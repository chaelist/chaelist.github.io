{"0": {
    "doc": "Amazon Bestselling Books",
    "title": "Amazon Bestselling Books",
    "content": ". | ë°ì´í„° íŒŒì•… . | ê°’ì˜ ë¶„í¬ í™•ì¸ | ë°ì´í„° ì‚¬ì „ ì²˜ë¦¬ | ì¤‘ë³µê°’ í™•ì¸ | . | ì±…ë³„ bestselling íšŸìˆ˜ ë¹„êµ . | bestselling íšŸìˆ˜ ë¶„í¬ í™•ì¸ | bestselling íšŸìˆ˜ê°€ ë§ì€ ì±… í™•ì¸ | . | ì±…ë³„ bestselling íšŸìˆ˜ ë¹„êµ . | ì‘ê°€ëª… í‘œê¸° í™•ì¸ | bestselling íšŸìˆ˜ ë¶„í¬ í™•ì¸ | bestselling íšŸìˆ˜ê°€ ë§ì€ ì‘ê°€ í™•ì¸ | . | ì—°ë„ë³„ ë³€í™” í™•ì¸ . | ì—°ë„ë³„ ì¥ë¥´ë³„ ë¹„ì¤‘ | ì—°ë„ë³„ Rating, Review, Price í‰ê· ì˜ ë³€í™” | . | Rating, Review, Price ìƒìœ„ê¶Œ ë„ì„œ í™•ì¸ . | User Ratingì´ ë†’ì€ ì±… í™•ì¸ | Reviewê°€ ë§ì€ ì±… í™•ì¸ | Priceê°€ ë†’ì€ ì±… í™•ì¸ | . | . *ë¶„ì„ ëŒ€ìƒ ë°ì´í„°ì…‹: Amazon Bestselling Books . | ë°ì´í„°ì…‹ ì¶œì²˜ | 2009ë…„ ~ 2019ë…„ì˜ Amazonâ€™s Top 50 bestselling books ë°ì´í„° (11ë…„ * 50 = 550ê¶Œ) | í•œ ì±…ì´ ì—¬ëŸ¬ í•´ì— ê±¸ì¹œ bestsellerì˜€ë‹¤ë©´, ì—¬ëŸ¬ ë²ˆ ì¤‘ë³µë˜ì–´ í¬í•¨ë˜ì–´ ìˆìŒ (ì¤‘ë³µì„ ì œì™¸í•˜ê³  ê³„ì‚°í•˜ë©´ ì´ 351ê¶Œ) | ê°™ì€ ì±…ì´ì—¬ë„ User Rating, Price, Reviews ì •ë³´ê°€ ë‹¬ë¼ì§€ëŠ” ê²½ìš°ë„ ìˆìŒ | Columns (7ê°œ): â€˜Nameâ€™, â€˜Authorâ€™, â€˜User Ratingâ€™, â€˜Reviewsâ€™, â€˜Priceâ€™, â€˜Yearâ€™, â€˜Genreâ€™ | . ",
    "url": "https://chaelist.github.io/docs/kaggle/amazon_bestsellers/",
    "relUrl": "/docs/kaggle/amazon_bestsellers/"
  },"1": {
    "doc": "Amazon Bestselling Books",
    "title": "ë°ì´í„° íŒŒì•…",
    "content": "# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ import import pandas as pd import numpy as np from matplotlib import pyplot as plt import seaborn as sns import scipy.stats as stats . books_df = pd.read_csv('data/bestsellers with categories.csv') books_df.head() . | Â  | Name | Author | User Rating | Reviews | Price | Year | Genre | . | 0 | 10-Day Green Smoothie Cleanse | JJ Smith | 4.7 | 17350 | 8 | 2016 | Non Fiction | . | 1 | 11/22/63: A Novel | Stephen King | 4.6 | 2052 | 22 | 2011 | Fiction | . | 2 | 12 Rules for Life: An Antidote to Chaos | Jordan B. Peterson | 4.7 | 18979 | 15 | 2018 | Non Fiction | . | 3 | 1984 (Signet Classics) | George Orwell | 4.7 | 21424 | 6 | 2017 | Fiction | . | 4 | 5,000 Awesome Facts (About Everything!) (National Geographic Kids) | National Geographic Kids | 4.8 | 7665 | 12 | 2019 | Non Fiction | . ê°’ì˜ ë¶„í¬ í™•ì¸ . | nullê°’ ì—¬ë¶€, data type í™•ì¸ . books_df.info() . &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 550 entries, 0 to 549 Data columns (total 7 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Name 550 non-null object 1 Author 550 non-null object 2 User Rating 550 non-null float64 3 Reviews 550 non-null int64 4 Price 550 non-null int64 5 Year 550 non-null int64 6 Genre 550 non-null object dtypes: float64(1), int64(3), object(3) memory usage: 30.2+ KB . | ìˆ«ìí˜• ì»¬ëŸ¼: ê°’ì˜ ë¶„í¬ë¥¼ í™•ì¸ . books_df[['User Rating', 'Reviews', 'Price']].describe() . | Â  | User Rating | Reviews | Price | . | count | 550.00 | 550.00 | 550.00 | . | mean | 4.62 | 11953.28 | 13.10 | . | std | 0.23 | 11731.13 | 10.84 | . | min | 3.30 | 37.00 | 0.00 | . | 25% | 4.50 | 4058.00 | 7.00 | . | 50% | 4.70 | 8580.00 | 11.00 | . | 75% | 4.80 | 17253.25 | 16.00 | . | max | 4.90 | 87841.00 | 105.00 | . | Priceì˜ ìµœì†Ÿê°’ì´ 0 â†’ bestselling books ëª…ë‹¨ì´ë¯€ë¡œ ì‹¤ì œ ê°€ê²©ì´ 0ì¼ë¦¬ëŠ” ì—†ë‹¤ê³  ìƒê°. (ê°€ê²©ì´ 0 = ì‚¬ì€í’ˆì´ë¼ê³  ê°„ì£¼í•  ìˆ˜ ìˆëŠ”ë°, ê·¸ë ‡ë‹¤ë©´ bestselling ëª…ë‹¨ì— ì˜¬ë¦¬ì§€ ì•Šì•˜ì„ ê²ƒ.) | . | uniqueí•œ ê°’ ìˆ˜ í™•ì¸ . books_df.nunique() . Name 351 Author 248 User Rating 14 Reviews 346 Price 40 Year 11 Genre 2 dtype: int64 . | . ë°ì´í„° ì‚¬ì „ ì²˜ë¦¬ . | Price ì»¬ëŸ¼ì˜ 0ê°’ ì±„ì›Œì£¼ê¸° . | bestselling books ëª…ë‹¨ì´ë¯€ë¡œ Priceê°€ 0ì¼ë¦¬ëŠ” ì—†ë‹¤ê³  ìƒê°. â†’ 0ì¸ ê°’ì€ ì•„ë§ˆ ì¡°ì‚¬ ê³¼ì •ì—ì„œ ëˆ„ë½ëœ ê²ƒì´ë¼ ìƒê°ë¨ | 0ê°’ = ê²°ì¸¡ì¹˜ë¡œ ê°„ì£¼í•˜ê³ , í•´ë‹¹ ì±…ì´ ì†í•œ ì—°ë„ &amp; ì¥ë¥´ì˜ Price í‰ê· ê°’ìœ¼ë¡œ ì±„ì›Œë„£ì–´ì¤€ë‹¤ | . books_df[books_df['Price'] == 0] # Priceê°€ 0ì¸ row: ì´ 12ê°œ . | Â  | Name | Author | User Rating | Reviews | Price | Year | Genre | . | 42 | Cabin Fever (Diary of a Wimpy Kid, Book 6) | Jeff Kinney | 4.8 | 4505 | 0 | 2011 | Fiction | . | 71 | Diary of a Wimpy Kid: Hard Luck, Book 8 | Jeff Kinney | 4.8 | 6812 | 0 | 2013 | Fiction | . | 116 | Frozen (Little Golden Book) | RH Disney | 4.7 | 3642 | 0 | 2014 | Fiction | . | 193 | JOURNEY TO THE ICE P | RH Disney | 4.6 | 978 | 0 | 2014 | Fiction | . | 219 | Little Blue Truck | Alice Schertle | 4.9 | 1884 | 0 | 2014 | Fiction | . | 358 | The Constitution of the United States | Delegates of the Constitutional | 4.8 | 2774 | 0 | 2016 | Non Fiction | . | 381 | The Getaway | Jeff Kinney | 4.8 | 5836 | 0 | 2017 | Fiction | . | 461 | The Short Second Life of Bree Tanner: An Eclipse Novella (The Twilight Saga) | Stephenie Meyer | 4.6 | 2122 | 0 | 2010 | Fiction | . | 505 | To Kill a Mockingbird | Harper Lee | 4.8 | 26234 | 0 | 2013 | Fiction | . | 506 | To Kill a Mockingbird | Harper Lee | 4.8 | 26234 | 0 | 2014 | Fiction | . | 507 | To Kill a Mockingbird | Harper Lee | 4.8 | 26234 | 0 | 2015 | Fiction | . | 508 | To Kill a Mockingbird | Harper Lee | 4.8 | 26234 | 0 | 2016 | Fiction | . â†’ 0ê°’(ê²°ì¸¡ì¹˜ë¡œ ê°„ì£¼) ì²˜ë¦¬: í•´ë‹¹ ì±…ì´ ì†í•œ ì—°ë„ &amp; ì¥ë¥´ì˜ Price í‰ê· ê°’ìœ¼ë¡œ ëŒ€ì²´ . zero_price_index = books_df[books_df['Price'] == 0].index for index in zero_price_index: year = books_df.loc[index, 'Year'] genre = books_df.loc[index, 'Genre'] avg_price = books_df[(books_df['Genre'] == genre) &amp; (books_df['Year'] == year)]['Price'].mean() books_df.loc[index, 'Price'] = avg_price books_df.loc[zero_price_index] . | Â  | Name | Author | User Rating | Reviews | Price | Year | Genre | . | 42 | Cabin Fever (Diary of a Wimpy Kid, Book 6) | Jeff Kinney | 4.8 | 4505 | 11.619 | 2011 | Fiction | . | 71 | Diary of a Wimpy Kid: Hard Luck, Book 8 | Jeff Kinney | 4.8 | 6812 | 10.7083 | 2013 | Fiction | . | 116 | Frozen (Little Golden Book) | RH Disney | 4.7 | 3642 | 10.1724 | 2014 | Fiction | . | 193 | JOURNEY TO THE ICE P | RH Disney | 4.6 | 978 | 10.5232 | 2014 | Fiction | . | 219 | Little Blue Truck | Alice Schertle | 4.9 | 1884 | 10.8861 | 2014 | Fiction | . | 358 | The Constitution of the United States | Delegates of the Constitutional | 4.8 | 2774 | 13.5161 | 2016 | Non Fiction | . | 381 | The Getaway | Jeff Kinney | 4.8 | 5836 | 8.83333 | 2017 | Fiction | . | 461 | The Short Second Life of Bree Tanner: An Eclipse Novella (The Twilight Saga) | Stephenie Meyer | 4.6 | 2122 | 9.7 | 2010 | Fiction | . | 505 | To Kill a Mockingbird | Harper Lee | 4.8 | 26234 | 11.1545 | 2013 | Fiction | . | 506 | To Kill a Mockingbird | Harper Lee | 4.8 | 26234 | 11.2614 | 2014 | Fiction | . | 507 | To Kill a Mockingbird | Harper Lee | 4.8 | 26234 | 9.35294 | 2015 | Fiction | . | 508 | To Kill a Mockingbird | Harper Lee | 4.8 | 26234 | 12.6316 | 2016 | Fiction | . | string ë°ì´í„° ì²˜ë¦¬ . | Nameê³¼ Authorì˜ ê²½ìš°, ì• ë’¤ ê³µë°± ë•Œë¬¸ì— ë‹¤ë¥´ê²Œ ì¸ì‹ë  ê°€ëŠ¥ì„±ì„ ì—†ì• ê¸° ìœ„í•´ strip()ì„ í•´ì¤€ë‹¤ | . books_df['Name'] = books_df['Name'].str.strip() books_df['Author'] = books_df['Author'].str.strip() . | . ì¤‘ë³µê°’ í™•ì¸ . | ì¤‘ë³µê°’ í™•ì¸ # ëª¨ë“  ì—´ì´ ì¤‘ë³µëœ ê°’ì´ í¬í•¨ë˜ì–´ ìˆë‚˜ í™•ì¸ books_df.duplicated().sum() . 0 . | ê°™ì€ ì±…ì´ ì—¬ëŸ¬ ë²ˆ ë‚˜ì˜¨ ê²½ìš°ë„ í™•ì¸ . # ì œëª©ê³¼ ì‘ê°€ë§Œ ì¤‘ë³µë˜ëŠ” ê°’ì´ ëª‡ ê°œì¸ì§€ ì²´í¬ books_df.duplicated(subset=['Name', 'Author']).sum() . 199 . | ê°™ì€ ì±…ì´ë©´ User Rating, Price, Reviews ì •ë³´ê°€ ê°™ì€ì§€ í™•ì¸ . nunique_df = books_df.groupby(['Name'])[['User Rating', 'Price', 'Reviews', 'Year']].nunique().reset_index() nunique_df[(nunique_df['User Rating'] &gt; 1) | (nunique_df['Price'] &gt; 1) | (nunique_df['Reviews'] &gt; 1)] . | Â  | Name | User Rating | Price | Reviews | Year | . | 104 | Gone Girl | 1 | 2 | 1 | 3 | . | 193 | Quiet: The Power of Introverts in a World That Canâ€™t Stop Talking | 1 | 2 | 1 | 2 | . | 219 | The 7 Habits of Highly Effective People: Powerful Lessons in Personal Change | 2 | 2 | 2 | 7 | . | 240 | The Fault in Our Stars | 1 | 2 | 1 | 3 | . | 248 | The Girl on the Train | 1 | 2 | 1 | 2 | . | 258 | The Help | 1 | 3 | 1 | 3 | . | 263 | The Immortal Life of Henrietta Lacks | 1 | 2 | 1 | 3 | . | 322 | To Kill a Mockingbird | 1 | 5 | 1 | 5 | . | 328 | Unbroken: A World War II Story of Survival, Resilience, and Redemption | 1 | 2 | 1 | 4 | . | ê°™ì€ ì±…ì´ì—¬ë„ í•´ì— ë”°ë¼ User Rating, Price, Reviews ì •ë³´ê°€ ë‹¬ë¼ì§€ëŠ” ê²½ìš°ë„ ìˆìŒ (To Kill a MockingbirdëŠ” ë‚´ê°€ ì„ì˜ë¡œ Priceë¥¼ ì±„ì›Œë„£ì—ˆìœ¼ë¯€ë¡œ ì œì™¸) | . | . ",
    "url": "https://chaelist.github.io/docs/kaggle/amazon_bestsellers/#%EB%8D%B0%EC%9D%B4%ED%84%B0-%ED%8C%8C%EC%95%85",
    "relUrl": "/docs/kaggle/amazon_bestsellers/#ë°ì´í„°-íŒŒì•…"
  },"2": {
    "doc": "Amazon Bestselling Books",
    "title": "ì±…ë³„ bestselling íšŸìˆ˜ ë¹„êµ",
    "content": ": ê° ì±…ë³„ë¡œ, ëª‡ ë²ˆ bestsellerì— ì´ë¦„ì„ ì˜¬ë ¸ëŠ”ì§€ íšŸìˆ˜ë¥¼ ê³„ì‚° . yearly_count = books_df.groupby(['Name', 'Genre', 'Author'])[['Year']].count().reset_index() yearly_count.sort_values(by='Year', ascending=False, inplace=True) yearly_count.head() . | Â  | Name | Genre | Author | Year | . | 191 | Publication Manual of the American Psychological Association, 6th Edition | Non Fiction | American Psychological Association | 10 | . | 209 | StrengthsFinder 2.0 | Non Fiction | Gallup | 9 | . | 178 | Oh, the Places Youâ€™ll Go! | Fiction | Dr. Seuss | 8 | . | 310 | The Very Hungry Caterpillar | Fiction | Eric Carle | 7 | . | 219 | The 7 Habits of Highly Effective People: Powerful Lessons in Personal Change | Non Fiction | Stephen R. Covey | 7 | . bestselling íšŸìˆ˜ ë¶„í¬ í™•ì¸ . | ìµœëŒ“ê°’, ìµœì†Ÿê°’, ì‚¬ë¶„ìœ„ê°’ í™•ì¸ . print(yearly_count.describe()) . Year count 351.000000 mean 1.566952 std 1.271868 min 1.000000 25% 1.000000 50% 1.000000 75% 2.000000 max 10.000000 . | ë¶„í¬ ì‹œê°í™” plt.figure(figsize=(6, 4)) sns.countplot(data=yearly_count, x='Year', palette='Purples_r'); . | í•œ ë²ˆë§Œ í¬í•¨ëœ ë¹„ìœ¨ í™•ì¸ one_year_percentage = len(yearly_count[yearly_count['Year'] == 1]) / len(yearly_count) * 100 print(f'í•œ í•´ë§Œ bestsellerì— í¬í•¨ëœ ì±…ì˜ ë¹„ìœ¨: {one_year_percentage :.0f}%') . í•œ í•´ë§Œ bestsellerì— í¬í•¨ëœ ì±…ì˜ ë¹„ìœ¨: 73% . | . &gt;&gt; ìµœëŒ€ 10ë²ˆì´ë‚˜ bestsellerì— í¬í•¨ëœ ì±…ë„ ìˆì§€ë§Œ, ë³´í†µì€ 1~2ë²ˆ bestsellerì— í¬í•¨ë˜ëŠ” ì •ë„ê°€ ì¼ë°˜ì . (73%ê°€ 1ë²ˆë§Œ ì´ë¦„ì„ ì˜¬ë¦¼) . bestselling íšŸìˆ˜ê°€ ë§ì€ ì±… í™•ì¸ . | Top 10 ì±… ì‹œê°í™”: ê°€ì¥ ë§ì´ bestsellerì— ì˜¬ë¼ì˜¨ ì±…ì´ ì–´ë–¤ ê²ƒì¸ì§€ . plt.figure(figsize=(6, 5)) sns.barplot(data=yearly_count.head(10), x='Year', y='Name', hue='Genre', palette='Purples'); . | ê°€ì¥ ë§ì´ bestsellerì— í¬í•¨ëœ ì±… 10ê¶Œ ì¤‘ 7ê¶Œì€ Non-Fiction | . | bestsellerì— ë§ì´ ì˜¬ë¼ì˜¨ ì±…ë“¤ì˜ ì—°ë„ë³„ bestselling ì—¬ë¶€ë¥¼ í™•ì¸ . pivot_df = pd.pivot_table(books_df, index='Name', columns='Year', values='Author', fill_value=0, aggfunc='count').reset_index() pd.merge(pivot_df, yearly_count, on='Name').sort_values(by='Year', ascending=False).head(10) . | . | Â  | Name | 2009 | 2010 | 2011 | 2012 | 2013 | 2014 | 2015 | 2016 | 2017 | 2018 | 2019 | Genre | Author | Year | . | 191 | Publication Manual of the American Psychological Association, 6th Edition | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 0 | Non Fiction | American Psychological Association | 10 | . | 209 | StrengthsFinder 2.0 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 0 | 0 | Non Fiction | Gallup | 9 | . | 178 | Oh, the Places Youâ€™ll Go! | 0 | 0 | 0 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | Fiction | Dr. Seuss | 8 | . | 310 | The Very Hungry Caterpillar | 0 | 0 | 0 | 0 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | Fiction | Eric Carle | 7 | . | 219 | The 7 Habits of Highly Effective People: Powerful Lessons in Personal Change | 1 | 0 | 1 | 1 | 1 | 0 | 1 | 1 | 1 | 0 | 0 | Non Fiction | Stephen R. Covey | 7 | . | 243 | The Four Agreements: A Practical Guide to Personal Freedom (A Toltec Wisdom Book) | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 1 | 1 | 1 | 1 | Non Fiction | Don Miguel Ruiz | 6 | . | 140 | Jesus Calling: Enjoying Peace in His Presence (with Scripture References) | 0 | 0 | 1 | 1 | 1 | 1 | 1 | 1 | 0 | 0 | 0 | Non Fiction | Sarah Young | 6 | . | 281 | The Official SAT Study Guide | 0 | 1 | 1 | 1 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | Non Fiction | The College Board | 5 | . | 322 | To Kill a Mockingbird | 0 | 0 | 0 | 0 | 1 | 1 | 1 | 1 | 0 | 0 | 1 | Fiction | Harper Lee | 5 | . | 216 | The 5 Love Languages: The Secret to Love That Lasts | 0 | 1 | 1 | 1 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | Non Fiction | Gary Chapman | 5 | . | ì—°ì†ìœ¼ë¡œ ê³„ì† bestsellerì— ì˜¬ë¼ì˜¨ ê²½ìš°ë§Œ ìˆëŠ” ê±´ ì•„ë‹ˆê³ , bestsellerì— ì˜¬ë¼ì˜¤ì§€ ì•Šì€ í•´ê°€ ì¤‘ê°„ì— ë¼ì–´ ìˆëŠ” ì±…ë“¤ë„ ì¡´ì¬í•¨ (ex. To Kill a Mockingbird) | . ",
    "url": "https://chaelist.github.io/docs/kaggle/amazon_bestsellers/#%EC%B1%85%EB%B3%84-bestselling-%ED%9A%9F%EC%88%98-%EB%B9%84%EA%B5%90",
    "relUrl": "/docs/kaggle/amazon_bestsellers/#ì±…ë³„-bestselling-íšŸìˆ˜-ë¹„êµ"
  },"3": {
    "doc": "Amazon Bestselling Books",
    "title": "ì±…ë³„ bestselling íšŸìˆ˜ ë¹„êµ",
    "content": "ì‘ê°€ëª… í‘œê¸° í™•ì¸ . : ê°™ì€ ì‘ê°€ì¸ë° ì´ë¦„ì´ ë‹¤ë¥´ê²Œ í‘œê¸°ëœ ê²½ìš°ê°€ ìˆëŠ”ì§€ í™•ì¸ . books_df['Author'].nunique() ## ì‘ê°€ëª… í‘œê¸°ë¥¼ ì •ë¦¬í•˜ê¸° ì „ì—ëŠ” ì¤‘ë³µ ì œì™¸í•˜ê³  248ê°œ . 248 . | ê° ì‘ê°€ì˜ ì„±(ê³µë°± ê¸°ì¤€ìœ¼ë¡œ ê°€ì¥ ë’·ë¶€ë¶„ë§Œ ì¶”ì¶œ)ë§Œ ë¶„ë¦¬í•´ì„œ ë³„ë„ì˜ dfë¡œ ì €ì¥ . books_df_copy = books_df[['Name', 'Author']] books_df_copy['Author_Last_Name'] = books_df_copy ['Author'].str.rsplit(n=1, expand=True)[1] books_df_copy.head() . | Â  | Name | Author | Author_Last_Name | . | 0 | 10-Day Green Smoothie Cleanse | JJ Smith | Smith | . | 1 | 11/22/63: A Novel | Stephen King | King | . | 2 | 12 Rules for Life: An Antidote to Chaos | Jordan B. Peterson | Peterson | . | 3 | 1984 (Signet Classics) | George Orwell | Orwell | . | 4 | 5,000 Awesome Facts (About Everything!) (National Geographic Kids) | National Geographic Kids | Kids | . | ê°™ì€ ì„±ì— 2ê°œ ì´ìƒì˜ ì‘ê°€ëª…ì´ ì¡´ì¬í•˜ëŠ” ê²½ìš°ë§Œ ë”°ë¡œ ì €ì¥ . authors_unique_df = books_df_copy.groupby(['Author_Last_Name'])[['Author']].nunique().reset_index() authors_unique_df[authors_unique_df['Author'] &gt; 1] . | Â  | Author_Last_Name | Author | . | 5 | Association | 2 | . | 20 | Brown | 4 | . | 26 | Campbell | 2 | . | 39 | Collins | 2 | . | 63 | Gaines | 2 | . | 121 | M.D. | 3 | . | 122 | MD | 3 | . | 125 | Martin | 3 | . | 164 | Press | 2 | . | 175 | Roth | 2 | . | 176 | Rowling | 2 | . | 191 | Smith | 3 | . | 221 | Young | 2 | . | last nameì´ 2ê°œ ì´ìƒìœ¼ë¡œ ë‚˜ì˜¨ Author ì´ë¦„ë§Œ ì•ŒíŒŒë²³ ìˆœìœ¼ë¡œ ë½‘ì•„ì„œ í™•ì¸ (ê°™ì€ ì‘ê°€ì¸ë° í‘œê¸°ê°€ ë‹¤ë¥¸ ê²½ìš°ê°€ ìˆëŠ”ì§€) . last_name_list = list(authors_unique_df[authors_unique_df['Author'] &gt; 1]['Author_Last_Name']) sorted(books_df_copy[books_df_copy['Author_Last_Name'].isin(last_name_list)]['Author'].unique()) . ['American Psychiatric Association', 'American Psychological Association', 'Bessel van der Kolk M.D.', 'BreneÌ Brown', 'Chip Gaines', 'Craig Smith', 'Dan Brown', 'Daniel James Brown', 'David Perlmutter MD', 'Dr. Steven R Gundry MD', 'Emily Winfield Martin', 'Geneen Roth', 'George R. R. Martin', 'George R.R. Martin', 'Ian K. Smith M.D.', 'J. K. Rowling', 'J.K. Rowling', 'JJ Smith', 'Jennifer Smith', 'Jim Collins', 'Joanna Gaines', 'Joel Fuhrman MD', 'Margaret Wise Brown', 'Mark Hyman M.D.', 'Paper Peony Press', 'Pretty Simple Press', 'Rod Campbell', 'Sarah Young', 'Suzanne Collins', 'Thomas Campbell', 'Veronica Roth', 'William P. Young'] . | ê³µë°± ì²˜ë¦¬ë¡œ ì¸í•´ ë‹¤ë¥´ê²Œ ì¸ì‹ë˜ëŠ” ì‘ê°€ëª…ì„ ìˆ˜ì •í•´ì¤Œ . # J.K.Rowlingê³¼ George R.R.Martinì˜ ê²½ìš°, ì¤‘ê°„ ê³µë°±ìœ¼ë¡œ ì¸í•´ ë‹¤ë¥´ê²Œ í‘œê¸°ëœ ê²½ìš°ê°€ ìˆìŒ â†’ ìˆ˜ì •í•´ì¤Œ books_df.replace({'Author': {'J.K. Rowling': 'J. K. Rowling', 'George R.R. Martin': 'George R. R. Martin'}}, inplace=True) . â†’ ì¤‘ë³µ ì œì™¸ ì‘ê°€ ìˆ˜ê°€ 246ëª…ìœ¼ë¡œ ì¤„ì–´ë“¦: . books_df['Author'].nunique() . 246 . | . bestselling íšŸìˆ˜ ë¶„í¬ í™•ì¸ . yearly_count2 = books_df.groupby(['Author'])[['Year']].count().reset_index() yearly_count2.sort_values(by='Year', ascending=False, inplace=True) yearly_count2.head() . | Â  | Author | Year | . | 118 | Jeff Kinney | 12 | . | 224 | Suzanne Collins | 11 | . | 195 | Rick Riordan | 11 | . | 92 | Gary Chapman | 11 | . | 11 | American Psychological Association | 10 | . | ìµœëŒ“ê°’, ìµœì†Ÿê°’, ì‚¬ë¶„ìœ„ê°’ í™•ì¸ print(yearly_count2.describe()) . Year count 246.000000 mean 2.235772 std 2.080350 min 1.000000 25% 1.000000 50% 1.000000 75% 2.000000 max 12.000000 . | ë¶„í¬ ì‹œê°í™” plt.figure(figsize=(6, 4)) sns.countplot(data=yearly_count2, x='Year', palette='Purples_r'); . | 1~2ë²ˆ í¬í•¨ëœ ë¹„ìœ¨ í™•ì¸ one_year_percentage2 = len(yearly_count2[yearly_count2['Year'] == 1]) / len(yearly_count2) * 100 print(f'í•œ í•´ë§Œ bestsellerì— í¬í•¨ëœ ì‘ê°€ì˜ ë¹„ìœ¨: {one_year_percentage2 :.0f}%') one_year_percentage2_2 = len(yearly_count2[yearly_count2['Year'] == 2]) / len(yearly_count2) * 100 print(f'ë‘ í•´ ë™ì•ˆ bestsellerì— í¬í•¨ëœ ì‘ê°€ì˜ ë¹„ìœ¨: {one_year_percentage2_2 :.0f}%') . í•œ í•´ë§Œ bestsellerì— í¬í•¨ëœ ì‘ê°€ì˜ ë¹„ìœ¨: 53% ë‘ í•´ ë™ì•ˆ bestsellerì— í¬í•¨ëœ ì‘ê°€ì˜ ë¹„ìœ¨: 24% . &gt;&gt; ìµœëŒ€ 12ë²ˆì´ë‚˜ bestsellerì— í¬í•¨ëœ ì‘ê°€ë„ ìˆì§€ë§Œ, ë³´í†µì€ 1~2ë²ˆ bestsellerì— í¬í•¨ë˜ëŠ” ì •ë„ê°€ ì¼ë°˜ì . (ê³¼ë°˜ìˆ˜ê°€ 1ë²ˆë§Œ ì´ë¦„ì„ ì˜¬ë ¸ê³ , 77%ê°€ 2ë²ˆ ì´í•˜ë¡œ ì´ë¦„ì„ ì˜¬ë¦¼) . | . bestselling íšŸìˆ˜ê°€ ë§ì€ ì‘ê°€ í™•ì¸ . | Top 10 ì‘ê°€ ì‹œê°í™”: ê°€ì¥ ë§ì´ bestsellerì— ì˜¬ë¼ì˜¨ ì‘ê°€ê°€ ëˆ„êµ¬ì¸ì§€ . plt.figure(figsize=(6, 5)) sns.barplot(data=yearly_count2.head(10), x='Year', y='Author', palette='Purples_r'); . | ìœ ëª…í•œ ì‹œë¦¬ì¦ˆë¬¼ì„ ë°œí‘œí•œ ì‘ê°€ë“¤ì´ ì£¼ë¡œ ìƒìœ„ê¶Œì— í¬ì§„. | Jeff Kinney: Diary of a Wimpy Kids ì‹œë¦¬ì¦ˆ | Suzanne Collins: The Hunger Game ì‹œë¦¬ì¦ˆ | Rick Riordan: The Heroes of Olympus &amp; The Kane Chronicles ì‹œë¦¬ì¦ˆ | J. K. Rowling: Harry Potter ì‹œë¦¬ì¦ˆ | Stephenie Meyer: Twilight ì‹œë¦¬ì¦ˆ | . | . | bestsellerì— ë§ì´ ì˜¬ë¼ì˜¨ ì‘ê°€ë“¤ì˜ ì—°ë„ë³„ bestselling ë¶„í¬ë¥¼ í™•ì¸ . pivot_df2 = pd.pivot_table(books_df, index='Author', columns='Year', values='Name', fill_value=0, aggfunc='count').reset_index() pd.merge(pivot_df2, yearly_count2, on='Author').sort_values(by='Year', ascending=False).head(10) . | Â  | Author | 2009 | 2010 | 2011 | 2012 | 2013 | 2014 | 2015 | 2016 | 2017 | 2018 | 2019 | Year | . | 118 | Jeff Kinney | 2 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 12 | . | 224 | Suzanne Collins | 0 | 3 | 4 | 4 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 11 | . | 195 | Rick Riordan | 1 | 4 | 2 | 2 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 11 | . | 92 | Gary Chapman | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 11 | . | 11 | American Psychological Association | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 0 | 10 | . | 73 | Dr. Seuss | 0 | 0 | 0 | 1 | 1 | 1 | 2 | 1 | 1 | 1 | 1 | 9 | . | 90 | Gallup | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 0 | 0 | 9 | . | 111 | J. K. Rowling | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 5 | 1 | 0 | 1 | 8 | . | 197 | Rob Elliott | 0 | 0 | 0 | 0 | 2 | 2 | 2 | 1 | 1 | 0 | 0 | 8 | . | 219 | Stephenie Meyer | 6 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 7 | . | Suzanne Collins, Rick Riordan, J. K. Rowling, Stephenie Meyer: ì‹œë¦¬ì¦ˆë¬¼ë¡œ ìœ ëª…í•œ ì‘ê°€ì—¬ì„œì¸ì§€, ëŒ€ì²´ë¡œ íŠ¹ì • ê¸°ê°„ ë‚´ì— ë³µìˆ˜ì˜ ì±…ì´ í•œë²ˆì— bestsellerì— ì˜¬ë¼ì˜´ | . | . ",
    "url": "https://chaelist.github.io/docs/kaggle/amazon_bestsellers/#%EC%B1%85%EB%B3%84-bestselling-%ED%9A%9F%EC%88%98-%EB%B9%84%EA%B5%90-1",
    "relUrl": "/docs/kaggle/amazon_bestsellers/#ì±…ë³„-bestselling-íšŸìˆ˜-ë¹„êµ-1"
  },"4": {
    "doc": "Amazon Bestselling Books",
    "title": "ì—°ë„ë³„ ë³€í™” í™•ì¸",
    "content": "ì—°ë„ë³„ ì¥ë¥´ë³„ ë¹„ì¤‘ . : ì—°ë„ë³„ë¡œ, ì–´ë–¤ ì¥ë¥´ì˜ ì±…ì´ bestsellerë¡œ ì´ë¦„ì„ ë” ë§ì´ ì˜¬ë ¸ëŠ”ì§€ í™•ì¸ . groupby_df1 = books_df.groupby(['Year', 'Genre'])[['Name']].count().reset_index() groupby_df1.rename(columns={'Name':'Count'}, inplace=True) groupby_df1.head(4) . | Â  | Year | Genre | Count | . | 0 | 2009 | Fiction | 24 | . | 1 | 2009 | Non Fiction | 26 | . | 2 | 2010 | Fiction | 20 | . | 3 | 2010 | Non Fiction | 30 | . â†’ ì‹œê°í™”: . plt.figure(figsize=(12, 5)) sns.lineplot(data=groupby_df1, x='Year', y='Count', hue='Genre', palette='Purples_r'); . | 2014ë…„ì„ ì œì™¸í•˜ë©´ ëŒ€ì²´ë¡œ Non-Fictionì´ ë” ë§ì´ bestsellerì— í¬í•¨ë˜ëŠ” ê²½í–¥ | . ì—°ë„ë³„ Rating, Review, Price í‰ê· ì˜ ë³€í™” . | ì—°ë„ë³„ ì¥ë¥´ë³„ User Rating í‰ê· ì˜ ì°¨ì´ plt.figure(figsize=(12, 5)) sns.lineplot(data=books_df, x='Year', y='User Rating', hue='Genre', palette='Purples'); . | ëŒ€ì²´ë¡œ í° ì°¨ì´ê°€ ì—†ìœ¼ë‚˜, 2017ë…„ ì´í›„ë¶€í„°ëŠ” Fiction ì¥ë¥´ì˜ í‰ê·  User Ratingì´ ì˜¬ë¼ê°€ë©´ì„œ Non-Fiction ì¥ë¥´ì™€ ë‹¤ì†Œ ì°¨ì´ê°€ ë²Œì–´ì§ | Fiction ì¥ë¥´ê°€ ëŒ€ì²´ë¡œ User Ratingì˜ í¸ì°¨ê°€ í¼ (ë³´ë‹¤ ì·¨í–¥ì´ ê°ˆë¦¬ëŠ” ì¥ë¥´ì´ê¸° ë•Œë¬¸ì´ë¼ê³  ì¶”ì •) | . | ì—°ë„ë³„ ì¥ë¥´ë³„ Reviews í‰ê· ì˜ ì°¨ì´ plt.figure(figsize=(12, 5)) sns.lineplot(data=books_df, x='Year', y='Reviews', hue='Genre', palette='Purples'); . | íŠ¹íˆ 2012 ~ 2016ë…„ ì‚¬ì´ì— ì¥ë¥´ ê°„ í‰ê·  Review ìˆ˜ê°€ í° ì°¨ì´ë¥¼ ë³´ì„ | 2017 ~ 2018ë…„ì€ ê±°ì˜ í‰ê· ì˜ ì°¨ì´ê°€ ì—†ë‹¤ê³  ë´ë„ ë¬´ë°©í•˜ë‚˜, 2019ë…„ë¶€í„° ë‹¤ì‹œ Fiction ì¥ë¥´ì˜ í‰ê·  Reviewìˆ˜ê°€ ì¦ê°€í•˜ëŠ” ê²½í–¥ì„ ë³´ì—¬, í–¥í›„ ì¶”ì´ë¥¼ ì§€ì¼œë´ì•¼ í•  ê²ƒìœ¼ë¡œ ìƒê°ë¨ | . | ì—°ë„ë³„ ì¥ë¥´ë³„ Price í‰ê· ì˜ ì°¨ì´ plt.figure(figsize=(12, 5)) sns.lineplot(data=books_df, x='Year', y='Price', hue='Genre', palette='Purples'); . | 2010 ~ 2014ë…„ ì‚¬ì´ì˜ bestsellerë“¤ì€ ëŒ€ì²´ë¡œ Non-Fictionì´ Fictionë³´ë‹¤ ë‹¤ì†Œ ë†’ì€ ê°€ê²©ì„ ë³´ì´ë‚˜, 2015ë…„ ì´í›„ì—ëŠ” ë³„ ì°¨ì´ë¥¼ ë³´ì´ì§€ ì•ŠìŒ | Non-Fiction ì¥ë¥´ê°€ ëŒ€ì²´ë¡œ Priceì˜ í¸ì°¨ê°€ í¼ (í•™ìˆ  ë„ì„œ ì¤‘ì—ëŠ” ìƒë‹¹íˆ ë¹„ì‹¼ ê°€ê²©ì˜ ì±…ë“¤ë„ ìˆê¸° ë•Œë¬¸ì´ë¼ê³  ì¶”ì •) | . | . ",
    "url": "https://chaelist.github.io/docs/kaggle/amazon_bestsellers/#%EC%97%B0%EB%8F%84%EB%B3%84-%EB%B3%80%ED%99%94-%ED%99%95%EC%9D%B8",
    "relUrl": "/docs/kaggle/amazon_bestsellers/#ì—°ë„ë³„-ë³€í™”-í™•ì¸"
  },"5": {
    "doc": "Amazon Bestselling Books",
    "title": "Rating, Review, Price ìƒìœ„ê¶Œ ë„ì„œ í™•ì¸",
    "content": ": ì „ê¸°ê°„ì˜ ëª¨ë“  bestselling ì±… ì¤‘, rating, review, priceê°€ ë†’ì€ ì±…ë“¤ì´ ì–´ë–¤ ê²ƒë“¤ì¸ì§€ í™•ì¸ . | â€» ëŒ€ì²´ë¡œ ê°™ì€ ì±…ì€ rating, review, priceê°€ ë™ì¼í•œ ê²½ìš°ê°€ ë§ìœ¼ë¯€ë¡œ, ê°™ì€ ì±…ì˜ ê²½ìš° ê°€ì¥ ìµœê·¼ record í•˜ë‚˜ë§Œ ë‚¨ê²¨ì„œ ë¶„ì„ì— í™œìš© | . # ì—°ë„ìˆœìœ¼ë¡œ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬ books_df_sorted = books_df.sort_values(by='Year', ascending=False) # ì¤‘ë³µëœ í–‰ë“¤ ì¤‘ ê°€ì¥ ìœ„ì— ìˆëŠ” í–‰ë§Œ ë‚¨ê¸°ê³  ë‹¤ ì‚­ì œ unique_books_df = books_df_sorted.drop_duplicates(subset=['Name', 'Author'], ignore_index=True) unique_books_df.reset_index(drop=True, inplace=True) print('books_df: ', len(books_df)) print('unique_books_df: ', len(unique_books_df)) . books_df: 550 unique_books_df: 351 . +) í‰ê·  í™•ì¸ . unique_books_df.groupby(['Genre'])[['User Rating', 'Reviews', 'Price']].mean() . | Genre | User Rating | Reviews | Price | . | Fiction | 4.61563 | 13111.1 | 12.0938 | . | Non Fiction | 4.60366 | 7001.66 | 13.7016 | . +) í‘œì¤€í¸ì°¨ í™•ì¸ . unique_books_df.groupby(['Genre'])[['User Rating', 'Reviews', 'Price']].std() . | Genre | User Rating | Reviews | Price | . | Fiction | 0.274388 | 13312.2 | 9.54815 | . | Non Fiction | 0.177815 | 7241.59 | 10.369 | . +) ë³€ìˆ˜ê°„ ìƒê´€ê´€ê³„ ì²´í¬ . | Ratingì´ ë†’ì€ ì±…ì´ Reviewë„ ë†’ê³ , Priceë„ ë†’ì€ ë“±ì˜ ê´€ê³„ê°€ ìˆëŠ”ì§€ íŒŒì•…í•˜ê¸° ìœ„í•¨ | . sns.heatmap(unique_books_df[['User Rating', 'Reviews', 'Price']].corr(), annot=True, cmap='Purples') plt.yticks(rotation=0); . | ë³€ìˆ˜ê°„ ì „í˜€ ìƒê´€ê´€ê³„ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŒ | . User Ratingì´ ë†’ì€ ì±… í™•ì¸ . | User Ratingì´ 4.9(=ìµœëŒ“ê°’)ì¸ ì±…ë“¤ ì¤‘ì— Fictionê³¼ Non Fictionì´ ëª‡ ê¶Œì”© í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸ . plot = sns.countplot(data=unique_books_df[unique_books_df['User Rating'] == 4.9], x='Genre', palette='Purples_r', order=unique_books_df[unique_books_df['User Rating'] == 4.9].groupby(['Genre'])[['Name']].nunique().index) # data labelì„ ê°ê°ì˜ bar ìœ„ì— ì¶”ê°€ for p, label in zip(plot.patches, unique_books_df[unique_books_df['User Rating'] == 4.9].groupby(['Genre'])[['Name']].nunique()['Name']): plot.annotate(label, (p.get_x() + 0.375, p.get_height() + 1)) plt.ylim(0, 25); . | ìµœê³ ì ì¸ 4.9ì ì„ ë°›ì€ ì±… ì¤‘ì—ì„œëŠ” ì•½ 78.6%ê°€ Fiction | Fictionì´ ì¥ë¥´ íŠ¹ì„±ìƒ í˜¸ë¶ˆí˜¸ê°€ ë” ëª…í™•í•˜ê¸° ë•Œë¬¸ì¸ ë“¯ (Fictionì´ ë” í‘œì¤€í¸ì°¨ê°€ í¼) | . | ì—°ë„ë³„ë¡œ, User Ratingì´ 4.9(=ìµœëŒ“ê°’)ì¸ ì±…ë“¤ì´ ëª‡ ê¶Œì”© í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸ . # unique_books_df ëŒ€ì‹  books_dfë¥¼ í™œìš© plot = sns.countplot(data=books_df[books_df['User Rating'] == 4.9], x='Year', palette='Purples', order=books_df[books_df['User Rating'] == 4.9].groupby(['Year'])[['Name']].nunique().index) # data labelì„ ê°ê°ì˜ bar ìœ„ì— ì¶”ê°€ for p, label in zip(plot.patches, books_df[books_df['User Rating'] == 4.9].groupby(['Year'])[['Name']].nunique()['Name']): plot.annotate(label, (p.get_x() + 0.27, p.get_height() + 0.7)) plt.ylim(0, 15); . | í•´ê°€ ì§€ë‚ ìˆ˜ë¡ bestsellerë“¤ì˜ í‰ê·  Ratingì´ ë†’ì•„ì§€ëŠ” ê²½í–¥ì´ ìˆì–´ì„œ ê·¸ëŸ° ë“¯ | . | 4.9ì  Ratingì˜ bestsellerë¥¼ 2ê¶Œ ì´ìƒ ë³´ìœ í•œ ì‘ê°€ë“¤ì„ í™•ì¸ . unique_books_df[unique_books_df['User Rating'] == 4.9].groupby(['Author'])[['Name']].count().sort_values(by='Name', ascending=False).head() . | Author | Name | . | Dav Pilkey | 6 | . | J. K. Rowling | 4 | . | Rush Limbaugh | 2 | . | Alice Schertle | 1 | . | Lin-Manuel Miranda | 1 | . | 2ê¶Œ ì´ìƒì˜ bestsellerê°€ 4.9ì ì„ ë°›ì€ ì‘ê°€ì˜ ê²½ìš°, ëª¨ë‘ ì‹œë¦¬ì¦ˆë¬¼ | . | . Reviewê°€ ë§ì€ ì±… í™•ì¸ . ## Review ê°€ì¥ ë§ì€ Top 10 ì±… ì‹œê°í™” plt.figure(figsize=(6, 5)) sns.barplot(data=unique_books_df.sort_values(by='Reviews', ascending=False).head(10), x='Reviews', y='Name', hue='Genre', palette='Purples_r'); . | Top 10 ì¤‘ í•œ ê¶Œë§Œ Non-Fiction ì¥ë¥´ | íŠ¹íˆ 2012 ~ 2016ì„ ì¤‘ì‹¬ìœ¼ë¡œ Reviewê°€ ë†’ì€ Fiction ì¥ë¥´ ì±…ì´ ë§ì•˜ê¸° ë•Œë¬¸ì¸ ë“¯ | . Priceê°€ ë†’ì€ ì±… í™•ì¸ . | Priceê°€ ê°€ì¥ ë†’ì€ Top 10 ì±… ì‹œê°í™” plt.figure(figsize=(6, 5)) sns.barplot(data=unique_books_df.sort_values(by='Price', ascending=False).head(10), x='Price', y='Name', hue='Genre', palette='Purples'); . | 4ê¶Œì´ Fiction, 6ê¶Œì´ Non-Fiction | ë‹¤ë§Œ, Fiction ì¤‘ ê°€ì¥ ê°€ê²©ì´ ë†’ì€ 2ê°œì˜ ìƒí’ˆì€ Twilightê³¼ Harry Potter ì‹œë¦¬ì¦ˆ ì„¸íŠ¸ íŒë§¤ ìƒí’ˆ | . | Fiction ì¤‘ ì‹œë¦¬ì¦ˆ ì„¸íŠ¸ íŒë§¤ ìƒí’ˆë“¤ì„ íŒë³„ . condition1 = unique_books_df['Name'].str.contains('Saga Collection') condition2 = unique_books_df['Name'].str.contains('Boxed Set') condition3 = unique_books_df['Name'].str.contains('Box Set') condition4 = unique_books_df['Name'].str.contains('Fifty Shades of Grey / ') condition5 = unique_books_df['Name'].str.contains('A Game of Thrones /') unique_books_df[condition1 | condition2 | condition3 | condition4 | condition5] . | Â  | Name | Author | User Rating | Reviews | Price | Year | Genre | . | 130 | Harry Potter Paperback Box Set (Books 1-7) | J. K. Rowling | 4.8 | 13471 | 52 | 2016 | Fiction | . | 187 | A Game of Thrones / A Clash of Kings / A Storm of Swords / A Feast of Crows / A Dance with Dragons | George R. R. Martin | 4.7 | 19735 | 30 | 2014 | Fiction | . | 212 | Game of Thrones Boxed Set: A Game of Thrones/A Clash of Kings/A Storm of Swords/A Feast for Crows | George R. R. Martin | 4.6 | 5594 | 5 | 2013 | Fiction | . | 228 | The Hunger Games Trilogy Boxed Set (1) | Suzanne Collins | 4.8 | 16949 | 30 | 2012 | Fiction | . | 251 | Fifty Shades Trilogy (Fifty Shades of Grey / Fifty Shades Darker / Fifty Shades Freed) | E L James | 4.5 | 13964 | 32 | 2012 | Fiction | . | 300 | Percy Jackson and the Olympians Paperback Boxed Set (Books 1-3) | Rick Riordan | 4.8 | 548 | 2 | 2010 | Fiction | . | 330 | The Twilight Saga Collection | Stephenie Meyer | 4.7 | 3801 | 82 | 2009 | Fiction | . | ì‹œë¦¬ì¦ˆ ì„¸íŠ¸ íŒë§¤ ìƒí’ˆì„ ì œì™¸í•œ Price Top 10 temp = unique_books_df[~(condition1 | condition2 | condition3 | condition4 | condition5)] plt.figure(figsize=(6, 5)) sns.barplot(data=temp.sort_values(by='Price', ascending=False).head(10), x='Price', y='Name', hue='Genre', palette='Purples'); . | Fiction 3ê¶Œ, Non-Fiction 7ê¶Œ | . | ì‹œë¦¬ì¦ˆ ì„¸íŠ¸ íŒë§¤ ìƒí’ˆì„ ì œì™¸í•œ Price ë¶„í¬ë¥¼ íŒŒì•… . sns.catplot(data=temp, x='Genre', y='Price', kind='box', palette='Purples'); . | ì‹œë¦¬ì¦ˆ ì„¸íŠ¸ íŒë§¤ ìƒí’ˆì„ ì œì™¸í•œ ì¥ë¥´ë³„ í‰ê·  ì°¨ì´ë¥¼ í™•ì¸ . sns.barplot(data=temp, x='Genre', y='Price', palette='Purples'); . â†’ ë…ë¦½í‘œë³¸ t-testë¡œ í‰ê· ì˜ ì°¨ì´ê°€ ìœ ì˜ë¯¸í•œì§€ í™•ì¸ . temp_fiction_df = temp[temp['Genre'] == 'Fiction'] temp_non_fiction_df = temp[temp['Genre'] == 'Non Fiction'] # Leveneì˜ ë“±ë¶„ì‚° ê²€ì • lev_result = stats.levene(temp_fiction_df['Price'], temp_non_fiction_df['Price']) print('LeveneResult(F) : %.2f \\np-value : %.3f' % (lev_result)) . LeveneResult(F) : 2.50 p-value : 0.115 . # ë“±ë¶„ì‚°ì¸ ë…ë¦½í‘œë³¸ t-test ì‹¤í–‰ t_result = stats.ttest_ind(temp_fiction_df['Price'], temp_non_fiction_df['Price'], equal_var=True) print('t statistic : %.2f \\np-value : %.3f' % (t_result)) . t statistic : -2.67 p-value : 0.008 . | ì†Œì„¤ ì‹œë¦¬ì¦ˆ ì„¸íŠ¸ íŒë§¤ ìƒí’ˆì„ ì œì™¸í•˜ë©´, Non-Fictionì˜ ê°€ê²©ì´ ëŒ€ì²´ë¡œ ë‹¤ì†Œ ë†’ì€ í¸ì´ë¼ê³  í•  ìˆ˜ ìˆì„ ë“¯ | . | . ",
    "url": "https://chaelist.github.io/docs/kaggle/amazon_bestsellers/#rating-review-price-%EC%83%81%EC%9C%84%EA%B6%8C-%EB%8F%84%EC%84%9C-%ED%99%95%EC%9D%B8",
    "relUrl": "/docs/kaggle/amazon_bestsellers/#rating-review-price-ìƒìœ„ê¶Œ-ë„ì„œ-í™•ì¸"
  },"6": {
    "doc": "App Review ìˆ˜ì§‘",
    "title": "App Review ìˆ˜ì§‘",
    "content": ". | Google Play Store Scraping . | App Info ìˆ˜ì§‘ | App Review ìˆ˜ì§‘ | íŠ¹ì • ê¸°ê°„ì˜ App Reviewë§Œ ìˆ˜ì§‘ | . | App Store Scraping . | App Review ìˆ˜ì§‘ | . | . ",
    "url": "https://chaelist.github.io/docs/webscraping/app_review/",
    "relUrl": "/docs/webscraping/app_review/"
  },"7": {
    "doc": "App Review ìˆ˜ì§‘",
    "title": "Google Play Store Scraping",
    "content": ". | Google-Play-Scraper libraryë¥¼ í™œìš© | pip install google-play-scraperë¡œ ì„¤ì¹˜í•´ì„œ ì‚¬ìš© | . App Info ìˆ˜ì§‘ . from google_play_scraper import app # https://play.google.com/store/apps/details?id=com.frograms.wplay result = app( 'com.frograms.wplay', # app id (package name) lang='ko', # default: 'en' country='kr' # defaul: 'us' ) print(result) . {'title': 'ì™“ì± ', 'description': \"- ì˜í™”/ë“œë¼ë§ˆ/ì˜ˆëŠ¥/ì• ë‹ˆë©”ì´ì…˜/ë‹¤íë©˜í„°ë¦¬ ë¬´ì œí•œ ìŠ¤íŠ¸ë¦¬ë°/ë‹¤ìš´ë¡œë“œ ê°ìƒ ì•±.\\r\\n- ìŠ¤ë§ˆíŠ¸í°, íƒœë¸”ë¦¿, PC, ë§¥, ìŠ¤ë§ˆíŠ¸ TV, ì…‹í†±ë°•ìŠ¤, í¬ë¡¬ìºìŠ¤íŠ¸ì—ì„œ ëª¨ë‘, ìµœê³ ì˜ í™”ì§ˆë¡œ.\\r\\n- ì´ì œëŠ” ì˜¨ ê°€ì¡±ê³¼ í•¨ê»˜ ì¦ê¸°ì„¸ìš”!\\r\\n- ì™“ì± í”Œë ˆì´ê°€ ë§ì€ ë¶„ë“¤ì´ ë¶ˆëŸ¬ì£¼ì‹œëŠ” ê·¸ ì´ë¦„ ê·¸ëŒ€ë¡œ 'ì™“ì± 'ë¡œ ë‹¤ì‹œ íƒœì–´ë‚¬ìŠµë‹ˆë‹¤.\\r\\n\\r\\n[ì œí’ˆ ì„¤ëª…]\\r\\n\\r\\nâ€¢ ìŠ¤íŠ¸ë¦¬ë°ë¿ ì•„ë‹ˆë¼, ë‹¤ìš´ë¡œë“œ ë° ì˜¤í”„ë¼ì¸ ì¬ìƒ ê¸°ëŠ¥ê¹Œì§€.\\r\\nâ€¢ ìŠ¤ë§ˆíŠ¸í°, íƒœë¸”ë¦¿, PC, ë§¥, ìŠ¤ë§ˆíŠ¸ TV, ì…‹í†±ë°•ìŠ¤, í¬ë¡¬ìºìŠ¤íŠ¸ì—ì„œ ëª¨ë‘.\\r\\nâ€¢ HDë¶€í„° Ultra HD 4Kê¹Œì§€ ìµœê³ ì˜ í™”ì§ˆë¡œ.\\r\\nâ€¢ ì„¸ê³„ ìµœê³  ìˆ˜ì¤€ì˜ ì¶”ì²œ ì—”ì§„ìœ¼ë¡œ ë‚´ ì·¨í–¥ì— ë§ëŠ” ì‘í’ˆë§Œ.\\r\\nâ€¢ ì‹ ìƒì•„ë„ ì§ê´€ì ìœ¼ë¡œ ì¡°ì‘í•  ìˆ˜ ìˆëŠ”, ì‰½ê³  í¸í•œ ì¸í„°í˜ì´ìŠ¤.\\r\\nâ€¢ ì˜í™”, ë“œë¼ë§ˆ, ì˜ˆëŠ¥ë¿ ì•„ë‹ˆë¼ ë‹¤íë©˜í„°ë¦¬, ì• ë‹ˆë©”ì´ì…˜ê¹Œì§€.\\r\\nâ€¢ ê°€ì…í•˜ê³  ë‘˜ëŸ¬ë³´ëŠ” ê±´ í‰ìƒ ë¬´ë£Œ. ì¼ë‹¨ ë‹¤ìš´ë¡œë“œí•´ ë³´ì„¸ìš”.\\r\\n\\r\\n\\r\\n (ìƒëµ)} . App Review ìˆ˜ì§‘ . from google_play_scraper import Sort, reviews result, continuation_token = reviews( 'com.frograms.wplay', lang='ko', # default: 'en' country='kr', # default: 'us' sort=Sort.MOST_RELEVANT, # default: Sort.MOST_RELEVANT count=3, # default: 100 filter_score_with=5 # default: None (ëª¨ë“  í‰ì ì„ ë‹¤ ê°€ì ¸ì˜´) ) result . [{'at': datetime.datetime(2021, 10, 16, 8, 2, 12), 'content': 'ì§±êµ¬ ê·¹ì¥íŒë³´ë©° ìš°ëŠ” ì¤‘..... ìµœì‹ í™” ì—…ë°ì´íŠ¸ í•´ì£¼ì‹œë©´ ì•ˆë ê¹Œìš” ë³´ê³ ì‹¶ì€ë°ã… ã… ', 'repliedAt': None, 'replyContent': None, 'reviewCreatedVersion': '1.9.97', 'reviewId': 'gp:AOqpTOFVBSK8EhoqhobkcZbFrDmaaLdIdsIVAC2-oKDVJSgRX62jWkQzMVbMvosS2-GjoA_BC96wOkYq0DkoVnw', 'score': 5, 'thumbsUpCount': 1, 'userImage': 'https://play-lh.googleusercontent.com/a/AATXAJzCaKwsYBHvCTY6ztFEq_F-WmRahkWIF1hhHQct=mo', 'userName': 'ìŠ¬êµ¬'}, {'at': datetime.datetime(2021, 10, 1, 16, 17, 43), 'content': 'ì†ë„ì¡°ì ˆ ìƒê²¨ì„œ ì¢‹ì€ë° ì¢€ ë” ë””í…Œì¼í•œ ì†ë„ ì¡°ì ˆì´ì˜€ìœ¼ë©´ ì¢‹ê² ìŠµë‹ˆë‹¤. 1.25ë°°, 1.5ë°°ëŠ” ë„ˆë¬´ ë¹¨ë¼ì„œ ë³´ê¸°ê°€ ë¶ˆí¸í•©ë‹ˆë‹¤. 1.1ë°°ë¶€í„° ë‹¤ì–‘í•œ ì¡°ì ˆì´ ê°€ëŠ¥í–ˆìœ¼ë©´ ì¢‹ê² ë„¤ìš”. ê³°í”Œë„ ë˜ëŠ”ë° ì™“ì± ì—ì„œ ì•ˆë ìˆœì—†ì£ ~ ë¶€íƒë“œë¦½ë‹ˆë‹¤.', 'repliedAt': None, 'replyContent': None, 'reviewCreatedVersion': '1.9.96', 'reviewId': 'gp:AOqpTOFdH8PODeF-Al6Xo04_9IPHIDxdf3a5A2br0lHgtLrvi0ntmSW6hYmbUF9tv2x_6LcD5rIaAKSorWrBZZw', 'score': 5, 'thumbsUpCount': 20, 'userImage': 'https://play-lh.googleusercontent.com/a/AATXAJwiQJj1XEdXv4TfeilZfMufI8bKddTSyQFnKVGv=mo', 'userName': 'ì•ˆí¬ìˆ˜'}, {'at': datetime.datetime(2021, 10, 12, 8, 53, 16), 'content': 'ì™“ì±  ì˜ì‚¬ìš©í•˜ëŠ” ìœ ì €ì…ë‹ˆë‹¤ ì¹´ë“œë¡œê²°ì œí•œê±°ëŠ” í•´ì§€í•´ì„œ 10ì›”3ì¼ì¯¤ í•´ì§€ëœë‹¤ê³ í•˜ì…”ì„œ ì·¨ì†Œí•˜ê³  êµ¬ê¸€ê¸°í”„íŠ¸ì¹´ë“œë¡œ ê²°ì œí–‡ëŠ”ë° ì™œ ëˆì„ ì•ˆëŒë ¤ì£¼ì‹œì£ ?ëˆëŒë ¤ì£¼ì‹œë©´ ì¢‹ê²ŸìŠµë‹ˆë‹¤', 'repliedAt': None, 'replyContent': None, 'reviewCreatedVersion': '1.9.96', 'reviewId': 'gp:AOqpTOEqFS_DVBEaph1OgYa9IwVZxtghfWKYxgBALGM_jN6Q92zsy1MdlnSDJIKXmUJ4HY_VOWEt_XXWlEJh21c', 'score': 5, 'thumbsUpCount': 3, 'userImage': 'https://play-lh.googleusercontent.com/a-/AOh14GgNhr4vwFU8hQyPbFLYGnSoDfJCJFUBCXhxU5aO', 'userName': 'ì¤€íŠœë¸ŒJUNTUBE'}] . +) continuation_tokenì„ ì„¤ì •í•´ì£¼ë©´ ì´ì–´ì„œ ìˆ˜ì§‘ ê°€ëŠ¥ . result, _ = reviews( 'com.frograms.wplay', continuation_token=continuation_token # default: None (ì²˜ìŒë¶€í„° ìˆ˜ì§‘) ) result . [{'at': datetime.datetime(2021, 8, 2, 14, 59, 45), 'content': 'ì € ê°™ì€ ê²½ìš°ì—ëŠ” ì˜¤ë¥˜ë„ í•˜ë‚˜ë„ ì—†ê³  ì½˜í…ì¸ ë„ ë‹¤ì–‘í•´ì„œ ë„ˆë¬´ ì¢‹ìŠµë‹ˆë‹¤ã…œã…œã…œ ì˜ ì•Œë ¤ì§€ì§€ ì•Šì€ ì˜í™”ë¼ë˜ê°€ ë‹¨í¸ì´ë¼ë˜ê°€ ì•ìœ¼ë¡œë„ ë”ìš± ë§ì´ ë“¤ì—¬ì™€ì£¼ì‹œë©´ ê°ì‚¬í•˜ê² êµ¬ìš”ã…œ ê·¸ë¦¬ê³  í˜¹ì‹œ ê° ì‘í’ˆë§ˆë‹¤ ê³µìœ ë²„íŠ¼ì„ ë§Œë“¤ì–´ì£¼ì‹œë©´ ì™“ì± ë¥¼ í™ë³´í•˜ê¸° í›¨ì”¬ í¸í• ê²ƒê°™ì•„ìš”!ã…ã… ì•ìœ¼ë¡œë„ ì†Œë¹„ìë¥¼ ìœ„í•œ ì™“ì± ê°€ ë˜ì–´ì£¼ê¸¸ ë°”ëë‹ˆë‹¹â™¡', 'repliedAt': None, 'replyContent': None, 'reviewCreatedVersion': '1.9.85', 'reviewId': 'gp:AOqpTOEH6UJtiev2mz0oCU3vsn_N90fUIM1BYbLbxh6WU6Lu_elLAphl8J7mmFkOrF1DZ510b2AmURAuinSVncc', 'score': 5, 'thumbsUpCount': 10, 'userImage': 'https://play-lh.googleusercontent.com/a-/AOh14Gh5dTIVYgAtJZdgYj99tpKD3NMBaC7QBmRno8ZSng', 'userName': 'ì •ê·œë°˜ë°°í•˜ë¯¼'}, {'at': datetime.datetime(2021, 9, 21, 4, 52, 21), 'content': 'ì¢‹ì€ ì»¨í…ì¸  ê³„ì† ì œê³µí•´ì£¼ì…¨ìœ¼ë©´ ì¢‹ê² ì–´ìš”!!&gt;_&lt; ê°ì¢… ì˜í™”ì œì—ì„œ ìƒì„ ë°›ê±°ë‚˜ ìš°ìˆ˜í•œ í‰ê°€ë¥¼ ë°›ì•˜ë˜ ì‘í’ˆë“¤ë„,,, ë¶€íƒë“œë ¤ìš” ã…ã……ã…', 'repliedAt': None, 'replyContent': None, 'reviewCreatedVersion': '1.9.96', 'reviewId': 'gp:AOqpTOE1zthMN8AlbYemq_2IZpwlZ32xOK-46-s39ZSZZvcuZm_Ejasisx4EM1hHrd2nvcS9HzoNKEEVH54k0_Q', 'score': 5, 'thumbsUpCount': 5, 'userImage': 'https://play-lh.googleusercontent.com/a/AATXAJxrul7Th625uvpvsdeXIDcqQjmFlfw2_5t9y8ak=mo', 'userName': 'ë°©ì§€ì˜'}, {'at': datetime.datetime(2021, 9, 23, 13, 49, 42), 'content': 'ë„·í”Œë¦­ìŠ¤ë³´ë‹¤ ë³¼ê²Œ ë§ì•„ì„œ ì•„ì£¼ ì¢‹ìŠµë‹ˆë‹¤ ì•„ì‰¬ìš´ê±´ ì• ë‹ˆë“¤ ì–¸ì–´ ì„ íƒì´ ê°€ëŠ¥ í–ˆìœ¼ë©´ ì¢‹ê²ŸìŠµë‹ˆë‹¤ ì›í”¼ìŠ¤ ì›ì–´ë¡œ ë³´ê³  ì‹¶ì—ˆëŠ”ë° ã…œ', 'repliedAt': None, 'replyContent': None, 'reviewCreatedVersion': '1.9.96', 'reviewId': 'gp:AOqpTOE3L6Kscm9b2rdPafySeISRRREPHxa-qWa_N6HphWdcYqtqFVpU7F24zSk5i6xs-wb0AL5NUol29Coqi7U', 'score': 5, 'thumbsUpCount': 7, 'userImage': 'https://play-lh.googleusercontent.com/a-/AOh14GghVbKmfdpD-Ld65jdsw5k5l-oq05-IceGUJNnB', 'userName': 'ì§¬ë½•ì§œì¥ë©´'} . | ì•ì„œ ë°›ì•„ì˜¨ â€˜continuation_tokenâ€™ì„ ë„£ì–´ì£¼ë©´, ê·¸ ë‹¤ìŒë¶€í„° ì´ì–´ì„œ ìˆ˜ì§‘ì´ ê°€ëŠ¥í•˜ë‹¤. | ì„¤ì •ë„ ê·¸ëŒ€ë¡œ ìœ ì§€ë¨ (5ì  ë¦¬ë·°ë§Œ, ê´€ë ¨ë„ìˆœìœ¼ë¡œ 3ê°œ ìˆ˜ì§‘) | . íŠ¹ì • ê¸°ê°„ì˜ App Reviewë§Œ ìˆ˜ì§‘ . | ex) 2020 ~ 2021 ê¸°ê°„ì˜ Watcha ë¦¬ë·°ë§Œ ëª¨ë‘ ìˆ˜ì§‘í•´ì˜¤ê¸°: | . from google_play_scraper import Sort, reviews import pandas as pd year = 2021 token = None review_list = [] while year &gt;= 2020: # 2020ë…„ ~ 2021ë…„ë§Œ ìˆ˜ì§‘ result, continuation_token = reviews( 'com.frograms.wplay', lang = 'ko', # default: 'en' country = 'kr', # default: 'us' continuation_token = token, sort = Sort.NEWEST, # default: Sort.MOST_RELEVANT count = 100, # default: 100 filter_score_with = None # default: None (ëª¨ë“  í‰ì ì„ ë‹¤ ê°€ì ¸ì˜´) ) token = continuation_token year = result[-1]['at'].year for review in result: if review['at'].year &gt;= 2020: # 2020ë…„ì˜ ë¦¬ë·°ê¹Œì§€ë§Œ ì €ì¥ temp_list = [review['score'], review['content'], review['at']] review_list.append(temp_list) review_df = pd.DataFrame(review_list, columns=['score', 'content', 'date']) print(len(review_df)) review_df.head() . 7378 . | Â  | score | content | date | . | 0 | 1 | 2ì£¼ ë¬´ë£Œì²´í—˜ì„ í•˜ë ¤ê³  ë“±ë¡í•´ë†“ì•˜ë‹¤ê°€ 2ì£¼ ë¬´ë£Œì²´í—˜ì´ ì•ˆë˜ì„œ í•˜ì§€ë„ ì•Šì•˜ëŠ”ë° 2ì£¼ê°€â€¦ | 2021-10-19 09:29:50 | . | 1 | 5 | ì§„ì§œ | 2021-10-19 07:58:21 | . | 2 | 1 | ì™“ì±  ì•±ì§€ì›Œë„ ê²°ì œê³„ì†ë– ì„œ ê²°ì œì•ˆí•´ë„ ë ê±°ì— ê³„ì†í•´ì•¼í•˜ë‹ˆ ì§œì¦ë‚˜ìš”. í•´ì§€ë„ ì•Šë˜ê³  | 2021-10-19 01:15:30 | . | 3 | 5 | ì¼ë³¸ë“œë¼ë§ˆê°€ ë§ì´ ìˆì–´ì„œ ì¢‹ì•„ìš” | 2021-10-18 15:21:52 | . | 4 | 4 | ê¼­ ì§‘ì„ ìˆ˜ëŠ” ì—†ì§€ë§Œ ì¡°ê¸ˆ ì•„ì‰¬ì›€ì´â€¦.ğŸ˜ | 2021-10-18 13:41:19 | . ",
    "url": "https://chaelist.github.io/docs/webscraping/app_review/#google-play-store-scraping",
    "relUrl": "/docs/webscraping/app_review/#google-play-store-scraping"
  },"8": {
    "doc": "App Review ìˆ˜ì§‘",
    "title": "App Store Scraping",
    "content": ". | App-Store-Scraper libraryë¥¼ í™œìš© | pip install app-store-scraperë¡œ ì„¤ì¹˜í•´ì„œ ì‚¬ìš© | . App Review ìˆ˜ì§‘ . | app ì •ë³´ë¥¼ ì…ë ¥í•´ ê°ì²´ ìƒì„± from app_store_scraper import AppStore # https://apps.apple.com/kr/app/watcha/id1096493180 my_app = AppStore(country='kr', app_name='watcha', app_id='1096493180') # app_idëŠ” optional print(my_app) # ê°ì²´ ê²€ì¦ . Country | kr Name | watcha ID | 1096493180 URL | https://apps.apple.com/kr/app/watcha/id1096493180 Review count | 0 . | ë¦¬ë·° ìˆ˜ì§‘ . | ex) 2020 ~ 2021 ê¸°ê°„ì˜ Watcha ë¦¬ë·°ë§Œ ëª¨ë‘ ìˆ˜ì§‘í•´ì˜¤ê¸°: | . import datetime as dt import numpy as np my_app.review(after=dt.datetime(2020, 1, 1), sleep=np.random.randint(0, 2)) . | my_app.review(how_many, after, sleep)ì˜ í˜•íƒœ | how_many = int (ì ì–´ì£¼ì§€ ì•Šìœ¼ë©´ ëª¨ë“  ë¦¬ë·°ë¥¼ ë‹¤ ê°€ì ¸ì˜¨ë‹¤) | after = datetime (optional, íŠ¹ì • ì‹œê°„ ì´í›„ì˜ ë¦¬ë·°ë§Œ ê°€ì ¸ì˜¤ê² ë‹¤ëŠ” ì˜ë¯¸) | sleep = int (optional, parameter to specify seconds to sleep between each call) | . | ê°€ì ¸ì˜¨ ë¦¬ë·°ë¥¼ dataframeìœ¼ë¡œ ì •ë¦¬ . fetched_reviews = my_app.reviews ios_review_df = pd.DataFrame(fetched_reviews) print(len(ios_review_df)) ios_review_df .head() . 5069 . | Â  | userName | isEdited | rating | review | title | date | developerResponse | . | 0 | ì €ë‚´ë¦¼ | False | 5 | ê°œë°œì ë° ê²½ì˜ì§„ë¶„ë“¤ ê¼­ ë³´ì‹œë¼ê³  ë³„5ê°œ ë‹µë‹ˆë‹¤????? ê¼­ ì½ìœ¼ì„¸ìš”â€¦ | ë²„í¼ë§ì‹¤í™”ëƒ??? | 2020-05-26 12:27:32 | nan | . | 1 | dkqjfsqfe | False | 5 | ë„·í”Œë¦­ìŠ¤ì—ì„œ ì™“ì± ë¡œ ë„˜ì–´ì˜¨ì§€ 2ê°œì›”ì§¸ì…ë‹ˆë‹¹\\nìŒ ì˜í™”ë¥¼ ì¢‹ì•„í•˜ì‹œëŠ” ë¶„ì´ë¼ë©´â€¦ | ë„ˆë¬´ì¢‹ì•„ìš”â¤ï¸ | 2020-04-06 06:40:28 | nan | . | 2 | jsa38 | False | 5 | ì•ˆë…•í•˜ì„¸ìš”! ì½”ë¡œë‚˜ë•Œë¬¸ì— ë„ˆë¬´ ì‹¬ì‹¬í•´ì„œ ë„·í”Œë¡œ ë¨¼ì € ì‹œì‘í–ˆëŠ”ë° ë„ˆë¬´ ë³¼ ê²ƒì´ ì—†ì–´â€¦ | ì™“ì±  ìµœê³ ì˜ˆìš”â¤ï¸ | 2020-08-25 01:03:45 | nan | . | 3 | ìƒŒë“œìœ„ì¹˜ëƒ ëƒ ëƒ  | False | 5 | ë„·í”Œë¦­ìŠ¤ë„ ì“°ê³  ì™“ì± ë„ ì“°ëŠ”ë° ê°œì¸ì ìœ¼ë¡œ ë„·í”Œë¦­ìŠ¤ëŠ” ì˜¤ë¦¬ì§€ë„ì´ ìˆì§€ë§Œ ì™“ì± ê°€ ë³¼ê±´â€¦ | ë„ˆë¬´ ì¢‹ìŠµë‹ˆë‹¤ | 2020-04-26 16:13:03 | nan | . | 4 | dkjfew | False | 5 | ë§¥ë¶ìœ¼ë¡œ ë³´ì‹œë ¤ë©´ ì•±ìŠ¤í† ì–´ì—ì„œ ì•„ì´íŒ¨ë“œìš© ì•±ìœ¼ë¡œ ë‹¤ìš´ ë°›ìœ¼ì‹œë©´ ë©ë‹ˆë‹¤~ ê·¸ë¦¬ê³ â€¦ | ë§¥ë¶ìœ¼ë¡œ ì™“ì±  ë³¼ ìˆ˜ ìˆì–´ìš”~! | 2021-03-01 10:45:08 | nan | . | . ",
    "url": "https://chaelist.github.io/docs/webscraping/app_review/#app-store-scraping",
    "relUrl": "/docs/webscraping/app_review/#app-store-scraping"
  },"9": {
    "doc": "Classification 1",
    "title": "Classification 1",
    "content": ". | Classification | KNN (K Nearest Neighbors) . | ê¸°ë³¸ ê°œë… | scikit-learnìœ¼ë¡œ êµ¬í˜„í•˜ê¸° | . | Logistic Regression . | ê¸°ë³¸ ê°œë… | scikit-learnìœ¼ë¡œ êµ¬í˜„í•˜ê¸° | . | . ",
    "url": "https://chaelist.github.io/docs/ml_basics/classification1/",
    "relUrl": "/docs/ml_basics/classification1/"
  },"10": {
    "doc": "Classification 1",
    "title": "Classification",
    "content": ": supervised learning ì¤‘, ì¢…ì†ë³€ìˆ˜ê°€ ë²”ì£¼í˜• ë³€ìˆ˜ì¸ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ë°©ì‹. | cf) ì¢…ì†ë³€ìˆ˜ê°€ ì—°ì† ë³€ìˆ˜ì¸ ë¬¸ì œëŠ” Linear Regression ëª¨í˜•ì´ ì í•© | . Â  . *Classification problems . | Binary classification: ì¢…ì†ë³€ìˆ˜ì˜ ê°’ì´ 2ê°€ì§€ â†’ ë³´í†µ 0ê³¼ 1ë¡œ í‘œí˜„ . | ex) ì–´ë–¤ ì‚¬ëŒì´ ê°ê¸°ì¸ì§€ ì•„ë‹Œì§€ | ex) ì˜í™”ê°€ ì„±ê³µì¸ì§€ ì‹¤íŒ¨ì¸ì§€ | ex) ì†Œë¹„ìê°€ Aì œí’ˆì„ ì‚¬ëŠ”ì§€ ì•ˆì‚¬ëŠ”ì§€ | . | Multiclass classification: ì¢…ì†ë³€ìˆ˜ ê°’ì´ 3ê°€ì§€ ì´ìƒ. | ex) ë‰´ìŠ¤ ê¸°ì‚¬ì˜ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ | ex) ì˜í™”ì˜ ì¥ë¥´ ë¶„ë¥˜ | ex) í•™ì  ë¶„ë¥˜ (A ~ F) | . | . ",
    "url": "https://chaelist.github.io/docs/ml_basics/classification1/#classification",
    "relUrl": "/docs/ml_basics/classification1/#classification"
  },"11": {
    "doc": "Classification 1",
    "title": "KNN (K Nearest Neighbors)",
    "content": ": ê°€ì¥ ê°€ê¹Œìš´ Kê°œì˜ neighborê°€ ì†í•œ classë¡œ ë°°ì •í•˜ëŠ” classification ë°©ì‹. ê¸°ë³¸ ê°œë… . | ex) K=1ì´ë¼ë©´, ìƒˆë¡œìš´ datapointì˜ classëŠ” ê°€ì¥ ê°€ê¹Œìš´ í•˜ë‚˜ì˜ neighborì˜ classì— ë”°ë¼ ë°°ì •ëœë‹¤. | ex) K=5ë¼ë©´, ìƒˆë¡œìš´ datapointì˜ classëŠ” ê°€ì¥ ê°€ê¹Œìš´ 5ê°œì˜ neighbor ì¤‘ ê°€ì¥ ë§ì€ ìˆ˜ê°€ ì†í•œ classì— ë”°ë¼ ë°°ì •ëœë‹¤. | ë³´í†µ, class(ë°°ì •í•˜ê²Œ ë˜ëŠ” ì§‘ë‹¨)ì˜ ìˆ˜ê°€ ì§ìˆ˜ì¼ ê²½ìš°, KëŠ” í™€ìˆ˜ë¡œ í•œë‹¤ | ìµœì ì˜ Kê°’ì€ datasetë§ˆë‹¤ ë‹¤ë¥´ë¯€ë¡œ, ë‹¤ì–‘í•œ Kê°’ì¼ ë•Œì˜ ì„±ëŠ¥ì„ ì²´í¬í•´ë³´ëŠ” ê²Œ ì¢‹ë‹¤ | . (ì¶œì²˜: datacamp) . Â  . *vectorê°„ ìœ ì‚¬ë„ ê³„ì‚° ë°©ë²• . | Euclidean distance: xy = sqrt(sum((x - y)2)) . | x, yëŠ” ê°ê° í•˜ë‚˜ì˜ ë²¡í„°(ë°ì´í„°í¬ì¸íŠ¸)ë¥¼ ì˜ë¯¸ | ê°€ì¥ ê¸°ë³¸ì ì¸ ê±°ë¦¬ ê³„ì‚° ë°©ì‹ | ë†’ì€ ì°¨ì›ì—ì„œì˜ ê³„ì‚°ì—ëŠ” íš¨ê³¼ê°€ í¬ì§€ ì•Šì„ ìˆ˜ ìˆë‹¤ (cosine ìœ ì‚¬ë„ê°€ ë” ì¢‹ì€ ê²°ê³¼ë¥¼ ë‚¼ ìˆ˜ ìˆìŒ) | . | Cosine ìœ ì‚¬ë„: cosÎ¸ = xâˆ™y / lxllyl . | xâˆ™yëŠ” ë²¡í„° ê°„ ë‚´ì ê³±, lxl: ì›ì ì—ì„œ ë²¡í„°xê¹Œì§€ì˜ ê±°ë¦¬ | cosÎ¸ ê°’ì´ 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ë²¡í„° ì‚¬ì´ ìœ ì‚¬ë„ê°€ í° ê²ƒ. (cosÎ¸ê°€ 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì‚¬ì´ê°ì´ 0ì— ê°€ê¹Œì›€) | ì •í™•íˆ â€˜ê¸¸ì´â€™ì˜ ì°¨ì´ë¥¼ ë°˜ì˜í•˜ëŠ” ê²ƒì€ ì•„ë‹ˆë¼ëŠ” ë‹¨ì  (ì‚¬ì´ê°ì´ ì‘ì•„ë„ ë‘ ë²¡í„° ê°„ ê±°ë¦¬ê°€ ë©€ ìˆ˜ ìˆìŒ) | í•˜ì§€ë§Œ normalization / standardization ê³¼ì •ì„ ê±°ì¹˜ê³  ë‚˜ë©´ ê¸¸ì´ì˜ ì°¨ì´ëŠ” ìœ ì˜ë¯¸í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì— cosine ìœ ì‚¬ë„ë¡œ ë²¡í„° ê°„ ìœ ì‚¬ë„ë¥¼ ì¶©ë¶„íˆ êµ¬ë¶„í•  ìˆ˜ ìˆìŒ. | . | . scikit-learnìœ¼ë¡œ êµ¬í˜„í•˜ê¸° . from sklearn.datasets import load_iris # iris ë°ì´í„° ë¶„ëŸ¬ì˜¤ê¸° from sklearn.model_selection import train_test_split # ë°ì´í„°ì…‹ì„ training set / test set ë‚˜ëˆ„ê¸° ìœ„í•œ í•¨ìˆ˜ from sklearn.neighbors import KNeighborsClassifier # KNN Classifier import pandas as pd . 1. ë°ì´í„° ì¤€ë¹„ . iris_dataset = load_iris() # ë°ì´í„°ì…‹ì„ ê°€ì ¸ì™€ì¤€ë‹¤ . +) print(iris_dataset.DESCR)ë¥¼ í•´ì£¼ë©´ ë°ì´í„°ì…‹ì— ëŒ€í•œ ì •ë³´ë¥¼ ì‚´í´ë³¼ ìˆ˜ ìˆìŒ . | ë…ë¦½ë³€ìˆ˜: Sepal(ê½ƒë°›ì¹¨) length, Sepal width, Petl(ê½ƒì) length, Petal widthì˜ 4ê°œ | ê²°ê³¼ë³€ìˆ˜: Iris-Setosa, Iris-Versicolour, Iris-Virginica ì´ë ‡ê²Œ 3ì¢…ë¥˜ . | ë°ì´í„°ì…‹ì—ëŠ” 3ì¢…ë¥˜ì˜ ë¶“ê½ƒ(iris)ê°€ 1/3ì”©(=50ê°œì”©) í¬í•¨ë˜ì–´ ìˆìŒ | . | . Â  . â†’ ë°ì´í„° ì •ë¦¬ . # Xì— boston datasetì˜ ì…ë ¥ë³€ìˆ˜ë“¤ &amp; í•´ë‹¹ ì…ë ¥ë³€ìˆ˜ ëª…ì¹­ë“¤ ì •ë¦¬ X = pd.DataFrame(iris_dataset.data, columns=iris_dataset.feature_names) X.head() . | Â  | sepal length (cm) | sepal width (cm) | petal length (cm) | petal width (cm) | . | 0 | 5.1 | 3.5 | 1.4 | 0.2 | . | 1 | 4.9 | 3 | 1.4 | 0.2 | . | 2 | 4.7 | 3.2 | 1.3 | 0.2 | . | 3 | 4.6 | 3.1 | 1.5 | 0.2 | . | 4 | 5 | 3.6 | 1.4 | 0.2 | . Â  . # ëª©í‘œë³€ìˆ˜ë„ dataframeìœ¼ë¡œ ì •ë¦¬ y = pd.DataFrame(iris_dataset.target, columns=['Class']) y.head() . | Â  | Class | . | 0 | 0 | . | 1 | 0 | . | 2 | 0 | . | 3 | 0 | . | 4 | 0 | . Â  . y.Class.unique() ## 0: Iris-Setosa, 1: Iris-Versicolour, 2: Iris-Virginica . array([0, 1, 2]) . 2. train_test_split . X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=5) # 20%ë¥¼ test setìœ¼ë¡œ ì„ íƒ # ì˜ ë‚˜ë‰˜ì—ˆë‚˜ í™•ì¸ print(X_train.shape) print(X_test.shape) print(y_train.shape) print(y_test.shape) . (120, 4) (30, 4) (120, 1) (30, 1) . 3. ëª¨ë¸ í•™ìŠµì‹œí‚¤ê¸° . y_train = y_train.values.ravel() #ravel(): ë‹¤ì°¨ì› arrayë¥¼ 1ì°¨ì› arrayë¡œ í‰í‰í•˜ê²Œ í´ì£¼ëŠ” í•¨ìˆ˜. ì•ˆì¨ë„ ë˜ì§€ë§Œ ì•ˆì“°ë©´ ê²½ê³ ê°€ ëœ¸. model = KNeighborsClassifier(n_neighbors=5) # ê°€ì¥ ê·¼ì ‘í•œ 5ê°œ ì´ì›ƒì˜ classì— ë”°ë¼ ë¶„ë¥˜í•˜ê² ë‹¤ëŠ” ëœ» model.fit(X_train, y_train) . KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski', metric_params=None, n_jobs=None, n_neighbors=5, p=2, weights='uniform') . | metric: ì‚¬ìš©í•  distance metric. (ë²¡í„° ê°„ ìœ ì‚¬ë„(ê±°ë¦¬)ë¥¼ ì–´ë–¤ ë°©ì‹ìœ¼ë¡œ ì¸¡ì •í•  ê²ƒì¸ì§€) . | default=â€™minkowskiâ€™ (+. ê°€ëŠ¥í•œ metrics) | minkowski ë°©ì‹ì—ì„œ p=2 (default)ë©´ standard Euclidean metricì™€ ë™ì¼í•œ ê³„ì‚°ì‹ | . | . 4. test dataë¡œ ì„±ëŠ¥ ì²´í¬ . model.predict(X_test) # ì–´ë–»ê²Œ ë¶„ë¥˜í–ˆë‚˜ í™•ì¸ . array([1, 2, 2, 0, 2, 1, 0, 2, 0, 1, 1, 2, 2, 2, 0, 0, 2, 2, 0, 0, 1, 2, 0, 2, 1, 2, 1, 1, 1, 2]) . y_test['class'].to_numpy() ## ì‹¤ì œ y_testì™€ ë¹„êµ . array([1, 2, 2, 0, 2, 1, 0, 1, 0, 1, 1, 2, 2, 2, 0, 0, 2, 2, 0, 0, 1, 2, 0, 1, 1, 2, 1, 1, 1, 2]) . # ëª‡ í¼ì„¼íŠ¸ê°€ ì˜¬ë°”ë¥´ê²Œ ë¶„ë¥˜ë˜ì—ˆëŠ”ì§€ í™•ì¸ model.score(X_test, y_test) . 0.9333333333333333 . ì•½ 93% ì •ë„ê°€ ì˜¬ë°”ë¥´ê²Œ ë¶„ë¥˜ë˜ì—ˆë‹¤ëŠ” ì˜ë¯¸ . ",
    "url": "https://chaelist.github.io/docs/ml_basics/classification1/#knn-k-nearest-neighbors",
    "relUrl": "/docs/ml_basics/classification1/#knn-k-nearest-neighbors"
  },"12": {
    "doc": "Classification 1",
    "title": "Logistic Regression",
    "content": "ê¸°ë³¸ ê°œë… . (ì¶œì²˜: incredible.ai) . | ê°€ì„¤ í•¨ìˆ˜: sigmoid í•¨ìˆ˜ (í•™ìŠµ = ë°ì´í„°ì— ê°€ì¥ ì˜ ë§ëŠ” sigmoid í•¨ìˆ˜ë¥¼ ì°¾ëŠ” ê²ƒ) . | sigmoid í•¨ìˆ˜ëŠ” 0ê³¼ 1 ì‚¬ì´ì˜ ì—°ì†ì ì¸ ê²°ê³¼ê°’ì„ ê°–ê¸°ì— â€˜regression(íšŒê·€)â€™ë¼ê³  ì´ë¦„ì´ ë¶™ì§€ë§Œ, ë³´í†µ sigmoid í•¨ìˆ˜ì˜ ê²°ê³¼ê°’ì´ 0.5ë³´ë‹¤ í°ì§€ ì‘ì€ì§€ë¥¼ ë³´ê³  â€˜ë¶„ë¥˜â€™ë¥¼ í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ì£¼ë¡œ ì‚¬ìš©í•œë‹¤ | . | 0ê³¼ 1 ì–´ëŠ ìª½ì— ê°€ê¹Œìš´ì§€ë¥¼ íŒë³„í•˜ëŠ” binary classification ëª¨ë¸ì´ì§€ë§Œ, 3ê°œ ì´ìƒì˜ ë¶„ë¥˜ì—ë„ ì‚¬ìš© ê°€ëŠ¥í•˜ë‹¤ | *3ê°œ ì´ìƒì˜ Classë¥¼ ë¶„ë¥˜í•˜ëŠ” ë°©ë²•: (ex. ë©”ì¼ì„ ì—…ë¬´ ë©”ì¼ / ì‚¬ì  ë©”ì¼ / ìŠ¤íŒ¸ ë©”ì¼ 3ì¢…ë¥˜ë¡œ ë¶„ë¥˜í•  ë•Œ) . | ì—…ë¬´ ë©”ì¼ì¸ì§€ ì•„ë‹Œì§€ íŒë³„í•˜ëŠ” í™œì„± í•¨ìˆ˜ â†’ ì—…ë¬´ ë©”ì¼ì¼ í™•ë¥ ì„ êµ¬í•¨ | ì‚¬ì  ë©”ì¼ì¸ì§€ ì•„ë‹Œì§€ íŒë³„í•˜ëŠ” í™œì„± í•¨ìˆ˜ â†’ ì‚¬ì  ë©”ì¼ì¼ í™•ë¥ ì„ êµ¬í•¨ | ìŠ¤íŒ¸ ë©”ì¼ì¸ì§€ ì•„ë‹Œì§€ íŒë³„í•˜ëŠ” í™œì„± í•¨ìˆ˜ â†’ ìŠ¤íŒ¸ ë©”ì¼ì¼ í™•ë¥ ì„ êµ¬í•¨ | ê° datapointë¥¼ ê°€ì¥ í™•ë¥ ì´ ë†’ì€ ìª½ìœ¼ë¡œ ë¶„ë¥˜. (ex. ìŠ¤íŒ¸ ë©”ì¼ì¼ í™•ë¥ ì´ ê°€ì¥ ë†’ë‹¤ë©´, ìŠ¤íŒ¸ ë©”ì¼ë¡œ ë¶„ë¥˜) | . | ì†ì‹¤í•¨ìˆ˜: ë¡œê·¸ ì†ì‹¤ = log loss = cross entropy . (ì¶œì²˜: codeit) . | yê°€ 0ì¸ ê²½ìš°: 0ì— ê°€ê¹ê²Œ ë¶„ë¥˜í• ìˆ˜ë¡ ë¡œê·¸ ì†ì‹¤ì´ 0ì— ê°€ê¹ê³ , 1ì— ê°€ê¹ê²Œ ë¶„ë¥˜í• ìˆ˜ë¡ ë¡œê·¸ ì†ì‹¤ì´ ë¬´í•œëŒ€ì— ê°€ê¹Œì›Œì§ | yê°€ 1ì¸ ê²½ìš°: 1ì— ê°€ê¹ê²Œ ë¶„ë¥˜í• ìˆ˜ë¡ ë¡œê·¸ ì†ì‹¤ì´ 0ì— ê°€ê¹ê³ , 0ì— ê°€ê¹ê²Œ ë¶„ë¥˜í• ìˆ˜ë¡ ë¡œê·¸ ì†ì‹¤ì´ ë¬´í•œëŒ€ì— ê°€ê¹Œì›Œì§ | sklearnì˜ Logistic Regressionì€ Gradient Descent ë°©ì‹ìœ¼ë¡œ ì†ì‹¤í•¨ìˆ˜ê°€ 0ì— ê°€ê¹Œì›Œì§€ëŠ” ì ì„ ì°¾ëŠ”ë‹¤ | . | . scikit-learnìœ¼ë¡œ êµ¬í˜„í•˜ê¸° . from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression import pandas as pd . 1. ë°ì´í„° ì¤€ë¹„ . # KNNì—ì„œì™€ ë™ì¼í•˜ê²Œ iris data ì‚¬ìš© iris_data = load_iris() # dataframeìœ¼ë¡œ ì˜®ê¸°ê¸° X = pd.DataFrame(iris_data.data, columns=iris_data.feature_names) y = pd.DataFrame(iris_data.target, columns=['class']) . X.head() . | Â  | sepal length (cm) | sepal width (cm) | petal length (cm) | petal width (cm) | . | 0 | 5.1 | 3.5 | 1.4 | 0.2 | . | 1 | 4.9 | 3 | 1.4 | 0.2 | . | 2 | 4.7 | 3.2 | 1.3 | 0.2 | . | 3 | 4.6 | 3.1 | 1.5 | 0.2 | . | 4 | 5 | 3.6 | 1.4 | 0.2 | . Â  . y.head() . | Â  | Class | . | 0 | 0 | . | 1 | 0 | . | 2 | 0 | . | 3 | 0 | . | 4 | 0 | . 2. train_test_split &amp; í•™ìŠµ . X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=5) y_train = y_train.values.ravel() #ravel(): ë‹¤ì°¨ì› arrayë¥¼ 1ì°¨ì› arrayë¡œ í‰í‰í•˜ê²Œ í´ì£¼ëŠ” í•¨ìˆ˜. model = LogisticRegression(solver='saga', max_iter=2000) . | solver: ëª¨ë¸ì„ ìµœì í™”í•  ë•Œ ì–´ë–¤ ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í• ì§€ ì„ íƒí•˜ëŠ” ê²ƒ. (=ì–´ë–¤ ê²½ì‚¬í•˜ê°•ë²•ì„ ì‚¬ìš©í• ì§€) . | SAGA: SAG(Stochastic Average Gradient) ì•Œê³ ë¦¬ì¦˜ì„ ê°œì„ í•œ ë²„ì „ì˜ ì•Œê³ ë¦¬ì¦˜. ìœ ì‚¬í•˜ì§€ë§Œ ë” ì„±ëŠ¥ì´ ì¢‹ë‹¤ê³  í•¨ | +. ê°€ëŠ¥í•œ solver (datasetì˜ íŠ¹ì„±ì— ë§ê²Œ ì ì ˆíˆ ì„ íƒ) | defaultëŠ” lbfgs. | . | max_iter: ìµœì í™” ê³¼ì •ì„ ëª‡ ë²ˆ ë°˜ë³µí•  ì§€ë¥¼ ì •í•´ì£¼ëŠ” ê²ƒ. | max_iter=2000ì´ë¼ê³  í•´ë„ ë§Œì•½ 2000ë²ˆ ëŒê¸° ì „ì— ìµœì í™”ë˜ë©´ ê·¸ ë•Œ ì´ë¯¸ ë©ˆì¶˜ë‹¤ | defaultëŠ” 100. | . | . model.fit(X_train, y_train) . LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True, intercept_scaling=1, l1_ratio=None, max_iter=2000, multi_class='auto', n_jobs=None, penalty='l2', random_state=None, solver='saga', tol=0.0001, verbose=0, warm_start=False) . | C: Inverse of regularization strength. 0 ~ 1 ì‚¬ì´ì˜ float ê°’ì„ ì§€ì •í•  ìˆ˜ ìˆìœ¼ë©°, ê¸°ë³¸ ì„¸íŒ…ì€ 1.0 . | ìˆ«ìê°€ ì‘ì„ìˆ˜ë¡ penaltyë¥¼ ê°•í•˜ê²Œ ì¤€ë‹¤ëŠ” ëœ» (= stronger regularization) | Î» = 1 / C â†’ Cê°€ ì‘ì„ìˆ˜ë¡ Î» (penalty)ê°€ ì»¤ì§„ë‹¤ | Î»ëŠ” Î¸ê°’(ë³€ìˆ˜ë³„ ê°€ì¤‘ì¹˜)ì— ë¶™ëŠ” penaltyì˜ ê°œë…. (Î¸ê°’ì´ ë„ˆë¬´ ì»¤ì§€ëŠ” ê²ƒì„ ë°©ì§€í•´ì¤€ë‹¤) | . | penalty: L1ê³¼ L2 Regularization ì¤‘ ì–´ëŠ ê²ƒì„ ì„ íƒí• ì§€ë¥¼ ì˜ë¯¸. (defaultëŠ” L2) . | (Regularizationì— ëŒ€í•´ì„œëŠ” ì¶”í›„ ìì„¸íˆ ì„¤ëª… ì˜ˆì •) | . | multi_class: defaultë¡œ autoë¼ê³  ë˜ì–´ ìˆëŠ”ë°, binary problemì´ê±°ë‚˜ solver = â€˜liblinearâ€™ì¸ ê²½ìš°ì—ëŠ” â€˜ovrâ€™ë¡œ ì„ íƒí•˜ê³ , ê·¸ ì™¸ì—ëŠ” â€˜multinomialâ€™ë¡œ ì„ íƒí•´ì¤€ë‹¤ . | ì´ ê²½ìš°, multiclass problemì´ë¯€ë¡œ ì•Œì•„ì„œ â€˜multinomialâ€™ë¡œ ì„ íƒë˜ì—ˆì„ ê²ƒ. | . | . 3. test dataë¡œ ì„±ëŠ¥ ì²´í¬ . model.predict(X_test) # ì–´ë–»ê²Œ ë¶„ë¥˜í–ˆë‚˜ í™•ì¸ . array([1, 2, 2, 0, 2, 1, 0, 2, 0, 1, 1, 2, 2, 2, 0, 0, 2, 2, 0, 0, 1, 2, 0, 1, 1, 2, 1, 1, 1, 2]) . y_test['class'].to_numpy() ## ì‹¤ì œ y_testì™€ ë¹„êµ . array([1, 2, 2, 0, 2, 1, 0, 1, 0, 1, 1, 2, 2, 2, 0, 0, 2, 2, 0, 0, 1, 2, 0, 1, 1, 2, 1, 1, 1, 2]) . # ëª‡ í¼ì„¼íŠ¸ê°€ ì˜¬ë°”ë¥´ê²Œ ë¶„ë¥˜ë˜ì—ˆëŠ”ì§€ í™•ì¸ model.score(X_test, y_test) . 0.9666666666666667 . ì•½ 97% ì •ë„ê°€ ì˜¬ë°”ë¥´ê²Œ ë¶„ë¥˜ë˜ì—ˆë‹¤ëŠ” ì˜ë¯¸ . ",
    "url": "https://chaelist.github.io/docs/ml_basics/classification1/#logistic-regression",
    "relUrl": "/docs/ml_basics/classification1/#logistic-regression"
  },"13": {
    "doc": "Classification 2",
    "title": "Classification 2",
    "content": ". | Decision Tree (ê²°ì • íŠ¸ë¦¬) . | ê¸°ë³¸ ê°œë… | scikit-learnìœ¼ë¡œ ë°ì´í„° í•™ìŠµ | tree êµ¬ì¡° í™•ì¸ | ì†ì„± ì¤‘ìš”ë„ í™•ì¸ | . | Random Forest . | ê¸°ë³¸ ê°œë… | scikit-learnìœ¼ë¡œ ë°ì´í„° í•™ìŠµ | ì†ì„± ì¤‘ìš”ë„ í™•ì¸ | . | AdaBoost . | ê¸°ë³¸ ê°œë… | scikit-learnìœ¼ë¡œ ë°ì´í„° í•™ìŠµ | ì†ì„± ì¤‘ìš”ë„ í™•ì¸ | . | . ",
    "url": "https://chaelist.github.io/docs/ml_basics/classification2/",
    "relUrl": "/docs/ml_basics/classification2/"
  },"14": {
    "doc": "Classification 2",
    "title": "Decision Tree (ê²°ì • íŠ¸ë¦¬)",
    "content": ": ì˜ˆ/ì•„ë‹ˆì˜¤ë¡œ ë‹µí•  ìˆ˜ ìˆëŠ” ì–´ë–¤ ì§ˆë¬¸ë“¤ì´ ìˆê³ , ê·¸ ì§ˆë¬¸ë“¤ì˜ ë‹µì„ ë”°ë¼ê°€ë©´ì„œ ë°ì´í„°ë¥¼ ë¶„ë¥˜í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜ . ê¸°ë³¸ ê°œë… . | ê°€ì¥ ìœ„ì— ìˆëŠ” ì§ˆë¬¸ ë…¸ë“œë¥¼ â€˜root ë…¸ë“œâ€™ë¼ê³  í•˜ê³ , íŠ¸ë¦¬ì˜ ê°€ì¥ ëì— ìˆëŠ” ë¶„ë¥˜ ë…¸ë“œë“¤ì„ â€˜leaf ë…¸ë“œâ€™ë¼ê³  í•œë‹¤. | leaf nodeëŠ” ì‚¬ë§/ìƒì¡´, or íŒ½ê·„/ëŒê³ ë˜ ì´ëŸ° ì‹ìœ¼ë¡œ íŠ¹ì • ì˜ˆì¸¡ê°’ì„ ê°€ì§€ê³  ìˆê³  (= ë¶„ë¥˜ ë…¸ë“œ) , ë‚˜ë¨¸ì§€ ë…¸ë“œë“¤ì€ ì˜ˆ/ì•„ë‹ˆì˜¤(True/False)ë¡œ ë‹µí•  ìˆ˜ ìˆëŠ” ì§ˆë¬¸ë“¤ì„ ê°€ì§€ê³  ìˆë‹¤ | . (ì¶œì²˜: javatpoint.com) . Â  . *íŠ¸ë¦¬ì˜ ë…¸ë“œ ë§Œë“¤ê¸° . | ì—¬ëŸ¬ ì§ˆë¬¸ ë…¸ë“œì™€ ë¶„ë¥˜ ë…¸ë“œì˜ â€˜ì§€ë‹ˆ ë¶ˆìˆœë„â€™ë¥¼ ê³„ì‚°í•´ì„œ, ì§€ë‹ˆ ë¶ˆìˆœë„ê°€ ê°€ì¥ ë‚®ì€ ì§ˆë¬¸ì„ ë…¸ë“œë¡œ ë§Œë“¤ì–´ì¤€ë‹¤. | ì§ˆë¬¸ í›„ë³´ë“¤ë³´ë‹¤ ë¶„ë¥˜ ë…¸ë“œ(ex. â€˜ë…ê°ìœ¼ë¡œ ë¶„ë¥˜â€™)ì˜ ì§€ë‹ˆ ë¶ˆìˆœë„ê°€ ë‚®ìœ¼ë©´ ê·¸ëƒ¥ ë°”ë¡œ ë¶„ë¥˜í•´ì£¼ëŠ” ë‹¨ê³„ë¡œ ë„˜ì–´ê°„ë‹¤ (leaf node) | ex) ë…ê° / ê°ê¸° ë¶„ë¥˜: íŠ¹ì • ë°ì´í„°ì…‹ì„ 1) ëª¨ë‘ ë…ê°ìœ¼ë¡œ ë¶„ë¥˜(ë¶„ë¥˜ ë…¸ë“œ)í•  ë•Œì˜ ì§€ë‹ˆ ë¶ˆìˆœë„ê°€ 0.49, 2) â€˜ê³ ì—´ì´ ìˆë‚˜ìš”?â€™ ì§ˆë¬¸ ë…¸ë“œë¡œ ë¶„ë¥˜í•  ë•Œì˜ ì§€ë‹ˆ ë¶ˆìˆœë„ê°€ 0.34, 3) â€˜ëª¸ì‚´ì´ ìˆë‚˜ìš”?â€™ ì§ˆë¬¸ ë…¸ë“œë¡œ ë¶„ë¥˜í•  ë•Œì˜ ì§€ë‹ˆ ë¶ˆìˆœë„ê°€ 0.33ì´ë¼ë©´ â€˜ëª¸ì‚´â€™ ì§ˆë¬¸ ë…¸ë“œë¥¼ ê³¨ë¼ì¤€ë‹¤! | â€»ì§ˆë¬¸ë…¸ë“œì˜ ì§€ë‹ˆë¶ˆìˆœë„ëŠ” ì´ë¡œ ì¸í•´ ë¶„ë¥˜ëœ 2ê°œì˜ ë°ì´í„°ì…‹ì˜ ì§€ë‹ˆ ë¶ˆìˆœë„ì˜ í‰ê· ìœ¼ë¡œ ê³„ì‚° | ë°ì´í„°ê°€ ìˆ«ìí˜•ì¸ ê²½ìš°(ex. ì²´ì˜¨): ë°ì´í„°ë¥¼ ì •ë ¬í•œ í›„, ê° ì—°ì†ëœ ë°ì´í„°ì˜ í‰ê· ì„ ê³„ì‚° â†’ ì´ í‰ê· ì„ ì´ìš©í•´ ì§ˆë¬¸ì„ í•˜ë‚˜ì”© ë§Œë“¦ (ex. 36.4ë„ê°€ ë„˜ë‚˜ìš”?, 36.6ë„ê°€ ë„˜ë‚˜ìš”?,â€¦) â†’ ì´ ì¤‘ ì§€ë‹ˆ ë¶ˆìˆœë„ê°€ ê°€ì¥ ë‚®ì€ ì§ˆë¬¸ì„ ì„ ì • | . Â  . *ì§€ë‹ˆ ë¶ˆìˆœë„(Gini Impurity) . | ë°ì´í„°ì…‹ì˜ ë°ì´í„°ë“¤ì´ ì–¼ë§ˆë‚˜ í˜¼í•©ë˜ì–´ ìˆëŠ”ì§€ ë‚˜íƒ€ë‚´ëŠ” ìˆ˜ì¹˜. | ì§€ë‹ˆ ë¶ˆìˆœë„ê°€ ì‘ì„ìˆ˜ë¡ ë°ì´í„°ì…‹ì´ ìˆœìˆ˜í•˜ë‹¤ëŠ” ëœ». | ë…ê° &amp; ì¼ë°˜ ê°ê¸° ë°ì´í„°ê°€ ì„ì—¬ ìˆëŠ” ë°ì´í„°ì…‹ì˜ ì§€ë‹ˆ ë¶ˆìˆœë„: 1 - pflu2 - pnot_flu2 (*pëŠ” í™•ë¥ ) . | ex) ë°ì´í„° 100ê°œ ì¤‘ 70ê°œê°€ ë…ê° 30ê°œê°€ ì¼ë°˜ ê°ê¸° â†’ 1 - 0.72 - 0.32 = 0.42 | ex) 50ê°œ ë…ê° 50ê°œ ì¼ë°˜ ê°ê¸° â†’ 1 - 0.52 - 0.52 = 0.5 | ex) 100ê°œ ëª¨ë‘ ë…ê° â†’ 1 - 1.02 - 0.02 = 0 | . | ì„¸ ê°€ì§€ classê°€ ì„ì—¬ìˆëŠ” ê²½ìš°: 1 - pclass12 - pclass22 - pclass32 | . scikit-learnìœ¼ë¡œ ë°ì´í„° í•™ìŠµ . from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.tree import DecisionTreeClassifier import matplotlib.pyplot as plt import numpy as np import pandas as pd . 1. ë°ì´í„° ì¤€ë¹„ . # ë°ì´í„° ì¤€ë¹„: KNN, Logistic Regressionì—ì„œì™€ ë™ì¼í•˜ê²Œ iris data ì‚¬ìš© iris_data = load_iris() X = pd.DataFrame(iris_data.data, columns=iris_data.feature_names) y = pd.DataFrame(iris_data.target, columns=['class']) . X.head() . | Â  | sepal length (cm) | sepal width (cm) | petal length (cm) | petal width (cm) | . | 0 | 5.1 | 3.5 | 1.4 | 0.2 | . | 1 | 4.9 | 3 | 1.4 | 0.2 | . | 2 | 4.7 | 3.2 | 1.3 | 0.2 | . | 3 | 4.6 | 3.1 | 1.5 | 0.2 | . | 4 | 5 | 3.6 | 1.4 | 0.2 | . Â  . y.head() . | Â  | Class | . | 0 | 0 | . | 1 | 0 | . | 2 | 0 | . | 3 | 0 | . | 4 | 0 | . 2. train_test_split &amp; í•™ìŠµ . X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=5) y_train = y_train.values.ravel() #ravel(): ë‹¤ì°¨ì› arrayë¥¼ 1ì°¨ì› arrayë¡œ í‰í‰í•˜ê²Œ í´ì£¼ëŠ” í•¨ìˆ˜. model = DecisionTreeClassifier(max_depth=4) . | max_depth: maximum depth of the tree. (íŠ¸ë¦¬ê°€ ëª‡ ì¸µê¹Œì§€ ë‚´ë ¤ ê°€ëŠ”ì§€ = depth of tree) . | max_depthë¥¼ ì„¤ì •í•´ì£¼ì§€ ì•Šìœ¼ë©´ ëª¨ë“  leafê°€ pureí•´ì§ˆ ë•Œê¹Œì§€ OR ëª¨ë“  leafê°€ min_samples_splitë³´ë‹¤ ì ì€ ì–‘ì˜ ë°ì´í„°ë¥¼ ê°€ì§ˆ ë•Œê¹Œì§€ ë¬´í•œì • nodeê°€ ìƒì„±ëœë‹¤ | depthê°€ ë„ˆë¬´ ê¹Šìœ¼ë©´ training dataì— ê³¼ì í•©ë  ìˆ˜ ìˆê¸°ì—, max_depthë¥¼ ì ì ˆíˆ ì„¸íŒ…í•´ì£¼ë©´ ì¢‹ë‹¤ | . | . model.fit(X_train, y_train) . DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini', max_depth=4, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, presort='deprecated', random_state=None, splitter='best') . | criterion: splitì˜ qulaityë¥¼ ì¸¡ì •í•˜ëŠ” ë°©ì‹. â€˜giniâ€™(=Gini impurity)ì™€ â€˜entropyâ€™(=information gain) ë‘ ê°€ì§€ ì˜µì…˜ì´ ìˆìœ¼ë©°, giniê°€ default. | giniì™€ entropyëŠ” ì‚¬ì‹¤ ê³„ì‚°ì˜ ì°¨ì´ê°€ í¬ì§€ëŠ” ì•Šê¸°ì—, ì–´ë–¤ ê±¸ ì‚¬ìš©í•˜ë“ ì§€ í° ìƒê´€ ì—†ë‹¤ | . | . 3. test dataë¡œ ì„±ëŠ¥ ì²´í¬ . model.predict(X_test) # ì–´ë–»ê²Œ ë¶„ë¥˜í–ˆë‚˜ í™•ì¸ . array([1, 1, 2, 0, 2, 2, 0, 2, 0, 1, 1, 1, 2, 2, 0, 0, 2, 2, 0, 0, 1, 2, 0, 1, 1, 2, 1, 1, 1, 2]) . y_test['class'].to_numpy() ## ì‹¤ì œ y_testì™€ ë¹„êµ . array([1, 2, 2, 0, 2, 1, 0, 1, 0, 1, 1, 2, 2, 2, 0, 0, 2, 2, 0, 0, 1, 2, 0, 1, 1, 2, 1, 1, 1, 2]) . # ëª‡ í¼ì„¼íŠ¸ê°€ ì˜¬ë°”ë¥´ê²Œ ë¶„ë¥˜ë˜ì—ˆëŠ”ì§€ í™•ì¸ model.score(X_test, y_test) . 0.8666666666666667 . ì•½ 87% ì •ë„ê°€ ì˜¬ë°”ë¥´ê²Œ ë¶„ë¥˜ë˜ì—ˆë‹¤ëŠ” ì˜ë¯¸ . tree êµ¬ì¡° í™•ì¸ . â€» ê²°ì •íŠ¸ë¦¬ëŠ” ë¶„ë¥˜ ê³¼ì •ì„ ì§ê´€ì ìœ¼ë¡œ ì´í•´í•  ìˆ˜ ìˆê³ , ì†ì„±ë³„ ì¤‘ìš”ë„ë¥¼ ì‰½ê²Œ í•´ì„í•  ìˆ˜ ìˆë‹¤ (ì •í™•ë„ê°€ ì•„ì£¼ ë†’ì€ ëª¨ë¸ì€ ì•„ë‹ˆì§€ë§Œ, í•´ì„ ë° ì ìš©ì´ ì‰½ë‹¤ëŠ” ê²ƒì´ í° ì¥ì !) . | sklearn.tree í™œìš© from sklearn import tree plt.figure(figsize=(7, 7)) tree.plot_tree(model, feature_names = iris_data.feature_names, class_names = iris_data.target_names, filled = True); # ìƒ‰ì„ ì¹ í•´ì„œ êµ¬ë¶„í•˜ê² ë‹¤ëŠ” ëœ» . | dtreeviz í™œìš© . | installí•´ì•¼ ì‚¬ìš© ê°€ëŠ¥ https://github.com/parrt/dtreeviz | . from dtreeviz.trees import dtreeviz viz = dtreeviz(model, iris_data.data, # Xê°’ iris_data.target, # yê°’: df í˜•íƒœ ë§ê³  array í˜•íƒœë¡œ ë„£ì–´ì£¼ê¸° target_name = \"target\", feature_names = iris_data.feature_names, class_names = list(iris_data.target_names)) viz # +) viz.save(\"íŒŒì¼ëª….svg\") ì´ë ‡ê²Œ í•´ì„œ ì €ì¥ . | . ì†ì„± ì¤‘ìš”ë„ í™•ì¸ . Â  . *ì†ì„± ì¤‘ìš”ë„(Feature Importance) . | ì†ì„±ì˜ â€˜í‰ê·  ì§€ë‹ˆ ê°ì†Œ(Mean Gini Decrease)â€™ë¼ê³  ë¶€ë¥´ê¸°ë„ í•¨ | . *ê³„ì‚°í•˜ëŠ” ë²•: . | ëª¨ë“  ì§ˆë¬¸ ë…¸ë“œì˜ ì¤‘ìš”ë„(Node Importance)ë¥¼ ê³„ì‚° | íŠ¹ì • ì†ì„±ì˜ ì¤‘ìš”ë„: í•´ë‹¹ ì†ì„± ì§ˆë¬¸ ë…¸ë“œì˜ ì¤‘ìš”ë„ í•© / ëª¨ë“  ë…¸ë“œì˜ ì¤‘ìš”ë„ í•© . | ì „ì²´ì ìœ¼ë¡œ ë‚®ì¶°ì§„ ë¶ˆìˆœë„ì—ì„œ, íŠ¹ì • ì†ì„± í•˜ë‚˜ê°€ ë¶ˆìˆœë„ë¥¼ ì–¼ë§ˆë‚˜ ë‚®ì·„ëŠ”ì§€ í™•ì¸ â†’ ê·¸ ì†ì„±ì˜ ì¤‘ìš”í•œ ì •ë„ë¥¼ ê³„ì‚°í•˜ëŠ” ê²ƒ! | ex) â€˜ê³ ì—´ ì—¬ë¶€â€™ ë³€ìˆ˜ì˜ ì¤‘ìš”ë„: ê³ ì—´ ì§ˆë¬¸ì„ ê°–ëŠ” ëª¨ë“  ë…¸ë“œì˜ ì¤‘ìš”ë„ í•© / íŠ¸ë¦¬ ì•ˆì— ìˆëŠ” ëª¨ë“  ë…¸ë“œì˜ ì¤‘ìš”ë„ í•© | . | . *ë…¸ë“œ ì¤‘ìš”ë„(Node Importance) . | íŠ¹ì • ë…¸ë“œ ì „í›„ë¡œ ë¶ˆìˆœë„ê°€ ì–¼ë§ˆë‚˜ ë‚®ì•„ì¡ŒëŠ”ì§€ë¡œ í•´ë‹¹ ë…¸ë“œì˜ ì¤‘ìš”ë„ë¥¼ íŒë‹¨. | ë‚˜ëˆ ì§€ëŠ” ë°ì´í„° ì…‹ë“¤ì— ëŒ€í•´ì„œ ì ì  ë” ì•Œì•„ê°„ë‹¤, ë˜ëŠ” â€œë” ë§ì€ ì •ë³´ë¥¼ ì–»ëŠ”ë‹¤â€ë¼ê³  í•´ì„œ ì´ ìˆ˜ì¹˜ë¥¼ ì •ë³´ ì¦ê°€ëŸ‰ (information gain)ì´ë¼ê³ ë„ ë¶€ë¦„. (ë¶ˆìˆœë„ê°€ ë‚®ì•„ì§ˆìˆ˜ë¡ ì ì  ë°ì´í„°ê°€ ì˜ ë‚˜ëˆ ì§€ê³  ìˆëŠ” ê±°ë‹ˆê¹Œ) | . *ê³„ì‚°í•˜ëŠ” ë²•: . | $ ni = \\dfrac{n}{m}GI - \\dfrac{n_{left}}{m}GI_{left} - \\dfrac{n_{right}}{m}GI_{right} $ . | n: ì¤‘ìš”ë„ë¥¼ ê³„ì‚°í•˜ë ¤ëŠ” ë…¸ë“œê¹Œì§€ ì˜¤ëŠ” í•™ìŠµ ë°ì´í„°ì˜ ìˆ˜ | GI: ì´ ë…¸ë“œê¹Œì§€ ì˜¤ëŠ” ë°ì´í„° ì…‹ì˜ ë¶ˆìˆœë„ | m: ì „ì²´ í•™ìŠµ ë°ì´í„°ì˜ ìˆ˜ | . | í•œ ë…¸ë“œì—ì„œ ë°ì´í„°ë¥¼ ë‘ ê°œë¡œ ë‚˜ëˆ´ì„ ë•Œ, ë°ì´í„° ìˆ˜ì— ë¹„ë¡€í•´ì„œ ë¶ˆìˆœë„ê°€ ì–¼ë§ˆë‚˜ ì¤„ì–´ë“¤ì—ˆëŠ”ì§€ë¥¼ ê³„ì‚°í•˜ëŠ” ê²ƒ! | . Â  . *pythonìœ¼ë¡œ ê³„ì‚° . # ì†ì„±ë“¤ì˜ ì¤‘ìš”ë„ë¥¼ í™•ì¸ model.feature_importances_ # numpy ë°°ì—´ë¡œ ì •ë¦¬ë˜ì–´ì„œ ë‚˜ì˜´ . array([0.04642857, 0. , 0. , 0.95357143]) . # dfë¥¼ ë§Œë“¤ì–´ importanceê°€ í° ìˆœì„œëŒ€ë¡œ ì •ë ¬ df = pd.DataFrame(list(zip(iris_data.feature_names, model.feature_importances_)), columns=['feature', 'importance']).sort_values('importance', ascending=False) df = df.reset_index(drop=True) df . | Â  | feature | importance | . | 0 | petal length (cm) | 0.953571 | . | 1 | petal width (cm) | 0.046429 | . | 2 | sepal length (cm) | 0.000000 | . | 3 | sepal width (cm) | 0.000000 | . +) zip(*iterable)ì€ ë™ì¼í•œ ê°œìˆ˜ë¡œ ì´ë£¨ì–´ì§„ ìë£Œí˜•ì„ ë¬¶ì–´ ì£¼ëŠ” ì—­í• ì„ í•˜ëŠ” í•¨ìˆ˜ . list(zip(\"abc\", \"def\")) . [('a', 'd'), ('b', 'e'), ('c', 'f')] . Â  . import seaborn as sns plt.figure() plt.title('Feature Importances') sns.barplot(data=df, y='feature', x='importance', palette='RdPu') # palette ì˜µì…˜: https://seaborn.pydata.org/generated/seaborn.color_palette.html#seaborn.color_palette plt.show(); . ",
    "url": "https://chaelist.github.io/docs/ml_basics/classification2/#decision-tree-%EA%B2%B0%EC%A0%95-%ED%8A%B8%EB%A6%AC",
    "relUrl": "/docs/ml_basics/classification2/#decision-tree-ê²°ì •-íŠ¸ë¦¬"
  },"15": {
    "doc": "Classification 2",
    "title": "Random Forest",
    "content": ": ê²°ì • íŠ¸ë¦¬ ì•™ìƒë¸” ì•Œê³ ë¦¬ì¦˜ ì¤‘ í•˜ë‚˜. ìˆ˜ë§ì€ íŠ¸ë¦¬ë“¤ì„ ì„ì˜ë¡œ ë§Œë“¤ê³ , ì´ ëª¨ë¸ë“¤ì˜ ê²°ê³¼ë¥¼ ë‹¤ìˆ˜ê²° íˆ¬í‘œë¡œ ì¢…í•©í•´ì„œ ì˜ˆì¸¡í•˜ëŠ” ëª¨ë¸. (Bagging ë°©ì‹ì˜ ì•™ìƒë¸” ëŸ¬ë‹) . | *Ensemble Learning: í•˜ë‚˜ì˜ ëª¨ë¸ì„ ì“°ëŠ” ëŒ€ì‹ , ìˆ˜ë§ì€ ëª¨ë¸ë“¤ì„ ë§Œë“¤ê³  ì´ ëª¨ë¸ë“¤ì˜ ì˜ˆì¸¡ì„ í•©ì³ì„œ ì¢…í•©ì ì¸ ì˜ˆì¸¡ì„ í•˜ëŠ” ê¸°ë²• | ê²°ì • íŠ¸ë¦¬ ìì²´ëŠ” ì•„ì£¼ ì„±ëŠ¥ì´ ì¢‹ì€ ëª¨ë¸ì€ ì•„ë‹ˆì§€ë§Œ, ì•™ìƒë¸” ê¸°ë²•ìœ¼ë¡œ ì‚¬ìš©í•˜ë©´ ì„±ëŠ¥ì´ ì¢‹ì€ ëª¨ë¸ì„ ë§Œë“¤ ìˆ˜ ìˆìŒ | . ê¸°ë³¸ ê°œë… . Â  . *Random Forest ëª¨ë¸ì˜ ì‘ë™ ë°©ì‹: . | Bootstrappingìœ¼ë¡œ ì„ì˜ë¡œ ë°ì´í„°ì…‹ì„ ë§Œë“ ë‹¤ . | Bootstrapping: ë°ì´í„°ì…‹ì—ì„œ ì„ì˜ë¡œ ë°ì´í„°ë¥¼ ê³¨ë¼ì™€ì„œ ìƒˆë¡œìš´ ë°ì´í„°ì…‹ì„ ë§Œë“¤ì–´ì£¼ëŠ” ë°©ë²•. (â€»ì¤‘ë³µì„ í—ˆìš©í•´ì„œ ë°ì´í„° ì„ì˜ ì„ íƒ) | +) Bagging: Bootstrap Aggregatingì˜ ì•½ì–´. (bootsrap ë°ì´í„° ì…‹ì„ ë§Œë“¤ì–´ë‚´ê³ , ì´ë¥¼ í™œìš©í•œ ëª¨ë¸ë“¤ì˜ ê²°ì •ì„ ì¢…í•©(aggregate)í•´ì„œ ì˜ˆì¸¡í•˜ëŠ” ì•™ìƒë¸” ê¸°ë²•ì„ ì˜ë¯¸) | Bootstrap ë°ì´í„°ì…‹ì„ ë§Œë“œëŠ” ì´ìœ : ì•™ìƒë¸” ê¸°ë²•ì„ ì‚¬ìš©í•  ë•Œ, ëª¨ë¸ë“¤ì„ ë‹¤ ë˜‘ê°™ì€ ë°ì´í„°ì…‹ìœ¼ë¡œ í•™ìŠµì‹œí‚¤ë©´ ê²°ê³¼ê°€ ë‹¤ ë¹„ìŠ·í•˜ê²Œ ë‚˜ì™€ë²„ë¦´ ìˆ˜ë„ ìˆê¸°ì—, ëª¨ë¸ì„ ë§Œë“¤ ë•Œë§ˆë‹¤ ê°ê° ì„ì˜ë¡œ ë§Œë“  bootstrap ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•´ì„œ í•™ìŠµì‹œí‚¤ëŠ” ê²ƒ. | . | ì„ì˜ë¡œ ìˆ˜ë§ì€ ê²°ì • íŠ¸ë¦¬ë¥¼ ë§Œë“ ë‹¤ (ì§ˆë¬¸ ë…¸ë“œë“¤ì„ ì–´ëŠ ì •ë„ëŠ” ì„ì˜ë¡œ ë§Œë“¦) . | ê° ì†ì„±ì„ ì‚¬ìš©í•œ ì§ˆë¬¸ë“¤ì˜ ì§€ë‹ˆ ë¶ˆìˆœë„ë¥¼ ëª¨ë‘ êµ¬í•˜ê³  ê°€ì¥ ë‚®ì€ ê²ƒìœ¼ë¡œ ë…¸ë“œë¥¼ ë§Œë“œëŠ” ëŒ€ì‹ , ì—¬ëŸ¬ ì†ì„± ì¤‘ 2ê°œ ì •ë„ë¥¼ ì„ì˜ë¡œ ì„ íƒ (ì†ì„±ì´ ë§ìœ¼ë©´ ë” ë§ì´ ê³ ë¥¼ ìˆ˜ë„ ìˆìŒ) | 2ê°œ ì¤‘ ë¶ˆìˆœë„ê°€ ë” ë‚®ì€ ê²ƒìœ¼ë¡œ root ë…¸ë“œì˜ ì§ˆë¬¸ ì„ íƒ | ê·¸ ë‹¤ìŒì—ë„ ë˜‘ê°™ì´ ì†ì„± 2ê°œ ì •ë„ë¥¼ ì„ì˜ë¡œ ì„ íƒí•´ì„œ ì§€ë‹ˆ ë¶ˆìˆœë„ ë‚®ì„ ê±¸ ì‚¬ìš© â€¦ (ë°˜ë³µ) | . | ì´ë ‡ê²Œ 1, 2ë‹¨ê³„ë¥¼ ë°˜ë³µí•˜ë‹¤ë³´ë©´, ì„œë¡œ ì¡°ê¸ˆì”© ë‹¤ë¥¸ ê²°ì • íŠ¸ë¦¬ë“¤ì„ ë§ì´ ë§ë“¤ ìˆ˜ ìˆë‹¤. â†’ ì´ë ‡ê²Œ ë§Œë“  íŠ¸ë¦¬ë“¤ì— ë°ì´í„°ë¥¼ ë„£ì€ í›„, ê° íŠ¸ë¦¬ì˜ ì˜ˆì¸¡ ê°’ì„ ë‹¤ìˆ˜ê²° íˆ¬í‘œë¡œ ì¢…í•©í•´ì„œ ìµœì¢… ê²°ì •! (ex. 40ê°œì˜ íŠ¸ë¦¬ëŠ” â€˜ë…ê°â€™ì´ë¼ê³  ì˜ˆì¸¡, 60ê°œì˜ íŠ¸ë¦¬ëŠ” â€˜ì¼ë°˜ ê°ê¸°â€™ë¼ê³  ì˜ˆì¸¡ â†’ ì¼ë°˜ ê°ê¸°ë¼ê³  ìµœì¢… ì˜ˆì¸¡) | . (ì¶œì²˜: dinhanhthi.com) . scikit-learnìœ¼ë¡œ ë°ì´í„° í•™ìŠµ . from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.ensemble import RandomForestClassifier import matplotlib.pyplot as plt import numpy as np import pandas as pd . 1. ë°ì´í„° ì¤€ë¹„ . iris_data = load_iris() X = pd.DataFrame(iris_data.data, columns=iris_data.feature_names) y = pd.DataFrame(iris_data.target, columns=['class']) . X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=5) y_train = y_train.values.ravel() #ravel(): ë‹¤ì°¨ì› arrayë¥¼ 1ì°¨ì› arrayë¡œ í‰í‰í•˜ê²Œ í´ì£¼ëŠ” í•¨ìˆ˜ . 2. í•™ìŠµ . model = RandomForestClassifier(n_estimators=100, max_depth=4) . | max_depth: ê²°ì •íŠ¸ë¦¬ì™€ ë§ˆì°¬ê°€ì§€ë¡œ, íŠ¸ë¦¬ì˜ ìµœëŒ€ ê¹Šì´ë¥¼ ì •í•˜ëŠ” ë³€ìˆ˜. ì´ ëœë¤í¬ë ˆìŠ¤íŠ¸ ëª¨ë¸ì´ ë§Œë“œëŠ” ëª¨ë“  íŠ¸ë¦¬ë“¤ì˜ ìµœëŒ€ ê¹Šì´ë¥¼ ì •í•´ì¤Œ. | . model.fit(X_train, y_train) . RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None, criterion='gini', max_depth=4, max_features='auto', max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None, oob_score=False, random_state=None, verbose=0, warm_start=False) . | bootstrap: Falseë¼ê³  í•˜ë©´ ëª¨ë“  treeì— ë‹¤ whole datasetì„ ì‚¬ìš©. (default=True) | max_samples: bootstrap=Trueì¸ ê²½ìš°, ê° tree í•™ìŠµì„ ìœ„í•´ Xì—ì„œ ëª‡ ê°œì”©ì˜ sampleì„ ì¶”ì¶œí•  ê²ƒì¸ì§€ . | None(default)ì¸ ê²½ìš°, ìë™ìœ¼ë¡œ X.shape[0]ê°œì˜ sampleì„ ì¶”ì¶œ (ex. ì´ ê²½ìš°, X_train.shape[0]=120ì´ë¯€ë¡œ ê°ê° 120ê°œì˜ elementê°€ ë‹´ê¸´ sampleì„ êµ¬ì„±) | . | n_estimators: ëª‡ ê°œì˜ ê²°ì •íŠ¸ë¦¬ë¥¼ ë§Œë“¤ì–´ì„œ ì˜ˆì¸¡í•  ê²ƒì¸ì§€ ì •í•´ì£¼ëŠ” ë³€ìˆ˜. (default=100) | . 3. test dataë¡œ ì„±ëŠ¥ ì²´í¬ . model.predict(X_test) # ì–´ë–»ê²Œ ë¶„ë¥˜í–ˆë‚˜ í™•ì¸ . array([1, 1, 2, 0, 2, 1, 0, 2, 0, 1, 1, 1, 2, 2, 0, 0, 2, 2, 0, 0, 1, 2, 0, 1, 1, 2, 1, 1, 1, 2]) . y_test['class'].to_numpy() ## ì‹¤ì œ y_testì™€ ë¹„êµ . array([1, 2, 2, 0, 2, 1, 0, 1, 0, 1, 1, 2, 2, 2, 0, 0, 2, 2, 0, 0, 1, 2, 0, 1, 1, 2, 1, 1, 1, 2]) . # ëª‡ í¼ì„¼íŠ¸ê°€ ì˜¬ë°”ë¥´ê²Œ ë¶„ë¥˜ë˜ì—ˆëŠ”ì§€ í™•ì¸ model.score(X_test, y_test) . 0.9 . ì•½ 90% ì •ë„ê°€ ì˜¬ë°”ë¥´ê²Œ ë¶„ë¥˜ë˜ì—ˆë‹¤ëŠ” ì˜ë¯¸ . ì†ì„± ì¤‘ìš”ë„ í™•ì¸ . | ê²°ì •íŠ¸ë¦¬ë¥¼ ì‚¬ìš©í•œ ëª¨ë¸ì´ê¸°ì—, ê²°ì • íŠ¸ë¦¬ì™€ ë§ˆì°¬ê°€ì§€ë¡œ í‰ê·  ì§€ë‹ˆ ê°ì†Œë¥¼ ì´ìš©í•´ ì†ì„± ì¤‘ìš”ë„ ê³„ì‚°ì´ ê°€ëŠ¥ | ëœë¤ í¬ë ˆìŠ¤íŠ¸ì—ì„œì˜ ì†ì„± ì¤‘ìš”ë„ëŠ” ê·¸ ì•ˆì˜ ìˆ˜ë§ì€ ê²°ì • íŠ¸ë¦¬ë“¤ì˜ ì†ì„± ì¤‘ìš”ë„ì˜ í‰ê· ê°’ | . # ì†ì„± ì¤‘ìš”ë„ í™•ì¸ model.feature_importances_ . array([0.09846022, 0.01962833, 0.35241878, 0.52949267]) . # dfë¥¼ ë§Œë“¤ì–´ importanceê°€ í° ìˆœì„œëŒ€ë¡œ ì •ë ¬ df = pd.DataFrame(list(zip(iris_data.feature_names, model.feature_importances_)), columns=['feature', 'importance']).sort_values('importance', ascending=False) df = df.reset_index(drop=True) df . | Â  | feature | importance | . | 0 | petal width (cm) | 0.529493 | . | 1 | petal length (cm) | 0.352419 | . | 2 | sepal length (cm) | 0.098460 | . | 3 | sepal width (cm) | 0.019628 | . Â  . import seaborn as sns plt.figure() plt.title('Feature Importances') sns.barplot(data=df, y='feature', x='importance', palette='RdPu') plt.show(); . | ë³´í†µ, random forest ëª¨ë¸ì´ decision tree ëª¨ë¸ë³´ë‹¤ ê° featureë¥¼ ê³¨ê³ ë£¨ ë°˜ì˜í•´ì„œ ì˜ˆì¸¡í•œë‹¤. (does not depend highly on any specific set of features) | . ",
    "url": "https://chaelist.github.io/docs/ml_basics/classification2/#random-forest",
    "relUrl": "/docs/ml_basics/classification2/#random-forest"
  },"16": {
    "doc": "Classification 2",
    "title": "AdaBoost",
    "content": ": Adaptive Boosting. - Boosting ê¸°ë²•ì„ ì‚¬ìš©í•œ ì•™ìƒë¸” ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ ì¤‘ í•˜ë‚˜. ê¸°ë³¸ ê°œë… . Â  . *Boosting ê¸°ë²•: . | ì¼ë¶€ëŸ¬ ì„±ëŠ¥ì´ ì•ˆì¢‹ì€ ëª¨ë¸(weak learner)ì„ ì‚¬ìš© | ë” ë¨¼ì € ë§Œë“  ëª¨ë¸ì˜ ì„±ëŠ¥ì— ë”°ë¼ ë’¤ì— ìˆëŠ” ëª¨ë¸ì´ ì‚¬ìš©í•  ë°ì´í„°ì…‹ì„ ë°”ê¾¼ë‹¤ | ëª¨ë¸ë³„ ì„±ëŠ¥ì˜ ì°¨ì´ë¥¼ ë°˜ì˜í•´ì„œ ëª¨ë¸ì˜ ì˜ˆì¸¡ì„ ì¢…í•©í•œë‹¤ (ì„±ëŠ¥ì´ ì¢‹ì€ ëª¨ë¸ì˜ ì˜ˆì¸¡ì„ ë” ë°˜ì˜) | . Â  . *AdaBoost ì‘ë™ ë°©ì‹: . | ìŠ¤í…€í”„(stump)ë¥¼ ì‚¬ìš© . | ìŠ¤í…€í”„: root ë…¸ë“œ í•˜ë‚˜ì™€ ë¶„ë¥˜ ë…¸ë“œ ë‘ ê°œë¥¼ ê°–ëŠ” ì–•ì€ ê²°ì • íŠ¸ë¦¬. ë³´í†µ 50%ë³´ë‹¤ ì¡°ê¸ˆ ë‚˜ì€ ì •ë„ì˜ ì„±ëŠ¥. | . | íŠ¹ì • ê²°ì • ìŠ¤í…€í”„ê°€ ë¶„ë¥˜í•œ ê²°ê³¼ë¥¼ ë³´ê³ , ë§ê²Œ ë¶„ë¥˜í•œ ì• ë“¤ì€ ì¤‘ìš”ë„ë¥¼ ë‚®ì¶”ê³  í‹€ë¦¬ê²Œ ë¶„ë¥˜í•œ ì• ë“¤ì€ ì¤‘ìš”ë„ë¥¼ ë†’ì—¬ì¤€ë‹¤ | ë‹¤ìŒ ìŠ¤í…€í”„ëŠ” ì•ì˜ ìŠ¤í…€í”„ì˜ ë¶„ë¥˜ ê²°ê³¼ì— ë”°ë¼ ì¤‘ìš”ë„ê°€ ì¡°ì •ëœ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•´ í•™ìŠµí•œë‹¤ . | ë’¤ì˜ ìŠ¤í…€í”„ê°€ ì•ì˜ ìŠ¤í…€í”„ì˜ ì‹¤ìˆ˜ë¥¼ ë” ì˜ ë§ì¶”ê²Œ ë˜ëŠ” ë°©í–¥ìœ¼ë¡œ ë§Œë“¤ì–´ì§€ëŠ” ê²ƒ. | . | ìˆ˜ë§ì€ ìŠ¤í…€í”„ë¥¼ ë§Œë“¤ì–´ì¤€ í›„ì—, ê° ìŠ¤í…€í”„ì˜ ì„±ëŠ¥ì„ ê³ ë ¤í•´ ì¢…í•©ì ìœ¼ë¡œ ê²°ê³¼ë¥¼ ì˜ˆì¸¡í•œë‹¤ . | ë‹¤ìˆ˜ê²°ë¡œ ê²°ê³¼ë¥¼ ê²°ì •í•˜ë˜, ì„±ëŠ¥ì´ ì¢‹ì€ ê²°ì • ìŠ¤í…€í”„ì¼ìˆ˜ë¡ ì˜ˆì¸¡ ì˜ê²¬ì˜ ë¹„ì¤‘ì„ ë†’ê²Œ ë°˜ì˜ | . | . Â  . *stumpì˜ ì„±ëŠ¥ ê³„ì‚°í•˜ê¸°: . | $ \\dfrac{1}{2}\\log(\\dfrac{1- total \\, error}{total \\, error}) $ . | total error: ì˜ëª» ë¶„ë¥˜í•œ ë°ì´í„°ë“¤ì˜ ì¤‘ìš”ë„ì˜ í•© | ëŠ˜ ëª¨ë“  ë°ì´í„°ì˜ ì¤‘ìš”ë„ì˜ í•©ì€ 1ë¡œ ìœ ì§€ë˜ë¯€ë¡œ, total errorì˜ ìµœëŒ“ê°’ì€ 1 | total errorê°€ 1ì¸ ê²½ìš° (=ëª¨ë“  ë°ì´í„°ë¥¼ ë‹¤ í‹€ë¦¬ê²Œ ì˜ˆì¸¡í•œ ê²½ìš°) ì„±ëŠ¥ì´ ë¬´í•œí•˜ê²Œ ì‘ì•„ì§„ë‹¤ | total errorê°€ 0ì¸ ê²½ìš° (=ëª¨ë“  ë°ì´í„°ë¥¼ ë‹¤ ë§ê²Œ ì˜ˆì¸¡í•œ ê²½ìš°) ì„±ëŠ¥ì´ ë¬´í•œí•˜ê²Œ ì»¤ì§„ë‹¤ | total errorê°€ 0.5ì¸ ê²½ìš° (= ë”± ë°˜ë§Œ ë§ê²Œ ì˜ˆì¸¡í•œ ê²½ìš°) ì„±ëŠ¥ì€ 0 | . | . (ì¶œì²˜: codeit) . Â  . *stump ì¶”ê°€í•˜ê¸°: . | ì²« ìŠ¤í…€í”„ëŠ” ê²°ì •íŠ¸ë¦¬ë¥¼ ë§Œë“¤ ë•Œì²˜ëŸ¼ ì§€ë‹ˆë¶ˆìˆœë„ë¥¼ ê³„ì‚°í•´ì„œ root ë…¸ë“œë¥¼ ê³ ë¥¸ë‹¤ | ê·¸ í›„ ìŠ¤í…€í”„ ì¶”ê°€í•˜ê¸°: . | ê° ë°ì´í„°ì˜ ì¤‘ìš”ë„ë¥¼ ê°€ì§€ê³  ë²”ìœ„ë¥¼ ë§Œë“¤ì–´ì¤€ë‹¤. (ex. ì²«ë²ˆì§¸ ë°ì´í„°ëŠ” ì¤‘ìš”ë„ê°€ 0.1 â†’ ë²”ìœ„ê°€ 0 ~ 0.1, ë‘ë²ˆì§¸ ë°ì´í„°ëŠ” ì¤‘ìš”ë„ê°€ 0.2 â†’ ë²”ìœ„ê°€ 0.1 ~ 0.3, ì„¸ë²ˆì§¸ ë°ì´í„°ëŠ” ì¤‘ìš”ë„ê°€ 0.15 â†’ ë²”ìœ„ê°€ 0.3 ~ 0.45, â€¦) | 0ê³¼ 1 ì‚¬ì´ì˜ ì„ì˜ì˜ ìˆ«ìë¥¼ ê³¨ë¼, ê·¸ ìˆ«ìê°€ ì†í•˜ëŠ” ë²”ìœ„ì˜ ë°ì´í„°ë¥¼ ë°ì´í„°ì…‹ì— ì¶”ê°€í•œë‹¤ | â€» ì¤‘ìš”ë„ê°€ ë†’ì€ ë°ì´í„°ëŠ” ë²”ìœ„ë„ í¬ê¸° ë•Œë¬¸ì— ì„ íƒë  í™•ë¥ ì´ ë†’ì•„ì§€ëŠ” ê²ƒ! | . | . â†’ ìƒˆë¡œìš´ ë°ì´í„° ì…‹ì€ ì „ ìŠ¤í…€í”„ë“¤ì´ í‹€ë¦°, ì¤‘ìš”ë„ê°€ ë†’ì€ ë°ì´í„°ë“¤ì´ í™•ë¥ ì ìœ¼ë¡œ ë” ë§ì´ ë“¤ì–´ìˆê¸° ë•Œë¬¸ì— ì–˜ë„¤ë¥¼ ë” ì˜ ë§ì¶œ ìˆ˜ ìˆê²Œ ë¨. Â  . *ë°ì´í„° ì¤‘ìš”ë„ ë°”ê¾¸ê¸°: . | ì²« ìŠ¤í…€í”„ë¥¼ ë§Œë“¤ ë•ŒëŠ” ëª¨ë“  ë°ì´í„°ì˜ ì¤‘ìš”ë„ê°€ ê°™ë‹¤ (ex. ë°ì´í„°ê°€ 10ê°œë©´ ê°ê°ì˜ ì¤‘ìš”ë„ëŠ” 1/10) | í‹€ë¦¬ê²Œ ë¶„ë¥˜í•œ ë°ì´í„°: $ weight_{new} = weight_{old} * e^{p_{tree}} $ . | $ e $: ìì—°ìƒìˆ˜. 2.71â€¦ | $ p_{tree} $: ìŠ¤í…€í”„ì˜ ì„±ëŠ¥ | . | ë§ê²Œ ë¶„ë¥˜í•œ ë°ì´í„°: $ weight_{new} = weight_{old} * e^{-p_{tree}} $ | . | ì„±ëŠ¥ì´ 0ì´ë©´ $ weight_{new} $ = 1 | í‹€ë¦° ë°ì´í„°ëŠ” $ weight_{old} $ì— 1ë³´ë‹¤ í° ê°’ì„ ê³±í•˜ê²Œ ë˜ë¯€ë¡œ ì›ë˜ë³´ë‹¤ ì¤‘ìš”ë„ê°€ ì»¤ì§ | ë§ì€ ë°ì´í„°ëŠ” $ weight_{old} $ì— 1ë³´ë‹¤ ì‘ì€ ê°’ì„ ê³±í•˜ê²Œ ë˜ë¯€ë¡œ ì›ë˜ë³´ë‹¤ ì¤‘ìš”ë„ê°€ ì‘ì•„ì§ | . scikit-learnìœ¼ë¡œ ë°ì´í„° í•™ìŠµ . from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.ensemble import AdaBoostClassifier import matplotlib.pyplot as plt import numpy as np import pandas as pd . 1. ë°ì´í„° ì¤€ë¹„ . iris_data = load_iris() X = pd.DataFrame(iris_data.data, columns=iris_data.feature_names) y = pd.DataFrame(iris_data.target, columns=['class']) . X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=5) y_train = y_train.values.ravel() #ravel(): ë‹¤ì°¨ì› arrayë¥¼ 1ì°¨ì› arrayë¡œ í‰í‰í•˜ê²Œ í´ì£¼ëŠ” í•¨ìˆ˜ . 2. í•™ìŠµ . model = AdaBoostClassifier(n_estimators=100) . | n_estimators: ìµœëŒ€ ëª‡ ê°œì˜ ê²°ì • ìŠ¤í…€í”„(stump)ë¥¼ ë§Œë“¤ì–´ì„œ ì˜ˆì¸¡í•  ê²ƒì¸ì§€ ì •í•´ì£¼ëŠ” ë³€ìˆ˜. ê¸°ë³¸ê°’ì€ 50. | . model.fit(X_train, y_train) . AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0, n_estimators=100, random_state=None) . | base_estimator: ì–´ë–¤ estimatorë¥¼ ë°”íƒ•ìœ¼ë¡œ boosted ensembleì„ êµ¬ì¶•í•  ê²ƒì¸ì§€. | default: Noneì´ë©°, Noneì¸ ê²½ìš° max_depth=1ì˜ DecisionTreeClassifierë¥¼ ì‚¬ìš© (=ê²°ì • ìŠ¤í…€í”„ë¥¼ ì‚¬ìš©) | . | learning_rate: ê° classifierì˜ ê¸°ì—¬ë„ë¥¼ ë‚®ì¶°ì¤„ ìˆ˜ ìˆìŒ. (float numberë¥¼ ì…ë ¥) | . 3. test dataë¡œ ì„±ëŠ¥ ì²´í¬ . model.predict(X_test) # ì–´ë–»ê²Œ ë¶„ë¥˜í–ˆë‚˜ í™•ì¸ . array([1, 1, 2, 0, 2, 2, 0, 2, 0, 1, 1, 1, 2, 2, 0, 0, 2, 2, 0, 0, 1, 2, 0, 1, 1, 2, 1, 1, 1, 2]) . y_test['class'].to_numpy() ## ì‹¤ì œ y_testì™€ ë¹„êµ . array([1, 2, 2, 0, 2, 1, 0, 1, 0, 1, 1, 2, 2, 2, 0, 0, 2, 2, 0, 0, 1, 2, 0, 1, 1, 2, 1, 1, 1, 2]) . # ëª‡ í¼ì„¼íŠ¸ê°€ ì˜¬ë°”ë¥´ê²Œ ë¶„ë¥˜ë˜ì—ˆëŠ”ì§€ í™•ì¸ model.score(X_test, y_test) . 0.8666666666666667 . ì•½ 87% ì •ë„ê°€ ì˜¬ë°”ë¥´ê²Œ ë¶„ë¥˜ë˜ì—ˆë‹¤ëŠ” ì˜ë¯¸ . ì†ì„± ì¤‘ìš”ë„ í™•ì¸ . | ê²°ì •íŠ¸ë¦¬ë¥¼ ì‚¬ìš©í•œ ëª¨ë¸ì´ê¸°ì—, ê²°ì • íŠ¸ë¦¬ì™€ ë§ˆì°¬ê°€ì§€ë¡œ í‰ê·  ì§€ë‹ˆ ê°ì†Œë¥¼ ì´ìš©í•´ ì†ì„± ì¤‘ìš”ë„ ê³„ì‚°ì´ ê°€ëŠ¥ | ì—ë‹¤ë¶€ìŠ¤íŠ¸ì—ì„œì˜ ì†ì„± ì¤‘ìš”ë„ëŠ” ê° ê²°ì • ìŠ¤í…€í”„ë“¤ì˜ ì†ì„± ì¤‘ìš”ë„ì˜ weighted average (ê° ìŠ¤í…€í”„ì˜ ì„±ëŠ¥ ì°¨ì´ë¥¼ ë°˜ì˜í•´ í‰ê· ëƒ„) | . # ì†ì„± ì¤‘ìš”ë„ í™•ì¸ model.feature_importances_ . array([0.17, 0.03, 0.39, 0.41]) . # dfë¥¼ ë§Œë“¤ì–´ importanceê°€ í° ìˆœì„œëŒ€ë¡œ ì •ë ¬ df = pd.DataFrame(list(zip(iris_data.feature_names, model.feature_importances_)), columns=['feature', 'importance']).sort_values('importance', ascending=False) df = df.reset_index(drop=True) df . | Â  | feature | importance | . | 0 | petal width (cm) | 0.41 | . | 1 | petal length (cm) | 0.39 | . | 2 | sepal length (cm) | 0.17 | . | 3 | sepal width (cm) | 0.03 | . Â  . import seaborn as sns plt.figure() plt.title('Feature Importances') sns.barplot(data=df, y='feature', x='importance', palette='RdPu') plt.show(); . ",
    "url": "https://chaelist.github.io/docs/ml_basics/classification2/#adaboost",
    "relUrl": "/docs/ml_basics/classification2/#adaboost"
  },"17": {
    "doc": "Clustering",
    "title": "Clustering",
    "content": ". | Clustering | K-Means Clustering . | ê¸°ë³¸ ê°œë… | scikit-learnìœ¼ë¡œ êµ¬í˜„ | ìµœì ì˜ Kê°’ ì°¾ê¸° | ìµœì ì˜ Kê°’ ì°¾ê¸°: yellowbrick | . | Hierarchical Clustering . | ê¸°ë³¸ ê°œë… | Dendrogram ê·¸ë ¤ë³´ê¸° | scikit-learnìœ¼ë¡œ êµ¬í˜„ | ìµœì ì˜ Kê°’ ì°¾ê¸°: yellowbrick | . | . ",
    "url": "https://chaelist.github.io/docs/ml_basics/clustering/",
    "relUrl": "/docs/ml_basics/clustering/"
  },"18": {
    "doc": "Clustering",
    "title": "Clustering",
    "content": ": ê±°ë¦¬ë¥¼ ê³„ì‚°í•´ ìœ ì‚¬í•œ data pointë¼ë¦¬ ê°™ì€ Clusterë¡œ ë¬¶ì–´ì£¼ëŠ” ê²ƒ. | ë¹„ì§€ë„í•™ìŠµ. ì¢…ì†ë³€ìˆ˜ê°€ ì—†ê³ , ë…ë¦½ë³€ìˆ˜ë§Œ ì‚¬ìš©í•´ì„œ ë‹µì„ ì°¾ëŠ” ë°©ì‹. | ëŒ€ì²´ë¡œ íƒìƒ‰ì  ë°ì´í„° ë¶„ì„ì˜ ì¼ë¶€ë¡œ ìˆ˜í–‰ëœë‹¤. (ex. ì†Œë¹„ì Clusterë¥¼ ë‚˜ëˆˆ í›„ â†’ ì–´ë–¤ Clusterê°€ Aì œí’ˆì„ ë§ì´ ì‚¬ëŠ”ì§€ Regression ë¬¸ì œ í’€ê¸°) | . ",
    "url": "https://chaelist.github.io/docs/ml_basics/clustering/",
    "relUrl": "/docs/ml_basics/clustering/"
  },"19": {
    "doc": "Clustering",
    "title": "K-Means Clustering",
    "content": "ê¸°ë³¸ ê°œë… . | ë°ì´í„° í¬ì¸íŠ¸ (ë²¡í„°) ê°„ì˜ ê±°ë¦¬ë¥¼ ê³„ì‚°í•´, ê±°ë¦¬ê°€ ì§§ì€ ì• ë“¤ë¼ë¦¬ ë¬¶ì–´ Kê°œì˜ Clusterë¥¼ í˜•ì„±í•´ì£¼ëŠ” ë°©ì‹. | K ê°’ì„ ì˜ ì •í•´ì£¼ëŠ” ê²ƒì´ ì¤‘ìš”í•˜ë‹¤! | ë³´í†µ ìœ í´ë¦¬ë””ì•ˆ ë°©ì‹ìœ¼ë¡œ ê±°ë¦¬ë¥¼ ê³„ì‚°. | . (ì¶œì²˜: tcpschool.com) . Â  . *K-Means Clustering ê³¼ì •: . | Kì˜ ê°’ì„ ì •í•œë‹¤ (ëª‡ ê°œì˜ Clusterë¡œ ë‚˜ëˆŒ ê²ƒì¸ì§€) | ë°ì´í„°ì…‹ì—ì„œ ì„ì˜ë¡œ Kê°œì˜ ì¤‘ì‹¬ì ì„ ì„ íƒ. | ê° ì ì„ Kê°œì˜ ì¤‘ì‹¬ì  ì¤‘ ê°€ì¥ ê°€ê¹Œìš´ ì ì´ ì†í•œ Clusterë¡œ assign. | ê° ê·¸ë£¹ì— ì†í•˜ëŠ” ì ë“¤ì˜ í‰ê· ê°’ì„ ìƒˆë¡œìš´ ì¤‘ì‹¬ì ìœ¼ë¡œ í•¨. | ìƒ‰ì´ ë³€í•˜ëŠ” ì ì´ ì—†ì„ ë•Œê¹Œì§€ 3, 4ë²ˆì„ ê³„ì† ë°˜ë³µ. | . scikit-learnìœ¼ë¡œ êµ¬í˜„ . from sklearn.datasets import load_iris from sklearn.cluster import KMeans import pandas as pd import numpy as np import seaborn as sns import matplotlib.pyplot as plt . 1. ë°ì´í„° ì¤€ë¹„ . # ì˜ˆì‹œì´ë¯€ë¡œ, make_blobsë¥¼ ì‚¬ìš©í•´ clusteringí•˜ê¸° ì‰¬ìš´ ë°ì´í„°ë¥¼ ì¤€ë¹„ from sklearn.datasets import make_blobs X, y = make_blobs(n_samples=100, centers=3, n_features=2) # ë³€ìˆ˜ 2ê°œ, ìƒ˜í”Œ 100ê°œ, ì¤‘ì‹¬ì  3ê°œë¡œ blob ë§Œë“¤ê¸° X = pd.DataFrame(X, columns=['a', 'b']) X.head() . | Â  | a | b | . | 0 | 8.66962 | -2.95918 | . | 1 | -10.3818 | 0.959058 | . | 2 | 9.45125 | -2.50409 | . | 3 | -4.03838 | -9.18607 | . | 4 | 10.9778 | -2.85563 | . Â  . sns.scatterplot(data=X, x=\"a\", y=\"b\"); . 2. Clustering . model = KMeans(n_clusters=3) . | n_clusters: ëª‡ ê°œì˜ clusterë¡œ ë‚˜ëˆŒ ê²ƒì¸ì§€ ì„¤ì • (K) | . model.fit(X) . KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300, n_clusters=3, n_init=10, n_jobs=None, precompute_distances='auto', random_state=None, tol=0.0001, verbose=0) . | init: initialization ë°©ë²•. k-means++, random, í˜¹ì€ ì§ì ‘ array í˜•íƒœë¡œ ì§€ì • . | defaultëŠ” k-means++. ë§¨ ì²˜ìŒ ì¤‘ì‹¬ì ì„ ë³´ë‹¤ ì „ëµì ìœ¼ë¡œ ë°°ì¹˜í•˜ëŠ” ë°©ì‹. | ìš°ì„  ë°ì´í„°í¬ì¸íŠ¸ 1ê°œë¥¼ ì²«ë²ˆì§¸ ì¤‘ì‹¬ì ìœ¼ë¡œ ì„ íƒí•˜ê³ , ì´ì™€ ìµœëŒ€í•œ ë¨¼ ê³³ì— ìˆëŠ” ë°ì´í„°í¬ì¸íŠ¸ë¥¼ ë‹¤ìŒ ì¤‘ì‹¬ì ìœ¼ë¡œ ì„ íƒ, â€¦ | ë§¨ ì²˜ìŒ ì¤‘ì‹¬ì ë“¤ì´ ì„œë¡œ ê·¼ì ‘í•˜ê²Œ ìœ„ì¹˜í•˜ëŠ” ê²ƒì„ ë°©ì§€í•´ì£¼ê¸° ë•Œë¬¸ì— ë‹¨ìˆœíˆ randomí•˜ê²Œ ê³ ë¥´ëŠ” ê²ƒë³´ë‹¤ ë” ìµœì ì˜, íš¨ìœ¨ì ì¸ clusteringì´ ê°€ëŠ¥í•˜ë‹¤ | . | random: ë§¨ ì²˜ìŒ ì¤‘ì‹¬ì ì„ ë§ ê·¸ëŒ€ë¡œ ë¬´ì‘ìœ„ë¡œ Kê°œ ê³ ë¥´ëŠ” ë°©ì‹ | . | max_iter: ì¤‘ì‹¬ê³¼ ë‹¤ë¥¸ ë°ì´í„°í¬ì¸íŠ¸ ê°„ì˜ ê±°ë¦¬ë¥¼ ê³„ì‚°í•´ì„œ ê³„ì†í•´ì„œ clusterë¥¼ updateí•´ì£¼ëŠ” ê²ƒì„ ìµœëŒ€ ëª‡ ë²ˆ ë°˜ë³µí•  ê²ƒì¸ì§€ ì§€ì •. max_iter ìˆ˜ë¥¼ ì‘ê²Œ ì§€ì •í•´ì£¼ë©´ ì†ë„ê°€ ë¹¨ë¼ì§€ì§€ë§Œ ì •í™•ë„ëŠ” ë–¨ì–´ì§. default=300 | n_init: ì„œë¡œ ë‹¤ë¥¸ ì´ˆê¸° ì¤‘ì‹¬ì ì„ ë°”íƒ•ìœ¼ë¡œ ëª‡ ë²ˆ ì•Œê³ ë¦¬ì¦˜ì„ ë°˜ë³µí•  ê²ƒì¸ì§€ ì§€ì •. default=10. ìµœì¢… ê²°ê³¼ëŠ” inertia ê³„ì‚°ê°’ì´ ê°€ì¥ ì˜ ë‚˜ì˜¤ëŠ” ê²°ê³¼ë¬¼ë¡œ ì¶œë ¥ë¨. | inertia: í´ëŸ¬ìŠ¤í„° ë‚´ ì˜¤ì°¨ì œê³±í•©. Sum of squared distances of samples to their closest cluster center. | . | . clusters = model.predict(X) # model.labels_ë¼ê³  í•´ë„ ë™ì¼í•œ ê²°ê³¼ clusters # ê° ì ì´ ì–´ëŠ clusterì— ë°°ì •ë˜ì—ˆëŠ”ì§€ í™•ì¸ . array([0, 1, 0, 2, 0, 2, 2, 0, 2, 0, 0, 2, 0, 1, 0, 0, 1, 1, 1, 1, 2, 0, 1, 1, 1, 2, 2, 1, 0, 1, 1, 1, 2, 2, 1, 0, 0, 1, 2, 1, 2, 2, 0, 1, 2, 2, 1, 1, 2, 1, 2, 0, 2, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 1, 0, 0, 1, 2, 2, 1, 0, 2, 0, 2, 1, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 2, 1, 1, 1], dtype=int32) . | clusters = model.fit_predict(X)ë¼ê³  í•˜ë©´ fitê³¼ predictë¥¼ ë™ì‹œì— í•  ìˆ˜ ìˆìŒ. (clusteringì€ ì‚¬ì‹¤ fitê³¼ predictê°€ í•˜ë‚˜ì˜ ê³¼ì •ì´ë¼ì„œ) | . 3. Clustering ê²°ê³¼ í™•ì¸ . result = X.copy() result[\"cluster\"] = clusters result.head() . | Â  | a | b | cluster | . | 0 | 8.66962 | -2.95918 | 0 | . | 1 | -10.3818 | 0.959058 | 1 | . | 2 | 9.45125 | -2.50409 | 0 | . | 3 | -4.03838 | -9.18607 | 2 | . | 4 | 10.9778 | -2.85563 | 0 | . Â  . sns.scatterplot(data=result, x=\"a\", y=\"b\", hue=\"cluster\", palette=\"Set2\"); . ìµœì ì˜ Kê°’ ì°¾ê¸° . | make_blobsë¡œ ë§Œë“  ì˜ˆì‹œ ë°ì´í„°ì˜ ê²½ìš°, ì‚¬ì „ì— 3ê°œì˜ ë¶„ë¥˜ì„ì„ ì•Œê³  ìˆì—ˆê¸°ì— K=3ìœ¼ë¡œ ì§€ì •í•´ì„œ ì§„í–‰í–ˆì§€ë§Œ, ë³´í†µì€ ëª‡ ê°œì˜ clusterë¡œ ë‚˜ëˆ ì•¼ í•  ì§€ ì‰½ê²Œ ì•Œ ìˆ˜ ì—†ë‹¤. | 2ì°¨ì›ì¸ ê²½ìš°, ì‹œê°í™”í•´ì„œ ëˆˆìœ¼ë¡œ íŒë‹¨í•˜ëŠ” ê²ƒë„ ê°€ëŠ¥í•˜ì§€ë§Œ, ë³´í†µì˜ ë°ì´í„°ëŠ” 3ì°¨ì› ì´ìƒì´ê¸°ì—,,, ì•„ë˜ ë°©ì‹ë“¤ì„ ì‚¬ìš©í•˜ë©´ ì¢‹ìŒ! | . Â  . 1. Elbow ë°©ì‹ . | Kê°’ì„ 1ë¶€í„° ì°¨ë¡€ë¡œ ë„£ì–´ë³´ë©´ì„œ, ê° ê²°ê³¼ì˜ inertia(í´ëŸ¬ìŠ¤í„° ë‚´ ì˜¤ì°¨ì œê³±í•©)ë¥¼ êµ¬í•œë‹¤ | Kê°’ì— ë”°ë¥¸ inertiaì˜ ë³€í™”ë¥¼ ë³´ê³ , ê·¸ë˜í”„ì˜ íŒ”ê¿ˆì¹˜ ë¶€ë¶„ì— í•´ë‹¹í•˜ëŠ” ì§€ì ì„ Kê°’ìœ¼ë¡œ ì„ íƒ (intertiaê°€ ê°ì†Œí•˜ëŠ” ì •ë„ê°€ ë‚®ì•„ì§€ëŠ” ì§€ì ) | â€» inertiaëŠ” cluster ìˆ˜ê°€ ì¦ê°€í•  ìˆ˜ë¡ ê°ì†Œí•¨. (trade-off ê´€ê³„.) | . inertias = [] for i in range(1, 11): kmeans = KMeans(n_clusters=i) kmeans.fit(X) inertias.append(kmeans.inertia_) plt.plot(range(1,11), inertias, marker='o') plt.xlabel('Num_Clusters') plt.ylabel('Inertia') plt.show() # ê²°ê³¼: 3ì´ ìµœì ì˜ Cluster . 2. Silhouette Score (Silhouette Coefficient) . | $ \\frac{(b-a)}{max(a, b)} $ë¡œ ê³„ì‚° . | a: íŠ¹ì •í•œ sample ië¡œë¶€í„° ê°™ì€ Classì— ì†í•œ ë‹¤ë¥¸ ì ë“¤ê¹Œì§€ì˜ í‰ê·  ê±°ë¦¬ (mean intra-cluster distance) | b: íŠ¹ì •í•œ sample ië¡œë¶€í„° ê°€ì¥ ê°€ê¹Œìš´ ì˜† Classì— ì†í•œ ì ë“¤ê¹Œì§€ì˜ í‰ê·  ê±°ë¦¬ (mean nearest-cluster distance) | . | ìˆ«ìê°€ í´ìˆ˜ë¡ ì˜ ë¶„ë¥˜ëœ ê²ƒ. (ìˆ«ìê°€ í¬ë‹¤ëŠ” ê²ƒì€ íƒ€ clusterì™€ëŠ” ê±°ë¦¬ê°€ ìˆê³ , ê°™ì€ cluster ë‚´ì—ì„œëŠ” ì˜ ëª¨ì—¬ ìˆë‹¤ëŠ” ì˜ë¯¸) | . from sklearn.metrics import silhouette_score silhouette = [] for i in range(2, 11): kmeans = KMeans(n_clusters=i) kmeans.fit(X) silhouette.append(silhouette_score(X, kmeans.labels_)) plt.plot(range(2,11), silhouette, marker='o') plt.xlabel('Num_Clusters') plt.ylabel('Silhouette Score') plt.show() # ê²°ê³¼: 3ì´ ìµœì ì˜ Cluster . 3. CH Index (Calinski-Harabasz Index) . | ì˜ ë¶„ë¥˜ëœ í´ëŸ¬ìŠ¤í„°ëŠ” (1) ë‚´ë¶€ì˜ ì ë“¤ë¼ë¦¬ compactí•˜ê²Œ ëª¨ì—¬ ìˆê³ , (2) ë‚˜ë¨¸ì§€ clusterë¡œë¶€í„°ëŠ” ë©€ë¦¬ ë–¨ì–´ì ¸ìˆì–´ì•¼ í•œë‹¤ëŠ” ì ì— ì°©ì•ˆí•œ Index | $ \\frac{BCV}{k-1} * \\frac{n-k}{WCV} $ë¡œ ê³„ì‚° . | BCV: Between-Cluster Variation: ì„œë¡œ ë‹¤ë¥¸ í´ëŸ¬ìŠ¤í„°ë¼ë¦¬ ì–¼ë§ˆë‚˜ ë–¨ì–´ì ¸ìˆëŠ”ì§€. â€“ í´ìˆ˜ë¡ ì¢‹ìŒ | WCV: Within-Vluster Variation: ì„œë¡œ ê°™ì€ í´ëŸ¬ìŠ¤í„°ì— ìˆëŠ” ì ë¼ë¦¬ ì–¼ë§ˆë‚˜ ë–¨ì–´ì ¸ìˆëŠ”ì§€. â€“ ì‘ì„ìˆ˜ë¡ ì¢‹ìŒ | k: # of clusters | n: # of datapoints | . | ìˆ«ìê°€ í´ìˆ˜ë¡ ì˜ ë¶„ë¥˜ëœ ê²ƒ. (ìˆ«ìê°€ í¬ë‹¤ëŠ” ê²ƒì€ íƒ€ clusterì™€ëŠ” ê±°ë¦¬ê°€ ìˆê³ , ê°™ì€ cluster ë‚´ì—ì„œëŠ” ì˜ ëª¨ì—¬ ìˆë‹¤ëŠ” ì˜ë¯¸) | . from sklearn.metrics import calinski_harabasz_score ch_index = [] for i in range(2, 11): kmeans = KMeans(n_clusters=i) kmeans.fit(X) ch_index.append(calinski_harabasz_score(X, kmeans.labels_)) plt.plot(range(2, 11), ch_index, marker='o') plt.xlabel('Num_Clusters') plt.ylabel('CH Index') plt.show() # ê²°ê³¼: 3ì´ ìµœì ì˜ Cluster . ìµœì ì˜ Kê°’ ì°¾ê¸°: yellowbrick . *yellowbrick: machine learning visualization library (https://www.scikit-yb.org/en/latest/) . | ìµœì ì˜ clusterë¥¼ ìë™ìœ¼ë¡œ ì°¾ì•„ì£¼ê³ , clusteringì— ê±¸ë¦¬ëŠ” ì‹œê°„ ë“±ë„ ê°„í¸í•˜ê²Œ ì‹œê°í™”í•  ìˆ˜ ìˆì–´ì„œ í¸ë¦¬í•˜ë‹¤ | . # ìš°ì„  ì„¤ì¹˜í•´ì¤˜ì•¼ ì‚¬ìš© ê°€ëŠ¥ import sys !{sys.executable} -m pip install yellowbrick . 1. Elbow Method . # Import ElbowVisualizer from yellowbrick.cluster import KElbowVisualizer model = KMeans() visualizer = KElbowVisualizer(model, k=(1,30), timings=True) # k is range of number of clusters. visualizer.fit(X) # Fit the data to the visualizer visualizer.show(); . 2. Silhouette Score . model = KMeans() visualizer = KElbowVisualizer(model, k=(2,30), metric='silhouette', timings=True) visualizer.fit(X) visualizer.show(); . 3. CH Index . model = KMeans() visualizer = KElbowVisualizer(model, k=(2,30), metric='calinski_harabasz', timings=True) visualizer.fit(X) visualizer.show(); . ",
    "url": "https://chaelist.github.io/docs/ml_basics/clustering/#k-means-clustering",
    "relUrl": "/docs/ml_basics/clustering/#k-means-clustering"
  },"20": {
    "doc": "Clustering",
    "title": "Hierarchical Clustering",
    "content": "(ê³„ì¸µì  êµ°ì§‘ ë¶„ì„) . ê¸°ë³¸ ê°œë… . | K Meansì™€ ë‹¬ë¦¬, ì¤‘ì‹¬ì„ ë¨¼ì € ì¡ê³  ì‹œì‘í•˜ëŠ” ê²Œ ì•„ë‹ˆë¼, ì¼ë‹¨ ëª¨ë“  ë²¡í„°ì˜ ê±°ë¦¬ë¥¼ ë‹¤ ê³„ì‚° â†’ ê±°ë¦¬ê°€ ê°€ì¥ ì§§ì€ ê²ƒë¼ë¦¬ ì°¨ê·¼ì°¨ê·¼ ë¬¶ì–´ë‚˜ê°. (ê·¸ë£¹ì˜ ìˆ˜ë¥¼ ì‚¬ì „ì— ì •í•˜ì§€ ì•ŠìŒ) | ê³„ì† ì—°ê²°í•´ë‚˜ê°€ì„œ í•˜ë‚˜ì˜ ì™„ë²½í•œ clusterë¡œ ë¬¶ì¼ ë•Œê¹Œì§€ ë¬¶ëŠ” ì‘ì—…ì„ ê³„ì†í•¨ | Dendrogram ë³´ê³  cluster ê°œìˆ˜ë¥¼ ì–¼ë§ˆ ì •ë„ë¡œ í•  ì§€ ê³ ë ¤í•´ì„œ ì ë‹¹íˆ ì˜ë¼ì¤Œ | . Â  . *Cluster ê°„ ê±°ë¦¬ë¥¼ ê³„ì‚°í•˜ëŠ” ë²•: (â€» ë°ì´í„°í¬ì¸íŠ¸ ê°„ì˜ ê±°ë¦¬ëŠ” Euclideanì´ë‚˜ Cosign ë°©ì‹ ë“±ìœ¼ë¡œ ê³„ì‚°) . | Single: ê° í´ëŸ¬ìŠ¤í„°ë¥¼ êµ¬ì„±í•˜ëŠ” ë°ì´í„°í¬ì¸íŠ¸ ì¤‘ ê°€ì¥ ê°€ê¹Œìš´ ë°ì´í„°í¬ì¸íŠ¸ ê°„ì˜ ê±°ë¦¬ë¡œ ê³„ì‚° | Complete: ê° í´ëŸ¬ìŠ¤í„°ë¥¼ êµ¬ì„±í•˜ëŠ” ë°ì´í„°í¬ì¸íŠ¸ ì¤‘ ê°€ì¥ ë¨¼ ë°ì´í„°í¬ì¸íŠ¸ ê°„ì˜ ê±°ë¦¬ë¡œ ê³„ì‚° | Average: ê° í´ëŸ¬ìŠ¤í„°ë¥¼ êµ¬ì„±í•˜ëŠ” ë°ì´í„°í¬ì¸íŠ¸ë“¤ì˜ í‰ê· ì  ê°„ì˜ ê±°ë¦¬ë¡œ ê³„ì‚° | Ward: ë‘ ê°œì˜ í´ëŸ¬ìŠ¤í„°ê°€ í•©ì³ì¡Œì„ ë•Œì˜ ë°ì´í„°í¬ì¸íŠ¸ë“¤ì´ ê°–ëŠ” ë¶„ì‚° (ì˜¤ì°¨ì œê³±í•©)ì´ ê°€ì¥ ì‘ì€ í´ëŸ¬ìŠ¤í„°ë¼ë¦¬ ë¬¶ì–´ì£¼ëŠ” ë°©ì‹ | . â€» 4ê°œì˜ linkage typeì€ ë°ì´í„°ì…‹ì˜ ë¶„í¬ì— ë”°ë¼ ê²°ê³¼ê°€ ìƒì´í•˜ë¯€ë¡œ, ë°ì´í„°ì…‹ì— ë”°ë¼ ì ì ˆíˆ ì„ íƒ . Dendrogram ê·¸ë ¤ë³´ê¸° . from sklearn.datasets import load_iris from sklearn.cluster import AgglomerativeClustering import pandas as pd import numpy as np import seaborn as sns import matplotlib.pyplot as plt from scipy.cluster.hierarchy import dendrogram, linkage . 1. ë°ì´í„° ì¤€ë¹„ . # iris dataset ì‚¬ìš© iris_data = load_iris() X = pd.DataFrame(iris_data.data, columns=iris_data.feature_names) X.head() . | Â  | sepal length (cm) | sepal width (cm) | petal length (cm) | petal width (cm) | . | 0 | 5.1 | 3.5 | 1.4 | 0.2 | . | 1 | 4.9 | 3 | 1.4 | 0.2 | . | 2 | 4.7 | 3.2 | 1.3 | 0.2 | . | 3 | 4.6 | 3.1 | 1.5 | 0.2 | . | 4 | 5 | 3.6 | 1.4 | 0.2 | . 2. Dendrogram ê·¸ë ¤ë³´ê¸° . | Scipy í™œìš© | ì–´ë–»ê²Œ ë¬¶ì¼ì§€ ì‹œë®¬ë ˆì´ì…˜ + ëª‡ ê°œ Clusterë¡œ ë‚˜ëˆŒì§€ ê³ ë¯¼ | ì–´ë–¤ linkage typeì„ ì“°ë©´ ì¢‹ì„ì§€, ëª‡ ê°œì˜ Clusterë¥¼ ì“°ë©´ ì¢‹ì„ì§€ ê³ ë¯¼ | . plt.figure(figsize=(15, 5)) L = linkage(X, 'single') dn = dendrogram(L) plt.title(\"Dendrograms: Single\") plt.show() . plt.figure(figsize=(15, 5)) L = linkage(X, 'ward') dn = dendrogram(L) plt.title(\"Dendrograms: Ward\") plt.show() . | singleë¡œ ë¬¶ëŠ”ë‹¤ë©´ 2ê°œì˜ clusterë¡œ, wardë¡œ ë¬¶ëŠ”ë‹¤ë©´ 2-3ê°œì˜ clusterë¡œ ë¬¶ëŠ” ê²Œ ì¢‹ì„ ê²ƒ ê°™ë‹¤ê³  íŒë‹¨ | completeë‚˜ average ë°©ì‹ë„ êµ¬í˜„í•´ë³´ê³  ë¹„êµí•´ë³´ë©´ ì¢‹ìŒ | . scikit-learnìœ¼ë¡œ êµ¬í˜„ . 3. Clustering . model = AgglomerativeClustering(n_clusters=3) # iris datasetì´ë¯€ë¡œ, n_clusters=3ìœ¼ë¡œ ì„ íƒ model.fit(X) . AgglomerativeClustering(affinity='euclidean', compute_full_tree='auto', connectivity=None, distance_threshold=None, linkage='ward', memory=None, n_clusters=3) . | affinity: ë°ì´í„°í¬ì¸íŠ¸ ê°„ì˜ ê±°ë¦¬ë¥¼ ì–´ë–»ê²Œ ê³„ì‚°í• ì§€ ê²°ì •. euclidean, cosine, l1, l2, manhattan ì¤‘ì— ê³ ë¥¼ ìˆ˜ ìˆìœ¼ë©°, defaultëŠ” euclidean. | linkage=â€™wardâ€™ë¡œ í•˜ë ¤ë©´ euclideanë°–ì— ì„ íƒí•  ìˆ˜ ì—†ìŒ | . | linkage: ward, complete, average, single ì¤‘ì— ì„ íƒ. (defaultëŠ” ward) | . 4. Clustering ê²°ê³¼ êµ¬í˜„ . result = X.copy() result[\"cluster\"] = model.labels_ result.head() . | Â  | sepal length (cm) | sepal width (cm) | petal length (cm) | petal width (cm) | cluster | . | 0 | 5.1 | 3.5 | 1.4 | 0.2 | 1 | . | 1 | 4.9 | 3 | 1.4 | 0.2 | 1 | . | 2 | 4.7 | 3.2 | 1.3 | 0.2 | 1 | . | 3 | 4.6 | 3.1 | 1.5 | 0.2 | 1 | . | 4 | 5 | 3.6 | 1.4 | 0.2 | 1 | . +) pairplotìœ¼ë¡œ clusteringì´ ì˜ ë˜ì—ˆë‚˜ ì‚´í´ë³´ê¸° . # 4ì°¨ì›ìœ¼ë¡œ ì‹œê°í™”í•  ìˆ˜ëŠ” ì—†ì§€ë§Œ, seabornì˜ pairplotìœ¼ë¡œ ì–´ëŠ ì •ë„ ë‹¤ê°ë„ë¡œ ì‚´í´ë³¼ ìˆ˜ëŠ” ìˆìŒ sns.pairplot(result, hue='cluster') plt.show() . ìµœì ì˜ Kê°’ ì°¾ê¸°: yellowbrick . | ë§ˆì°¬ê°€ì§€ë¡œ, iris dataì²˜ëŸ¼ cluster ìˆ˜ë¥¼ ì•Œê³  ìˆëŠ” ìƒí™©ì´ ì•„ë‹ˆë¼ë©´, ì•„ë˜ì™€ ê°™ì€ ë°©ì‹ë“¤ì„ ì‚¬ìš©í•´ ìµœì ì˜ Kê°’ì„ ì°¾ì•„ë³´ê³  clusteringì„ êµ¬í˜„í•˜ëŠ” ê²ƒì´ ì¢‹ë‹¤ | . 1. Elbow Method . # Import ElbowVisualizer from yellowbrick.cluster import KElbowVisualizer model = AgglomerativeClustering() visualizer = KElbowVisualizer(model, k=(1,30), timings=True) visualizer.fit(X) visualizer.show(); . 2. Silhouette Score . model = AgglomerativeClustering() visualizer = KElbowVisualizer(model, k=(2,30), metric='silhouette', timings=True) visualizer.fit(X) visualizer.show(); . 3. CH Index . model = AgglomerativeClustering() visualizer = KElbowVisualizer(model, k=(2,30), metric='calinski_harabasz', timings=True) visualizer.fit(X) visualizer.show(); . | ëŒ€ì²´ë¡œ 2-3ê°œì˜ clusterë¡œ ë‚˜ëˆ„ëŠ” ê²ƒì´ ì¢‹ë‹¤ëŠ” ê²°ë¡  (ì‚¬ì‹¤ iris dataì—ì„œ virginicaì™€ versicolorëŠ” ê½¤ ìœ ì‚¬í•˜ê¸° ë•Œë¬¸) | . ",
    "url": "https://chaelist.github.io/docs/ml_basics/clustering/#hierarchical-clustering",
    "relUrl": "/docs/ml_basics/clustering/#hierarchical-clustering"
  },"21": {
    "doc": "Control Flow (ì œì–´ë¬¸)",
    "title": "Control Flow (ì œì–´ë¬¸)",
    "content": ". | if-elseë¬¸ (ì¡°ê±´ë¬¸) . | Comparison tests | â€˜passâ€™ | if - elif - else | . | for ë°˜ë³µë¬¸ . | range() function | â€˜continueâ€™ì™€ â€˜breakâ€™ | List Comprehension | . | while ë°˜ë³µë¬¸ . | â€˜continueâ€™ì™€ â€˜breakâ€™ | . | try - exceptë¬¸ (ì˜¤ë¥˜ ì˜ˆì™¸ ì²˜ë¦¬) | . ",
    "url": "https://chaelist.github.io/docs/python_basics/controlflow/",
    "relUrl": "/docs/python_basics/controlflow/"
  },"22": {
    "doc": "Control Flow (ì œì–´ë¬¸)",
    "title": "if-elseë¬¸ (ì¡°ê±´ë¬¸)",
    "content": ". | íŠ¹ì •í•œ ì¡°ê±´ì´ ì¶©ì¡±ë  ë•Œë§Œ(= Trueì¼ ë•Œë§Œ) íŠ¹ì •í•œ Python ì½”ë“œë“¤ì„ ì‹¤í–‰í•˜ê²Œ í•¨ | if ì¡°ê±´ë¬¸ì˜ ê²°ê³¼ë¡œ ì‹¤í–‰ë˜ì–´ì•¼ í•˜ëŠ” ì•„ë˜ ì½”ë“œë“¤ì—ëŠ” indentation(ë“¤ì—¬ì“°ê¸°)ë¥¼ í•´ì£¼ì–´ì•¼ í•¨ | . *í˜•ì‹: . if condition1: body1 # bodyì—ëŠ” ì—¬ëŸ¬ ì¤„ì˜ ì½”ë“œê°€ ë“¤ì–´ê°€ë„ ë¨. (ë“¤ì—¬ì“°ê¸°ë¡œ êµ¬ê°„ì´ ì˜ í‘œí˜„ë§Œ ë˜ë©´) else: # elseëŠ” ì•ˆì¨ë„ ë¨ body2 . *ì˜ˆì‹œ: . num1 = -1 if num1 &gt; 0: # num1ì´ 0ë³´ë‹¤ í´ ê²½ìš° ì•„ë˜ ì½”ë“œë“¤ì´ ì‹¤í–‰ print(num1) print('positive') else: # num1ì´ 0ë³´ë‹¤ í¬ì§€ ì•Šì„ ê²½ìš° ì•„ë˜ ì½”ë“œë“¤ì´ ì‹¤í–‰ print(num1) print('negative') . -1 negative . Comparison tests . | a &lt; b (bë³´ë‹¤ ì‘ë‹¤), a &lt;= b (bë³´ë‹¤ ì‘ê±°ë‚˜ ê°™ë‹¤) | a &gt; b (bë³´ë‹¤ í¬ë‹¤), a &gt;= b (bë³´ë‹¤ í¬ê±°ë‚˜ ê°™ë‹¤) | == (ê°™ë‹¤), != (ë‹¤ë¥´ë‹¤) | â€˜inâ€™ (í¬í•¨ë˜ì–´ ìˆë‹¤), â€˜not inâ€™ (í¬í•¨ë˜ì–´ ìˆì§€ ì•Šë‹¤) | . # equality test ì˜ˆì‹œ a = 2 b = 3 a != b # ë‘˜ì´ ë‹¤ë¥´ë¯€ë¡œ True . True . # in / not in x = [3, 5, 9] if 3 in x: print('included') else: print('not included') . included . â€˜passâ€™ . : ì¡°ê±´ì´ Trueì¼ ê²½ìš° ì•„ë¬´ ì¼ë„ ì¼ì–´ë‚˜ì§€ ì•Šê²Œ ì„¤ì •í•˜ê³  ì‹¶ë‹¤ë©´, â€˜passâ€™ë¥¼ ì‚¬ìš©í•˜ë©´ ëœë‹¤ . # ì˜ˆì‹œ: bag = ['apple', 'water', 'cell phone'] if 'apple' in bag: pass # appleì´ bagì— ìˆê¸° ë•Œë¬¸ì—, ì•„ë¬´ ê²°ê³¼ê°’ë„ ì¶œë ¥ë˜ì§€ ì•ŠëŠ”ë‹¤. else: print('need an apple') . if - elif - else . | for more than one condition, we use â€˜if - elif - elseâ€™ statement | elifëŠ” ë¬´ìˆ˜íˆ ë§ì´ ì‚¬ìš© ê°€ëŠ¥. | . *í˜•ì‹: . if condition1: body1 elif condition2: body2 elif condition3: body3 else: body4 . ** if-elif-else êµ¬ë¬¸ì€ ìœ„ì—ì„œë¶€í„° ì ì  ë°‘ìœ¼ë¡œ ë‚´ë ¤ê°€ë©° testë˜ë‹¤ê°€, ì¶©ì¡±ë˜ëŠ” conditionì„ ë§Œë‚˜ë©´ í•´ë‹¹ bodyê°€ ì‹¤í–‰ë˜ë©° ë” ì´ìƒ ì•„ë˜ ì¡°ê±´ë“¤ì€ testë˜ì§€ ì•ŠëŠ”ë‹¤ . # ì˜ˆì‹œ: a = 35 if a &gt; 30: # ì²« ë²ˆì§¸ ì¡°ê±´ì´ ì¶©ì¡±ë˜ë¯€ë¡œ ì—¬ê¸°ì„œ processê°€ ëë‚˜ê³ , 25ê°€ ì¶œë ¥ë¨ a= a-10 print(a) elif 20 &lt;a &lt;= 40: a = a-5 print(a) else: print(a) # ì²«ë²ˆì§¸ì™€ ë‘ë²ˆì§¸ ì¡°ê±´ì— ê²¹ì¹˜ëŠ” ë¶€ë¶„ì´ ìˆì§€ë§Œ, ì´ë¯¸ ì²«ë²ˆì§¸ ì¡°ê±´ì´ ì¶©ì¡±ë˜ì–´ì„œ processê°€ ëë‚˜ê¸°ì— ë” ì´ìƒ ë°‘ìœ¼ë¡œ ì§„í–‰ë˜ì§€ ì•ŠëŠ”ë‹¤. ## í•˜ì§€ë§Œ ë¬¼ë¡  ì¡°ê±´ë“¤ì€ ì„œë¡œ ê²¹ì¹˜ì§€ ì•Šê²Œ ì‘ì„±í•˜ëŠ” ê²Œ ë” ì¢‹ìŒ! . 25 . ** cf) if-elif ëŒ€ì‹  ifë§Œ ë°˜ë³µí•´ì„œ ì“°ë©´, ë…ë¦½ì ì¸ ì„¸íŠ¸ë¡œ ê°„ì£¼ë˜ì–´ ëª¨ë‘ testë¨. # if-elif ëŒ€ì‹  ifë§Œ ë‘ ê°œ ì“°ëŠ” ê²½ìš° a = 3 if a &gt; 0: a = a+1 print(a) if a &gt; 2: print(a) # ifê°€ ë‘ ê°œë©´, ì„œë¡œ ë…ë¦½ì ì¸ ë‘ ê°œì˜ ë‹¤ë¥¸ ì„¸íŠ¸ë¡œ ê°„ì£¼ë¨. # ì²«ë²ˆì¬ ì¡°ê±´ì—ì„œ trueì—¬ë„ ë‘ë²ˆì§¸ ì¡°ê±´ë„ testë˜ê³ , ë‘ë²ˆì§¸ ì¡°ê±´ë„ ë˜ trueì´ë©´ ë‘˜ ë‹¤ ì‹¤í–‰ë¨. 4 4 . +) ì˜ˆì‹œ: ìë™ í•™ì  ê³„ì‚°ê¸° . ## 'input()' í•¨ìˆ˜ë¡œ ì ìˆ˜ ë°›ì•„ì„œ í•™ì  ê³„ì‚°í•´ë³´ê¸° num = input('Enter your score: ') x = int(num) # input()ì„ í†µí•´ ë°›ì€ ìˆ«ìëŠ” string typeìœ¼ë¡œ ì €ì¥ë˜ë¯€ë¡œ, int()ë¡œ ë³€í™˜í•´ì•¼ ê³„ì‚° ê°€ëŠ¥ if x &lt; 50: print('F') elif 50 &lt;= x &lt; 60: print('D') elif 60 &lt;= x &lt;70: print('C') elif 70 &lt;= x &lt; 80: print('B') else: print('A') . Enter your score: 70 B . ",
    "url": "https://chaelist.github.io/docs/python_basics/controlflow/#if-else%EB%AC%B8-%EC%A1%B0%EA%B1%B4%EB%AC%B8",
    "relUrl": "/docs/python_basics/controlflow/#if-elseë¬¸-ì¡°ê±´ë¬¸"
  },"23": {
    "doc": "Control Flow (ì œì–´ë¬¸)",
    "title": "for ë°˜ë³µë¬¸",
    "content": "*í˜•ì‹: . for item in list-like variable: body . | itemì€ í•´ë‹¹ list-like variableì˜ ê° ê°’ë“¤ì„ ì·¨í•˜ëŠ” temporary variable | íŠ¹ì • list-like variableì— ìˆëŠ” ê°’ë“¤ì„ í•˜ë‚˜ì”© ì·¨í•´ì„œ ì°¨ë¡€ë¡œ íŠ¹ì • body êµ¬ë¬¸ì„ ì‹¤í–‰ | . *ì˜ˆì‹œ: . | forë¬¸ ì˜ˆì‹œ1: list ì•ˆì˜ variable ì°¨ë¡€ë¡œ printí•˜ê¸° x = ['a', 'b', 'c'] for k in x: print(k) . a b c . | forë¬¸ ì˜ˆì‹œ2: adding up all elements in a list x = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] total = 0 for k in x: total = total + k # x ì•ˆì˜ ê°’ì„ í•˜ë‚˜ì”© ì·¨í•´ì„œ ì°¨ë¡€ë¡œ ë”í•´ì¤Œ print(total) . 55 . | forë¬¸ ì˜ˆì‹œ3: adding up even numbers only . # forë¬¸ê³¼ ifë¬¸ì„ í•¨ê»˜ ì‚¬ìš©í•´, íŠ¹ì • ì¡°ê±´ì˜ ê°’ë“¤ë§Œ ë”í•˜ê¸° x = [2, 3, 1, 4, 5] total = 0 for k in x: if k % 2 == 0: total = total + k print(total) . 6 . | . range() function . | for a number n, range(n) returns a sequence of 0, 1, â€¦ n-1 . | ex) range(5) â€“&gt; 0, 1, 2, 3, 4 ì°¨ë¡€ë¡œ ì ‘ê·¼ | . | . **ì¶”ê°€: . | range(a, b): ì‹œì‘ê°’ê³¼ ëê°’ì„ ì§€ì • . | a, a+1, â€¦ b-1 | . | range(a, b, k): ì‹œì‘ê°’, ëê°’, ê°„ê²©ì„ ì§€ì • . | a, a+k, a+2k, â€¦ b-1 ì´ëŸ° ì‹ìœ¼ë¡œ k ê°„ê²©ìœ¼ë¡œ a ~ b-1 ì‚¬ì´ì˜ ê°’ë“¤ì„ ì ‘ê·¼ | ex) range(2, 10, 2) -&gt; 2, 4, 6, 8 | . | . | range(n) ì˜ˆì‹œ for k in range(5): print(k) . 0 1 2 3 4 . | range(a, b, k) ì˜ˆì‹œ for a in range(3, 10, 2): print(a) . 3 5 7 9 . | ì‘ìš©: range(len(s))ë¥¼ í†µí•´ ë¦¬ìŠ¤íŠ¸ â€˜sâ€™ì— ìˆëŠ” ê°’ë“¤ì˜ index numberì— ì ‘ê·¼ s = ['a', 'b', 'c'] for k in range(len(s)): print(s[k]) ### ë¬¼ë¡  `for k in s: print(k)` ì´ë ‡ê²Œ í•˜ëŠ” ê±°ë‘ ê²°ê³¼ëŠ” ë˜‘ê°™ìŒ,,, . a b c . | . â€˜continueâ€™ì™€ â€˜breakâ€™ . : íŠ¹ì • condition ì•„ë˜ â€˜forâ€™ loopë¥¼ ë©ˆì¶”ê±°ë‚˜ escapeí•˜ëŠ” ë°©ë²• . | continue: íŠ¹ì • ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” elementì— ëŒ€í•´ì„œëŠ” ë‚˜ë¨¸ì§€ bodyê°€ ì‹¤í–‰ë˜ì§€ X, ë‹¤ìŒ elementë¡œ processê°€ ë„˜ì–´ê°„ë‹¤ (ë‚¨ì•„ìˆëŠ” elementê°€ ìˆë‹¤ë©´ forë¬¸ ìì²´ëŠ” ê³„ì†ë˜ëŠ” ê²ƒ) . x = [2, -1, 3] total = 0 for k in x: if k &lt; 0: # kê°€ ìŒìˆ˜ì¼ ê²½ìš°ì—ëŠ” continue continue ## ì•„ë˜ total = total + k ë¶€ë¶„ì´ ì‹¤í–‰ë˜ì§€ ì•Šê³ , ë‹¤ìŒ elementë¡œ ë„˜ì–´ê°„ë‹¤ total = total + k print(total) # ê²°êµ­, ì–‘ìˆ˜ë§Œ ë”í•˜ëŠ” ê²°ê³¼ê°€ ë‚˜ì˜´. 5 . | break: íŠ¹ì • ì¡°ê±´ì´ ì¶©ì¡±ë˜ëŠ” ìˆœê°„, forë¬¸ì´ ì•„ì˜ˆ ë‹¤ ë©ˆì¶°ë²„ë¦°ë‹¤ (ë‹¤ìŒ elementê°€ ë‚¨ì•„ìˆì–´ë„ ë„˜ì–´ê°€ì§€ ì•ŠìŒ) . x = [2, -1, 3] total = 0 for k in x: if k &lt; 0: # kê°€ ìŒìˆ˜ì¼ ê²½ìš°ì— ì•„ì˜ˆ í•´ë‹¹ 'for' processê°€ ë‹¤ ë©ˆì¶¤. ë‹¤ìŒ elementë¡œë„ ë„˜ì–´ê°€ì§€ ì•ŠìŒ. break total = total + k print(total) ## 2ëŠ” ì¼ë‹¨ total = total + kê¹Œì§€ ë‚´ë ¤ê°€ì„œ ë”í•´ì§€ì§€ë§Œ, -1ì—ì„œ ifê°€ ì¶©ì¡±ë˜ë©´ì„œ breakë¥¼ ë§Œë‚˜ forë¬¸ì´ ë©ˆì¶”ë©´ì„œ ê·¸ëŒ€ë¡œ totalì€ 2ì—ì„œ ë©ˆì¶¤ . 2 . | . List Comprehension . | ë¦¬ìŠ¤íŠ¸ ë‚´í¬ (List Comprehension): ë¦¬ìŠ¤íŠ¸ ì•ˆì— forë¬¸ì„ ë‚´í¬í•´ ê°„ê²°í•˜ê²Œ ì‘ì„± # list comprehension ì˜ˆì‹œ a = [1, 2, 3, 4] result = [num * 3 for num in a] print(result) . [3, 6, 9, 12] . | if ì¡°ê±´ì„ í•¨ê»˜ ë‚´í¬í•˜ëŠ” ê²ƒë„ ê°€ëŠ¥ # List comprehension ì˜ˆì‹œ2: if ì¡°ê±´ë„ í•¨ê»˜ ë‚´í¬ a = [1, 2, 3, 4] result = [num * 3 for num in a if num % 2 == 0] print(result) . [6, 12] . | . ",
    "url": "https://chaelist.github.io/docs/python_basics/controlflow/#for-%EB%B0%98%EB%B3%B5%EB%AC%B8",
    "relUrl": "/docs/python_basics/controlflow/#for-ë°˜ë³µë¬¸"
  },"24": {
    "doc": "Control Flow (ì œì–´ë¬¸)",
    "title": "while ë°˜ë³µë¬¸",
    "content": ". | íŠ¹ì • ì¡°ê±´ì´ Trueì¸ ë™ì•ˆ ì˜ì›íˆ ëŒì•„ê°„ë‹¤. (ì¡°ê±´ì´ Falseê°€ ë  ë•Œê¹Œì§€) | ì¡°ê±´ì„ ì˜ ì„¤ì •í•˜ì§€ ì•Šìœ¼ë©´ ì •ë§ â€˜ì˜ì›íˆâ€™ ëŒì•„ê°€ë‹ˆ, ì¡°ì‹¬! | . *í˜•ì‹: . while condition: body . *ì˜ˆì‹œ: . a = 10 while a &gt; 0: print(a) a = a - 1 . 10 9 8 7 6 5 4 3 2 1 . â€˜continueâ€™ì™€ â€˜breakâ€™ . : whileë¬¸ë„ continueì™€ breakë¥¼ í™œìš©í•´ íŠ¹ì • elementë¥¼ skipí•˜ê±°ë‚˜ ì „ì²´ loopë¥¼ ê°•ì œë¡œ ì¤‘ë‹¨ì‹œí‚¬ ìˆ˜ ìˆë‹¤ . | continue ì‚¬ìš© ì˜ˆì‹œ . a = 0 while a &lt; 10: a = a + 1 if a % 2 == 0: # 2ì˜ ë°°ìˆ˜ì¼ ê²½ìš°ì—ëŠ” print(a)ê°€ ì‹¤í–‰ë˜ì§€ ì•Šì€ ì±„ ë‹¤ì‹œ ë§¨ ì²˜ìŒìœ¼ë¡œ ëŒì•„ê°„ë‹¤ continue print(a) . 1 3 5 7 9 . | break ì‚¬ìš© ì˜ˆì‹œ . a = 0 while a &lt; 10: a = a + 1 if a % 3 == 0: # 2ì˜ ë°°ìˆ˜ë¥¼ ë§Œë‚˜ë©´ while loopê°€ ì•„ì˜ˆ ì¤‘ë‹¨ëœë‹¤ break print(a) . 1 2 . | . ",
    "url": "https://chaelist.github.io/docs/python_basics/controlflow/#while-%EB%B0%98%EB%B3%B5%EB%AC%B8",
    "relUrl": "/docs/python_basics/controlflow/#while-ë°˜ë³µë¬¸"
  },"25": {
    "doc": "Control Flow (ì œì–´ë¬¸)",
    "title": "try - exceptë¬¸ (ì˜¤ë¥˜ ì˜ˆì™¸ ì²˜ë¦¬)",
    "content": ". | ê¸°ë³¸ try - exceptë¬¸ . | ì˜¤ë¥˜ê°€ ë°œìƒí•  ì‹œ except ë¸”ë¡ì´ ì‹¤í–‰ëœë‹¤ | . try: ì‹¤í–‰í•´ì•¼ í•˜ëŠ” ì½”ë“œ except: ì˜¤ë¥˜ê°€ ë‚  ê²½ìš° ì‹¤í–‰í•  ì½”ë“œ . | ë°œìƒ ì˜¤ë¥˜ ì¢…ë¥˜ë¥¼ í¬í•¨í•œ exceptë¬¸ . | exceptë¬¸ì— ë¯¸ë¦¬ ì •í•´ë‘” ì˜¤ë¥˜ ì´ë¦„ê³¼ ì¼ì¹˜í•  ë•Œë§Œ except ë¸”ë¡ ì‹¤í–‰ | . try: ì‹¤í–‰í•´ì•¼ í•˜ëŠ” ì½”ë“œ except ë°œìƒ ì˜¤ë¥˜: ì˜¤ë¥˜ê°€ ë‚  ê²½ìš° ì‹¤í–‰í•  ì½”ë“œ . *ì˜ˆì‹œ: . try: a = [1,2] print(a[3]) except IndexError: print(\"ì¸ë±ì‹± ë¶ˆê°€ëŠ¥\") . ì¸ë±ì‹± ë¶ˆê°€ëŠ¥ . +) ì—¬ëŸ¬ ê°œì˜ ì˜¤ë¥˜ë¥¼ ì²˜ë¦¬í•˜ëŠ” ê²ƒë„ ê°€ëŠ¥ . try: ì‹¤í–‰í•´ì•¼ í•˜ëŠ” ì½”ë“œ except ë°œìƒ ì˜¤ë¥˜1: ì˜¤ë¥˜1ì´ ë‚  ê²½ìš° ì‹¤í–‰í•  ì½”ë“œ except ë°œìƒ ì˜¤ë¥˜2: ì˜¤ë¥˜2ê°€ ë‚  ê²½ìš° ì‹¤í–‰í•  ì½”ë“œ . | try â€¦ finally . | finallyì ˆì€ tryë¬¸ ìˆ˜í–‰ ë„ì¤‘ ì˜ˆì™¸ ë°œìƒ ì—¬ë¶€ì™€ ìƒê´€ì—†ì´ í•­ìƒ ìˆ˜í–‰ëœë‹¤ | . try: ì‹¤í–‰í•´ì•¼ í•˜ëŠ” ì½”ë“œ finally: ì˜¤ë¥˜ ì—¬ë¶€ì™€ ìƒê´€ ì—†ì´ ì‹¤í–‰ë˜ì–´ì•¼ í•˜ëŠ” ì½”ë“œ . *ì˜ˆì‹œ: . f = open('message.txt', 'w') try: ì‹¤í–‰í•´ì•¼ í•˜ëŠ” ì½”ë“œ finally: f.close() # tryë¬¸ ì¤‘ ì˜¤ë¥˜ ì—¬ë¶€ì™€ ìƒê´€ì—†ì´ ë°˜ë“œì‹œ ì‹¤í–‰ë˜ì–´ì•¼ í•˜ëŠ” ì½”ë“œ ## ì˜ˆì™¸ ë°œìƒ ì—¬ë¶€ì™€ ìƒê´€ì—†ì´ finallyì ˆì—ì„œ f.close()ë¡œ ì—´ë¦° íŒŒì¼ì„ ë‹«ì„ ìˆ˜ ìˆë‹¤. | â€˜passâ€™ë¡œ ì˜¤ë¥˜ íšŒí”¼í•˜ê¸° try: ì‹¤í–‰í•´ì•¼ í•˜ëŠ” ì½”ë“œ except: pass # ì˜¤ë¥˜ê°€ ë‚˜ë©´ ê·¸ëƒ¥ passí•´ë²„ë¦¬ê¸° . *ì˜ˆì‹œ: . # ì˜¤ë¥˜ íšŒí”¼ ì˜ˆì‹œ try: f = open(\"ì—†ëŠ” íŒŒì¼\", 'r') except FileNotFoundError: pass ## íŒŒì¼ì´ ì°¾ì•„ì§€ì§€ ì•Šìœ¼ë©´ ê·¸ëƒ¥ pass. | . ",
    "url": "https://chaelist.github.io/docs/python_basics/controlflow/#try---except%EB%AC%B8-%EC%98%A4%EB%A5%98-%EC%98%88%EC%99%B8-%EC%B2%98%EB%A6%AC",
    "relUrl": "/docs/python_basics/controlflow/#try---exceptë¬¸-ì˜¤ë¥˜-ì˜ˆì™¸-ì²˜ë¦¬"
  },"26": {
    "doc": "Telco Customer Churn",
    "title": "Telco Customer Churn",
    "content": ". | ë°ì´í„° íŒŒì•… | ë³€ìˆ˜ ê°„ ìƒê´€ê´€ê³„ íŒŒì•… | chrun ì—¬ë¶€ ì˜ˆì¸¡ì— ì£¼ìš”í•œ ë³€ìˆ˜ íŒŒì•… . | Logistic Regression ëª¨ë¸ ê³„ì‚° | ë³€ìˆ˜ë³„ coefficient ë¹„êµ | . | ë³€ìˆ˜ë³„ ë¹„êµ . | ì£¼(state)ë³„ ì´íƒˆë¥  | planë³„ ì´íƒˆë¥  | ë‚® í†µí™”ì‹œê°„ì— ë”°ë¥¸ ì´íƒˆë¥  | customer service callsì— ë”°ë¥¸ ì´íƒˆë¥  | ë³´ì´ìŠ¤ë©”ì¼ ë©”ì‹œì§€ ìˆ˜ì— ë”°ë¥¸ ì´íƒˆë¥  | . | . *ë¶„ì„ ëŒ€ìƒ ë°ì´í„°ì…‹: Telecommunications Customer Churn . | ë°ì´í„°ì…‹ ì¶œì²˜ | ê³ ê°ì´ telecommunications providerë¥¼ ë°”ê¾¸ëŠ” í–‰ë™(churn)ì„ ì˜ˆì¸¡í•˜ëŠ” ì§•í›„ë¥¼ íŒŒì•…í•˜ê¸° ìœ„í•œ dataset | 4250ê°œ sample. ì•½ 86%ëŠ” stay, 14%ëŠ” churn | . Â  . *ë³€ìˆ˜ ì„¤ëª…: . | â€œstateâ€, string. 2-letter code of the US state of customer residence | â€œaccount_lengthâ€, numerical. Number of months the customer has been with the current telco provider | â€œarea_codeâ€, string=â€area_code_AAAâ€ where AAA = 3 digit area code. | â€œinternational_planâ€, . The customer has international plan. | â€œvoice_mail_planâ€, . The customer has voice mail plan. | â€œnumber_vmail_messagesâ€, numerical. Number of voice-mail messages. | â€œtotal_day_minutesâ€, numerical. Total minutes of day calls. | â€œtotal_day_callsâ€, numerical. Total minutes of day calls. | â€œtotal_day_chargeâ€, numerical. Total charge of day calls. | â€œtotal_eve_minutesâ€, numerical. Total minutes of evening calls. | â€œtotal_eve_callsâ€, numerical. Total number of evening calls. | â€œtotal_eve_chargeâ€, numerical. Total charge of evening calls. | â€œtotal_night_minutesâ€, numerical. Total minutes of night calls. | â€œtotal_night_callsâ€, numerical. Total number of night calls. | â€œtotal_night_chargeâ€, numerical. Total charge of night calls. | â€œtotal_intl_minutesâ€, numerical. Total minutes of international calls. | â€œtotal_intl_callsâ€, numerical. Total number of international calls. | â€œtotal_intl_chargeâ€, numerical. Total charge of international calls | â€œnumber_customer_service_callsâ€, numerical. Number of calls to customer service | â€œchurnâ€, . Customer churn - target variable. | . ",
    "url": "https://chaelist.github.io/docs/kaggle/customer_churn/",
    "relUrl": "/docs/kaggle/customer_churn/"
  },"27": {
    "doc": "Telco Customer Churn",
    "title": "ë°ì´í„° íŒŒì•…",
    "content": "# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ import import pandas as pd import numpy as np from matplotlib import pyplot as plt import seaborn as sns import scipy.stats as stats . customer_df = pd.read_csv('data/customer_churn_2020.csv') customer_df.head() . | Â  | state | account_length | area_code | international_plan | voice_mail_plan | number_vmail_messages | total_day_minutes | total_day_calls | total_day_charge | total_eve_minutes | total_eve_calls | total_eve_charge | total_night_minutes | total_night_calls | total_night_charge | total_intl_minutes | total_intl_calls | total_intl_charge | number_customer_service_calls | churn | . | 0 | OH | 107 | area_code_415 | no | yes | 26 | 161.6 | 123 | 27.47 | 195.5 | 103 | 16.62 | 254.4 | 103 | 11.45 | 13.7 | 3 | 3.7 | 1 | no | . | 1 | NJ | 137 | area_code_415 | no | no | 0 | 243.4 | 114 | 41.38 | 121.2 | 110 | 10.3 | 162.6 | 104 | 7.32 | 12.2 | 5 | 3.29 | 0 | no | . | 2 | OH | 84 | area_code_408 | yes | no | 0 | 299.4 | 71 | 50.9 | 61.9 | 88 | 5.26 | 196.9 | 89 | 8.86 | 6.6 | 7 | 1.78 | 2 | no | . | 3 | OK | 75 | area_code_415 | yes | no | 0 | 166.7 | 113 | 28.34 | 148.3 | 122 | 12.61 | 186.9 | 121 | 8.41 | 10.1 | 3 | 2.73 | 3 | no | . | 4 | MA | 121 | area_code_510 | no | yes | 24 | 218.2 | 88 | 37.09 | 348.5 | 108 | 29.62 | 212.6 | 118 | 9.57 | 7.5 | 7 | 2.03 | 3 | no | . | nullê°’ ì—¬ë¶€, data type í™•ì¸ . customer_df.info() . &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 4250 entries, 0 to 4249 Data columns (total 20 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 state 4250 non-null object 1 account_length 4250 non-null int64 2 area_code 4250 non-null object 3 international_plan 4250 non-null object 4 voice_mail_plan 4250 non-null object 5 number_vmail_messages 4250 non-null int64 6 total_day_minutes 4250 non-null float64 7 total_day_calls 4250 non-null int64 8 total_day_charge 4250 non-null float64 9 total_eve_minutes 4250 non-null float64 10 total_eve_calls 4250 non-null int64 11 total_eve_charge 4250 non-null float64 12 total_night_minutes 4250 non-null float64 13 total_night_calls 4250 non-null int64 14 total_night_charge 4250 non-null float64 15 total_intl_minutes 4250 non-null float64 16 total_intl_calls 4250 non-null int64 17 total_intl_charge 4250 non-null float64 18 number_customer_service_calls 4250 non-null int64 19 churn 4250 non-null object dtypes: float64(8), int64(7), object(5) memory usage: 664.2+ KB . | ìˆ«ìí˜• ì»¬ëŸ¼: ê°’ì˜ ë¶„í¬ë¥¼ í™•ì¸ . customer_df.describe() . | Â  | account_length | number_vmail_messages | total_day_minutes | total_day_calls | total_day_charge | total_eve_minutes | total_eve_calls | total_eve_charge | total_night_minutes | total_night_calls | total_night_charge | total_intl_minutes | total_intl_calls | total_intl_charge | number_customer_service_calls | . | count | 4250 | 4250 | 4250 | 4250 | 4250 | 4250 | 4250 | 4250 | 4250 | 4250 | 4250 | 4250 | 4250 | 4250 | 4250 | . | mean | 100.236 | 7.63176 | 180.26 | 99.9073 | 30.6447 | 200.174 | 100.176 | 17.015 | 200.528 | 99.8395 | 9.02389 | 10.2561 | 4.42635 | 2.76965 | 1.55906 | . | std | 39.6984 | 13.4399 | 54.0124 | 19.8508 | 9.1821 | 50.2495 | 19.9086 | 4.27121 | 50.3535 | 20.0932 | 2.26592 | 2.7601 | 2.46307 | 0.745204 | 1.31143 | . | min | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . | 25% | 73 | 0 | 143.325 | 87 | 24.365 | 165.925 | 87 | 14.1025 | 167.225 | 86 | 7.5225 | 8.5 | 3 | 2.3 | 1 | . | 50% | 100 | 0 | 180.45 | 100 | 30.68 | 200.7 | 100 | 17.06 | 200.45 | 100 | 9.02 | 10.3 | 4 | 2.78 | 1 | . | 75% | 127 | 16 | 216.2 | 113 | 36.75 | 233.775 | 114 | 19.8675 | 234.7 | 113 | 10.56 | 12 | 6 | 3.24 | 2 | . | max | 243 | 52 | 351.5 | 165 | 59.76 | 359.3 | 170 | 30.54 | 395 | 175 | 17.77 | 20 | 20 | 5.4 | 9 | . | ì¤‘ë³µê°’ í™•ì¸ . customer_df.duplicated().sum() . 0 . | uniqueí•œ ê°’ ìˆ˜ í™•ì¸ . customer_df.nunique() . state 51 account_length 215 area_code 3 international_plan 2 voice_mail_plan 2 number_vmail_messages 46 total_day_minutes 1843 total_day_calls 120 total_day_charge 1843 total_eve_minutes 1773 total_eve_calls 123 total_eve_charge 1572 total_night_minutes 1757 total_night_calls 128 total_night_charge 992 total_intl_minutes 168 total_intl_calls 21 total_intl_charge 168 number_customer_service_calls 10 churn 2 dtype: int64 . | . ",
    "url": "https://chaelist.github.io/docs/kaggle/customer_churn/#%EB%8D%B0%EC%9D%B4%ED%84%B0-%ED%8C%8C%EC%95%85",
    "relUrl": "/docs/kaggle/customer_churn/#ë°ì´í„°-íŒŒì•…"
  },"28": {
    "doc": "Telco Customer Churn",
    "title": "ë³€ìˆ˜ ê°„ ìƒê´€ê´€ê³„ íŒŒì•…",
    "content": "plt.figure(figsize=(14, 9)) sns.heatmap(customer_df.corr(), annot=True, cmap='Greens'); . | total_%_chargeì™€ total_%_minutesëŠ” ëª¨ë‘ ì—°ê´€ì„±ì´ ì•„ì£¼ ê°•í•¨ (ìƒê´€ê³„ìˆ˜ = 1) | minuteì— ì •í™•íˆ ë¹„ë¡€í•´ì„œ chargeê°€ ê²°ì •ë˜ëŠ” êµ¬ì¡°ì¸ ë“¯. | . Â  . â†’ â€˜total_%_chargeâ€™ í˜•íƒœì˜ ì´ë¦„ì„ ê°€ì§„ ì»¬ëŸ¼ì€ ì‚­ì œí•´ì¤Œ . | total_%_chargeëŠ” total_%_minutesì— ë¹„ë¡€í•´ì„œ ê²°ì •ë˜ëŠ” ê°’ì´ê¸°ì—, ë‘ ì¢…ë¥˜ì˜ ì»¬ëŸ¼ì„ ëª¨ë‘ ì‚´í´ë³¼ í•„ìš”ëŠ” ì—†ë‹¤ê³  íŒë‹¨ë¨. customer_df.drop(['total_day_charge', 'total_eve_charge', 'total_night_charge', 'total_intl_charge'], axis='columns', inplace=True) customer_df.head() . | . | Â  | state | account_length | area_code | international_plan | voice_mail_plan | number_vmail_messages | total_day_minutes | total_day_calls | total_eve_minutes | total_eve_calls | total_night_minutes | total_night_calls | total_intl_minutes | total_intl_calls | number_customer_service_calls | churn | . | 0 | OH | 107 | area_code_415 | no | yes | 26 | 161.6 | 123 | 195.5 | 103 | 254.4 | 103 | 13.7 | 3 | 1 | no | . | 1 | NJ | 137 | area_code_415 | no | no | 0 | 243.4 | 114 | 121.2 | 110 | 162.6 | 104 | 12.2 | 5 | 0 | no | . | 2 | OH | 84 | area_code_408 | yes | no | 0 | 299.4 | 71 | 61.9 | 88 | 196.9 | 89 | 6.6 | 7 | 2 | no | . | 3 | OK | 75 | area_code_415 | yes | no | 0 | 166.7 | 113 | 148.3 | 122 | 186.9 | 121 | 10.1 | 3 | 3 | no | . | 4 | MA | 121 | area_code_510 | no | yes | 24 | 218.2 | 88 | 348.5 | 108 | 212.6 | 118 | 7.5 | 7 | 3 | no | . ",
    "url": "https://chaelist.github.io/docs/kaggle/customer_churn/#%EB%B3%80%EC%88%98-%EA%B0%84-%EC%83%81%EA%B4%80%EA%B4%80%EA%B3%84-%ED%8C%8C%EC%95%85",
    "relUrl": "/docs/kaggle/customer_churn/#ë³€ìˆ˜-ê°„-ìƒê´€ê´€ê³„-íŒŒì•…"
  },"29": {
    "doc": "Telco Customer Churn",
    "title": "chrun ì—¬ë¶€ ì˜ˆì¸¡ì— ì£¼ìš”í•œ ë³€ìˆ˜ íŒŒì•…",
    "content": ": Logitsic Regressionìœ¼ë¡œ ê° ë³€ìˆ˜ì˜ coefficientë¥¼ ê³„ì‚°í•´, ì–´ë–¤ ë³€ìˆ˜ê°€ churn ì—¬ë¶€ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë°ì— ì¤‘ìš”í•˜ê²Œ ì‘ìš©í•˜ëŠ”ì§€ íŒŒì•… . Logistic Regression ëª¨ë¸ ê³„ì‚° . # scikit-learnì„ ì‚¬ìš© from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression from sklearn import preprocessing . | churn = yes / noë¥¼ 1ê³¼ 0ìœ¼ë¡œ ë³€í™˜í•œ ì¹¼ëŸ¼ì„ ë³„ë„ë¡œ ì €ì¥ . customer_df['churn_0_1'] = customer_df['churn'].apply(lambda x: 1 if x == 'yes' else 0) customer_df[['churn', 'churn_0_1']].head() . | Â  | churn | churn_0_1 | . | 0 | no | 0 | . | 1 | no | 0 | . | 2 | no | 0 | . | 3 | no | 0 | . | 4 | no | 0 | . | ì…ë ¥ë³€ìˆ˜(X), ëª©í‘œë³€ìˆ˜(y)ë¥¼ ë‚˜ëˆ„ì–´ ì €ì¥ . X = customer_df.iloc[:, :-2] # ~ 'number_customer_service_calls' ì»¬ëŸ¼ê¹Œì§€ ì €ì¥ y = customer_df.iloc[:, -1] # 'churn_0_1' ì»¬ëŸ¼ë§Œ ì €ì¥ . | ì¹´í…Œê³ ë¦¬ ë³€ìˆ˜ë¥¼ ë”ë¯¸ë³€ìˆ˜í™” . # ê°’ì´ 3ê°œ ì´ìƒì¸ ì»¬ëŸ¼ì€ pd.get_dummies()ë¡œ ë”ë¯¸ë³€ìˆ˜í™” X = pd.get_dummies(data=X, columns=['state', 'area_code']) # ê°’ì´ yes, no 2ê°œì¸ ì»¬ëŸ¼: yesë¥¼ 1, noë¥¼ 0ìœ¼ë¡œ ë³€í™˜ X['international_plan'] = X['international_plan'].apply(lambda x: 1 if x == 'yes' else 0) X['voice_mail_plan'] = X['voice_mail_plan'].apply(lambda x: 1 if x == 'yes' else 0) X.head() . | Â  | account_length | international_plan | voice_mail_plan | number_vmail_messages | total_day_minutes | total_day_calls | total_eve_minutes | total_eve_calls | total_night_minutes | total_night_calls | total_intl_minutes | total_intl_calls | number_customer_service_calls | state_AK | state_AL | state_AR | state_AZ | state_CA | state_CO | state_CT | state_DC | state_DE | state_FL | state_GA | state_HI | state_IA | state_ID | state_IL | state_IN | state_KS | state_KY | state_LA | state_MA | state_MD | state_ME | state_MI | state_MN | state_MO | state_MS | state_MT | state_NC | state_ND | state_NE | state_NH | state_NJ | state_NM | state_NV | state_NY | state_OH | state_OK | state_OR | state_PA | state_RI | state_SC | state_SD | state_TN | state_TX | state_UT | state_VA | state_VT | state_WA | state_WI | state_WV | state_WY | area_code_area_code_408 | area_code_area_code_415 | area_code_area_code_510 | . | 0 | 107 | 0 | 1 | 26 | 161.6 | 123 | 195.5 | 103 | 254.4 | 103 | 13.7 | 3 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | . | 1 | 137 | 0 | 0 | 0 | 243.4 | 114 | 121.2 | 110 | 162.6 | 104 | 12.2 | 5 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | . | 2 | 84 | 1 | 0 | 0 | 299.4 | 71 | 61.9 | 88 | 196.9 | 89 | 6.6 | 7 | 2 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | . | 3 | 75 | 1 | 0 | 0 | 166.7 | 113 | 148.3 | 122 | 186.9 | 121 | 10.1 | 3 | 3 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | . | 4 | 121 | 0 | 1 | 24 | 218.2 | 88 | 348.5 | 108 | 212.6 | 118 | 7.5 | 7 | 3 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | . | ì…ë ¥ë³€ìˆ˜ í‘œì¤€í™”(standardization) . | â€» Xê°’ì„ í‘œì¤€í™”í•´ì£¼ì§€ ì•Šìœ¼ë©´, ê³„ì‚°ë˜ëŠ” coefficientë¥¼ ë™ì¼ì„ ìƒì—ì„œ ë¹„êµí•  ìˆ˜ ì—†ìŒ | . scaler = preprocessing.StandardScaler() X_scaled = scaler.fit_transform(X) . | train_test_split &amp; í•™ìŠµ . X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2) y_train = y_train.values.ravel() #ravel(): ë‹¤ì°¨ì› arrayë¥¼ 1ì°¨ì› arrayë¡œ í‰í‰í•˜ê²Œ í´ì£¼ëŠ” í•¨ìˆ˜. logistic_model = LogisticRegression(solver='saga', max_iter=2000, penalty='l1') logistic_model.fit(X_train, y_train) logistic_model.score(X_test, y_test) # accuracy_score . 0.8670588235294118 . | . ë³€ìˆ˜ë³„ coefficient ë¹„êµ . # dfë¥¼ ë§Œë“¤ì–´ coefficientì˜ ì ˆëŒ€ê°’ì´ í° ìˆœì„œëŒ€ë¡œ ì •ë ¬ temp = pd.DataFrame(list(zip(X.columns, np.absolute(logistic_model.coef_[0]))), columns=['feature', 'coefficient']).sort_values('coefficient', ascending=False).reset_index() # coefficientê°€ í° Top 20 ë³€ìˆ˜ë§Œ ì‹œê°í™” plt.figure(figsize=(8, 6)) sns.barplot(data=temp.head(20), y='feature', x='coefficient', palette='Greens_r'); . | voice_mail_plan, total_day_minutes, number_customer_service_calls, international_plan, number_vmail_messagesê°€ ê³ ê° ì´íƒˆ ì˜ˆì¸¡ì— ì£¼ìš”í•œ ë³€ìˆ˜ë¼ê³  íŒë‹¨ë¨ | . ",
    "url": "https://chaelist.github.io/docs/kaggle/customer_churn/#chrun-%EC%97%AC%EB%B6%80-%EC%98%88%EC%B8%A1%EC%97%90-%EC%A3%BC%EC%9A%94%ED%95%9C-%EB%B3%80%EC%88%98-%ED%8C%8C%EC%95%85",
    "relUrl": "/docs/kaggle/customer_churn/#chrun-ì—¬ë¶€-ì˜ˆì¸¡ì—-ì£¼ìš”í•œ-ë³€ìˆ˜-íŒŒì•…"
  },"30": {
    "doc": "Telco Customer Churn",
    "title": "ë³€ìˆ˜ë³„ ë¹„êµ",
    "content": "ì£¼(state)ë³„ ì´íƒˆë¥  . | ì£¼ë³„ ì´íƒˆë¥  ë¹„êµ # churn: yes=1, no=0ì¸ ìƒíƒœì—ì„œ í‰ê· ì„ êµ¬í•˜ë©´ churn rateì™€ ê°™ë‹¤ churn_state = customer_df.groupby(['state'])[['churn_0_1']].mean() * 100 churn_state.rename(columns={'churn_0_1':'churn rate (%)'}, inplace=True) churn_state.reset_index(inplace=True) # ì´íƒˆë¥  ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬ churn_state.sort_values(by='churn rate (%)', ascending=False, inplace=True) churn_state.head() . | Â  | state | churn rate (%) | . | 31 | NJ | 27.0833 | . | 4 | CA | 25.641 | . | 47 | WA | 22.5 | . | 20 | MD | 22.093 | . | 26 | MT | 21.25 | . â†’ ì£¼ë³„ ì´íƒˆë¥  ì‹œê°í™”í•´ì„œ í™•ì¸ . plt.figure(figsize=(16, 6)) sns.barplot(data=churn_state, x='state', y='churn rate (%)', palette='Greens_r'); . | NJ(New Jersey)ì™€ CA(California)ê°€ íŠ¹íˆ churn rateì´ ë†’ì€ ì£¼ë¡œ íŒëª…ë¨ | VA(Virginia)ì™€ HI(Hawaii)ëŠ” íŠ¹íˆ churn rateì´ ë‚®ì€ ì£¼ë¡œ íŒëª…ë¨ | . Â  . | ì£¼ë³„ ì´íƒˆì ìˆ˜ ë¹„êµ . | â€˜ì´íƒˆë¥ â€™ì´ ë†’ì€ ì£¼ê°€ ì‹¤ì œ â€˜ì´íƒˆì ìˆ˜â€™ë„ ë§ì€ ê²ƒì¸ì§€, ë‹¨ìˆœíˆ ê°€ì…ì ìˆ˜ê°€ ì ì–´ì„œì¸ì§€ í™•ì¸í•˜ê¸° ìœ„í•¨ | . state_churn_count = pd.pivot_table(data=customer_df, index='state', columns='churn', values='account_length', aggfunc='count').reset_index() state_churn_count.sort_values(by='yes', ascending=False, inplace=True) state_churn_count.head() . | Â  | state | no | yes | . | 31 | NJ | 70 | 26 | . | 43 | TX | 79 | 19 | . | 23 | MN | 89 | 19 | . | 49 | WV | 120 | 19 | . | 20 | MD | 67 | 19 | . â†’ ì£¼ë³„ ì´íƒˆì ìˆ˜ ì‹œê°í™”í•´ì„œ í™•ì¸ . plt.figure(figsize=(16, 6)) sns.barplot(data=state_churn_count, x='state', y='yes', palette='Greens_r'); . | NJ(New Jersey)ëŠ” ì‹¤ì œë¡œë„ ì´íƒˆì ìˆ˜ê°€ ê°€ì¥ ë§ì€ ì£¼ë¡œ íŒëª…ë¨. íŠ¹íˆ ê³ ê° ê²½í—˜ ê´€ë¦¬ì— ì‹ ê²½ì„ ì“°ë©´ ì¢‹ì€ ì£¼ë¼ê³  ìƒê°. | VA(Virginia)ì™€ HI(Hawaii)ëŠ” ì‹¤ì œë¡œë„ ì´íƒˆì ìˆ˜ê°€ ë§¤ìš° ì ì€ ì£¼ë¡œ íŒëª…ë¨. | . | . planë³„ ì´íƒˆë¥  . : voice_mail_plan, international_plan ê³ ê° íƒ€ì…ë³„ ì´íƒˆë¥  . | voice_mail_plan ì‚¬ìš© ì—¬ë¶€ì— ë”°ë¥¸ ì´íƒˆë¥  . # churn: yes=1, no=0ì¸ ìƒíƒœì—ì„œ í‰ê· ì„ êµ¬í•˜ë©´ churn rateì™€ ê°™ë‹¤ sns.barplot(data=customer_df, x='voice_mail_plan', y='churn_0_1', palette='Greens'); . | voice_mail_planì„ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ê³ ê°ì˜ ì´íƒˆë¥ ì„ ì•½ 16%, ì‚¬ìš©í•˜ëŠ” ê³ ê°ì˜ ì´íƒˆë¥ ì€ ì•½ 7% | . â†’ ë¹„ìœ¨ ì°¨ì´ê°€ ìœ ì˜ë¯¸í•œì§€ t-test (ì‚¬ì‹¤ ë¹„ìœ¨ì€ ê²°êµ­ í‰ê· ê³¼ ê°™ì€ ê°œë…ì´ë¼ê³  í•  ìˆ˜ ìˆê¸°ì—, í‰ê·  ì°¨ì´ì²˜ëŸ¼ t-testë¥¼ í™œìš©í•´ë„ ê´œì°®ë‹¤) . temp1 = customer_df[customer_df['voice_mail_plan'] == 'yes'] temp2 = customer_df[customer_df['voice_mail_plan'] == 'no'] # Leveneì˜ ë“±ë¶„ì‚° ê²€ì • lev_result = stats.levene(temp1['churn_0_1'], temp2['churn_0_1']) print('LeveneResult(F) : %.2f \\np-value : %.3f' % (lev_result)) . LeveneResult(F) : 56.57 p-value : 0.000 . # ì´ë¶„ì‚°ì¸ ë…ë¦½í‘œë³¸ t-test ì‹¤í–‰ t_result = stats.ttest_ind(temp1['churn_0_1'], temp2['churn_0_1'], equal_var=False) print('t statistic : %.2f \\np-value : %.3f' % (t_result)) . t statistic : -8.84 p-value : 0.000 . | p &lt; 0.01ì´ê³ , ì‹œê°í™”í•´ì„œ ì‚´í´ë´¤ì„ ë•Œë„ ê½¤ ì°¨ì´ê°€ ë‚˜ë¯€ë¡œ, voice_mail_planì„ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ê³ ê°ì´ ë” ì´íƒˆë¥ ì´ ë†’ë‹¤ê³  í•  ìˆ˜ ìˆìŒ | . Â  . | international_plan ì‚¬ìš© ì—¬ë¶€ì— ë”°ë¥¸ ì´íƒˆë¥  . sns.barplot(data=customer_df, x='international_plan', y='churn_0_1', palette='Greens'); . | international_planì„ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ê³ ê°ì˜ ì´íƒˆë¥ ì„ ì•½ 11%, ì‚¬ìš©í•˜ëŠ” ê³ ê°ì˜ ì´íƒˆë¥ ì€ ì•½ 42% | . â†’ ë¹„ìœ¨ ì°¨ì´ê°€ ìœ ì˜ë¯¸í•œì§€ t-test . temp1 = customer_df[customer_df['international_plan'] == 'yes'] temp2 = customer_df[customer_df['international_plan'] == 'no'] # Leveneì˜ ë“±ë¶„ì‚° ê²€ì • lev_result = stats.levene(temp1['churn_0_1'], temp2['churn_0_1']) print('LeveneResult(F) : %.2f \\np-value : %.3f' % (lev_result)) . LeveneResult(F) : 305.58 p-value : 0.000 . # ì´ë¶„ì‚°ì¸ ë…ë¦½í‘œë³¸ t-test ì‹¤í–‰ t_result = stats.ttest_ind(temp1['churn_0_1'], temp2['churn_0_1'], equal_var=False) print('t statistic : %.2f \\np-value : %.3f' % (t_result)) . t statistic : 12.22 p-value : 0.000 . | p &lt; 0.01ì´ê³ , ì‹œê°í™”í•´ì„œ ì‚´í´ë´¤ì„ ë•Œë„ í° ì°¨ì´ê°€ ë‚˜ë¯€ë¡œ, international_planì„ ì‚¬ìš©í•˜ëŠ” ê³ ê°ì´ ë” ì´íƒˆë¥ ì´ ë†’ë‹¤ê³  í•  ìˆ˜ ìˆìŒ | international_planì„ ì‚¬ìš©í•˜ëŠ” ê³ ê°ì˜ ì´íƒˆë¥ ì€ 42%ë‚˜ ë˜ë¯€ë¡œ, í”Œëœì˜ ë¶ˆë§Œì¡± ìš”ì¸ì„ ì „ë°˜ì ìœ¼ë¡œ ì ê²€í•´ë³¼ í•„ìš”ê°€ ìˆë‹¤ê³  ìƒê°ë¨. | . | . ë‚® í†µí™”ì‹œê°„ì— ë”°ë¥¸ ì´íƒˆë¥  . | ì´íƒˆ ì—¬ë¶€ì— ë”°ë¥¸ total_day_minutes ë¶„í¬ &amp; í‰ê·  ë¹„êµ . fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(10, 5)) sns.boxplot(data=customer_df, x='churn', y='total_day_minutes', palette='Greens', ax=ax1) sns.barplot(data=customer_df, x='churn', y='total_day_minutes', palette='Greens', ax=ax2) plt.close(2) plt.close(3) plt.tight_layout() . | ì´íƒˆí•œ ê³ ê°ë“¤ì´ ì „ë°˜ì ìœ¼ë¡œ total_day_minutesê°€ ë†’ì€ í¸ìœ¼ë¡œ ë³´ì„ | . | total_day_minutes 4ë¶„ìœ„ë³„ ì´íƒˆë¥  . # ì‚¬ë¶„ìœ„ë¥¼ ê³„ì‚°í•´ì„œ ì¹¼ëŸ¼ì„ ìƒˆë¡œ ìƒì„± temp_df = customer_df[['total_day_minutes', 'churn', 'churn_0_1']] q1, q2, q3 = np.percentile(temp_df['total_day_minutes'], [25, 50, 75]) def get_quarter(x): if x &lt; q1: quarter = '1st_q' elif x &lt; q2: quarter = '2nd_q' elif x &lt; q3: quarter = '3rd_q' else: quarter = '4th_q' return quarter temp_df['total_day_minutes_quartile'] = temp_df['total_day_minutes'].apply(lambda x: get_quarter(x)) temp_df.head() . | Â  | total_day_minutes | churn | churn_0_1 | total_day_minutes_quartile | . | 0 | 161.6 | no | 0 | 2nd_q | . | 1 | 243.4 | no | 0 | 4th_q | . | 2 | 299.4 | no | 0 | 4th_q | . | 3 | 166.7 | no | 0 | 2nd_q | . | 4 | 218.2 | no | 0 | 4th_q | . â†’ total_day_minutes ì‚¬ë¶„ìœ„ë³„ churn rate ì‹œê°í™”í•´ ë¹„êµ . sns.barplot(data=temp_df, y='total_day_minutes_quartile', x='churn_0_1', order=['4th_q', '3rd_q', '2nd_q', '1st_q'], palette='Greens'); . | íŠ¹íˆ total_day_minutesê°€ ê°€ì¥ ë§ì€ ì‚¬ë¶„ìœ„ì— ì†í•˜ëŠ” ê³ ê°ë“¤ì´ ë†’ì€ ì´íƒˆë¥ ì„ ë³´ì„. | . | . customer service callsì— ë”°ë¥¸ ì´íƒˆë¥  . | ì´íƒˆ ì—¬ë¶€ì— ë”°ë¥¸ number_customer_service_calls ë¶„í¬ &amp; í‰ê·  ë¹„êµ . fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(10, 5)) sns.boxplot(data=customer_df, x='churn', y='number_customer_service_calls', palette='Greens', ax=ax1) sns.barplot(data=customer_df, x='churn', y='number_customer_service_calls', palette='Greens', ax=ax2) plt.close(2) plt.close(3) plt.tight_layout() . | ì´íƒˆí•œ ê³ ê°ë“¤ì´ ì „ë°˜ì ìœ¼ë¡œ number_customer_service_callsê°€ ë§ì•˜ë˜ ê²ƒìœ¼ë¡œ ë³´ì„ | . | number_customer_service_callsë³„ ì´íƒˆìì™€ ë¹„ì´íƒˆì ë¶„í¬ . plt.figure(figsize=(8, 6)) sns.countplot(data=customer_df, x='number_customer_service_calls', hue='churn', palette='Greens_r'); . | number_customer_service_calls ê·¸ë£¹ë³„ ì´íƒˆë¥  . # ì „í™” íšŸìˆ˜ë¥¼ 4ë²ˆ ì´ìƒ / 3ë²ˆ ì´í•˜ë¡œ ë‚˜ëˆ„ì–´ flagë¥¼ ë¶™ì„ temp_df = customer_df[['number_customer_service_calls', 'churn', 'churn_0_1']] temp_df['customer_service_calls_flag'] = temp_df['number_customer_service_calls'].apply(lambda x: 'more than 4' if x &gt;= 4 else 'less than 3') # number_customer_service_calls ê·¸ë£¹ë³„ churn rate ë¹„êµ sns.barplot(data=temp_df, y='customer_service_calls_flag', x='churn_0_1', palette='Greens'); . | customer service callì„ 4ë²ˆ ì´ìƒ í•œ ê³ ê°ì˜ 50%ê°€ ì´íƒˆ â†’ customer service callì„ 4ë²ˆ ì´ìƒ í•œ ê³ ê°ì€ íŠ¹ë³„í•œ ê³ ê° ê²½í—˜ ê´€ë¦¬ê°€ í•„ìš”í•˜ë‹¤ê³  ìƒê°ë¨. | . | . ë³´ì´ìŠ¤ë©”ì¼ ë©”ì‹œì§€ ìˆ˜ì— ë”°ë¥¸ ì´íƒˆë¥  . | ì´íƒˆ ì—¬ë¶€ì— ë”°ë¥¸ number_vmail_messages ë¶„í¬ &amp; í‰ê·  ë¹„êµ . fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(10, 5)) sns.boxplot(data=customer_df, x='churn', y='number_vmail_messages', palette='Greens', ax=ax1) sns.barplot(data=customer_df, x='churn', y='number_vmail_messages', palette='Greens', ax=ax2) plt.close(2) plt.close(3) plt.tight_layout() . | ì´íƒˆí•œ ê³ ê° ì¤‘ voice_mail_planì„ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ê²½ìš°ê°€ ë§ì•„ì„œ vmail_messagesê°€ 0ì¸ ê²½ìš°ê°€ ë§ì€ ë“¯. | . | â€˜voice_mail_plan = noâ€™ì¸ ê²½ìš°ë¥¼ ì œì™¸í•˜ê³  ë¹„êµ . | voice_mail_planì„ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ê²½ìš°, vmail_messagesê°€ 0ì´ê¸° ë•Œë¬¸ | . vmail_customer = customer_df[customer_df['voice_mail_plan'] == 'yes'] fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(10, 5)) sns.boxplot(data=vmail_customer, x='churn', y='number_vmail_messages', palette='Greens', ax=ax1) sns.barplot(data=vmail_customer, x='churn', y='number_vmail_messages', palette='Greens', ax=ax2) plt.close(2) plt.close(3) plt.tight_layout() . | voice_mail_planì„ ì‚¬ìš©í•œ ê³ ê°ë§Œìœ¼ë¡œ ë¹„êµí•˜ë©´, ì´íƒˆ ì—¬ë¶€ì— ë”°ë¼ number_vmail_messagesê°€ í¬ê²Œ ë‹¬ë¼ì§€ì§€ëŠ” ì•ŠëŠ” ê²ƒìœ¼ë¡œ í™•ì¸ë¨. | . | number_vmail_messagesë³„ ì´íƒˆìì™€ ë¹„ì´íƒˆì ë¶„í¬ . | number_vmail_messsages = 0ì¸ ê²½ìš°ëŠ” ì œì™¸í•˜ê³  ì‹œê°í™” (0ì¸ ê²½ìš°ê°€ ì••ë„ì ìœ¼ë¡œ ë§ì•„ì„œ) | . plt.figure(figsize=(15, 6)) sns.countplot(data=customer_df[customer_df['number_vmail_messages'] != 0], x='number_vmail_messages', hue='churn', palette='Greens_r'); . | number_vmail_messagesë³„ ì´íƒˆë¥  ë¹„êµ . plt.figure(figsize=(15, 6)) sns.barplot(data=customer_df, x='number_vmail_messages', y ='churn_0_1', palette='Greens_r', ci=None); . | number_vmail_messages = 0ì¸ ê²½ìš°ë¥¼ ì œì™¸í•˜ë©´, number_vmail_messagesê°€ ë‚®ë‹¤ê³  ì´íƒˆìê°€ ë§ì€ ê²ƒì€ ì•„ë‹˜ | number_vmail_messagesë§Œ ê°€ì§€ê³ ëŠ” ì´íƒˆí•  ê³ ê°ì¸ì§€ íŒë³„í•˜ê¸° ì–´ë ¤ìš¸ ë“¯ | . | . ",
    "url": "https://chaelist.github.io/docs/kaggle/customer_churn/#%EB%B3%80%EC%88%98%EB%B3%84-%EB%B9%84%EA%B5%90",
    "relUrl": "/docs/kaggle/customer_churn/#ë³€ìˆ˜ë³„-ë¹„êµ"
  },"31": {
    "doc": "Data Handling",
    "title": "Data Handling",
    "content": " ",
    "url": "https://chaelist.github.io/docs/data_handling",
    "relUrl": "/docs/data_handling"
  },"32": {
    "doc": "Datetime ë‹¤ë£¨ê¸°",
    "title": "Datetime ë‹¤ë£¨ê¸°",
    "content": ". | datetime ê°ì²´ ìƒì„± | datetime ì •ë³´ ì¶œë ¥ . | â€» strftime(), strptime() í¬ë§· ì½”ë“œ | . | datetime ì—°ì‚° | . ",
    "url": "https://chaelist.github.io/docs/data_handling/datetime/",
    "relUrl": "/docs/data_handling/datetime/"
  },"33": {
    "doc": "Datetime ë‹¤ë£¨ê¸°",
    "title": "datetime ê°ì²´ ìƒì„±",
    "content": ". | ì§ì ‘ ì •ë³´ë¥¼ ì…ë ¥í•´ ìƒì„± . import datetime as dt # importí•´ì„œ ì‚¬ìš©. ë³„ë„ì˜ ì„¤ì¹˜ëŠ” í•„ìš” ì—†ë‹¤ custom_date = dt.datetime(2020, 11, 12) # 2020ë…„ 11ì›” 22ì¼ print(custom_date) # ì‹œê°„ ì •ë³´ë¥¼ ì¶”ê°€í•˜ì§€ ì•Šìœ¼ë©´ ìë™ìœ¼ë¡œ 00ì‹œ 00ë¶„ 00ì´ˆë¡œ ìƒì„±ë¨ . 2020-11-12 00:00:00 . +) ì‹œê°„ ì •ë³´ ì¶”ê°€: . custom_date = dt.datetime(2020, 11, 12, 10, 11, 12) print(custom_date) . 2020-11-12 10:11:12 . | í˜„ì¬ datetimeìœ¼ë¡œ ìƒì„± . now = dt.datetime.now() print(now) . 2022-01-09 06:16:51.301662 . +) ì‹œê°„ê¹Œì§€ ì„¸ì„¸í•˜ê²Œ ê°€ì ¸ì˜¤ëŠ” ëŒ€ì‹ , ì˜¤ëŠ˜ì˜ ë‚ ì§œë§Œ ê°€ì ¸ì˜¤ê¸°: . now = dt.date.today() print(now) . 2022-01-09 . | íŠ¹ì • str í¬ë§·ìœ¼ë¡œë¶€í„° ìƒì„± . | datetime.datetim.strptime('str í˜•íƒœì˜ ë‚ ì§œ', 'í¬ë§·')ì˜ êµ¬ì¡°ë¡œ ì‘ì„± | . str_date = '2020-11-12' custom_date = dt.datetime.strptime(str_date, '%Y-%m-%d') print(custom_date) . 2020-11-12 00:00:00 . | . ",
    "url": "https://chaelist.github.io/docs/data_handling/datetime/#datetime-%EA%B0%9D%EC%B2%B4-%EC%83%9D%EC%84%B1",
    "relUrl": "/docs/data_handling/datetime/#datetime-ê°ì²´-ìƒì„±"
  },"34": {
    "doc": "Datetime ë‹¤ë£¨ê¸°",
    "title": "datetime ì •ë³´ ì¶œë ¥",
    "content": ". | ë‚ ì§œ, ì‹œê°„ ì •ë³´ ì¶”ì¶œ . now = dt.datetime.now() print(now.date()) print(now.time()) . 2022-01-09 06:16:51.301662 . +) ì—°ë„, ì›”, ì¼, ì‹œê°„, ë¶„, ì´ˆ ëª¨ë‘ êµ¬ë¶„í•´ì„œ ì¶”ì¶œ: . print(now.year) print(now.month) print(now.day) print(now.hour) print(now.minute) print(now.second) . 2022 1 9 6 16 51 . | ìš”ì¼ ì •ë³´ ì¶”ì¶œ . print(now.weekday()) # 0: ì›”ìš”ì¼ ~ 6: ì¼ìš”ì¼ print(now.isoweekday()) # ISO ë‹¬ë ¥ í˜•ì‹: 1: ì›”ìš”ì¼ ~ 7: ì¼ìš”ì¼ . 6 7 . | íŠ¹ì • í¬ë§·ìœ¼ë¡œ ì •ë³´ ì¶œë ¥í•˜ê¸° . print('isoformat:', now.isoformat()) # yyyy-mm-dd'T'HH:mm:ss.SSSXXX í˜•íƒœ print('ctime:', now.ctime()) # ìš”ì¼ ì›” ì¼ì HH:mm:ss ì—°ë„ í˜•íƒœ . isoformat: 2022-01-09T06:16:51.301662 ctime: Sun Jan 9 06:16:51 2022 . | ì›í•˜ëŠ” str í¬ë§·ìœ¼ë¡œ ì •ë³´ ì¶œë ¥í•˜ê¸° . | strptime()ê³¼ ë°˜ëŒ€ì˜ ê°œë… | datetime_object.strftime('í¬ë§·')ì˜ êµ¬ì¡°ë¡œ ì‘ì„± | . print(now.strftime('%Y.%m.%d')) print(now.strftime('%Y/%m/%d %Hì‹œ %Më¶„ %Sì´ˆ')) . 2022.01.09 2022/01/09 06ì‹œ 16ë¶„ 51ì´ˆ . | . â€» strftime(), strptime() í¬ë§· ì½”ë“œ . | í¬ë§·ì½”ë“œ | ì„¤ëª… | ì˜ˆ | . | %a | ìš”ì¼ ì¤„ì„ë§ | Sun, Mon, â€¦ Sat | . | %A | ìš”ì¼ | Sunday, Monday, â€¦, Saturday | . | %w | ìš”ì¼ì„ ìˆ«ìë¡œ í‘œì‹œ, ì›”ìš”ì¼~ì¼ìš”ì¼, 0~6 | 0, 1, â€¦, 6 | . | %d | ì¼ | 01, 02, â€¦, 31 | . | %b | ì›” ì¤„ì„ë§ | Jan, Feb, â€¦, Dec | . | %B | ì›” | January, February, â€¦, December | . | %m | ìˆ«ì ì›” | 01, 02, â€¦, 12 | . | %y | ë‘ ìë¦¿ìˆ˜ ì—°ë„ | 01, 02, â€¦, 99 | . | %Y | ë„¤ ìë¦¿ìˆ˜ ì—°ë„ | 0001, 0002, â€¦, 2017, 2018, 9999 | . | %H | ì‹œê°„(24ì‹œê°„) | 00, 01, â€¦, 23 | . | %I | ì‹œê°„(12ì‹œê°„) | 01, 02, â€¦, 12 | . | %p | AM, PM | AM, PM | . | %M | ë¶„ | 00, 01, â€¦, 59 | . | %S | ì´ˆ | 00, 01, â€¦, 59 | . | %Z | ì‹œê°„ëŒ€ | (ë¹„ì–´ìˆìŒ)1), UTC, GMT | . | %j | 1ì›” 1ì¼ë¶€í„° ê²½ê³¼í•œ ì¼ìˆ˜ | 001, 002, â€¦, 366 | . | %U | 1ë…„ì¤‘ ì£¼ì°¨, ì¼ìš”ì¼ì´ í•œ ì£¼ì˜ ì‹œì‘ìœ¼ë¡œ | 00, 01, â€¦, 53 | . | %W | 1ë…„ì¤‘ ì£¼ì°¨, ì›”ìš”ì¼ì´ í•œ ì£¼ì˜ ì‹œì‘ìœ¼ë¡œ | 00, 01, â€¦, 53 | . | %c | ë‚ ì§œ, ìš”ì¼, ì‹œê°„ì„ ì¶œë ¥, í˜„ì¬ ì‹œê°„ëŒ€ ê¸°ì¤€ | Sat May 19 11:14:27 2018 | . | %x | ë‚ ì§œë¥¼ ì¶œë ¥, í˜„ì¬ ì‹œê°„ëŒ€ ê¸°ì¤€ | 05/19/18 | . | %X | ì‹œê°„ì„ ì¶œë ¥, í˜„ì¬ ì‹œê°„ëŒ€ ê¸°ì¤€ | â€˜11:44:22â€™ | . 1) naive datetime objectì˜ ê²½ìš°(timezone ì •ë³´ê°€ ë“¤ì–´ ìˆì§€ ì•Šì€ ê²½ìš°), ì•„ë¬´ê²ƒë„ ì¶œë ¥ë˜ì§€ ì•ŠëŠ”ë‹¤ . ",
    "url": "https://chaelist.github.io/docs/data_handling/datetime/#datetime-%EC%A0%95%EB%B3%B4-%EC%B6%9C%EB%A0%A5",
    "relUrl": "/docs/data_handling/datetime/#datetime-ì •ë³´-ì¶œë ¥"
  },"35": {
    "doc": "Datetime ë‹¤ë£¨ê¸°",
    "title": "datetime ì—°ì‚°",
    "content": ". | datetime ê°„ ë¹„êµ . | datetime objectëŠ” intì²˜ëŸ¼ &gt;, &lt;, ==ë¥¼ ì‚¬ìš©í•´ ë¹„êµí•  ìˆ˜ ìˆë‹¤ | . date1 = dt.datetime(2020, 11, 12) date2 = dt.datetime(2021, 1, 19) if date1 &gt; date2: print('date1ì´ ë” ë‚˜ì¤‘') elif date1 &lt; date2: print('date2ê°€ ë” ë‚˜ì¤‘') . date2ê°€ ë” ë‚˜ì¤‘ . | datetime - datetime . | ë‘ ì‹œì  ì‚¬ì´ì˜ ê¸°ê°„ì´ timedelta í˜•íƒœë¡œ ì €ì¥ë¨ | . period = date2 - date1 # ë‘ ì‹œì  ì‚¬ì´ì˜ ê¸°ê°„ì´ timedelta objectë¡œ ì €ì¥ë¨ print(period) print(type(period)) print(period.days) . 68 days, 0:00:00 &lt;class 'datetime.timedelta'&gt; 68 . +) datetime Â± timedelta: . timedelta1 = dt.timedelta(days=2, hours=5) print(date1 + timedelta1) print(date1 - timedelta1) . 2020-11-14 05:00:00 2020-11-09 19:00:00 . | . ",
    "url": "https://chaelist.github.io/docs/data_handling/datetime/#datetime-%EC%97%B0%EC%82%B0",
    "relUrl": "/docs/data_handling/datetime/#datetime-ì—°ì‚°"
  },"36": {
    "doc": "ë°ì´í„°ë² ì´ìŠ¤ & í…Œì´ë¸” êµ¬ì¶•",
    "title": "ë°ì´í„°ë² ì´ìŠ¤ &amp; í…Œì´ë¸” êµ¬ì¶•",
    "content": ". | ë°ì´í„°ë² ì´ìŠ¤ ìƒì„± | í…Œì´ë¸” ìƒì„± . | ì¹¼ëŸ¼ì˜ ì†ì„± | ì¹¼ëŸ¼ì˜ ë°ì´í„° íƒ€ì… | . | í…Œì´ë¸”ì— row ì¶”ê°€/ì—…ë°ì´íŠ¸/ì‚­ì œ | . ",
    "url": "https://chaelist.github.io/docs/sql/db_table_create/#%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B2%A0%EC%9D%B4%EC%8A%A4--%ED%85%8C%EC%9D%B4%EB%B8%94-%EA%B5%AC%EC%B6%95",
    "relUrl": "/docs/sql/db_table_create/#ë°ì´í„°ë² ì´ìŠ¤--í…Œì´ë¸”-êµ¬ì¶•"
  },"37": {
    "doc": "ë°ì´í„°ë² ì´ìŠ¤ & í…Œì´ë¸” êµ¬ì¶•",
    "title": "ë°ì´í„°ë² ì´ìŠ¤ ìƒì„±",
    "content": ". | CREATE DATABASEë¡œ ìƒì„± CREATE DATABASE ë°ì´í„°ë² ì´ìŠ¤ëª…; . | +) ì´ë¯¸ ì €ì¥ë˜ì–´ ìˆëŠ” ë°ì´í„°ë² ì´ìŠ¤ëª…ìœ¼ë¡œ ë˜ ìƒì„±í•  ê²½ìš°ì˜ error í”¼í•˜ê¸°: CREATE DATABASE IF NOT EXISTS ë°ì´í„°ë² ì´ìŠ¤ëª…; . | . ",
    "url": "https://chaelist.github.io/docs/sql/db_table_create/#%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B2%A0%EC%9D%B4%EC%8A%A4-%EC%83%9D%EC%84%B1",
    "relUrl": "/docs/sql/db_table_create/#ë°ì´í„°ë² ì´ìŠ¤-ìƒì„±"
  },"38": {
    "doc": "ë°ì´í„°ë² ì´ìŠ¤ & í…Œì´ë¸” êµ¬ì¶•",
    "title": "í…Œì´ë¸” ìƒì„±",
    "content": ". | CREATE TABLEë¡œ ìƒì„± CREATE TABLE `ë°ì´í„°ë² ì´ìŠ¤ëª…`.`í…Œì´ë¸”ëª…` ( `id` INT NOT NULL AUTO_INCREMENT, `name` VARCHAR(50) NOT NULL, `follower_count` INT NULL, PRIMARY KEY (id)); . | í…Œì´ë¸” ì—­ì‹œ, CREATE TABLE IF NOT EXISTSë¡œ ì‘ì„±í•˜ë©´ ì´ë¯¸ ì €ì¥ëœ ì´ë¦„ìœ¼ë¡œ ì¤‘ë³µ ìƒì„±í•  ë•Œì˜ errorë¥¼ í”¼í•  ìˆ˜ ìˆë‹¤ | ì–´ë–¤ ë°ì´í„°ë² ì´ìŠ¤ ë‚´ì— ìƒì„±í• ì§€ ëª…í™•íˆ í‘œì‹œí–ˆë‹¤ë©´, CREATE TABLE í…Œì´ë¸”ëª…ì´ë¼ê³ ë§Œ ì ëŠ” ê²ƒë„ ê°€ëŠ¥ CREATE TABLE IF NOT EXISTS `í…Œì´ë¸”ëª…` ( `id` INT NOT NULL AUTO_INCREMENT, `name` VARCHAR(50) NOT NULL, `follower_count` INT NULL, PRIMARY KEY (id)); . | â€» backtick(`): ì‹ë³„ì(identifier)ì„ì„ ë‚˜íƒ€ë‚´ê¸° ìœ„í•´ ì‚¬ìš©í•˜ëŠ” ê¸°í˜¸ . | identifier: ë°ì´í„°ë² ì´ìŠ¤, í…Œì´ë¸”, ì¹¼ëŸ¼ ë“±ì˜ objectì— ë¶™ëŠ” ì´ë¦„ | ì‹ë³„ìë¥¼ `ë¡œ ê°ì‹¸ì£¼ì§€ ì•Šì•„ë„ ì‹¤í–‰ì—ëŠ” ë¬¸ì œê°€ ì—†ìŒ. ì´ë¦„ì„ì„ í™•ì‹¤íˆ ë“œëŸ¬ë‚´ê³  ì‹¶ì–´ì„œ ì¨ì£¼ëŠ” ê²½ìš°ê°€ ë§ì„ ë¿. | cf) â€˜(ì‘ì€ ë”°ì˜´í‘œ)ë‚˜ â€œ(í° ë”°ì˜´í‘œ)ëŠ” ë¬¸ìì—´ ê°’ì„ ë‚˜íƒ€ë‚¼ ë•Œ ì‚¬ìš©ë˜ë‹ˆ, `ì™€ êµ¬ë¶„í•´ì„œ ì‚¬ìš© | . | . Â  . ì¹¼ëŸ¼ì˜ ì†ì„± . | PRIMARY KEY(PK): í•´ë‹¹ í…Œì´ë¸”ì—ì„œ ê° rowë¥¼ ì‹ë³„í•  ìˆ˜ ìˆê²Œ í•´ì£¼ëŠ” ì¹¼ëŸ¼. í…Œì´ë¸”ë³„ë¡œ 1ê°œì”©ë§Œ ì§€ì • ê°€ëŠ¥í•˜ë‹¤ | NOT NULL(NN): ì¹¼ëŸ¼ì— NULLê°’ì„ í—ˆìš©í•˜ì§€ ì•Šê² ë‹¤ëŠ” ì œí•œ ì¡°ê±´ PRIMARY KEYëŠ” ë°˜ë“œì‹œ NOT NULLì´ì—¬ì•¼ í•œë‹¤ | AUTO_INCREMENT(AI): ë”°ë¡œ ê°’ì„ ì…ë ¥í•´ì£¼ì§€ ì•Šì•„ë„ ìë™ìœ¼ë¡œ ì¹¼ëŸ¼ì˜ ê°’ì´ 1ì”© ëŠ˜ì–´ë‚œë‹¤. | PRIMARY KEYë¡œ ì“°ëŠ” ì¹¼ëŸ¼(ex.id ì¹¼ëŸ¼)ì˜ ê²½ìš°, AUTO_INCREMENT ì†ì„±ì„ ë‹¬ì•„ì£¼ëŠ” ê²ƒì´ í¸í•˜ë‹¤ | . | . Â  . ì¹¼ëŸ¼ì˜ ë°ì´í„° íƒ€ì… . Â  . 1. Numeric types (ìˆ«ìí˜• íƒ€ì…) . 1) ì •ìˆ˜í˜• íƒ€ì… . | TINYINT: ì‘ì€ ë²”ìœ„ì˜ ì •ìˆ˜ë¥¼ ì €ì¥í•  ë•Œ ì“°ëŠ” ë°ì´í„° íƒ€ì… . | ìµœì†Œ -128 ~ ìµ€ëŒ€ 127ê¹Œì§€ì˜ ì •ìˆ˜ë¥¼ ì €ì¥ ê°€ëŠ¥ | â€» SIGNED: ì–‘ìˆ˜-0-ìŒìˆ˜ / UNSIGNED: 0-ì–‘ìˆ˜ë¥¼ ì˜ë¯¸ | TINYINT SIGNED: -128 ~ 127 | TINYINT UNSIGNED: 0 ~ 255 | *ê·¸ëƒ¥ TINYINTë¼ê³ ë§Œ í•˜ë©´ SIGNEDê°€ ë¶™ì€ ê²ƒìœ¼ë¡œ ê°„ì£¼í•˜ëŠ” ê²ƒì´ default. | . | SMALLINT . | SMALLINT SIGNED: -32768 ~ 32767 | SMALLINT UNSIGNED : 0 ~ 65535 | . | MEDIUMINT . | MEDIUMINT SIGNED : -8388608 ~ 8388607 | MEDIUMINT UNSIGNED : 0 ~ 16777215 | . | INT . | INT SIGNED : -2147483648 ~ 2147483647 | INT UNSIGNED : 0 ~ 4294967295 | . | BIGINT . | BIGINT SIGNED : -9223372036854775808 ~ 9223372036854775807 | BIGINT UNSIGNED : 0 ~ 18446744073709551615 | . | . 2) ì‹¤ìˆ˜í˜• íƒ€ì… . | DECIMAL: ë³´í†µ DECIMAL(M, D)ì˜ í˜•ì‹ìœ¼ë¡œ ë‚˜íƒ€ë‚´ëŠ”ë°, Mì€ ìµœëŒ€ë¡œ ì“¸ ìˆ˜ ìˆëŠ” ì „ì²´ ìˆ«ìì˜ ìë¦¬ìˆ˜, DëŠ” ìµœëŒ€ë¡œ ì“¸ ìˆ˜ ìˆëŠ” ì†Œìˆ˜ì  ë’¤ ìë¦¬ìˆ˜ë¥¼ ì˜ë¯¸ . | ex) DECIMAL (5, 2)ë¼ë©´ -999.99 ë¶€í„° 999.99 ê¹Œì§€ì˜ ì‹¤ìˆ˜ê°€ ê°€ëŠ¥ | Mì€ ìµœëŒ€ 65, DëŠ” 30ê¹Œì§€ì˜ ê°’ì„ ê°€ì§ˆ ìˆ˜ ìˆë‹¤ | DECIMALì´ë¼ëŠ” ë‹¨ì–´ ëŒ€ì‹  DEC, NUMERIC, FIXEDë¥¼ ì¨ë„ ëœë‹¤ | . | FLOAT . | -3.402823466E+38 ~ -1.175494351E-38, 0, 1.175494351E-38 ~ 3.402823466E+38 ë²”ìœ„ì˜ ì‹¤ìˆ˜ë“¤ì„ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆëŠ” ë°ì´í„° íƒ€ì… | fyi) -3.402823466E+38 ì€ (-3.402823466) X (10ì˜ 38ì œê³±), -1.175494351E-38 ì€ (-1.175494351) X (10ì˜ 38ì œê³± ë¶„ì˜ 1) | . | DOUBLE . | -1.7976931348623157E+308 ~ -2.2250738585072014E-308, 0, .2250738585072014E-308 ~ 1.7976931348623157E+308 ë²”ìœ„ì˜ ì‹¤ìˆ˜ë“¤ì„ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆëŠ” ë°ì´í„° íƒ€ì… | . | . 2. ë‚ ì§œ ë° ì‹œê°„ íƒ€ì… (Date and Time Types) . | DATE: ë‚ ì§œë¥¼ ì €ì¥í•˜ëŠ” ë°ì´í„° íƒ€ì… . | YYYY-MM-DD í˜•ì‹ìœ¼ë¡œ í‘œê¸° - ex. â€˜2020-03-26â€™ | . | DATETIME: ë‚ ì§œì™€ ì‹œê°„ì„ ì €ì¥í•˜ëŠ” ë°ì´í„° íƒ€ì… . | YYYY-MM-DD HH:MM:SS í˜•ì‹ìœ¼ë¡œ í‘œê¸° - ex. â€˜2020-03-26 09:30:27â€™ | . | TIMESTAMP: ë‚ ì§œì™€ ì‹œê°„ì„ ì €ì¥í•˜ëŠ” ë°ì´í„° íƒ€ì… . | DATETIMEê³¼ ë§ˆì°¬ê°€ì§€ë¡œ YYYY-MM-DD HH:MM:SS í˜•ì‹ìœ¼ë¡œ í‘œê¸° | . â€» DATETIMEê³¼ TIMESTAMPì˜ ì°¨ì´ . | TIMESTAMPëŠ” íƒ€ì„ ì¡´(time_zone) ì •ë³´ë„ í•¨ê»˜ ì €ì¥í•œë‹¤ëŠ” ì ì—ì„œ DATETIMEê³¼ ì°¨ë³„í™”ëœë‹¤ | ex) SET time_zone = '-11:00'; ì´ë ‡ê²Œ ì‹œê°„ëŒ€ë¥¼ UTC-11ë¡œ ë°”ê¿”ì£¼ë©´ DATETIMEìœ¼ë¡œ ì €ì •í•œ ì‹œê°„ì€ ê·¸ëŒ€ë¡œì§€ë§Œ, TIMESTAMPë¡œ ì €ì¥í•œ ì‹œê°„ì€ ë°”ë€œ (TIMESTAMPë¡œ ì €ì¥í•  ë‹¹ì‹œì—ëŠ” UTC+9 ì‹œê°„ëŒ€ë¡œ ì €ì¥ë¨) | *UTC (Coordinated Universal Time): êµ­ì œ ì‚¬íšŒì—ì„œ í†µìš©ë˜ëŠ” í‘œì¤€ ì‹œê°„ ì²´ê³„ë¡œ â€˜êµ­ì œ í‘œì¤€ì‹œâ€™ë¼ê³ ë„ í•¨. ì˜êµ­ ëŸ°ë˜ì´ ê¸°ì¤€. â†’ í•œêµ­ì€ UTC+9 ì‹œê°„ëŒ€ | . | TIME: ì‹œê°„ì„ ë‚˜íƒ€ë‚´ëŠ” ë°ì´í„° íƒ€ì…. | HH:MM:SSì˜ í˜•ì‹ìœ¼ë¡œ í‘œê¸° - ex. â€˜09:27:31â€™ | . | . 3. ë¬¸ìì—´ íƒ€ì… (String type) . | CHAR: ë¬¸ìì—´ì„ ë‚˜íƒ€ë‚´ëŠ” ê¸°ë³¸ íƒ€ì…ìœ¼ë¡œ, Characterì˜ ì¤„ì„ë§ . | CHAR(30) ì´ëŸ°ì‹ìœ¼ë¡œ í‘œê¸°í•˜ëŠ”ë°, ê´„í˜¸ ì•ˆì˜ ìˆ«ìëŠ” ë¬¸ìë¥¼ ìµœëŒ€ ëª‡ ìê¹Œì§€ ì €ì¥í•  ìˆ˜ ìˆëŠ”ì§€ë¥¼ ì˜ë¯¸ (30ì´ë¼ê³  ì“°ë©´ ìµœëŒ€ 30ìì˜ ë¬¸ìì—´ì„ ì €ì¥í•  ìˆ˜ ìˆë‹¤ëŠ” ëœ») | () ì•ˆì—ëŠ” 0ë¶€í„° 255ê¹Œì§€ì˜ ìˆ«ìë¥¼ ì ì„ ìˆ˜ ìˆìŒ | . | VARCHAR: CHARì²˜ëŸ¼ ë¬¸ìì—´ì˜ ìµœëŒ€ ê¸¸ì´ë¥¼ ì§€ì •í•  ìˆ˜ ìˆëŠ” ë¬¸ìì—´ íƒ€ì… - ex. VARCHAR(30) . | () ì•ˆì— ìµœì†Œ 0ë¶€í„° ìµœëŒ€ 65,535 (216 âˆ’ 1)ë¥¼ ì“¸ ìˆ˜ ìˆë‹¤ | . â€» CHARì™€ VARCHARì˜ ì°¨ì´ . | VARCHARëŠ” â€˜ê°€ë³€ ê¸¸ì´ íƒ€ì…â€™ â€“ VARCHARì´ë¼ëŠ” ë‹¨ì–´ ìì²´ê°€ ê°€ë³€ ë¬¸ìì—´(varying character)ì˜ ì¤„ì„ë§ | CHARëŠ” â€˜ê³ ì • ê¸¸ì´ íƒ€ì…â€™ | CHAR(10)ì€ ì–´ë–¤ ê¸¸ì´ì˜ ë¬¸ìì—´ì´ ì €ì¥ë˜ë”ë¼ë„ í•­ìƒ ê·¸ ê°’ì´ 10ë§Œí¼ì˜ ì €ì¥ ìš©ëŸ‰ì„ ì°¨ì§€í•˜ëŠ” ë°˜ë©´, VARCHAR(10)ì˜ ê²½ìš° ë§Œì•½ ê°’ì´ â€˜Helloâ€™ì™€ ê°™ì´ 5ìë¼ë©´ ì €ì¥ ìš©ëŸ‰ë„ 5ë§Œí¼ë§Œ ì°¨ì§€í•œë‹¤ (ì €ì¥ ìš©ëŸ‰ì´ ì„¤ì •ëœ ìµœëŒ€ ê¸¸ì´ì— ë§ê²Œ ê³ ì •ë˜ëŠ” ê²Œ ì•„ë‹ˆë¼ ì‹¤ì œ ì €ì¥ëœ ê°’ì— ë§ê²Œ ìµœì í™”ë˜ëŠ” ê²ƒ!) | ëŒ€ì‹ , VARCHAR íƒ€ì…ìœ¼ë¡œ ê°’ì´ ì €ì¥ë  ë•ŒëŠ” í•´ë‹¹ ê°’ì˜ ì‚¬ì´ì¦ˆë¥¼ ë‚˜íƒ€ë‚´ëŠ” ë¶€ë¶„(1byte ë˜ëŠ” 2byte)ì´ ì €ì¥ ìš©ëŸ‰ì— ì¶”ê°€ëœë‹¤ â†’ ê°’ì˜ ê¸¸ì´ê°€ í¬ê²Œ ë³€í•˜ì§€ ì•Šì„ ì»¬ëŸ¼ì—ëŠ” CHAR íƒ€ì…ì„ ì‚¬ìš©í•˜ê³ , ê¸¸ì´ê°€ ë“¤ì‘¥ë‚ ì‘¥í•  ì»¬ëŸ¼ì—ëŠ” VARCHAR íƒ€ì…ì„ ì“°ëŠ” ê²Œ ì¢‹ë‹¤ | . | TEXT: ë¬¸ìì—´ì„ ì €ì¥í•˜ëŠ” ë°ì´í„° íƒ€ì…ìœ¼ë¡œ ìµœëŒ€ 65,535ìê¹Œì§€ ì €ì¥ ê°€ëŠ¥ . | VARCHARì™€ ë‚´ë¶€ êµ¬í˜„ ë©´ì—ì„œ ì¼ë¶€ ì°¨ì´ê°€ ìˆì–´ì„œ, ë³´í†µ ê¸¸ì´ê°€ ê¸´ ë¬¸ìì—´ì€ TEXTê³„ì—´ì˜ íƒ€ì…ìœ¼ë¡œ ì €ì¥ (ë©”ì‹œì§€, ëŒ“ê¸€ ë“±) | . | MEDIUM TEXT: 16,777,215 (224 âˆ’ 1) ìê¹Œì§€ ì €ì¥ ê°€ëŠ¥ | LONGTEXT: 4,294,967,295 (232 âˆ’ 1) ìê¹Œì§€ ì €ì¥ ê°€ëŠ¥ | . Â  . ",
    "url": "https://chaelist.github.io/docs/sql/db_table_create/#%ED%85%8C%EC%9D%B4%EB%B8%94-%EC%83%9D%EC%84%B1",
    "relUrl": "/docs/sql/db_table_create/#í…Œì´ë¸”-ìƒì„±"
  },"39": {
    "doc": "ë°ì´í„°ë² ì´ìŠ¤ & í…Œì´ë¸” êµ¬ì¶•",
    "title": "í…Œì´ë¸”ì— row ì¶”ê°€/ì—…ë°ì´íŠ¸/ì‚­ì œ",
    "content": "Â  . 1. í…Œì´ë¸”ì— row ì¶”ê°€ . | INSERT INTO í…Œì´ë¸”ëª… (ì¹¼ëŸ¼ëª… ë‚˜ì—´) VALUES (ì¶”ê°€í•  ê°’ë“¤ ë‚˜ì—´);ì˜ êµ¬ì¡°ë¡œ ì‘ì„± . INSERT INTO members (id, name, gender, email, phone, sign_up_date) VALUES (1, 'Chaelist', 'F', 'chaechae@chaelist.com', '010-1234-5678', '2015-05-22'); . | ëª¨ë“  ì¹¼ëŸ¼ì— ê°’ì´ ìˆëŠ” rowë¥¼ ì¶”ê°€í•œë‹¤ë©´ ì•„ë˜ì™€ ê°™ì´ ì¹¼ëŸ¼ëª…ì€ ì œì™¸í•˜ê³  ì ì–´ë„ ëœë‹¤: . INSERT INTO members VALUES (1, 'Chaelist', 'F', 'chaechae@chaelist.com', '010-1234-5678', '2015-05-22'); . | ì¼ë¶€ ì¹¼ëŸ¼ì—ë§Œ ê°’ì„ ì¶”ê°€í•´ì¤„ ê²½ìš°, í•´ë‹¹ rowì˜ ë‚˜ë¨¸ì§€ columnì€ NULLê°’ìœ¼ë¡œ ì±„ì›Œì§„ë‹¤. (NOT NULL ì œì•½ì´ ì—†ëŠ” ê²½ìš°) . INSERT INTO members (id, name, gender, sign_up_date) VALUES (1, 'Chaelist', 'F', '2015-05-22'); . | AUTO_INCREMENT ì†ì„±ì„ ê°–ëŠ” ì¹¼ëŸ¼ â€˜idâ€™ì˜ ê²½ìš°, ê°’ì„ ì…ë ¥í•´ì£¼ì§€ ì•Šì•„ë„ ìë™ìœ¼ë¡œ ì´ì „ ê°’ë³´ë‹¤ 1 í° ìˆ˜ê°€ í• ë‹¹ëœë‹¤ . INSERT INTO members (name, gender, email, sign_up_date) VALUES ('Honu', 'M', 'honu@racoon.com', '2016-01-19'); . | ì—¬ëŸ¬ rowë¥¼ ì‚½ì…í•  ê²½ìš°, INSERT INTOë¬¸ì„ ê³„ì† ì ì–´ì£¼ëŠ” ëŒ€ì‹  ê°ê°ì„ tupleë¡œ ë¬¶ì–´ì„œ ë‚˜ì—´í•´ì¤„ ìˆ˜ ìˆë‹¤ . INSERT INTO food_menu (menu, price, ingredient) VALUES ('ë–¡ë³¶ì´', 4500, 'ì–´ë¬µ, ë–¡, ì–‘íŒŒ..'), ('ì°¸ì¹˜ê¹€ë°¥', 3000, 'ì°¸ì¹˜, ê¹€, ë‹¨ë¬´ì§€..'), ('ì˜¤ë¯€ë¼ì´ìŠ¤', 7000, 'ë‹¬ê±€, ì–‘íŒŒ, ì• í˜¸ë°•..'); . | INSERT INTOë¬¸ì„ ì—¬ëŸ¬ ë²ˆ ë°˜ë³µí•´ì„œ ì¨ì£¼ëŠ” ê²ƒë³´ë‹¤, ê°ê°ì„ tupleë¡œ ë¬¶ì–´ì„œ í•˜ë‚˜ì˜ ì¿¼ë¦¬ë¡œ ë„£ì–´ì£¼ëŠ” ê²ƒì´ ì†ë„ê°€ ë¹ ë¥´ë‹¤ (í•œë²ˆì— ë‹¤ëŸ‰ì˜ ë°ì´í„°ë¥¼ ë„£ì„ ë•ŒëŠ” í™•ì—°í•œ ì†ë„ ì°¨ì´ë¥¼ ë³´ì„) | . | . 2. row ì •ë³´ ì—…ë°ì´íŠ¸ . | UPDATE í…Œì´ë¸”ëª… SET ë³€ê²½í•  ì»¬ëŸ¼ ì •ë³´ WHERE ì¡°ê±´;ì˜ êµ¬ì¡°ë¡œ ì‘ì„± . UPDATE members SET phone = '010-8765-4321' WHERE id = 1; . | ê°™ì€ rowì˜ ì •ë³´ë¥¼ í•œ ë²ˆì— 2ê°œ ì´ìƒ ë°”ê¿€ ë•ŒëŠ” SET ë’¤ì— ë‚˜ì—´í•´ì£¼ë©´ ë¨ . UPDATE members SET phone = '010-8765-4321', name = 'ChaeChae' WHERE id = 1; . | â€» ì •ë³´ë¥¼ ì—…ë°ì´íŠ¸í•  ë•ŒëŠ”, WHEREì ˆì„ ì˜ ì ì–´ì£¼ëŠ” ê²ƒì´ ì¤‘ìš”í•˜ë‹¤ . | ex) UPDATE members SET name = 'ChaeChae'ë¼ê³ ë§Œ ì ìœ¼ë©´ â€˜nameâ€™ ì¹¼ëŸ¼ì˜ ëª¨ë“  ê°’ì´ ë‹¤ ë³€ê²½ë© | . | +) workbenchì—ì„œ safe update ëª¨ë“œê°€ ì„¤ì •ë˜ì–´ ìˆìœ¼ë©´, primary key ì™¸ì˜ ì»¬ëŸ¼ì„ WHERE ì ˆì—ì„œ ì‚¬ìš©í•˜ëŠ” UPDATEë¬¸ì€ ì‹¤í–‰ì´ ì•ˆëœë‹¤. | MySQLì¸ ê²½ìš°, [MYSQLWorkbench &gt; Preferences í´ë¦­ &gt; SQL Editor ì„ íƒ &gt; ì•„ë˜ Others ë¶€ë¶„ì— â€˜Safe Updatesâ€™ì— ì²´í¬ë˜ì–´ ìˆëŠ” ê²ƒì„ í•´ì œ]í•œ í›„ ì¬ì ‘ì†í•˜ë©´ primary key ì™¸ì˜ ì»¬ëŸ¼ìœ¼ë¡œë„ ì ‘ê·¼í•´ì„œ UPDATE ê°€ëŠ¥. | . | . +) ì»¬ëŸ¼ì˜ ê¸°ì¡´ ê°’ì„ ê¸°ì¤€ìœ¼ë¡œ ê°’ ì—…ë°ì´íŠ¸í•˜ê¸° . | ex) ê¸°ë§ê³ ì‚¬ ì„±ì ì— ì±„ì  ì˜¤ë¥˜ê°€ ìˆì—ˆì–´ì„œ, ëª¨ë“  rowì˜ scoreì— +3ì„ í•´ì¤˜ì•¼ í•˜ëŠ” ìƒí™©: . UPDATE final_exam_result SET score = score + 3; -- ì´ë ‡ê²Œ í•´ì£¼ë©´ ëª¨ë“  rowì˜ ì ìˆ˜ê°€ 3ì”© ë”í•´ì ¸ì„œ ì €ì¥ë¨ . | . 3. row ì‚­ì œ . | DELETE FROM í…Œì´ë¸”ëª… WHERE ì¡°ê±´;ì˜ êµ¬ì¡°ë¡œ ì‘ì„± . DELETE FROM members WHERE id = 4; . | â€» ì—…ë°ì´íŠ¸í•  ë•Œì™€ ë§ˆì°¬ê°€ì§€ë¡œ, DELETEí•  ë•Œì—ë„ WHEREì ˆì„ ì‹ ê²½ì¨ì•¼ í•œë‹¤ . | ex) DELETE FROM memebers;ë¼ê³ ë§Œ í•˜ë©´ í…Œì´ë¸”ì˜ ëª¨ë“  rowê°€ ì‚­ì œë˜ì–´ ë²„ë¦°ë‹¤ | . | . +) ë¬¼ë¦¬ ì‚­ì œ vs ë…¼ë¦¬ ì‚­ì œ . | ë¬¼ë¦¬ ì‚­ì œ: ì‚­ì œí•´ì•¼ í•  rowë¥¼ ë¬¼ë¦¬ì ìœ¼ë¡œ â€˜DELETEâ€™í•´ì„œ ì—†ì• ë²„ë¦¬ëŠ” ê²ƒ . | ex) DELETE FROM order WHERE id = 4; | . | ë…¼ë¦¬ ì‚­ì œ: DELETE ëŒ€ì‹ , â€˜ì‚­ì œ ì—¬ë¶€â€™ ì»¬ëŸ¼ì„ ë³„ë„ë¡œ ë§Œë“¤ì–´ì„œ, â€˜YESâ€™ì™€ ê°™ì´ ê°’ì„ ì—…ë°ì´íŠ¸í•´ì¤˜ì„œ rowê°€ ì‚­ì œë˜ì—ˆìŒì„ í‘œì‹œí•´ì£¼ëŠ” ê²ƒ . | ex) UPDATE order SET cancelled = 'Y' WHERE id = 4; | . | ì¶”í›„ ë¶„ì„ì— í™œìš©í•  ìˆ˜ ìˆë„ë¡ ë°ì´í„°ë¥¼ ë‚¨ê²¨ë‘ê³  ì‹¶ì„ ë•ŒëŠ” ë…¼ë¦¬ ì‚­ì œë¥¼ ì´ìš©í•˜ëŠ” ê²ƒì´ ì¢‹ë‹¤ . | ex) ì‚¬ìš©ìê°€ ì£¼ë¬¸ì„ ì·¨ì†Œí•œ ê²½ìš°, ì•„ì˜ˆ row ìì²´ë¥¼ ì‚­ì œí•´ë²„ë¦¬ê¸°ë³´ë‹¤ rowëŠ” ê·¸ëŒ€ë¡œ ë‘ê³  ì·¨ì†Œëœ ì£¼ë¬¸ì„ì„ ëª…ì‹œí•´ì£¼ëŠ” ë°©ì‹ì„ ì‚¬ìš©í•˜ë©´ ì¶”í›„ì— ì·¨ì†Œëœ ì£¼ë¬¸ ë°ì´í„°ë¥¼ ë”°ë¡œ ëª¨ì•„ì„œ ë¶„ì„ ê°€ëŠ¥ | . | ë‹¤ë§Œ, ë…¼ë¦¬ ì‚­ì œ ë°©ì‹ìœ¼ë¡œ ì‚­ì œí•˜ë©´ 1) ì‚­ì œë˜ì§€ ì•Šì€ ìœ íš¨í•œ rowë“¤ë§Œ ì¡°íšŒí•  ë•Œ WHEREì ˆì— ë§¤ë²ˆ ë³„ë„ ì¡°ê±´ì„ ë„£ì–´ì¤˜ì•¼ í•˜ë©°, 2) ì‚­ì œëœ rowì—¬ë„ ì‹¤ì œ DELETEëœ ê²ƒì€ ì•„ë‹ˆê¸°ì— DB ìš©ëŸ‰ì„ ê³„ì† ì°¨ì§€í•œë‹¤ëŠ” ë‹¨ì ì´ ìˆë‹¤ | . ",
    "url": "https://chaelist.github.io/docs/sql/db_table_create/#%ED%85%8C%EC%9D%B4%EB%B8%94%EC%97%90-row-%EC%B6%94%EA%B0%80%EC%97%85%EB%8D%B0%EC%9D%B4%ED%8A%B8%EC%82%AD%EC%A0%9C",
    "relUrl": "/docs/sql/db_table_create/#í…Œì´ë¸”ì—-row-ì¶”ê°€ì—…ë°ì´íŠ¸ì‚­ì œ"
  },"40": {
    "doc": "ë°ì´í„°ë² ì´ìŠ¤ & í…Œì´ë¸” êµ¬ì¶•",
    "title": "ë°ì´í„°ë² ì´ìŠ¤ & í…Œì´ë¸” êµ¬ì¶•",
    "content": " ",
    "url": "https://chaelist.github.io/docs/sql/db_table_create/",
    "relUrl": "/docs/sql/db_table_create/"
  },"41": {
    "doc": "Deep Learning ê¸°ì´ˆ",
    "title": "Deep Learning ê¸°ì´ˆ",
    "content": ". | ì¸ê³µì‹ ê²½ë§ ì´ë¡  . | Neural networkì˜ êµ¬ì¡° | ê°€ì„¤í•¨ìˆ˜ì™€ ì†ì‹¤í•¨ìˆ˜ | í™œì„± í•¨ìˆ˜ | . | Kerasë¡œ êµ¬í˜„í•˜ê¸° (ê¸°ì´ˆ) . | ë°ì´í„° ì¤€ë¹„ | Normalizer ì¤€ë¹„ | ëª¨ë¸ í˜•ì„± | í•™ìŠµ &amp; í‰ê°€ | . | . *Deep Learning: ML ì—°êµ¬ë°©ë²• ì¤‘ í•˜ë‚˜ë¡œ, Neural Networkë¥¼ í™œìš©í•˜ëŠ” ë°©ì‹. Â  . *pythonìœ¼ë¡œ Deep Learningì„ í•  ë•Œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” framework: . | Tensorflow: Googleì—ì„œ ê°œë°œ | Keras: tensorflowë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë§Œë“ , ë³´ë‹¤ ì‰¬ìš´ framework . | ì…ë¬¸ììš©ìœ¼ë¡œ ì í•©. ì „ë¬¸ ì—°êµ¬ìê°€ ì•„ë‹ˆê³  ë³¸ì¸ í•„ë“œì— ê°€ë³ê²Œ ì‘ìš©í•˜ëŠ” ì •ë„ë©´ Kerasë¡œ ì¶©ë¶„ | TensorFlow v1.10.0ë¶€í„° tf.kerasë¡œ í…ì„œí”Œë¡œìš° ì•ˆì—ì„œ ì¼€ë¼ìŠ¤ë¥¼ ì‚¬ìš© | . | PyTorch: Facebookì—ì„œ ê°œë°œ | . ",
    "url": "https://chaelist.github.io/docs/ml_basics/deep_learning/",
    "relUrl": "/docs/ml_basics/deep_learning/"
  },"42": {
    "doc": "Deep Learning ê¸°ì´ˆ",
    "title": "ì¸ê³µì‹ ê²½ë§ ì´ë¡ ",
    "content": "Neural networkì˜ êµ¬ì¡° . â€» layerì˜ ìˆ˜ë¥¼ ì…€ ë•Œ, ë³´í†µ hidden layerì™€ output layer ìˆ˜ë¡œ í‘œí˜„. (input layerëŠ” 0ë²ˆì§¸ ì¸µìœ¼ë¡œ ê°„ì£¼) â†’ L = 3ì´ë©´ hidden layer 2ê°œì— output layer 1ê°œì¸ êµ¬ì¡° . â€» ê° layerì— í¬í•¨ëœ nodeë“¤ì„ neuronì´ë¼ê³ ë„ í•¨ (ì¸ê³µ ë‰´ëŸ°) . Â  . (ì¶œì²˜: dzone.com) . Â  . 1. Input layer : ì…ë ¥ì¸µ. ê°€ì¥ ì•ì— ìˆëŠ” ë ˆì´ì–´ë¡œ, ë…ë¦½ë³€ìˆ˜ë¥¼ ë°ì´í„°ë¡œ ë°›ëŠ”ë‹¤. | ë…ë¦½ë³€ìˆ˜(feature)ì˜ ìˆ˜ë§Œí¼ì˜ input nodeë¡œ êµ¬ì„± (ê° nodeëŠ” ê° ë…ë¦½ë³€ìˆ˜ì˜ ê°’ì„ ì…ë ¥ë°›ìŒ) . | ex) ë‘ ê°œì˜ ë…ë¦½ë³€ìˆ˜(í‰ìˆ˜, ì—°ì‹)ì„ í†µí•´ ì•„íŒŒíŠ¸ ê°€ê²©ì„ ì˜ˆì¸¡í•  ê²½ìš°, input nodeëŠ” 2ê°œ | . | . 2. Hidden layer : ì€ë‹‰ì¸µ. ì¶”ê°€ì ì¸ ì‘ì—…ì„ í†µí•´ ì¢…ì†ë³€ìˆ˜ì˜ ì •í™•í•œ ì˜ˆì¸¡ì— ê¸°ì—¬í•˜ëŠ” nodeë“¤ì„ ë½‘ì•„ë‚¸ë‹¤. | ì• layerì— ì¡´ì¬í•˜ëŠ” ì •ë³´ë“¤ ì¤‘ ì¢…ì†ë³€ìˆ˜ ì˜ˆì¸¡ì— ì¤‘ìš”í•œ ì •ë³´ë“¤ì„ ì¶”ì¶œí•´ì£¼ëŠ” ì—­í• ì„ í•œë‹¤. | hidden layerëŠ” 1ê°œ ì´ìƒ ì¡´ì¬í•  ìˆ˜ ìˆë‹¤. (ì—°êµ¬ìê°€ ê²°ì •í•˜ëŠ” ê²ƒ) | hidden layerê°€ ë„ˆë¬´ ë§ì•„ë„ ì˜¤íˆë ¤ ì„±ëŠ¥ì´ ë–¨ì–´ì§€ê¸°ì—, ë³´í†µ 1-3ê°œ ì‚¬ì´ ì¤‘ì—ì„œ ê³ ë¥´ëŠ” ë“¯? | ë³´í†µ hidden layer ìˆ˜ê°€ ë§ì€ ê²½ìš°ë¥¼ â€˜deep neural networksâ€™ = â€˜deep learningâ€™ì´ë¼ê³  ë¶€ë¥´ëŠ” ê²ƒ. | hidden layerì˜ node ê°œìˆ˜ë„ ì—°êµ¬ì ì¬ëŸ‰ì— ë”°ë¼ ê²°ì •. | ì²«ë²ˆì§¸ hidden layerì˜ node ìˆ˜ì™€ ë‘ë²ˆì§¸ hidden layerì˜ node ìˆ˜ê°€ ê°™ì§€ ì•Šì•„ë„ ë¨. í•˜ì§€ë§Œ ë³´í†µì€ ê·¸ ì „ layerì˜ node ìˆ˜ì™€ ê°™ê±°ë‚˜ ì ì€ ìˆ˜ë¥¼ ì‚¬ìš©í•œë‹¤ê³  í•¨ | hidden layerì˜ node ìˆ˜ëŠ” ë³´í†µ input layerì˜ node ìˆ˜ì™€ output layerì˜ node ìˆ˜ ì‚¬ì´ë¡œ ë§ì´ ì„¤ì • | . 3. Output layer : ì¶œë ¥ì¸µ. ì¢…ì†ë³€ìˆ˜ì— ëŒ€í•œ ì˜ˆì¸¡ì¹˜ê°€ ì¶œë ¥ë˜ëŠ” ë ˆì´ì–´. | ì¢…ì†ë³€ìˆ˜ì˜ í˜•íƒœì— ë”°ë¼ output node ìˆ˜ê°€ ë‹¬ë¼ì§ . | ì´ë¶„ì  ë¶„ë¥˜ ë¬¸ì œ: output nodeë¥¼ 2ê°œë¡œ í•˜ê±°ë‚˜ / 1ê°œë¡œ ë‘ê³ , ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ë¥¼ output layerì˜ í™œì„± í•¨ìˆ˜ë¡œ ì‚¬ìš©í•´ 0ì— ê°€ê¹Œìš´ì§€ 1ì— ê°€ê¹Œìš´ì§€ë¥¼ ê°€ì§€ê³  íŒë‹¨ | ë‹¤ì¤‘ ë¶„ë¥˜ ë¬¸ì œ: ê²°ê³¼ë³€ìˆ˜(DV)ê°€ ì·¨í•  ìˆ˜ ìˆëŠ” ê°’ (ì¹´í…Œê³ ë¦¬ì˜ ìˆ˜) = output nodeì˜ ìˆ˜ | íšŒê·€ ë¬¸ì œ: output nodeëŠ” 1ê°œ. (ì˜ˆì¸¡ì¹˜ = output node) | . | . +) Bias Node . | bias node = intercept (ìƒìˆ˜í•­)ì˜ ê°œë… (ì„ í˜•íšŒê·€ì—ì„œì˜ interceptì˜ ê¸°ëŠ¥ê³¼ ê°™ë‹¤) | input layerì™€ hidden layerì—ë§Œ bias nodeê°€ í•˜ë‚˜ì”© ë“¤ì–´ê°ˆ ìˆ˜ ìˆìœ¼ë©°, bias nodeê°€ ë“¤ì–´ê°€ë©´ ëª¨ë¸ì˜ flexibilityê°€ ì¦ê°€í•œë‹¤. | . +) Weights (Parameters) . | ë…ë¦½ë³€ìˆ˜ì™€ ì¢…ì†ë³€ìˆ˜ ê°„ì˜ ê´€ê³„ë¥¼ ì„¤ì • (ì„ í˜•íšŒê·€ì—ì„œì˜ thetaê°’ê³¼ ë™ì¼í•œ ê¸°ëŠ¥) | nodeì™€ nodeë¥¼ ì‡ëŠ” ê° ì„ ë§ˆë‹¤ ë³„ë„ì˜ weightê°€ ì¡´ì¬. | ex) 1ë²ˆì§¸ layerì˜ ëª¨ë“  nodeì™€ 2ë²ˆì§¸ layerì˜ ëª¨ë“  node ê°„ì—ëŠ” ë³„ë„ì˜ weightê°€ ì¡´ì¬ | . | . Â  . ê°€ì„¤í•¨ìˆ˜ì™€ ì†ì‹¤í•¨ìˆ˜ . | ê°€ì„¤í•¨ìˆ˜: ì£¼ì–´ì§„ ê°€ì¤‘ì¹˜(weight)ì™€ í¸í–¥(bias)ì— ë”°ë¼ output layer ë‰´ëŸ°ë“¤ì˜ ì¶œë ¥ì„ ê³„ì‚°í•´ë‚´ëŠ” í•¨ìˆ˜. | ì†ì‹¤í•¨ìˆ˜(ë¹„ìš©í•¨ìˆ˜): . | íšŒê·€ ë¬¸ì œ: ë³´í†µ MSE(í‰ê· ì œê³±ì˜¤ì°¨)ë¥¼ ë§ì´ ì‚¬ìš© | ë¶„ë¥˜ ë¬¸ì œ: ë³´í†µ ë¡œê·¸ ì†ì‹¤(cross entropy)ë¥¼ ë§ì´ ì‚¬ìš© | . | . â†’ ê²½ì‚¬í•˜ê°•ë²•ì„ ì‚¬ìš©í•´ì„œ ì†ì‹¤í•¨ìˆ˜ì˜ ìµœì†Œì ì„ ì°¾ëŠ”ë‹¤. (ì†ì‹¤í•¨ìˆ˜ë¥¼ ìµœì†Œí™”í•˜ëŠ” weight ê°’ë“¤ì„ êµ¬í•œë‹¤) . Â  . +) ë‹¤ì–‘í•œ ê²½ì‚¬ í•˜ê°•ë²• . | ë°°ì¹˜ ê²½ì‚¬ í•˜ê°•ë²• (batch gradient descent): . | í•œ ë²ˆ ê²½ì‚¬ í•˜ê°•ì„ í•  ë•Œ ëª¨ë“  í•™ìŠµ ë°ì´í„°ë¥¼ ì‚¬ìš© â†’ ë°ì´í„°ê°€ ë§ìœ¼ë©´ ë„ˆë¬´ ì˜¤ë˜ ê±¸ë¦°ë‹¤! | . | í™•ë¥ ì  ê²½ì‚¬ í•˜ê°•ë²• (stochastic gradient descent): . | í•œ ë²ˆ ê²½ì‚¬ í•˜ê°•ì„ í•  ë•Œ í•˜ë‚˜ì˜ í•™ìŠµ ë°ì´í„°ë§Œ ì‚¬ìš© â†’ ë¹ ë¥´ê²Œ ê³„ì‚°ì´ ê°€ëŠ¥í•˜ì§€ë§Œ, ê°€ì¥ ê²½ì‚¬ê°€ ê°€íŒŒë¥¸ ë°©í–¥ìœ¼ë¡œ í•˜ê°•í•˜ì§€ ì•Šìœ¼ë©° ê·¹ì†Œì  ê·¼ì²˜ì—ì„œë„ ê³„ì† ì£¼ë³€ì„ ë§´ëŒë©° ì‰½ê²Œ ìˆ˜ë ´í•˜ì§€ ì•Šì„ ìˆ˜ ìˆë‹¤ëŠ” ë‹¨ì ë„ ìˆìŒ | . | ë¯¸ë‹ˆ ë°°ì¹˜ ê²½ì‚¬ í•˜ê°•ë²• (mini batch gradient descent): . | ìœ„ ë‘ê°€ì§€ ë°©ë²•ì˜ íƒ€í˜‘ì . ë°ì´í„°ì…‹ì„ ì„ì˜ë¡œ ê°™ì€ í¬ê¸°ì˜ ì—¬ëŸ¬ ë°ì´í„°ì…‹ìœ¼ë¡œ ë‚˜ëˆ ì¤€ í›„, í•œ ë²ˆ ê²½ì‚¬ í•˜ê°•ì„ í•  ë•Œ â€˜mini batchâ€™ í•™ìŠµ ë°ì´í„°ë¥¼ ì‚¬ìš© (ex. í•™ìŠµ ë°ì´í„°ë¥¼ 50ê°œì”© ë‚˜ëˆ  ë†“ì€ í›„, í•œ ë²ˆì˜ ê²½ì‚¬í•˜ê°•ì— í•˜ë‚˜ì˜ mini batchë§Œ ì‚¬ìš©) | . | . â€» ê°€ì¥ ì¢‹ì€ ê²½ì‚¬ í•˜ê°•ë²•ì´ ì •í•´ì ¸ ìˆëŠ” ê±´ ì•„ë‹ˆì§€ë§Œ, ëŒ€ë¶€ë¶„ì˜ ê²½ìš° ë¯¸ë‹ˆ ë°°ì¹˜ ê²½ì‚¬ í•˜ê°•ë²•ì„ ê°€ì¥ ë§ì´ ì‚¬ìš©í•œë‹¤ . Â  . í™œì„± í•¨ìˆ˜ . â€» hidden &amp; output nodeë“¤ì€ ë³´í†µ ì…ë ¥ê°’ z (ì´ì „ ì¸µì—ì„œ ì „ë‹¬í•˜ëŠ” ì…ë ¥ê°’)ì„ ê·¸ëŒ€ë¡œ ì¶œë ¥í•˜ì§€ ì•Šê³ , í™œì„±í™” í•¨ìˆ˜ë¡œ ë³€í™˜í•´ f(z)ë¥¼ ì¶œë ¥í•œë‹¤. (ê° node ì•ˆì—ì„œ ì´ëŸ¬í•œ ë³€í™˜ì´ ì´ë£¨ì–´ì§) . 1. Hidden Layerì˜ í™œì„± í•¨ìˆ˜ . (ì¶œì²˜: adilmoujahid.com) . | ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜( = logistic function) . | $ \\frac{1}{(1 + e^{-z})} $ë¡œ ê³„ì‚°. inputì„ 0~1 ì‚¬ì´ì˜ ìˆ«ìë¡œ ë°”ê¿”ì„œ ì¶œë ¥í•´ì£¼ê²Œ ë¨ | . | Hyperbolic tangent (tanh) . | $ \\frac{sinh(z)}{cosh(z)} = \\frac{(e^z - e^{-z})}{(e^z + e^{-z})} $ë¡œ ê³„ì‚°. inputì„ -1 ~ 1 ì‚¬ì´ì˜ ê°’ìœ¼ë¡œ ë°”ê¿”ì„œ ì¶œë ¥í•´ì¤Œ | . | ReLU (Rectified Linear Unit) . | max(0, z)ë¡œ ê³„ì‚°. inputì´ 0ë³´ë‹¤ í¬ë©´ ê·¸ëŒ€ë¡œ ì¶œë ¥, 0ë³´ë‹¤ ì‘ê±°ë‚˜ ê°™ìœ¼ë©´ 0ì„ ì¶œë ¥. | ê²½ì‚¬ ê³„ì‚° ì†ë„ê°€ ë§¤ìš° ë¹ ë¥´ë‹¤ëŠ” ì¥ì . (zê°€ 0ë³´ë‹¤ í¬ë©´ ê²½ì‚¬ê°€ 1, ì‘ê±°ë‚˜ ê°™ìœ¼ë©´ 0) | . | Leaky ReLU . | ReLUë¥¼ ì•½ê°„ ë³€í˜•í•œ í˜•íƒœë¡œ, ì‚¬ë¼ì§€ëŠ” ê¸°ìš¸ê¸° ë¬¸ì œê°€ ëœí•´ì§„ë‹¤. | max(Îµz, z)ë¡œ ê³„ì‚°. Îµ(ì…ì‹¤ë¡ )ì€ ë³´í†µ 0.01 ì •ë„ì˜ ì‘ì€ ìƒìˆ˜ë¥¼ ì‚¬ìš© | . | . â€» ìµœê·¼ì—ëŠ” hidden layerì˜ í™œì„± í•¨ìˆ˜ë¡œ ReLUê°€ ê°€ì¥ ë§ì´ ì‚¬ìš©ëœë‹¤. â€» í™œì„±í™” í•¨ìˆ˜ê°€ ë¹„ì„ í˜•ì´ë©´ ì‹ ê²½ë§ë„ ë¹„ì„ í˜• í•¨ìˆ˜, í™œì„±í™” í•¨ìˆ˜ê°€ ì„ í˜•ì´ë©´ ì‹ ê²½ë§ë„ ê²°êµ­ ì„ í˜• í•¨ìˆ˜ (ì„ í˜•ì ì¸ ê²°ì •ê²½ê³„ë§Œ ì°¾ì•„ë‚¼ ìˆ˜ ìˆìŒ) â†’ ì€ë‹‰ì¸µì˜ í™œì„± í•¨ìˆ˜ë¡œëŠ” ì„ í˜• í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ”ë‹¤! . 2. Output Layerì˜ í™œì„± í•¨ìˆ˜ . | ë¶„ë¥˜ë¬¸ì œ (DVê°€ ë¶„ë¥˜ë¬¸ì œ) . | ì´ë¶„ì  ë¶„ë¥˜ ë¬¸ì œë¼ë©´ ë³´í†µ output layerì˜ í™œì„±í•¨ìˆ˜ë¡œ â€˜ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜â€™ë¥¼ ì‚¬ìš©. | output nodeë¥¼ 1ê°œë§Œ ë‘ê³ , 0ì— ê°€ê¹Œìš´ì§€ 1ì— ê°€ê¹Œìš´ì§€ì˜ â€˜í™•ë¥ â€™ì— ë”°ë¼ ë¶„ë¥˜ | . | ë‹¤ì¤‘ ë¶„ë¥˜ ë¬¸ì œë¼ë©´ ë³´í†µ â€˜Softmax í•¨ìˆ˜â€™ë¥¼ ì‚¬ìš©. | $ \\frac{e^z}{ê° nodeì˜ e^zì˜ í•©} $ìœ¼ë¡œ ê³„ì‚° (eëŠ” ìì—°ìƒìˆ˜. 2.71â€¦) â†’ ê·¸ë¦¬ê³  ê°€ì¥ ê°’ì´ í° ìª½ìœ¼ë¡œ ë¶„ë¥˜ | â€» Softmax í•¨ìˆ˜ëŠ” ëŠ˜ ì¶œë ¥ê°’ì˜ í•©ì´ 1ì´ ë˜ê¸°ì—, ë‹¤ì¤‘ ë¶„ë¥˜ì—ì„œ ë³´ë‹¤ í™•ë¥ ì ìœ¼ë¡œ ì˜ë¯¸ê°€ ëª…í™•í•˜ë‹¤ (ex. ê°•ì•„ì§€ì¼ í™•ë¥  70%) | . | . | íšŒê·€ë¬¸ì œ (DVê°€ ì—°ì†ë³€ìˆ˜): output layerì˜ í™œì„±í•¨ìˆ˜ë¡œ â€˜ì„ í˜•í•¨ìˆ˜â€™ë¥¼ ì‚¬ìš©. output nodeë¥¼ 1ê°œë§Œ ë‘ê³ , output nodeê°€ ë°›ëŠ” zê°’ì„ ê·¸ëŒ€ë¡œ ì¶œë ¥. (output layerì— í™œì„±í•¨ìˆ˜ê°€ ì—†ë‹¤ê³  ì´í•´í•´ë„ ë¬´ë°©) | . ",
    "url": "https://chaelist.github.io/docs/ml_basics/deep_learning/#%EC%9D%B8%EA%B3%B5%EC%8B%A0%EA%B2%BD%EB%A7%9D-%EC%9D%B4%EB%A1%A0",
    "relUrl": "/docs/ml_basics/deep_learning/#ì¸ê³µì‹ ê²½ë§-ì´ë¡ "
  },"43": {
    "doc": "Deep Learning ê¸°ì´ˆ",
    "title": "Kerasë¡œ êµ¬í˜„í•˜ê¸° (ê¸°ì´ˆ)",
    "content": ". | ë¨¼ì €, ì„¤ì¹˜í•´ì•¼ ì‚¬ìš©ì´ ê°€ëŠ¥í•˜ë‹¤: https://www.tensorflow.org/install/pip#virtual-environment-install | google colaboratoryì—ì„œëŠ” ë³„ë„ì˜ ì„¤ì¹˜ ì—†ì´ ë°”ë¡œ importí•´ì„œ ì‚¬ìš© ê°€ëŠ¥ | . import tensorflow as tf # google colabì—ì„œëŠ” ì„¤ì¹˜ ë”°ë¡œ ì•ˆí•´ë„ importí•´ì„œ ì‚¬ìš© ê°€ëŠ¥ from tensorflow import keras from tensorflow.keras import layers from tensorflow.keras.layers.experimental import preprocessing import numpy as np import pandas as pd import matplotlib.pyplot as plt # Make numpy printouts easier to read. np.set_printoptions(precision=2, suppress=True) . | precision=2: ì†Œìˆ˜ì  ë‘ë²ˆì§¸ ìë¦¬ê¹Œì§€ë§Œ ì¶œë ¥ | suppress=True: e-04ì™€ ê°™ì€ scientific notationì„ ì œê±° | . ë°ì´í„° ì¤€ë¹„ . | ê¸°ì´ˆ ì˜ˆì‹œì´ê¸°ì—, ê¸°ë³¸ìœ¼ë¡œ ì œê³µë˜ëŠ” boston_housing ë°ì´í„°ë¥¼ ì‚¬ìš© | . from tensorflow.keras.datasets import boston_housing (X_train, y_train), (X_test, y_test) = boston_housing.load_data() # test_split=0.2ì´ default. Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/boston_housing.npz 57344/57026 [==============================] - 0s 0us/step . print(X_train.shape, y_train.shape, X_test.shape, y_test.shape) ## train:testê°€ ëŒ€ëµ 4:1 ë¹„ìœ¨ë¡œ ë‚˜ë‰˜ì—ˆê³ , IVëŠ” 13ê°œ, DVëŠ” 1ê°œ . (404, 13) (404,) (102, 13) (102,) . Normalizer ì¤€ë¹„ . | preprocessingì„ ìœ„í•œ normalization layerë¥¼ ì¤€ë¹„í•´ì„œ ëª¨ë¸ì— ë„£ê²Œ ë¨ | . normalizer = preprocessing.Normalization() normalizer.adapt(np.array(X_train)) # ì•„ë˜ì™€ ê°™ì´ normalizerì— 13ê°œ IV ê°ê°ì˜ í‰ê· (mean)ê³¼ ë¶„ì‚°(variance) ì •ë³´ê°€ ì €ì¥ë¨ print(normalizer.mean.numpy()) print(normalizer.variance.numpy()) . [ 3.75 11.48 11.1 0.06 0.56 6.27 69.01 3.74 9.44 405.9 18.48 354.78 12.74] [ 85.18 563.51 46.28 0.06 0.01 0.5 778.75 4.11 75.47 27611.97 4.83 8834.99 52.5 ] . ## ì˜ˆì‹œë¡œ, ë°ì´í„°ë¥¼ ë„£ì—ˆì„ ë•Œ ì˜ normalizeë˜ëŠ”ì§€ í™•ì¸ ex_array = np.array(X_train[:1]) # X_trainì˜ ì²«ë²ˆì§¸ ê°’ (ì²«ë²ˆì§¸ ë„ì‹œì— ëŒ€í•œ 13ê°œ ì •ë³´ vector) print('Original:', ex_array) print('Normalized:', normalizer(ex_array).numpy()) # ì›ë˜ ì„œë¡œ ë‹¤ë¥¸ scaleì˜ ê°’ì´ì˜€ì§€ë§Œ ì˜ normalizeëœ ê²ƒì„ í™•ì¸ ê°€ëŠ¥! . Original: [[ 1.23 0. 8.14 0. 0.54 6.14 91.7 3.98 4. 307. 21. 396.9 18.72]] Normalized: [[-0.27 -0.48 -0.44 -0.26 -0.17 -0.18 0.81 0.12 -0.63 -0.6 1.15 0.45 0.83]] . ëª¨ë¸ í˜•ì„± . model = keras.Sequential([ normalizer, layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)), # hidden layer 1 layers.Dense(32, activation='relu'), # hidden layer 2 layers.Dense(1) # output layer: íšŒê·€ ë¬¸ì œì´ë¯€ë¡œ node 1ê°œ &amp; í™œì„± í•¨ìˆ˜ ì§€ì • X ]) model.summary() . Model: \"sequential\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= normalization (Normalization (None, 13) 27 _________________________________________________________________ dense_6 (Dense) (None, 64) 896 _________________________________________________________________ dense_7 (Dense) (None, 32) 2080 _________________________________________________________________ dense_8 (Dense) (None, 1) 33 ================================================================= Total params: 3,036 Trainable params: 3,009 Non-trainable params: 27 _________________________________________________________________ . *Prameter ìˆ˜ ê³„ì‚°: . | ì²«ë²ˆì§¸ normalization layerëŠ” 13*2 + 1 = 27 | ë‘ë²ˆì§¸: 14 * 64 = 896 (bias nodeê°€ í•˜ë‚˜ì”© ì¶”ê°€ë˜ë‹ˆê¹Œ 13ì´ ì•„ë‹ˆë¼ 14) | ì„¸ë²ˆì§¸: 65 * 32 = 2080 | ë§ˆì§€ë§‰: 33 * 1 = 33 | . model.compile(loss=keras.losses.MeanSquaredError(), optimizer=tf.optimizers.Adam(0.01)) . | ì†ì‹¤í•¨ìˆ˜: ë³´í†µ â€˜mean_absolute_errorâ€™ë‚˜ keras.losses.MeanSquaredError() ì‚¬ìš© | optimizerë¡œ ì–´ë–¤ ê²½ì‚¬ í•˜ê°•ë²•ì„ ì‚¬ìš©í• ì§€ ê²°ì •. ë³´í†µ Adam(Adaptive Moment Estimation)ì„ ë§ì´ ì‚¬ìš© | Adam(0.01): learning_rateë¥¼ 0.01ë¡œ ì§€ì •í•œ ê²ƒ. (default = 0.001) | Adam ê²½ì‚¬í•˜ê°•ë²•ì€ íŒŒë¼ë¯¸í„°ì— ë”°ë¼ learning rateë¥¼ ìë™ìœ¼ë¡œ ì¡°ì ˆí•´ì£¼ëŠ” ê¸°ëŠ¥ì´ ìˆìœ¼ë‚˜, ì´ˆê¸° learning rateëŠ” ì„¤ì •ì„ í•´ì¤˜ì•¼ í•¨ | . í•™ìŠµ &amp; í‰ê°€ . %%time history = model.fit( X_train, y_train, validation_split = 0.2, # 20%ì˜ trainig dataë¥¼ validation dataë¡œ ì‚¬ìš© -- ì‚¬ì‹¤ ì´ë¯¸ train_test_splitì„ í•´ë’€ê¸° ë•Œë¬¸ì— ê¼­ í•  í•„ìš”ëŠ” ì—†ë‹¤. epochs=100, # ì „ì²´ ë°ì´í„°ë¥¼ ëª‡ ë²ˆ ë°˜ë³µí•´ì„œ ì—…ë°ì´íŠ¸í•  ê²ƒì¸ì§€ batch_size = 16, # í•œ ë²ˆ ì—…ë°ì´íŠ¸í•  ë•Œ ë°ì´í„°í¬ì¸íŠ¸ë¥¼ ëª‡ ê°œ ì‚¬ìš©í• ì§€ verbose=0 # í•™ìŠµ ê³¼ì •ì„ ì•„ë˜ì— ì¶œë ¥í•˜ì§€ ì•Šê² ë‹¤ëŠ” ì˜ë¯¸ ) . CPU times: user 10.2 s, sys: 432 ms, total: 10.6 s Wall time: 9.49 s . | batch_size: defaultëŠ” 32. ë³´í†µ 2ì˜ ë°°ìˆ˜ë¥¼ ë§ì´ ì‚¬ìš©í•˜ëŠ” ë“¯. | verbose: 0, 1, 2 ì¤‘ í•˜ë‚˜ ì„ íƒ. 0 = silent, 1 = progress bar, 2 = one line per epoch | . +) batch_sizeì™€ epoch: . | ex) í•™ìŠµ ë°ì´í„°ê°€ 48ê°œ ìˆê³ , batch_size=16, epochs=100ì´ë¼ë©´, ì´ (48/16) * 100 = 300ë²ˆ ì—…ë°ì´íŠ¸ê°€ ì¼ì–´ë‚¨ . | 48ê°œì˜ ë°ì´í„°ë¥¼ 16ê°œì”© ìª¼ê°œì„œ 3ë²ˆ ë‚˜ëˆ ì„œ ì—…ë°ì´íŠ¸ë¥¼ í•´ì£¼ëŠ”ë°, ì´ë ‡ê²Œ ì „ì²´ ë°ì´í„°ë¥¼ ì—…ë°ì´íŠ¸í•˜ëŠ” ê³¼ì •ì„ 100ë²ˆ ë°˜ë³µí•˜ëŠ” ê²ƒ! | . | . def plot_loss(history): plt.plot(history.history['loss'], label='loss') ## ì‹¤ì œ trainingì— ì‚¬ìš©í–ˆë˜ dataì—ì„œ ì–´ëŠ ì •ë„ ì˜¤ì°¨ê°€ ë°œìƒí•˜ëŠ”ì§€ plt.plot(history.history['val_loss'], label='val_loss') ## validation dataë¡œ ì‚¬ìš©í–ˆë˜ 20%ì—ì„œ ì–´ëŠ ì •ë„ ì˜¤ì°¨ê°€ ë°œìƒí•˜ëŠ”ì§€ plt.ylim([0, 50]) plt.xlabel('Epoch') plt.ylabel('Loss') plt.legend() plt.grid(True) plot_loss(history) . # test dataë¡œ ì˜ˆì¸¡í–ˆì„ ë•Œì˜ ì†ì‹¤ í™•ì¸ model.evaluate(X_test, y_test) . 4/4 [==============================] - 0s 2ms/step - loss: 13.1361 13.136137008666992 . from sklearn.metrics import r2_score y_test_pred = model.predict(X_test) r2_score(y_test, y_test_pred) # rìŠ¤í€˜ì–´ê°’ í™•ì¸ . 0.8421969063118736 . ",
    "url": "https://chaelist.github.io/docs/ml_basics/deep_learning/#keras%EB%A1%9C-%EA%B5%AC%ED%98%84%ED%95%98%EA%B8%B0-%EA%B8%B0%EC%B4%88",
    "relUrl": "/docs/ml_basics/deep_learning/#kerasë¡œ-êµ¬í˜„í•˜ê¸°-ê¸°ì´ˆ"
  },"44": {
    "doc": "Dictionary, Tuple, Set",
    "title": "Dictionary, Tuple, Set",
    "content": ". | Dictionary . | Dictionary ê¸°ì´ˆ | Main Dictionary Functions | . | Tuple . | Tuple ê¸°ì´ˆ | Tupleì˜ í™œìš©ë²• ì˜ˆì‹œ | . | Set (ì§‘í•©ìë£Œí˜•) . | Set ê¸°ì´ˆ | êµì§‘í•©, í•©ì§‘í•©, ì°¨ì§‘í•© | . | . ",
    "url": "https://chaelist.github.io/docs/python_basics/dictionary_tuple_set/",
    "relUrl": "/docs/python_basics/dictionary_tuple_set/"
  },"45": {
    "doc": "Dictionary, Tuple, Set",
    "title": "Dictionary",
    "content": "{Key : Value}ì™€ ê°™ì€ ëª¨ì–‘ìœ¼ë¡œ í‘œí˜„ë¨ . | {key:value, key2:value2, â€¦} ì´ëŸ° ì‹ìœ¼ë¡œ pairë“¤ ë‚˜ì—´ | ì´ë ‡ê²Œ í‚¤ì™€ ê°’ì´ ì—°ê²°ë˜ëŠ” ê°œë…ì„ ë³´í†µ â€˜ì—°ê´€ ë°°ì—´ (Associative Arrays)â€™ì´ë¼ê³  í•œë‹¤. | listëŠ” ê°’ë“¤ì— ìˆœì„œëŒ€ë¡œ indexê°€ ë§¤ê²¨ì§€ì§€ë§Œ, dictionaryëŠ” ê°€ë°©ì— ì´ê²ƒì €ê²ƒ ë¼ë²¨ì„ ë¶™ì—¬ì„œ ë„£ëŠ” ê²ƒê³¼ ê°™ë‹¤. (dictionaries are like bags - no order!) | â€˜Keyâ€™ is unique in the dictionary and must be immutable | . # ë¹ˆ Dictionaryë¥¼ ìƒì„±í•˜ëŠ” ë°©ë²• a = {} b = dict() print(a, b) # ë‘ ë°©ë²• ëª¨ë‘ ë™ì¼ . Dictionary ê¸°ì´ˆ . | keyë¡œ valueì— ì ‘ê·¼í•˜ê¸° dict1 = {'Tom':23, 'John':34, 'Bob':12} print(dict1['Tom']) # 'Tom'ì˜ valueë¥¼ ì¶”ì¶œ . 23 . | Dictionaryì— ê°’ ë„£ê¸° purse = dict() # ë¹ˆ dictionary ìƒˆë¡œ ìƒì„± purse['money'] = 12 # keyê°€ 'money'ê³  valueê°€ 12ì¸ pair ì €ì¥ purse['candy'] = 3 # keyê°€ 'candy'ê³  valueê°€ 3ì¸ pair ì €ì¥ print(purse) . {'money': 12, 'candy': 3} . *ì£¼ì˜: íŠ¹ì • keyëŠ” í•˜ë‚˜ë¿ì´ë©°(unique), ë³€ê²½ë¶ˆê°€ëŠ¥í•˜ë‹¤(immutable) . # ê°™ì€ keyì— ë˜ ê°’ì„ assigní•´ì£¼ë©´ ê·¸ëƒ¥ ê·¸ keyì˜ valueê°€ ëŒ€ì²´ë  ë¿ì´ë‹¤. (a key must be unique!) dict1 = {'Tom': 23, 'John': 34, 'Bob': 12, 'Sarah': 35} dict1['Sarah'] = 54 ## {'Sarah':54}ê°€ ìƒˆë¡œ ì¶”ê°€ë˜ëŠ” ëŒ€ì‹ , ì›ë˜ ìˆë˜ 'Sarah' keyì˜ valueê°€ 54ë¡œ ë³€ê²½ë¨. print(dict1) . {'Tom': 23, 'John': 34, 'Bob': 12, 'Sarah': 54} . # ë˜í•œ, íŠ¹ì • keyëŠ” ë³€ê²½ ë¶ˆê°€ëŠ¥í•˜ê¸°ì—, {'Sarah': 35}ë¥¼ {'SARAH':35}ë¡œ ë°”ê¾¸ëŠ” ë°©ë²•ì€ ì—†ë‹¤ . | pairì˜ ìˆ˜ (=keyì˜ ìˆ˜) ì„¸ê¸°: len() # len() dict1 = {'Tom':23, 'John':34, 'Bob':12} len(dict1) # returns the number of keys (= number of pairs) . 3 . | dictionary ì‚­ì œí•˜ê¸°: del # dictionaryì˜ íŠ¹ì • element ì‚­ì œ dict1 = {'Tom':23, 'John':34, 'Bob':12} del dict1['Tom'] print(dict1) # dictionary ìì²´ë¥¼ ì‚­ì œí•˜ëŠ” ê²ƒë„ ê°€ëŠ¥ del dict1 print(dict1) # dict1 ìì²´ê°€ ì•„ì˜ˆ ì‚¬ë¼ì¡Œìœ¼ë¯€ë¡œ, ì¡´ì¬í•˜ì§€ ì•ŠëŠ”ë‹¤ê³  errorê°€ ë‚¨. {'John': 34, 'Bob': 12} --------------------------------------------------------------------------- NameError Traceback (most recent call last) &lt;ipython-input-15-ecedb4a10042&gt; in &lt;module&gt;() 5 6 del dict1 ----&gt; 7 print(dict1) NameError: name 'dict1' is not defined . | . Main Dictionary Functions . | dict1.clear(): dictionary ì•ˆì˜ ëª¨ë“  ë°ì´í„°ë¥¼ ì‚­ì œ # dict1.clear() dict1 = {'Tom':23, 'John':34, 'Bob':12} dict1.clear() # dict1 ì•ˆì˜ ëª¨ë“  ê°’ì„ ì œê±°. dict1ì´ë¼ëŠ” ê²ƒ ìì²´ëŠ” ë‚¨ì•„ ìˆìŒ ## cf) del dict1ì„ í•˜ë©´ í•´ë‹¹ dictionary ìì²´ê°€ ì‚¬ë¼ì§ print(dict1) . {} . | dict1.keys(): dictionary ì•ˆì˜ ëª¨ë“  keyë“¤ì„ list-like variableë¡œ ë°˜í™˜ # dict1.keys() dict1 = {'Tom':23, 'John':34, 'Bob':12} dict1.keys() . dict_keys(['Tom', 'John', 'Bob']) . *dict1.keys()ì˜ ê²°ê³¼ë¡œ ë°˜í™˜ë˜ëŠ” ê²°ê³¼ë¬¼ì€ list í˜•íƒœë¡œ ë³€í™˜ì´ ê°€ëŠ¥ (list-like variable: â€˜list(x)â€™ functionì„ ì‚¬ìš©í•´ì„œ listë¡œ ë³€ê²½ ê°€ëŠ¥) . # list(dict1.keys()) key_list = list(dict1.keys()) print(key_list) # list í˜•íƒœë¡œ ì¶œë ¥ë¨ . ['Tom', 'John', 'Bob'] . cf) list(dict1)ì„ í•´ë„ keyë§Œ listë¡œ ë°˜í™˜ë¨ . # list(dict1) print(list(dict1)) . ['Tom', 'John', 'Bob'] . | dict1.values(): dictionary ì•ˆì˜ ëª¨ë“  valueë“¤ì„ list-like variableë¡œ ë°˜í™˜ # dict1.values() dict1 = {'Tom':23, 'John':34, 'Bob':12} dict1.values() . dict_values([23, 34, 12]) . | values() functionì€ ë³„ë¡œ ì•ˆ ì¤‘ìš”. keys() functionë§Œ ê¸°ì–µí•´ë„ ëœë‹¤. (keyëŠ” uniqueí•˜ë‹ˆê¹Œ, keyë§Œ ì•Œë©´ ê·¸ valueì— access ê°€ëŠ¥. í•˜ì§€ë§Œ valueëŠ” ê²¹ì¹  ìˆ˜ ìˆì–´ì„œ, valueë§Œ ì•„ëŠ” ê±´ useless.) | . | dict1.update(another_dicationary): ë‹¤ë¥¸ dictionaryì˜ ë‚´ìš©ì„ ê°€ì ¸ì™€ì„œ dictionaryë¥¼ update. # dict1.update(another_dicationary) dict1 = {'Tom':21, 'Kai':20} dict2 = {'Sarah':22} dict1.update(dict2) # dict2ì˜ ì •ë³´ë¥¼ dict1ì— ì¶”ê°€ print(dict1) . {'Tom': 21, 'Kai': 20, 'Sarah': 22} . cf) updateí•  ë•Œ, ë‘ ê°œì˜ dictionaryê°€ ì¤‘ë³µë˜ëŠ” keyë¥¼ ê°–ê³  ìˆë‹¤ë©´? . dict1 = {'Tom':21, 'Kai':20} dict2 = {'Sarah':22, 'Tom':34} dict1.update(dict2) print(dict1) # 'Tom'ì˜ valueê°€ dict2ì˜ ê°’ìœ¼ë¡œ ì—…ë°ì´íŠ¸ë¨ . {'Tom': 34, 'Kai': 20, 'Sarah': 22} . | dict1.get(name, default_value) . | dictionaryì— ì¡´ì¬í•˜ëŠ” í‚¤ì¸ì§€ ì—¬ë¶€ì— ë”°ë¼ ê°’ì„ ë‹¤ë¥´ê²Œ ì²˜ë¦¬ | nameì´ë¼ëŠ” keyê°€ dict1ì— ìˆìœ¼ë©´ ê·¸ ê°’ì„ ê°€ì ¸ì˜¤ê³ , ì•„ë‹ˆë©´ ë”°ë¡œ ì…ë ¥í•œ default valueë¥¼ ë°˜í™˜ | . # 'get' method ì´ìš© ì˜ˆì‹œ ## 'names' ë¦¬ìŠ¤íŠ¸ì— ê° ì´ë¦„ì´ ëª‡ ê°œì”© ë‹´ê²¨ìˆëŠ”ì§€ ì„¸ì–´ì„œ 'counts'ë¼ëŠ” ë”•ì…”ë„ˆë¦¬ì— ë„£ëŠ” ê²ƒ. counts = dict() names = ['csev', 'cwen', 'csev', 'zqian', 'cwen'] for name in names: counts[name] = counts.get(name, 0) + 1 # counts.get(name, 0): counts ë”•ì…”ë„ˆë¦¬ì— name í‚¤ê°€ ì¡´ì¬í•  ê²½ìš° ì´ì˜ valueë¥¼ ë¶ˆëŸ¬ì˜¤ê³ , # ê·¸ë ‡ì§€ ì•Šì„ ê²½ìš°ì—ëŠ” counts ë”•ì…”ë„ˆë¦¬ì— {name : 0} ë°ì´í„°ë¥¼ ì¶”ê°€í•˜ë¼ëŠ” ì˜ë¯¸ print(counts) . {'csev': 2, 'cwen': 2, 'zqian': 1} . | dict1.items(): (key, value)ì˜ tupleì´ list-like variableë¡œ ì¶œë ¥ë¨ # dict1.items() dict1 = {'Tom':23, 'John':34, 'Bob':12} dict1.items() . dict_items([('Tom', 23), ('John', 34), ('Bob', 12)]) . +) dict1.items() í™œìš© ì˜ˆì‹œ . jjj = {'chuck':1, 'fred':42, 'jan':100} for aaa, bbb in jjj.items(): print(aaa, bbb) . chuck 1 fred 42 jan 100 . | . +) get()ê³¼ item() í•¨ìˆ˜ ì´ìš© ì˜ˆì‹œ: . dictionaryì˜ get()ê³¼ item() í•¨ìˆ˜ë¥¼ ì´ìš©í•´, ì˜ˆì‹œ í…ìŠ¤íŠ¸ì—ì„œ ê°€ì¥ ë§ì´ ë‚˜ì˜¨ ë‹¨ì–´ë¥¼ ì¶œë ¥í•˜ëŠ” ì½”ë“œ . text = \"love live love start hands shake you feel love lower less dry deny follow hi love live like yogurt timeline loyal love\" # ì—°ìŠµì„ ìœ„í•´, ëœë¤í•œ ë‹¨ì–´ë¥¼ ë‚˜ì—´í•œ ë‹¨ìˆœí•œ textë¥¼ ì‚¬ìš© counts = dict() words = text.split() for word in words: counts[word] = counts.get(word, 0) + 1 bigcount = None bigword = None for word, count in counts.items(): if bigword is None or count &gt; bigcount: bigword = word bigcount = count print(bigword, bigcount) # ê°€ì¥ ë§ì´ ë‚˜ì˜¨ ë‹¨ì–´ì™€ ê·¸ ì‚¬ìš© ë¹ˆë„ë¥¼ ì¶œë ¥ . love 5 . ",
    "url": "https://chaelist.github.io/docs/python_basics/dictionary_tuple_set/#dictionary",
    "relUrl": "/docs/python_basics/dictionary_tuple_set/#dictionary"
  },"46": {
    "doc": "Dictionary, Tuple, Set",
    "title": "Tuple",
    "content": ". | listì™€ ë¹„ìŠ·í•˜ë‚˜, [ ] ëŒ€ì‹  ( )ë¡œ êµ¬ì„±ë¨ | listì²˜ëŸ¼, ìˆœì„œê°€ ìˆì–´ì„œ index numberë¡œ ì ‘ê·¼ ê°€ëŠ¥ (indexing) | í•˜ì§€ë§Œ listì™€ ë‹¬ë¦¬ immutable. (í•œ ë²ˆ ë§Œë“  tupleì€ modifyí•  ìˆ˜ ì—†ë‹¤) | ë³€ê²½ë¶ˆê°€ëŠ¥ì´ê¸°ì— ë” simple, efficientí•˜ë‹¤ëŠ” ì¥ì  -&gt; preferred when making â€˜temporary variablesâ€™ | . Tuple ê¸°ì´ˆ . | tupleì— ì ìš© ê°€ëŠ¥í•œ í•¨ìˆ˜ë“¤ (tupleì€ listì™€ ë¹„ìŠ·í•˜ê¸°ì—, ëª‡ëª‡ list í•¨ìˆ˜ë“¤ì€ tupleì—ë„ ë™ì¼í•˜ê²Œ ì ìš©ëœë‹¤) # tuple ì˜ˆì‹œ x = ('Glenn', 'Sally', 'Joseph') print(x[0]) # indexing print(x[:2]) # slicing print(max(x)) # max, min ê¸°ëŠ¥ print(x.count('Glenn')) # x.count ê¸°ëŠ¥ (íŠ¹ì • elementì˜ ê°œìˆ˜ ì„¸ê¸°) print(x.index('Glenn')) # x.index ê¸°ëŠ¥ (íŠ¹ì • elementì˜ index number ì°¾ê¸°) . Glenn ('Glenn', 'Sally') Sally 1 0 . | tuple is immutable; í•œ ë²ˆ ë§Œë“  tupleì€ ë³€ê²½í•  ìˆ˜ ì—†ë‹¤ x = (9, 8, 7) x[2] = 6 ## index numberê°€ 2ì¸ elementë¥¼ 6ìœ¼ë¡œ ë°”ê¾¸ë ¤ ì‹œë„ --&gt; error. --------------------------------------------------------------------------- TypeError Traceback (most recent call last) &lt;ipython-input-4-0ca6b872377d&gt; in &lt;module&gt;() 1 x = (9, 8, 7) ----&gt; 2 x[2] = 6 ## index numberê°€ 2ì¸ elementë¥¼ 6ìœ¼ë¡œ ë°”ê¾¸ë ¤ ì‹œë„ --&gt; error. TypeError: 'tuple' object does not support item assignment . *ì£¼ì˜: tupleì€ í•œ ë²ˆ ìƒì„±ëœ í›„ì—ëŠ” ë³€ê²½ ë¶ˆê°€í•˜ê¸°ì—, x.sort(), x.append(), x.extend(), x.reverse() ë“±ì˜ ê¸°ëŠ¥ì´ ì—†ë‹¤. (listì— ì“°ëŠ” method ì¤‘, listë¥¼ modifyí•˜ëŠ” ì†ì„±ì˜ ì•„ì´ë“¤ì€ tupleì—ëŠ” ëª¨ë‘ ì ìš© ë¶ˆê°€!) . | . Tupleì˜ í™œìš©ë²• ì˜ˆì‹œ . | tupleì„ ì‚¬ìš©í•´ ì—¬ëŸ¬ ê°œ variableë¥¼ í•œ ë²ˆì— assign ê°€ëŠ¥ (x, y) = (4, 'fred') # ê°ê° x=4, y='fred'ë¡œ ì €ì¥ë¨ print(x) a, b = (99, 98) # ì´ë ‡ê²Œ ê´„í˜¸ë¥¼ ì§€ìš°ê³  assigní•´ë„ ëœë‹¤ print(a) a, b = 99, 98 # ì´ ê²ƒë„ ê°€ëŠ¥ print(b) ## ì´ íŠ¹ì„±ì„ ì´ìš©í•´ dictionary.items()ì˜ ê²°ê³¼ë¡œ ë‚˜ì˜¨ (key, value) tupleì— ì‰½ê²Œ ì ‘ê·¼ ê°€ëŠ¥ ## (ex. for k, v in di.items(): ì™€ ê°™ì€ ì½”ë“œ ì´ìš©) . 4 99 98 . | tuple ê°„ ë¹„êµ . | ì²«ë²ˆì§¸ë¶€í„° ìˆœì„œëŒ€ë¡œ ë¹„êµ; ë§Œì•½ ì²«ë²ˆì§¸ elementì´ ê°™ë‹¤ë©´ ë‹¤ìŒ elementë¼ë¦¬ ë¹„êµí•˜ê³ ,..ì´ëŸ° ì‹. | . print((0, 1, 2) &lt; (5, 1, 2)) print((0, 1, 3000) &lt; (0, 3, 4)) print(('Jones', 'Sally') &lt; ('Jones', 'Sam')) print(('Jones', 'Sally') &gt; ('Jones', 'Sam')) . True True True False . | dictionary ì •ë¦¬1: sorting by keys d = {'a':10, 'b':1, 'c':22} print(d.items()) print(sorted(d.items())) # list ì† tupleë“¤ì˜ ì²«ë²ˆì¬ elementë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë¹„êµí•´ì„œ sortí•´ì¤€ë‹¤ ## sorted()ë¥¼ í•˜ë©´ dict_items ëŒ€ì‹  list variableë¡œ ì¶œë ¥ë¨ . dict_items([('a', 10), ('b', 1), ('c', 22)]) [('a', 10), ('b', 1), ('c', 22)] . | dictionary ì •ë¦¬2: sorting by values (instead of keys) d = {'a':10, 'b':1, 'c':22} tmp = list() for k, v in d.items(): tmp.append((v, k)) # ë°˜ëŒ€ë¡œ ë’¤ì§‘ì–´ì„œ listì— ë„£ì–´ì¤Œ print(tmp) tmp = sorted(tmp, reverse=True) # ì—­ìˆœìœ¼ë¡œ ì •ë ¬ print(tmp) . [(10, 'a'), (1, 'b'), (22, 'c')] [(22, 'c'), (10, 'a'), (1, 'b')] . +) List Comprehension (ì°¸ê³ ) . # ìœ„ì—ì„œ ì¼ë˜ ë‹¤ìŒê³¼ ê°™ì€ ì½”ë“œë¥¼ tmp = list() for k, v in d.items(): tmp.append((v, k)) # --&gt; ì•„ë˜ì™€ ê°™ì´ ê°„ê²°í•˜ê²Œ ì‘ì„± ê°€ëŠ¥ tmp = [(v, k) for k, v in d.items()] . | . ",
    "url": "https://chaelist.github.io/docs/python_basics/dictionary_tuple_set/#tuple",
    "relUrl": "/docs/python_basics/dictionary_tuple_set/#tuple"
  },"47": {
    "doc": "Dictionary, Tuple, Set",
    "title": "Set (ì§‘í•©ìë£Œí˜•)",
    "content": ". | { } ì•ˆì— ê°’ë“¤ì´ ë‚˜ì—´ëœ í˜•íƒœ. (cf. dictionaryëŠ” { } ì•ˆì— â€˜key:valueâ€™ì˜ â€œì¡°í•©â€ë“¤ì´ ë‹´ê¸´ í˜•íƒœ) | ì¤‘ë³µì„ í—ˆìš©í•˜ì§€ ì•ŠëŠ”ë‹¤ | ìˆœì„œê°€ ì—†ë‹¤ (unordered) -&gt; indexing ë¶ˆê°€. (cf. dictionaryë„ ìˆœì„œê°€ ì—†ì–´ì„œ indexing ë¶ˆê°€) | í•˜ì§€ë§Œ, ì‰½ê²Œ listë‚˜ tupleë¡œ ë³€í™˜í•  ìˆ˜ ìˆê¸°ì—, listë‚˜ tupleë¡œ ë³€í™˜ í›„ indexingí•˜ë©´ ë¨ | . Set ê¸°ì´ˆ . | Set(ì§‘í•©) ë§Œë“¤ê¸° # 1. list ì…ë ¥í•´ì„œ set ë§Œë“¤ê¸° s1 = set([1, 2, 3]) print(s1) # 2. string ì…ë ¥í•´ì„œ set ë§Œë“¤ê¸° s2 = set(\"Hello\") print(s2) # ì¤‘ë³µì´ ì œê±°ë˜ì–´ ë‚˜ì˜¤ê³ , ìˆœì„œë„ ë’¤ì£½ë°•ì£½ìœ¼ë¡œ ë‚˜ì˜´ (ìˆœì„œê°€ ì—†ëŠ” ìë£Œí˜•ì´ë¼) . {1, 2, 3} {'H', 'o', 'l', 'e'} . | â€˜ì¤‘ë³µì„ í—ˆìš©í•˜ì§€ ì•ŠëŠ”ë‹¤â€™Â Â» ì¤‘ë³µ ì œê±° ëª©ì ìœ¼ë¡œ ì¢…ì¢… ì‚¬ìš© l1 = [1, 2, 3, 4, 1, 5, 2, 3, 6, 9] s1 = set(l1) print(s1) # ì¤‘ë³µë˜ëŠ” ìˆ«ìê°€ ë‹¤ ì œê±°ë¨ . {1, 2, 3, 4, 5, 6, 9} . | add(): ê°’ 1ê°œ ì¶”ê°€í•˜ê¸° # add s1 = set([1, 2, 3]) s1.add(4) # listì˜ append()ì™€ ìœ ì‚¬ s1 . {1, 2, 3, 4} . | update(): ê°’ ì—¬ëŸ¬ ê°œ ì¶”ê°€í•˜ê¸° # update s1 = set([1, 2, 3]) s1.update([4, 5, 6]) # listì˜ extend()ì™€ ìœ ì‚¬ s1 . {1, 2, 3, 4, 5, 6} . | remove(): ê°’ 1ê°œ ì œê±°í•˜ê¸° s1 = set([1, 2, 3]) s1.remove(2) s1 . {1, 3} . | . êµì§‘í•©, í•©ì§‘í•©, ì°¨ì§‘í•© . | êµì§‘í•© êµ¬í•˜ê¸°: &amp; ê¸°í˜¸ or intersection() í•¨ìˆ˜ # êµì§‘í•© s1 = set([1, 2, 3, 4]) s2 = set([3, 4, 5, 6]) print(s1 &amp; s2) # \"&amp;\" ê¸°í˜¸ë¡œ ê°„ë‹¨íˆ êµì§‘í•©ì„ êµ¬í•  ìˆ˜ ìˆë‹¤ print(s1.intersection(s2)) # intersection í•¨ìˆ˜ë¥¼ ì¨ë„ ë™ì¼í•œ ê²°ê³¼ . {3, 4} {3, 4} . | í•©ì§‘í•© êµ¬í•˜ê¸°: | ê¸°í˜¸ or union() í•¨ìˆ˜ # í•©ì§‘í•© s1 = set([1, 2, 3, 4]) s2 = set([3, 4, 5, 6]) print(s1 | s2) # \"|\" ê¸°í˜¸ë¡œ ê°„ë‹¨íˆ í•©ì§‘í•©ì„ êµ¬í•  ìˆ˜ ìˆë‹¤ print(s1.union(s2)) # union í•¨ìˆ˜ë¥¼ ì¨ë„ ë™ì¼í•œ ê²°ê³¼ . {1, 2, 3, 4, 5, 6} {1, 2, 3, 4, 5, 6} . | ì°¨ì§‘í•© êµ¬í•˜ê¸°: - ê¸°í˜¸ or difference() í•¨ìˆ˜ # ì°¨ì§‘í•© s1 = set([1, 2, 3, 4]) s2 = set([3, 4, 5, 6]) # s1ì— ëŒ€í•œ s2ì˜ ì°¨ì§‘í•© print(s1 - s2) # \"-\" ê¸°í˜¸ ì‚¬ìš© print(s1.difference(s2)) # difference í•¨ìˆ˜ ì‚¬ìš© # s2ì— ëŒ€í•œ s1ì˜ ì°¨ì§‘í•© print(s2 - s1) # \"-\" ê¸°í˜¸ ì‚¬ìš© print(s2.difference(s1)) # difference í•¨ìˆ˜ ì‚¬ìš© . {1, 2} {1, 2} {5, 6} {5, 6} . | . ",
    "url": "https://chaelist.github.io/docs/python_basics/dictionary_tuple_set/#set-%EC%A7%91%ED%95%A9%EC%9E%90%EB%A3%8C%ED%98%95",
    "relUrl": "/docs/python_basics/dictionary_tuple_set/#set-ì§‘í•©ìë£Œí˜•"
  },"48": {
    "doc": "ë¹ˆë„ ë¶„ì„ (English)",
    "title": "ë¹ˆë„ ë¶„ì„ (English)",
    "content": ". | nltk ì¤€ë¹„ &amp; text ìˆ˜ì§‘ | ì „ì²˜ë¦¬ (Preprocessing) . | Text Cleaning | Case Conversion | Tokenization | POS tagging | Lemmatization | Stopwords Removal | . | Frequency Analysis . | Counter | WordCloud | . | . ",
    "url": "https://chaelist.github.io/docs/text_analysis/english_text/",
    "relUrl": "/docs/text_analysis/english_text/"
  },"49": {
    "doc": "ë¹ˆë„ ë¶„ì„ (English)",
    "title": "nltk ì¤€ë¹„ &amp; text ìˆ˜ì§‘",
    "content": "*nltk: natural language tool kit (English text anlaysisì— í•„ìš”í•œ python module) . | pip install nltkë¡œ ì„¤ì¹˜ | ì²˜ìŒ ì‚¬ìš©í•  ë•ŒëŠ” ì•„ë˜ì™€ ê°™ì´ ë‹¤ìš´ë°›ì•„ì•¼ ì‚¬ìš© ê°€ëŠ¥ import nltk nltk.download('all') ## í•œ ì»´í“¨í„°ì— í•œ ë²ˆë§Œ í•´ë‘ë©´ ëœë‹¤ . | . *Frequency Analysisì— ì‚¬ìš©í•  text ìˆ˜ì§‘í•´ì˜¤ê¸° . import requests from bs4 import BeautifulSoup url = 'https://www.nytimes.com/2017/06/12/well/live/having-friends-is-good-for-you.html' r = requests.get(url) soup = BeautifulSoup(r.text, 'lxml') title = soup.title.text.strip() text_list = soup.select('p.css-axufdj') content = '' for text in text_list: content = content +' '+ text.text.strip() print(title, '\\n') print(content) . Social Interaction Is Critical for Mental and Physical Health - The New York Times Hurray for the HotBlack Coffee cafe in Toronto for declining to offer Wi-Fi to its customers. There are other such cafes, to be sure, including seven of the eight New York City locations of CafÃ© Grumpy. But itâ€™s HotBlackâ€™s reason for the electronic blackout that is cause for hosannas. As its president, Jimson Bienenstock, explained, his aim is to get customers to talk with one another instead of being buried in their portable devices. â€œItâ€™s about creating a social vibe,â€ he told a New York Times reporter. â€œWeâ€™re a vehicle for human interaction, otherwise itâ€™s just a commodity.â€ What a novel idea! Perhaps Mr. Bienenstock instinctively knows what medical science has been increasingly demonstrating for decades: Social interaction is a critically important contributor to good health and longevity. Personally, I donâ€™t need research-based evidence to appreciate... (ìƒëµ) . ",
    "url": "https://chaelist.github.io/docs/text_analysis/english_text/#nltk-%EC%A4%80%EB%B9%84--text-%EC%88%98%EC%A7%91",
    "relUrl": "/docs/text_analysis/english_text/#nltk-ì¤€ë¹„--text-ìˆ˜ì§‘"
  },"50": {
    "doc": "ë¹ˆë„ ë¶„ì„ (English)",
    "title": "ì „ì²˜ë¦¬ (Preprocessing)",
    "content": "Â  . *ì „ì²˜ë¦¬ ê³¼ì • (English): . | Text Cleaning: ë¶ˆí•„ìš”í•œ ê¸°í˜¸ / í‘œí˜„ ì—†ì• ê¸°(ì˜ˆ, !, ., â€œ, ; ë“±) | Case Conversion: ëŒ€ì†Œë¬¸ì ë³€í™˜. ì†Œë¬¸ì â†” ëŒ€ë¬¸ì | Tokenization: ë‹¨ì–´ (í˜¹ì€ Token) ë‹¨ìœ„ë¡œ ì˜ë¼ì£¼ê¸° | POS tagging: ë‹¨ì–´ì˜ í’ˆì‚¬ ì°¾ê¸° | ì›í•˜ëŠ” í’ˆì‚¬ì˜ ë‹¨ì–´ë“¤ë§Œ ì„ íƒ | Lemmatization: (=Stemming) ë‹¨ì–´ì˜ ì›í˜•(í˜¹ì€ ì–´ê·¼) ì°¾ê¸° | Stopwords Removal: ë¶ˆìš©ì–´ ì œê±° | . í•„ìš”ì— ë”°ë¼ì„œëŠ” ìœ„ ê³¼ì •ì˜ ìˆœì„œê°€ ë°”ë€”ìˆ˜ë„ ìˆê³ , ê°™ì€ ê³¼ì •ì„ ë°˜ë³µ ìˆ˜í–‰í•  ìˆ˜ë„ ìˆë‹¤ . Text Cleaning . : ë¶ˆí•„ìš”í•œ symbol / mark ë“± ì œê±° . | .replace() í˜¹ì€ ì •ê·œì‹ í™œìš© | â€» ë¶„ì„ ëª©ì ì— ë”°ë¼, â€˜ì–´ë– í•œ mark ë‚˜ symbol ë“¤ì„ ì´ ë‹¨ê³„ì—ì„œ ì œê±°í•  ê²ƒì¸ê°€?â€™ë¥¼ ê³ ë¯¼í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•¨ . | ex) í…ìŠ¤íŠ¸ ë¶„ì„ì´ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ì´ë£¨ì–´ì§€ëŠ” ê²½ìš°, ë¬¸ì¥ì˜ ëì„ ë‚˜íƒ€ë‚´ëŠ” ê¸°í˜¸ë“¤(ë§ˆì¹¨í‘œ(.) ë¬¼ìŒí‘œ(?) ëŠë‚Œí‘œ(!) ë“±)ì„ ì—†ì• ì§€ ì•Šì•„ì•¼ í•œë‹¤ | . | . # ë¶ˆí•„ìš”í•œ ê¸°í˜¸ ì—†ì• ê¸° - ì •ê·œì‹ ì‚¬ìš© import re filtered_content = re.sub('[^,.?!\\w\\s]','', content) ## ,.?!ì™€ ë¬¸ì+ìˆ«ì+_(\\w)ì™€ ê³µë°±(\\s)ë§Œ ë‚¨ê¹€ filtered_content . Hurray for the HotBlack Coffee cafe in Toronto for declining to offer WiFi to its customers. There are other such cafes, to be sure, including seven of the eight New York City locations of CafÃ© Grumpy. But its HotBlacks reason for the electronic blackout that is cause for hosannas. As its president, Jimson Bienenstock, explained, his aim is to get customers to talk with one another instead of being buried in their portable devices. Its about creating a social vibe, he told a New York Times reporter. Were a vehicle for human interaction, otherwise its just a commodity. What a novel idea! Perhaps Mr. Bienenstock instinctively knows what medical science has been increasingly demonstrating for decades Social interaction is a critically important contributor to good health and longevity. Personally, I dont need researchbased evidence to appreciate the value of making and maintaining social connections. I experience it daily during my morning walk with up to three women, then... (ìƒëµ) . +) ì•„ë˜ì™€ ê°™ì´ replaceë¥¼ ì‚¬ìš©í•´ì„œ ì œê±°í•  ìˆ˜ë„ ìˆë‹¤ . filtered_content = content.replace('[','').replace(']','').replace('\"','') filtered_content = filtered_content.replace('â€œ','').replace('â€','').replace('â€™',\"\") # +) ì•„ë˜ì™€ ê°™ì´ íŠ¹ì • text íŠ¹ì„±ìƒ ë¶ˆí•„ìš”í•œ ë¶€ë¶„ë„ ì¶”ê°€ë¡œ ì œê±° ê°€ëŠ¥ filtered_content = filtered_content.replace('The New York Times', '') . Case Conversion . | ì˜ì–´ì˜ ê²½ìš°, pythonì—ì„œëŠ” ê°™ì€ ë‹¨ì–´ë¼ë„ ëŒ€ë¬¸ìì™€ ì†Œë¬¸ìë¥¼ ë‹¤ë¥´ê²Œ ì¸ì‹í•˜ë¯€ë¡œ í†µì¼í•´ì¤˜ì•¼ í•œë‹¤ | . filtered_content = filtered_content.lower() # .lower()ë¥¼ ì‚¬ìš©í•´ì„œ ì†Œë¬¸ìë¡œ ë³€í™˜ filtered_content . hurray for the hotblack coffee cafe in toronto for declining to offer wi-fi to its customers. there are other such cafes, to be sure, including seven of the eight new york city locations of cafÃ© grumpy. but its hotblacks reason for the electronic blackout that is cause for hosannas. as its president, jimson bienenstock, explained, his aim is to get customers to talk with one another instead of being buried in their portable devices. its about creating a social vibe, he told a new york times reporter. were a vehicle for human interaction, otherwise its just a commodity. what a novel idea! perhaps mr. bienenstock instinctively knows what medical science has been increasingly demonstrating for decades: social interaction is a critically important contributor to good health and longevity. personally, i dont need research-based evidence to appreciate the value of making and maintaining social connections. i experience it daily during my morning walk with up to three women, then... (ìƒëµ) . Tokenization . | Token: ëœ»ì„ ê°–ê³  ì‚¬ìš©ë  ìˆ˜ ìˆëŠ” ê°€ì¥ ì‘ì€ ê¸€ì˜ ë‹¨ìœ„. ë³´í†µ í•˜ë‚˜ì˜ ë‹¨ì–´(word)ë¼ê³  ìƒê°í•˜ë©´ ëœë‹¤ | Tokenization:í† í°(ë‹¨ì–´) ë‹¨ìœ„ë¡œ ì˜ë¼ì£¼ëŠ” ê²ƒ | . *ë‘ ê°€ì§€ ë°©ë²•ì´ ê°€ëŠ¥: . | nltk.word_tokenize() í•¨ìˆ˜ ì‚¬ìš© | .split() í•¨ìˆ˜ë¡œ whitespace ê¸°ì¤€ìœ¼ë¡œ text split (ì˜ì–´ì˜ ê²½ìš° split()ìœ¼ë¡œë§Œ ì˜ë¼ì¤˜ë„ ë‹¨ì–´ ë‹¨ìœ„ë¡œ ì˜ ì˜ë¦¼) | . # nltk.word_tokenize() í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´ì„œ tokenize import nltk # importí•´ì„œ ì‚¬ìš© word_tokens = nltk.word_tokenize(filtered_content) print(word_tokens) ## ë‹¨ì–´ì˜ listë¡œ ë‚˜ì˜´ . ['hurray', 'for', 'the', 'hotblack', 'coffee', 'cafe', 'in', 'toronto', 'for', 'declining', 'to', 'offer', 'wi-fi', 'to', 'its', 'customers', '.', 'there', 'are', 'other', 'such', 'cafes', ',', 'to', 'be', 'sure', ',', 'including', 'seven', 'of', 'the', 'eight', 'new', 'york', 'city', 'locations', 'of', 'cafÃ©', 'grumpy', '.', 'but', 'its', 'hotblacks', 'reason', 'for', 'the', 'electronic', 'blackout', 'that', 'is', 'cause', 'for', 'hosannas', '.', 'as', 'its', 'president', ',', 'jimson', 'bienenstock', ',', 'explained', ',', 'his', 'aim', 'is', 'to', 'get', 'customers', 'to', 'talk', 'with', 'one', 'another', 'instead', 'of', 'being', 'buried', 'in', 'their', 'portable', 'devices', '.', 'its', 'about', 'creating', 'a', 'social', 'vibe', ',', 'he', 'told', 'a', 'new', 'york', 'times', 'reporter', '.', 'were', 'a', 'vehicle', 'for', 'human', 'interaction', ',', 'otherwise', 'its', 'just', 'a', 'commodity', '.', 'what', 'a', 'novel', 'idea', (ìƒëµ)] . POS tagging . : (=Parts-of-Speech tagging) ê° ë‹¨ì–´ì˜ í’ˆì‚¬ë¥¼ ì°¾ì•„ì„œ íƒœê¹…. (ex. â€˜thisâ€™: pronoun(ëŒ€ëª…ì‚¬), â€˜classâ€™: noun, â€˜isâ€™: beë™ì‚¬, â€˜aâ€™: article(ê´€ì‚¬), â€¦) . | tagëœ í’ˆì‚¬ì˜ ì¢…ë¥˜ í•´ì„í•˜ëŠ” ë²•: https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html | . # nltk.pos_tag() í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´ì„œ í’ˆì‚¬ íƒœê¹… tokens_pos = nltk.pos_tag(word_tokens) # pos_tag()ì˜ ì…ë ¥ê°’ìœ¼ë¡œëŠ” ë‹¨ì–´ì˜ ë¦¬ìŠ¤íŠ¸ê°€ ë“¤ì–´ê°€ì•¼ í•œë‹¤ print(tokens_pos) . [('hurray', 'NN'), ('for', 'IN'), ('the', 'DT'), ('hotblack', 'NN'), ('coffee', 'NN'), ('cafe', 'NN'), ('in', 'IN'), ('toronto', 'NN'), ('for', 'IN'), ('declining', 'VBG'), ('to', 'TO'), ('offer', 'VB'), ('wi-fi', 'JJ'), ('to', 'TO'), ('its', 'PRP$'), ('customers', 'NNS'), ('.', '.'), ('there', 'EX'), ('are', 'VBP'), ('other', 'JJ'), ('such', 'JJ'), ('cafes', 'NNS'), (',', ','), ('to', 'TO'), ('be', 'VB'), ('sure', 'JJ'), (',', ','), ('including', 'VBG'), ('seven', 'CD'), ('of', 'IN'), ('the', 'DT'), ('eight', 'CD'), ('new', 'JJ'), ('york', 'NN'), ('city', 'NN'), ('locations', 'NNS'), ('of', 'IN'), ('cafÃ©', 'NN'), ('grumpy', 'NN'), ('.', '.'), ('but', 'CC'), ('its', 'PRP$'), ('hotblacks', 'NNS'), ('reason', 'NN'), ('for', 'IN'), ('the', 'DT'), ('electronic', 'JJ'), ('blackout', 'NN'), ('that', 'WDT'), ('is', 'VBZ'), ('cause', 'NN'), ('for', 'IN'), ('hosannas', 'NN'), ('.', '.'), ('as', 'IN'), ('its', 'PRP$'), ('president', 'NN'), (ìƒëµ)] . +) íŠ¹ì • PoSì˜ ë‹¨ì–´ë“¤ë§Œ ì¶”ì¶œí•˜ê¸° . | ë³´í†µ, ì£¼ì œ/í‚¤ì›Œë“œë¥¼ ë¶„ì„í•˜ê³ ì í•  ë•ŒëŠ” nounë§Œ ì‚¬ìš©í•´ì„œ ë¶„ì„ | í•˜ì§€ë§Œ ê°ì • ë¶„ì„ì—ëŠ” í˜•ìš©ì‚¬/ë¶€ì‚¬ë„ ì¤‘ìš” â†’ ëª©ì ì— ë”°ë¼ ì ì ˆíˆ í•„ìš”í•œ í’ˆì‚¬ ì„ íƒ! | . # ëª…ì‚¬ ë‹¨ì–´ë§Œ ì¶”ì¶œí•˜ê¸° NN_words = [] for word, pos in tokens_pos: if 'NN' in pos: ## Noun ì¢…ë¥˜ 4ê°œëŠ” ëª¨ë‘ 'NN'ì„ í¬í•¨í•˜ê³  ìˆì–´ì„œ (NN, NNS, NNP, NNPS) NN_words.append(word) print(NN_words) . ['hurray', 'hotblack', 'coffee', 'cafe', 'toronto', 'customers', 'cafes', 'york', 'city', 'locations', 'cafÃ©', 'grumpy', 'hotblacks', 'reason', 'blackout', 'cause', 'hosannas', 'president', 'jimson', 'bienenstock', 'aim', 'customers', 'devices', 'vibe', 'york', 'times', 'vehicle', 'interaction', 'commodity', 'idea', 'bienenstock', 'science', 'decades', 'interaction', 'contributor', 'health', 'longevity', 'evidence', 'value', 'connections', 'experience', 'morning', 'walk', 'women', 'swim', (ìƒëµ)] . Lemmatization . : lemma=ì›í˜•. ë‹¨ì–´ë¥¼ ì›í˜•ìœ¼ë¡œ ë°”ê¿”ì¤Œ (ex. is -&gt; be, ate -&gt; eat, methods -&gt; method) . | ë™ì‚¬ëŠ” ë™ì‚¬ì›í˜•ìœ¼ë¡œ, ëª…ì‚¬ëŠ” singular(ë‹¨ìˆ˜í˜•)ìœ¼ë¡œ ë³€í™˜ | . # nltk.WordNetLemmatizer() ì‚¬ìš© wlem = nltk.WordNetLemmatizer() lemmatized_words = [] for word in NN_words: new_word = wlem.lemmatize(word) lemmatized_words.append(new_word) print(lemmatized_words) . ['hurray', 'hotblack', 'coffee', 'cafe', 'toronto', 'customer', 'cafe', 'york', 'city', 'location', 'cafÃ©', 'grumpy', 'hotblacks', 'reason', 'blackout', 'cause', 'hosanna', 'president', 'jimson', 'bienenstock', 'aim', 'customer', 'device', 'vibe', 'york', 'time', 'vehicle', 'interaction', 'commodity', 'idea', 'bienenstock', 'science', 'decade', (ìƒëµ)] . Stopwords Removal . : ë¶ˆìš©ì–´ ì œê±°. articles(a, an, the,â€¦), pronouns(this, that,â€¦) ë“± ë³„ ì˜ë¯¸ê°€ ì—†ëŠ” ë‹¨ì–´ë“¤ì„ ì œê±°í•´ì¤€ë‹¤. # 1ì°¨ì ìœ¼ë¡œ nltkì—ì„œ ì œê³µí•˜ëŠ” ë¶ˆìš©ì–´ì‚¬ì „ì„ ì´ìš©í•´ì„œ ë¶ˆìš©ì–´ë¥¼ ì œê±° from nltk.corpus import stopwords stopwords_list = stopwords.words('english') # nltkì—ì„œ ì œê³µí•˜ëŠ” ì˜ì–´ ë¶ˆìš©ì–´ì‚¬ì „ unique_NN_words = set(lemmatized_words) ## ì¤‘ë³µì„ ì œê±°í•˜ê¸° ìœ„í•´ set(ì§‘í•©í˜•)ìœ¼ë¡œ ë³€í™˜ final_NN_words = lemmatized_words # ë¶ˆìš©ì–´ ì œê±° for word in unique_NN_words: if word in stopwords_list: while word in final_NN_words: final_NN_words.remove(word) print(final_NN_words) . ['hurray', 'hotblack', 'coffee', 'cafe', 'toronto', 'customer', 'cafe', 'york', 'city', 'location', 'cafÃ©', 'grumpy', 'hotblacks', 'reason', 'blackout', 'cause', 'hosanna', 'president', 'jimson', 'bienenstock', 'aim', 'customer', 'device', 'vibe', 'york', 'time', 'vehicle', 'interaction', 'commodity', 'idea', 'bienenstock', 'science', 'decade', 'interaction', 'contributor', (ìƒëµ)] . +) ì œê±°í•˜ê³ ì í•˜ëŠ” ë‹¨ì–´ê°€ nltk ì œê³µ ì‚¬ì „ì— í¬í•¨ë˜ì–´ ìˆì§€ ì•Šë‹¤ë©´, ì•„ë˜ì™€ ê°™ì´ ì§ì ‘ ë¶ˆìš©ì–´ ì‚¬ì „ì„ ë§Œë“¤ì–´ ì¶”ê°€ë¡œ ë‹¨ì–´ë¥¼ ì œê±°í•´ë„ ëœë‹¤ . customized_stopwords = ['be', 'today', 'yesterday'] # ì§ì ‘ ë§Œë“  ë¶ˆìš©ì–´ ì‚¬ì „ unique_NN_words1 = set(final_NN_words) for word in unique_NN_words1: if word in customized_stopwords: while word in final_NN_words: final_NN_words.remove(word) . ",
    "url": "https://chaelist.github.io/docs/text_analysis/english_text/#%EC%A0%84%EC%B2%98%EB%A6%AC-preprocessing",
    "relUrl": "/docs/text_analysis/english_text/#ì „ì²˜ë¦¬-preprocessing"
  },"51": {
    "doc": "ë¹ˆë„ ë¶„ì„ (English)",
    "title": "Frequency Analysis",
    "content": "Counter . : ë‹¨ì–´ ì‚¬ìš© ë¹ˆë„ë¥¼ ê³„ì‚°í•´ì£¼ëŠ” ëª¨ë“ˆ. from collections import Counter # importí•´ì„œ ì‚¬ìš© c = Counter(final_NN_words) ## ë‹¨ì–´ ê°œìˆ˜ë¥¼ ì„¸ì–´ì¤€ë‹¤ print(c) . Counter({'health': 11, 'people': 11, 'researcher': 7, 'study': 6, 'tie': 6, 'interaction': 5, 'friend': 4, 'others': 4, 'exercise': 4, 'york': 3, 'time': 3, 'connection': 3, 'woman': 3, 'problem': 3, 'lifestyle': 3, 'smoking': 3, 'lack': 3, 'heart': 3, 'death': 3, 'connectedness': 3, 'disease': 3, 'loneliness': 3, 'research': 3, 'blood': 3, 'inflammation': 3, 'texas': 3, 'seppala': 3, 'cafe': 2, 'customer': 2, 'reason': 2, 'bienenstock': 2, 'device': 2, 'longevity': 2, 'evidence': 2, (ìƒëµ)}) . â†’ ê°€ì¥ ë§ì´ ë‚˜ì˜¨ ë‹¨ì–´ 10ê°œ ì¶”ì¶œ . print(c.most_common(10)) . [('health', 11), ('people', 11), ('researcher', 7), ('study', 6), ('tie', 6), ('interaction', 5), ('friend', 4), ('others', 4), ('exercise', 4), ('york', 3)] . WordCloud . : ë‹¨ì–´ì˜ ë¹ˆë„ë¥¼ í¬ê¸°ë¡œ í‘œí˜„í•´ ì›Œë“œí´ë¼ìš°ë“œë¥¼ ë§Œë“¤ì–´ì£¼ëŠ” ëª¨ë“ˆ . from wordcloud import WordCloud import matplotlib.pyplot as plt total_words = '' for word in final_NN_words: total_words = total_words+' '+word wordcloud = WordCloud(max_font_size=40, relative_scaling=.5).generate(total_words) plt.figure() plt.imshow(wordcloud) plt.axis(\"off\") plt.show() . | max_font_size: ê°€ì¥ í¬ê¸°ê°€ í¬ê²Œ ë‚˜ì˜¬ (=ê°€ì¥ ë¹ˆë„ê°€ ë†’ì€) ë‹¨ì–´ì˜ í¬ê¸°ì— ì œí•œì„ ë‘  | relative_scaling: ë¹ˆë„ ì°¨ì´ì— ë”°ë¥¸ scaling ì •ë„ë¥¼ ì§€ì •. | 1ì¼ ê²½ìš°, 2ë°°ë¡œ ë¹ˆë„ê°€ ë†’ìœ¼ë©´ 2ë°°ë¡œ í¬ê²Œ ê·¸ë ¤ì§. 0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ í¬ê¸° ì°¨ì´ê°€ ëœ ë‚˜ê²Œ ê·¸ë¦¬ëŠ” ê²ƒ. | . | . +) colormap, ë°°ê²½ìƒ‰ ì§€ì • . wordcloud = WordCloud(relative_scaling=.2, background_color='white', colormap='summer').generate(total_words) plt.imshow(wordcloud) plt.axis('off') plt.show() . +) ì›Œë“œí´ë¼ìš°ë“œë¥¼ íŒŒì¼ë¡œ ì €ì¥í•˜ê¸° . wordcloud.to_file('WordCloud.png') . ",
    "url": "https://chaelist.github.io/docs/text_analysis/english_text/#frequency-analysis",
    "relUrl": "/docs/text_analysis/english_text/#frequency-analysis"
  },"52": {
    "doc": "íŒŒì¼ & í´ë” ë‹¤ë£¨ê¸°",
    "title": "íŒŒì¼ &amp; í´ë” ë‹¤ë£¨ê¸°",
    "content": ". | ì¡°íšŒ &amp; ì •ë³´ íƒìƒ‰ . | ìš´ì˜ì²´ì œ &amp; ê²½ë¡œ | íŒŒì¼/í´ë” ì •ë³´ íƒìƒ‰ | íŒŒì¼/í´ë” ì¡°íšŒ | í´ë”/íŒŒì¼ ì—¬ë¶€ íŒë³„ | . | ë³µì‚¬ &amp; ì´ë™ &amp; ì‚­ì œ | ì••ì¶• íŒŒì¼ ìƒì„± . | shutil í™œìš© | zipfile í™œìš© | . | . *ì‚¬ìš©í•  ëª¨ë“ˆ: os, shutil, zipfile (ëª¨ë‘ ë³„ë„ì˜ ì„¤ì¹˜ ì—†ì´ importí•´ì„œ ì‚¬ìš© ê°€ëŠ¥) . import os import shutil import zipfile . ",
    "url": "https://chaelist.github.io/docs/data_handling/file_folder_handling/#%ED%8C%8C%EC%9D%BC--%ED%8F%B4%EB%8D%94-%EB%8B%A4%EB%A3%A8%EA%B8%B0",
    "relUrl": "/docs/data_handling/file_folder_handling/#íŒŒì¼--í´ë”-ë‹¤ë£¨ê¸°"
  },"53": {
    "doc": "íŒŒì¼ & í´ë” ë‹¤ë£¨ê¸°",
    "title": "ì¡°íšŒ &amp; ì •ë³´ íƒìƒ‰",
    "content": "ìš´ì˜ì²´ì œ &amp; ê²½ë¡œ . | í˜„ì¬ì˜ ìš´ì˜ì²´ì œ í™•ì¸ . os.name . | windowsë©´ â€˜ntâ€™, macOSë‚˜ Linuxë¼ë©´ â€˜posixâ€™ë¼ê³  ì¶œë ¥ë¨ | . | working directory í™•ì¸ (í˜„ì¬ ì´ ì½”ë“œ íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ” directory ìœ„ì¹˜) . print(os.getcwd()) # ì•„ë˜ì™€ ê°™ì´ ì ˆëŒ€ ê²½ë¡œë¡œ í‘œí˜„ë˜ì–´ ë‚˜ì˜¨ë‹¤ . C:\\Users\\admin\\Desktop\\python . | windowsë©´ \\ë¡œ, macOSë©´ /ë¡œ êµ¬ë¶„ëœë‹¤ | . | íŠ¹ì • íŒŒì¼/í´ë”ì˜ ì ˆëŒ€ ê²½ë¡œ í™•ì¸í•˜ê¸° . print(os.path.abspath('Python ê¸°ì´ˆ')) . C:\\Users\\admin\\Desktop\\python\\Python ê¸°ì´ˆ . | í˜„ì¬ ìš´ì˜ì²´ì œì—ì„œ ì‚¬ìš©í•˜ëŠ” ê²½ë¡œ êµ¬ë¶„ ê¸°í˜¸ í™•ì¸ . print(os.path.sep) . | windowsë©´ \\, macOSë©´ /ê°€ ì¶œë ¥ë¨ | . | os.path.joinì„ ì‚¬ìš©í•œ ê²½ë¡œ êµ¬ë¶„ . | os.path.joinì„ ì‚¬ìš©í•˜ë©´ ë‚´ê°€ ì‚¬ìš©í•˜ëŠ” ìš´ì˜ì²´ì œì— ì•Œë§ê²Œ êµ¬ë¶„ ê¸°í˜¸ë¥¼ ë¶™ì—¬ì„œ ê²½ë¡œë¥¼ ë§Œë“¤ì–´ì¤Œ (ìš´ì˜ì²´ì œë³„ ê²½ë¡œ êµ¬ë¶„ êµ¬ì• ë°›ì§€ ì•Šê³  ì‚¬ìš©í•˜ëŠ” ë°©ë²•) | . path = os.path.join('User', 'Folder', 'image.jpg') print(path) . User\\Folder\\image.jpg . | ê²½ë¡œ ê²€ì¦í•˜ê¸° (ì‹¤ì œ í•´ë‹¹ ê²½ë¡œì— í•´ë‹¹ íŒŒì¼/í´ë”ê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸) . # íŒŒì¼ / í´ë” ëª¨ë‘ ë™ì¼í•˜ê²Œ ê²€ì¦ ê°€ëŠ¥ print(os.path.exists('Python ê¸°ì´ˆ')) print(os.path.exists('ì—†ëŠ” íŒŒì¼.png')) . True False . | ê²½ë¡œë¥¼ ë‹¤ë£° ë•Œì—ëŠ”, if os.path.exists('path'):ì²˜ëŸ¼ ifë¬¸ìœ¼ë¡œ ê²½ë¡œì˜ ì¡´ì¬ ì—¬ë¶€ë¥¼ ë¨¼ì € ê²€ì¦í•˜ê³  ê·¸ ë‹¤ìŒ ì½”ë“œë¥¼ ì¨ì£¼ë©´ ë” ì•ˆì „í•˜ë‹¤ | . | . Â  . +) ê²½ë¡œ ê´€ë ¨ ì •ë³´: . | windowsì˜ ê²½ìš°, ë‹¨ìˆœíˆ â€˜\\â€˜ë¡œë§Œ ê²½ë¡œë¥¼ êµ¬ë¶„í•˜ê¸°ë³´ë‹¤, â€˜\\\\â€™ ì´ë ‡ê²Œ ë‘ ë²ˆ ì¨ì£¼ëŠ” ê²Œ ì¢‹ë‹¤. â€“ ex) â€˜user\\nineâ€™ ì´ëŸ° ê²½ìš°ì—ëŠ” â€˜\\nâ€™ = ì—”í„°ë¡œ ì¸ì‹ë˜ì–´ë²„ë¦¬ê¸° ë•Œë¬¸. | ì•„ë¬´ê²ƒë„ ì“°ì§€ ì•Šê±°ë‚˜ â€˜.â€™ì´ë¼ê³  ì“°ë©´ working directoryë¥¼ ì˜ë¯¸ | â€™..â€™ì´ë¼ê³  ì“°ë©´ wdì˜ ìƒìœ„ ë””ë ‰í† ë¦¬ë¥¼ ì˜ë¯¸, â€˜../..â€™ë¼ê³  ì“°ë©´ í•œ ë²ˆ ë” ê±°ìŠ¬ëŸ¬ ì˜¬ë¼ê°„ ìƒìœ„ ë””ë ‰í† ë¦¬ë¥¼ ì˜ë¯¸ | . íŒŒì¼/í´ë” ì •ë³´ íƒìƒ‰ . | ìš©ëŸ‰ í™•ì¸ . | íŒŒì¼ ìš©ëŸ‰: ì»´í“¨í„°ì˜ ì €ì¥ì¥ì¹˜(HDD, SSD)ì— ì°¨ì§€í•˜ëŠ” ê³µê°„ì˜ ì–‘ | . os.path.getsize('íŒŒì¼/í´ë”ëª…') . 4096 . | ì´ ë•Œ ì¶œë ¥ë˜ëŠ” ê°’ì€ byte ë‹¨ìœ„. | 1kB = 1000byte, 1MB = 1000kB, 1GB = 1000MB | . | í™•ì¥ì ì •ë³´ ë¶„ë¦¬ . | ì¼ë°˜ì ì¸ íŒŒì¼ëª…ì˜ ê²½ìš° â€˜.â€™ì€ ì˜ ì•ˆì“°ë‹ˆê¹Œ split(â€˜.â€™)ìœ¼ë¡œ ë‚˜ëˆ ë„ ë˜ì§€ë§Œ, os.path.splittextë¥¼ ì‚¬ìš©í•˜ë©´ ì•„ë˜ì™€ ê°™ì€ íŠ¹ìˆ˜ ê²½ìš°ì—ë„ ì•ˆì „í•˜ê²Œ í™•ì¥ì ì •ë³´ë¥¼ ì–»ì„ ìˆ˜ ìˆìŒ | . file_ex = 'quarter.report.pdf' filename, extension = os.path.splitext(file_ex) print('íŒŒì¼ëª…:', filename) print('í™•ì¥ì:', extension) . íŒŒì¼ëª…: quarter.report í™•ì¥ì: .pdf . | ìƒì„±í•œ ë‚ ì§œ í™•ì¸ . import datetime birthtimestamp = os.path.getctime('Python/test.txt') # íŒŒì¼/í´ë”ì— ëª¨ë‘ ì ìš© ê°€ëŠ¥ birthtime = datetime.datetime.fromtimestamp(birthtimestamp) print(birthtime) . 2020-04-01 00:13:57.907304 . | os.path.getctime('path')ì˜ ê²°ê³¼ëŠ” timestamp í˜•íƒœë¡œ returnë˜ë¯€ë¡œ, datetime.datetime.fromtimestampë¥¼ ì‚¬ìš©í•´ ë³€í™˜í•´ì¤˜ì•¼ í•œë‹¤ (*timestamp: 1970ë…„ 1ì›” 1ì¼ë¶€í„° ì§€ë‚œ ì‹œê°„ì„ ì´ˆë¡œ í™˜ì‚°í•œ ê²ƒ) | os.stat('path').st_ctimeì„ ì‚¬ìš©í•´ë„ ë™ì¼í•œ ê²°ê³¼ | . | ë§ˆì§€ë§‰ìœ¼ë¡œ ìˆ˜ì •í•œ ë‚ ì§œ í™•ì¸ . mtimestamp = os.path.getmtime('Python/test.txt') mtime = datetime.datetime.fromtimestamp(mtimestamp) print(mtime) . 2021-04-01 14:12:23.278385 . | os.stat('path').st_atimeì„ ì‚¬ìš©í•´ë„ ë™ì¼í•œ ê²°ê³¼ | . | ë§ˆì§€ë§‰ìœ¼ë¡œ accessí•œ ë‚ ì§œ í™•ì¸ . atimestamp = os.path.getatime('Python/test.txt') atime = datetime.datetime.fromtimestamp(atimestamp) print(atime) . 2021-04-01 14:09:57.907304 . | os.stat('path').st_mtimeì„ ì‚¬ìš©í•´ë„ ë™ì¼í•œ ê²°ê³¼ | . | . íŒŒì¼/í´ë” ì¡°íšŒ . | íŠ¹ì • directory ë‚´ ëª¨ë“  element ì¡°íšŒ . os.listdir('.') # walking directoryë¥¼ ì¡°íšŒí•  ê²½ìš°, os.listdir()ì´ë¼ê³ ë§Œ ì¨ë„ ë¨ . ['Python ê¸°ì´ˆ', 'ë°ì´í„° ì‚¬ì´ì–¸ìŠ¤', 'íŒŒì¼ ìë™í™”.ipynb'] . | os.listdir('path')ë¥¼ í™œìš©í•˜ë©´ í•´ë‹¹ ê²½ë¡œ ë‚´ì˜ ëª¨ë“  íŒŒì¼/í´ë” ì •ë³´ê°€ listë¡œ ë°˜í™˜ë¨ | . | íŠ¹ì • ê²½ë¡œ ë‚´ì˜ ëª¨ë“  í´ë”/íŒŒì¼ ëŒì•„ë³´ê¸° . | ê²½ë¡œì— ë“¤ì–´ ìˆëŠ” ëª¨ë“  í´ë”ì™€ íŒŒì¼ì„ ì¡°íšŒí•´ì£¼ê³ , ë˜ ê·¸ ì•ˆì˜ í´ë”ì˜ í´ë”ì™€ íŒŒì¼ì„ ì¡°íšŒí•´ì£¼ê³ â€¦.. (ë” ì´ìƒ ì¡°íšŒí•  í´ë”ê°€ ì—†ì„ ë•Œê¹Œì§€ ë°˜ë³µ) | . for path, dirs, files in os.walk('datascience_intro'): print('Path:', path) print('Folders:', dirs) print('Files:', files) print('-----------') . Path: datascience_intro Folders: ['.ipynb_checkpoints', 'data'] Files: ['Exploratory Data Analysis.ipynb', 'Pandas ê¸°ì´ˆ.ipynb', 'ë°ì´í„° ì •ì œ.ipynb'] ----------- Path: datascience_intro\\.ipynb_checkpoints Folders: [] Files: ['ë°ì´í„° ì •ì œ-checkpoint.ipynb'] ----------- Path: datascience_intro\\data Folders: ['empty_folder'] Files: ['iris.csv', 'test_scores.csv', 'titanic.csv'] ----------- Path: datascience_intro\\data\\empty_folder Folders: [] Files: [] ----------- . | os.walk('path')ëŠ” path, folders, files ìˆœì„œë¡œ 3ê°œì˜ elementë¥¼ ë°˜ë³µì ìœ¼ë¡œ returní•´ì¤Œ | . | . í´ë”/íŒŒì¼ ì—¬ë¶€ íŒë³„ . # dirì¸ì§€ íŒë³„ print(os.path.isdir('Python/test.txt')) print(os.path.isdir('Python')) print(os.path.isdir('.ipynb_checkpoints')) # íŒŒì¼ì¸ì§€ íŒë³„ print(os.path.isfile('Python/test.txt')) print(os.path.isfile('íŒŒì¼ ìë™í™”.ipynb')) print(os.path.isfile('Python')) . False True True True True False . | â€» ì£¼ì˜: ì´ë¦„ì´ í‹€ë¦° ê²½ìš°(í•´ë‹¹ ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ê²½ìš°)ì—ë„ ì—ëŸ¬ê°€ ë‚˜ì§€ëŠ” ì•Šê³ , ê·¸ëƒ¥ Falseê°€ ë°˜í™˜ë¨! | . ",
    "url": "https://chaelist.github.io/docs/data_handling/file_folder_handling/#%EC%A1%B0%ED%9A%8C--%EC%A0%95%EB%B3%B4-%ED%83%90%EC%83%89",
    "relUrl": "/docs/data_handling/file_folder_handling/#ì¡°íšŒ--ì •ë³´-íƒìƒ‰"
  },"54": {
    "doc": "íŒŒì¼ & í´ë” ë‹¤ë£¨ê¸°",
    "title": "ë³µì‚¬ &amp; ì´ë™ &amp; ì‚­ì œ",
    "content": ". | os.rename: ì´ë¦„ ë³€ê²½ / ê²½ë¡œ ì´ë™ . | ë‹¨ìˆœíˆ ì´ë¦„ë§Œ ë°”ê¿”ì„œ ì¨ì£¼ë©´ ê·¸ëŒ€ë¡œ ê°™ì€ dir ë‚´ì— ì´ë¦„ë§Œ ë°”ë€Œì–´ì„œ ì €ì¥ë¨ os.rename('ê¸°ì¡´ì´ë¦„', 'ìƒˆì´ë¦„') . | ê²½ë¡œ ìì²´ë¥¼ ë°”ê¿”ì„œ ì¨ì£¼ë©´ ìœ„ì¹˜ë„ ìƒˆ ê²½ë¡œë¡œ ì´ë™ì‹œí‚¬ ìˆ˜ ìˆìŒ os.rename('ê¸°ì¡´ê²½ë¡œ/ê¸°ì¡´ì´ë¦„.txt', 'ìƒˆê²½ë¡œ/ìƒˆì´ë¦„.txt') . | . +) shutil.move: íŒŒì¼ ì´ë™ (os.renameê³¼ ê±°ì˜ ë™ì¼í•˜ê²Œ ë™ì‘) . shutil.move('ê¸°ì¡´ê²½ë¡œ/ê¸°ì¡´ì´ë¦„.txt', 'ìƒˆê²½ë¡œ/ìƒˆì´ë¦„.txt') . | os.renameê³¼ì˜ ì°¨ì´: shutil.moveë¥¼ í™œìš©í•˜ë©´ ë‹¤ë¥¸ ë“œë¼ì´ë¸Œ / ë‹¤ë¥¸ íŒŒì¼ ì‹œìŠ¤í…œì„ ê°–ëŠ” ìœ„ì¹˜ë¡œë„ ì´ë™ ê°€ëŠ¥ (ex. Cë“œë¼ì´ë¸Œì—ì„œ Dë“œë¼ì´ë¸Œë¡œ ì´ë™) | ê°™ì€ ë“œë¼ì´ë¸Œ ë‚´ì—ì„œ ì´ë™í•˜ëŠ” ê²½ìš°, os.renameë§Œ ì‚¬ìš©í•´ë„ ì¶©ë¶„ | . | shutil.copy: íŒŒì¼ ë³µì‚¬ . shutil.copy('ì›ë³¸ íŒŒì¼ ì´ë¦„', 'ë³µì œë³¸ íŒŒì¼ ì´ë¦„') . | ë¬¼ë¡  shutil ëª¨ë“ˆ ì—†ì´ë„, ê·¸ëƒ¥ openìœ¼ë¡œ ë³µì œí•˜ëŠ” ê²ƒë„ ê°€ëŠ¥: . | read ëª¨ë“œë¡œ ë³µì œí•  íŒŒì¼ì„ ì½ì–´ì™€ì„œ f.read()ë¡œ ì „ì²´ ë‚´ìš©ì„ ì €ì¥ í›„, ë‹¤ì‹œ write ëª¨ë“œë¡œ ìƒˆ íŒŒì¼ì„ ë§Œë“¤ì–´ì„œ f.write()ìœ¼ë¡œ ì €ì¥í–ë‘” ë‚´ìš©ì„ ì ì–´ì£¼ë©´ ë¨ | . | . â€» ì£¼ì˜: â€˜ë³µì œë³¸ íŒŒì¼ ì´ë¦„â€™ê³¼ ê°™ì€ ê²½ë¡œ/ì´ë¦„ì˜ íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•  ê²½ìš°, ìƒˆ íŒŒì¼ì´ ê¸°ì¡´ íŒŒì¼ì„ ë®ì–´ì”Œìš°ê²Œ ëœë‹¤ (if os.path.extist(â€˜pathâ€™)ë¡œ ê²€ì¦í•˜ê³  ë³µì œí•˜ë„ë¡ ì½”ë“œë¥¼ ì ìœ¼ë©´ ë” ì•ˆì „) . | shutil.copytree: í´ë” ë³µì‚¬ . shutil.copytree('ê¸°ì¡´í´ë”ëª…', 'ìƒˆí´ë”ëª…') . | os.remove: íŒŒì¼ ì‚­ì œ . os.remove('íŒŒì¼ëª….txt') . | í´ë” ì‚­ì œ . | ë¹ˆ í´ë” ì‚­ì œ os.rmdir('í´ë”ëª…') # os.rmdirë¥¼ ì‚¬ìš©í•˜ë©´ ë‚´ìš©ë¬¼ì´ ì—†ëŠ” í´ë”ë§Œ ì‚­ì œ ê°€ëŠ¥í•˜ë‹¤ # ë‚´ìš©ë¬¼ì´ ìˆëŠ” í´ë”ë¥¼ os.rmdirë¡œ ì‚­ì œí•˜ë ¤ê³  í•˜ë©´ error . | ë‚´ìš©ë¬¼ì´ ìˆëŠ” í´ë” ì‚­ì œ shutil.rmtree('í´ë”ëª…') . | . | í´ë” ìƒì„± . | os.makedirs(â€˜í´ë”ëª…â€™): ì—¬ëŸ¬ ë””ë ‰í† ë¦¬ë¥¼ í•œ ë²ˆì— ìƒì„±í•˜ëŠ” ê²ƒë„ ê°€ëŠ¥ os.makedirs(r'years/2021') # í˜„ì¬ ê²½ë¡œì— yearsë¼ëŠ” í´ë”ì¡°ì°¨ ì—†ëŠ” ìƒí™© â†’ ì´ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ë©´ years ë””ë ‰í† ë¦¬ê°€ ìƒê¸°ê³  ê·¸ ì•ˆì— 2021 ë””ë ‰í† ë¦¬ë„ í•¨ê»˜ ìƒê¹€ . | os.mkdir(â€˜í´ë”ëª…â€™): í•˜ë‚˜ì˜ ë””ë ‰í† ë¦¬ë§Œ ìƒì„± ê°€ëŠ¥ . os.mkdir(r'years/2021') # í˜„ì¬ ê²½ë¡œì— yearsë¼ëŠ” í´ë”ê°€ ì—†ë‹¤ë©´ ì´ ì½”ë“œëŠ” errorë¥¼ ë°œìƒì‹œí‚´ os.mkdir('2021') # ìœ„ì™€ ê°™ì€ ì½”ë“œëŠ” ì •ìƒì ìœ¼ë¡œ ì‹¤í–‰ë¨ (í´ë” í•˜ë‚˜ë§Œ ìƒì„±) . | . | . ",
    "url": "https://chaelist.github.io/docs/data_handling/file_folder_handling/#%EB%B3%B5%EC%82%AC--%EC%9D%B4%EB%8F%99--%EC%82%AD%EC%A0%9C",
    "relUrl": "/docs/data_handling/file_folder_handling/#ë³µì‚¬--ì´ë™--ì‚­ì œ"
  },"55": {
    "doc": "íŒŒì¼ & í´ë” ë‹¤ë£¨ê¸°",
    "title": "ì••ì¶• íŒŒì¼ ìƒì„±",
    "content": "shutil í™œìš© . | í•˜ë‚˜ì˜ í´ë” ë‚´ ë‚´ìš©ë¬¼ì„ í•œë²ˆì— ì••ì¶• . shutil.make_archive('archive', 'zip', 'folder') # ë‘ ë²ˆì§¸ ì¸ìë¡œëŠ” ì••ì¶• ë°©ì‹ì„ ë„£ì–´ì£¼ë©´ ë¨ # folderë¼ëŠ” í´ë”ì˜ ëª¨ë“  ë‚´ìš©ë¬¼ì´ archive.zipì´ë¼ëŠ” ì••ì¶•íŒŒì¼ë¡œ ì €ì¥ë¨ . | . | ì••ì¶• í•´ì œ . shutil.unpack_archive('archive.zip', 'unpacked_folder') # archive.zip ì••ì¶•íŒŒì¼ì˜ ë‚´ìš©ë¬¼ì´ unpacked_folderë¼ëŠ” í´ë”ë¡œ ì••ì¶•í•´ì œê°€ ë˜ì–´ ì €ì¥ë¨ . | . zipfile í™œìš© . | ê°ê°ì˜ íŒŒì¼ì„ ì„ ë³„í•´ì„œ ì••ì¶• . with zipfile.ZipFile('archive.zip', 'w', compression=zipfile.ZIP_DEFLATED) as z: z.write('ì••ì¶•í•  íŒŒì¼ 1') z.write('ì••ì¶•í•  íŒŒì¼ 2') z.write('ì••ì¶•í•  íŒŒì¼ 3') # íŒŒì¼ 1~3ì´ archive.zipì´ë¼ëŠ” ì••ì¶•íŒŒì¼ë¡œ ì €ì¥ë¨ . | compression=zipfile.ZIP_DEFLATEDì„ ì•ˆì“°ë©´ íŒŒì¼ 1~3ì´ zip íŒŒì¼ë¡œ í•©ì³ì§€ê¸´ í•˜ì§€ë§Œ, ìš©ëŸ‰ì´ ì¤„ì§€ëŠ” ì•ŠìŒ | . | ì••ì¶• í•´ì œ . with zipfile.ZipFile('archive.zip', 'r') as z: z.extractall('unpacked_folder') # archive.zip ì••ì¶•íŒŒì¼ì˜ ë‚´ìš©ë¬¼ì´ unpacked_folderë¼ëŠ” í´ë”ë¡œ ì••ì¶•í•´ì œê°€ ë˜ì–´ ì €ì¥ë¨ . | . ",
    "url": "https://chaelist.github.io/docs/data_handling/file_folder_handling/#%EC%95%95%EC%B6%95-%ED%8C%8C%EC%9D%BC-%EC%83%9D%EC%84%B1",
    "relUrl": "/docs/data_handling/file_folder_handling/#ì••ì¶•-íŒŒì¼-ìƒì„±"
  },"56": {
    "doc": "íŒŒì¼ & í´ë” ë‹¤ë£¨ê¸°",
    "title": "íŒŒì¼ & í´ë” ë‹¤ë£¨ê¸°",
    "content": " ",
    "url": "https://chaelist.github.io/docs/data_handling/file_folder_handling/",
    "relUrl": "/docs/data_handling/file_folder_handling/"
  },"57": {
    "doc": "íŒŒì¼ ì½ê³  ì“°ê¸°",
    "title": "íŒŒì¼ ì½ê³  ì“°ê¸°",
    "content": ". | Accessing a file | Reading a file . | f.read() | f.readlines() | f.readline() | í™œìš© Tip | . | Writing a file . | ë‚´ìš© appendí•˜ê¸° | . | r+, w+, a+ . | r+ ëª¨ë“œ | w+ ëª¨ë“œ | a+ ëª¨ë“œ | . | . ",
    "url": "https://chaelist.github.io/docs/data_handling/file_input_output/",
    "relUrl": "/docs/data_handling/file_input_output/"
  },"58": {
    "doc": "íŒŒì¼ ì½ê³  ì“°ê¸°",
    "title": "Accessing a file",
    "content": ". | ìš°ì„  open('file_name', 'mode')ë¡œ íŒŒì¼ì— access | â€˜modeâ€™ì˜ ì¢…ë¥˜: â€˜râ€™: read, â€˜wâ€™: write, â€˜aâ€™: append | . # íŒŒì¼ì´ ê°™ì€ í´ë”ì— ìˆì„ ê²½ìš° f = open('example.txt', 'r') ## ì´ë ‡ê²Œ íŒŒì¼ëª…ë§Œ ì ì–´ì£¼ë©´ ë¨ # íŒŒì¼ì´ ë‹¤ë¥¸ í´ë”ì— ìˆì„ ê²½ìš° f = open('C:/Users/admin/Desktop/chaelist_blog/example.txt', 'r') ## ì ˆëŒ€ ê²½ë¡œë¡œ ì ‘ê·¼ # íŒŒì¼ì´ í˜„ì¬ python íŒŒì¼ì„ ì‹¤í–‰ì‹œí‚¨ í´ë” ë‚´ ë‹¤ë¥¸ í´ë”ì— ìˆì„ ê²½ìš° f = open('data/example.txt', 'r') ## dataë¼ëŠ” í´ë” ë‚´ì— íŒŒì¼ì´ ìˆëŠ” ê²½ìš° # íŒŒì¼ì´ í˜„ì¬ python íŒŒì¼ì„ ì‹¤í–‰ì‹œí‚¨ í´ë” ë°–ì˜ í´ë”ì— ìˆì„ ê²½ìš° f = open('../example.txt', 'r') ## ../ë¥¼ í†µí•´ traverse up the directory . ",
    "url": "https://chaelist.github.io/docs/data_handling/file_input_output/#accessing-a-file",
    "relUrl": "/docs/data_handling/file_input_output/#accessing-a-file"
  },"59": {
    "doc": "íŒŒì¼ ì½ê³  ì“°ê¸°",
    "title": "Reading a file",
    "content": "f.read() . : íŒŒì¼ ì „ì²´ë¥¼ í•˜ë‚˜ì˜ string valueë¡œ ì½ì–´ì¤€ë‹¤ . f = open('example.txt', 'r') # 'r'ì€ default modeì´ê¸° ë•Œë¬¸ì— ì•ˆì¨ì¤˜ë„ ê²°ê³¼ëŠ” ë™ì¼ f.read() # ì•„ë˜ì²˜ëŸ¼, í•˜ë‚˜ì˜ string valueë¡œ ì½ì–´ì˜´ . '1\\n2\\n3\\n4\\n5' . f.close() # íŠ¹ì • íŒŒì¼ì„ opení•œ ë‹¤ìŒ read / writeí•˜ê³  ë‚˜ë©´ ë‹«ì•„ì¤˜ì•¼ í•œë‹¤ . f.readlines() . : read the entire file, as a list type (ê° ì¤„ì„ ê°ê°ì˜ list variableë¡œ ë°›ì•„ì„œ, í•˜ë‚˜ì˜ listë¡œ ë°˜í™˜) . | ê°€ì¥ í”íˆ ì‚¬ìš©í•˜ëŠ” file reading method. | . f = open('example.txt', 'r') f.readlines() # ê° ì¤„ì„ elementë¡œ í•˜ëŠ” listë¥¼ ë°˜í™˜ . ['1\\n', '2\\n', '3\\n', '4\\n', '5'] . f.close() . f.readline() . : read the file line by line . | í•œì¤„ì”© ì°¨ë¡€ëŒ€ë¡œ readlineí•´ì¤˜ì•¼ í•´ì„œ, ì˜ ì•ˆ ì“°ëŠ” method. | . f = open('example.txt', 'r') f.readline() # ì´ëŸ° ì‹ìœ¼ë¡œ í•œì¤„ì”© ì½íŒë‹¤ . '1\\n' . f.readline() # ì´ëŸ° ì‹ìœ¼ë¡œ í•œì¤„ì”© ì½íŒë‹¤ . '2\\n' . f.close() . í™œìš© Tip . | ë³´í†µ, readlines()ì— forë¬¸ì„ ê²°í•©í•´ì„œ ì´ìš©í•œë‹¤. f = open('example.txt', 'r') for line in f.readlines(): print(line.strip()) # ë³´í†µ ì´ë ‡ê²Œ forë¬¸ì„ ì‚¬ìš©í•´ì„œ í•œ ì¤„ í•œ ì¤„ ì ‘ê·¼ f.close() . 1 2 3 4 5 . | readlines()ë¥¼ ì“°ì§€ ì•Šì•„ë„, forë¬¸ì„ ì‚¬ìš©í•˜ë©´ í•œ ì¤„ í•œ ì¤„ ì ‘ê·¼ì´ ê°€ëŠ¥í•˜ë‹¤ . f = open('example.txt', 'r') for line in f: print(line.strip()) f.close() . 1 2 3 4 5 . | txt íŒŒì¼ì—ì„œ ì½ì–´ì˜¨ ê°’ë“¤ì€ ì¼ë‹¨ ë‹¤ string í˜•íƒœë¡œ ì €ì¥ëœë‹¤. f = open('example.txt', 'r') lines_list = f.readlines() # ì´ë ‡ê²Œ ì €ì¥í•´ë‘ë©´ ë¶ˆëŸ¬ì˜¤ê¸° í¸í•˜ë‹¤ f.close() line = lines_list[0].strip() print(line, type(line)) # typeì„ ì¶œë ¥í•´ë³´ë©´, txt íŒŒì¼ì—ì„œ ì½ì–´ì˜¨ valueë“¤ì€ ë‹¤ ì¼ë‹¨ string í˜•íƒœ. num_line = int(line) # '' ì‚¬ì´ì˜ ê°’ì´ ìˆ«ìì¼ ê²½ìš°, ì´ë ‡ê²Œ int()ë¥¼ ì ìš©í•´ ìˆ«ìë¡œ ë°”ê¿”ì¤„ ìˆ˜ ìˆë‹¤ print(num_line, type(num_line)) # ì´ì œ typeì´ integerë¡œ ë°”ë€Œì–´ì„œ, ì‚¬ì¹™ì—°ì‚° ê°€ëŠ¥ . 1 &lt;class 'str'&gt; 1 &lt;class 'int'&gt; . | file open &amp; closeë¥¼ í•œ ì¤„ë¡œ í•´ê²°í•˜ëŠ” ë°©ë²•: . with open('íŒŒì¼ëª….txt', 'r', encoding='utf-8') as file: file.read() . | ì´ë ‡ê²Œ ì¨ì£¼ë©´ open()ê³¼ close()ë¥¼ ëª¨ë‘ ì¨ì¤€ ê²ƒê³¼ ë™ì¼ (with ì•„ë˜ì˜ ì½”ë“œê°€ ëª¨ë‘ ì‹¤í–‰ë˜ê³  ë‚˜ë©´ ì•Œì•„ì„œ íŒŒì¼ì´ ë‹«íŒë‹¤) | . | ì»¤ì„œ ì˜®ê¸°ê¸° (â€» íŒŒì¼ì„ ì½ê³  ì“°ëŠ” ê³¼ì •ì—ì„œ ì¼ì¢…ì˜ â€˜ì»¤ì„œâ€™ê°€ ê³„ì† ì¤„ì„ ë”°ë¼ ì´ë™í•˜ê²Œ ë˜ê¸° ë•Œë¬¸ì—, ì»¤ì„œ ìœ„ì¹˜ë¥¼ ìœ ì˜í•´ì•¼ í•  ë•Œê°€ ìˆìŒ) . | f.readline()ìœ¼ë¡œ í•œ ì¤„ì„ ì´ë¯¸ ì½ì–´ì¤€ í›„ë¼ì„œ, ì»¤ì„œì˜ ìœ„ì¹˜ëŠ” ë‘ë²ˆì§¸ ì¤„ë¡œ ì´ë™í•¨. ê·¸ ìƒíƒœì—ì„œ f.read()ë¡œ ì½ì–´ì£¼ë©´ ì»¤ì„œ ìœ„ì¹˜ë¶€í„° ê°€ì¥ ëê¹Œì§€ì˜ ë‚´ìš©ì´ ì¶œë ¥ë¨. f = open('sample.txt') print(f.readline()) print('---------') print(f.read()) . 1 --------- 2 3 4 5 . | seek()ì„ í™œìš©í•˜ë©´ ì»¤ì„œ ìœ„ì¹˜ë¥¼ ì˜®ê¸¸ ìˆ˜ ìˆìŒ (seek(0): íŒŒì¼ì˜ ê°€ì¥ ì²˜ìŒìœ¼ë¡œ ì»¤ì„œë¥¼ ì˜®ê²¨ì¤Œ) . f = open('sample.txt') print(f.readline()) print('---------') f.seek(0) print(f.read()) . 1 --------- 1 2 3 4 5 . | . | . ",
    "url": "https://chaelist.github.io/docs/data_handling/file_input_output/#reading-a-file",
    "relUrl": "/docs/data_handling/file_input_output/#reading-a-file"
  },"60": {
    "doc": "íŒŒì¼ ì½ê³  ì“°ê¸°",
    "title": "Writing a file",
    "content": ". | f.write('ë‚´ìš©')ì˜ í˜•íƒœë¡œ ì‘ì„± | ë‹¤ ì“´ fileì€ closeí•´ì£¼ì–´ì•¼ ì œëŒ€ë¡œ ì •ë³´ê°€ ì €ì¥ëœë‹¤ | . # ì˜ˆì‹œ f1 = open('name.txt', 'w') ## í•´ë‹¹ python íŒŒì¼ì´ ì‹¤í–‰ëœ ë™ì¼í•œ í´ë”ì— ì €ì¥ë¨ f1.write('Chung &amp; Cho') # ì“°ê³  ì‹¶ì€ ë‚´ìš©ì„ ì‘ì„± f1.close() . | writeí•˜ëŠ” ë‚´ìš©ì€ ë¬´ì¡°ê±´ string valueì—¬ì•¼ í•œë‹¤! . f2 = open('numbers.txt', 'w') num = 3 f2.write(num) # ì´ë ‡ê²Œ int í˜•íƒœì˜ ë°ì´í„°ë¥¼ writeí•˜ë ¤ê³  í•˜ë©´ error f2.close . --------------------------------------------------------------------------- TypeError Traceback (most recent call last) &lt;ipython-input-15-7b27a5c24a65&gt; in &lt;module&gt;() 1 f2 = open('numbers.txt', 'w') 2 num = 3 ----&gt; 3 f2.write(num) 4 f2.close TypeError: write() argument must be str, not int . cf) int í˜•íƒœì˜ ë°ì´í„°ëŠ” str(num)ìœ¼ë¡œ stringìœ¼ë¡œ ë°”ê¿”ì£¼ë©´ write ê°€ëŠ¥ . f2 = open('numbers.txt', 'w') num = 3 f2.write(str(num)) f2.close() . | í•œê¸€ ì²˜ë¦¬ ë°©ë²•: encoding='utf-8' f3 = open('í•œê¸€.txt', 'w', encoding='utf-8') # encoding ë°©ë²•ì„ ì§€ì • ê°€ëŠ¥ f3.write('ê¹€íƒœí¬') f3.close() . | ì—¬ëŸ¬ ë‚´ìš© ì´ì–´ì„œ write f_test = open('test.txt', 'w') f_test.write('test1'+'\\n') f_test.write('test2') # ì´ëŸ°ì‹ìœ¼ë¡œ ì´ì–´ì„œ f.write()ë¥¼ ë˜ í•´ì£¼ë©´ ê³„ì† ì´ì–´ì„œ ì¨ì§ ## f.close()ë¥¼ í•´ì£¼ê¸° ì „ê¹Œì§€ëŠ” ê³„ì†í•´ì„œ í•œ ì¤„ì”© ì¶”ê°€ ê°€ëŠ¥ f_test.close() . | . ë‚´ìš© appendí•˜ê¸° . (ì´ë¯¸ ë§Œë“  íŒŒì¼ì— ë‚´ìš© ì¶”ê°€ë¡œ ë” ì“°ê¸°) . f_test = open('test.txt', 'a') # 'a': append f_test.write('\\n'+'test3') # 'a'ë¡œ opení•œë‹¤ìŒì— write í•´ì£¼ë©´ ì´ë¯¸ ì¡´ì¬í•˜ëŠ” íŒŒì¼ì— ì´ì–´ì„œ ì“¸ ìˆ˜ ìˆìŒ. f_test.close() . â€» open('ì´ë¯¸ ìˆëŠ” íŒŒì¼ëª…', 'w') ì´ë ‡ê²Œ ì´ë¯¸ ìˆëŠ” íŒŒì¼ì„ â€˜wâ€™(write)ë¡œ ì—´ë©´ ê·¸ ìœ„ì— overwriteí•˜ê²Œ ë˜ê¸° ë•Œë¬¸ì—, ì´ì–´ì„œ ê³„ì† ì“°ê³  ì‹¶ë‹¤ë©´ â€˜aâ€™(append)ë¡œ ì—´ì–´ì¤˜ì•¼ í•œë‹¤ . ",
    "url": "https://chaelist.github.io/docs/data_handling/file_input_output/#writing-a-file",
    "relUrl": "/docs/data_handling/file_input_output/#writing-a-file"
  },"61": {
    "doc": "íŒŒì¼ ì½ê³  ì“°ê¸°",
    "title": "r+, w+, a+",
    "content": ". | open() í•¨ìˆ˜ì—ëŠ” +ëª¨ë“œë“¤ë„ ì¡´ì¬. ì½ê¸° &amp; ì“°ê¸° ê¸°ëŠ¥ì„ í•œë²ˆì— ì²˜ë¦¬í•˜ê³  ì‹¶ì€ ê²½ìš°ì— ìœ ìš©í•˜ë‹¤ | ì´ 6ê°œì˜ ëª¨ë“œ: r, w, a, r+, w+, a+ | . r+ ëª¨ë“œ . | r ëª¨ë“œë¡œ ì—´ë©´ readingë§Œ ê°€ëŠ¥í•˜ì§€ë§Œ, r+ ëª¨ë“œë¡œ ì—´ë©´ readingê³¼ writingì´ ëª¨ë‘ ê°€ëŠ¥í•˜ë‹¤ . | â€˜râ€™ ëª¨ë“œë¡œ ì—´ê³  â€˜writeâ€™ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ë©´ â€˜UnsupportedOperation: not writableâ€™ì´ë¼ê³  errorê°€ ë‚¨ | . | r+ ëª¨ë“œë¡œ ì—´ê³  â€˜writeâ€™í•˜ëŠ” ê²½ìš°, ìƒˆë¡œ ì ëŠ” ë‚´ìš©ì€ ì»¤ì„œ ìœ„ì¹˜ë¶€í„° ì‘ì„±ëœë‹¤. íŒŒì¼ì„ ì—° ì§í›„ì—ëŠ” ì»¤ì„œ ìœ„ì¹˜ê°€ 0 (ê°€ì¥ ì•)ì´ë¯€ë¡œ, ê·¸ ìƒíƒœì—ì„œ writeí•˜ëŠ” ê²½ìš° íŒŒì¼ì˜ ë§¨ ì²˜ìŒë¶€í„° ìƒˆë¡œìš´ ë‚´ìš©ì´ ë®ì–´ì”Œì›Œì§„ë‹¤ . | â€» r+ ëª¨ë“œë¡œ ì—´ì–´ì„œ ë¨¼ì € â€˜readâ€™ë¥¼ í•œ ë²ˆ í•˜ê³  ë‚˜ì„œ â€˜writeâ€™ë¥¼ í•˜ëŠ” ê²½ìš°, ì»¤ì„œê°€ ê°€ì¥ ë’¤ì— ê°€ ìˆìœ¼ë¯€ë¡œ ë’¤ì— ì´ì–´ì„œ ìƒˆë¡œìš´ ë‚´ìš©ì„ ì¶”ê°€í•  ìˆ˜ ìˆë‹¤ | . | . # ìƒ˜í”Œ íŒŒì¼ ì¤€ë¹„ with open('sample.txt', 'r') as f: print(f.read()) . first line second line . with open('sample.txt', 'r+') as f: ## ìƒˆë¡œìš´ ë‚´ìš©ì´ íŒŒì¼ì˜ ê°€ì¥ ì•ë¶€ë¶„ë¶€í„° ë®ì–´ì”Œì›Œì§ f.write('write new text') f.seek(0) # ì²˜ìŒë¶€í„° ì½ì–´ì£¼ê¸° ìœ„í•´ì„œëŠ” ì»¤ì„œ ìœ„ì¹˜ë¥¼ 0ìœ¼ë¡œ ë°”ê¿”ì¤˜ì•¼ í•¨ print(f.read()) . write new textcond line . w+ ëª¨ë“œ . | w+ ëª¨ë“œë¡œ ì—´ë©´ ê¸°ì¡´ ë‚´ìš©ì„ ì§€ìš°ê³  ìƒˆë¡­ê²Œ writeí•˜ëŠ” ê²ƒ ì™¸ì—ë„, read ê¸°ëŠ¥ë“¤ë„ ì‚¬ìš© ê°€ëŠ¥í•˜ë‹¤ (ex. ë‚´ìš©ì„ ì ê³  ë°”ë¡œ í™•ì¸í•˜ëŠ” ê²ƒì´ ê°€ëŠ¥) | . # ìƒ˜í”Œ íŒŒì¼ ì¤€ë¹„ with open('sample.txt', 'r') as f: print(f.read()) . first line second line . with open('sample.txt', 'w+') as f: # ê¸°ì¡´ ë‚´ìš©ì€ ì§€ì›Œì§€ê³ , ìƒˆë¡œìš´ ë‚´ìš©ì´ ì íŒë‹¤ f.write('write new text') f.seek(0) print(f.read()) . write new text . +) â€˜w+â€™ ëª¨ë“œë¡œ ì—¬ëŠ” ìˆœê°„ fileì€ ë°±ì§€í™”ëœë‹¤. w+ ëª¨ë“œë¡œ ì—´ìë§ˆì f.read()ìœ¼ë¡œ ì½ì–´ë“¤ì´ëŠ” ê²ƒì€ ì˜ë¯¸ê°€ ì—†ìŒ . with open('sample.txt', 'w+') as f: print(f.read()) # ì•„ë¬´ê²ƒë„ ì¶œë ¥ë˜ì§€ ì•ŠìŒ . a+ ëª¨ë“œ . | a+ ëª¨ë“œë¡œ ì—´ë©´ ê¸°ì¡´ ë‚´ìš©ì— ìƒˆë¡­ê²Œ ë‚´ìš©ì„ ë§ë¶™ì—¬ writeí•˜ëŠ” ê²ƒ ì™¸ì—ë„, read ê¸°ëŠ¥ë“¤ë„ ì‚¬ìš© ê°€ëŠ¥í•˜ë‹¤ | . # ìƒ˜í”Œ íŒŒì¼ ì¤€ë¹„ with open('sample.txt', 'r') as f: print(f.read()) . first line second line . with open('sample.txt', 'a+') as f: # íŒŒì¼ì˜ ê¸°ì¡´ ë‚´ìš©ì€ ìœ ì§€í•˜ê³ , ë§¨ ë’¤ì— ìƒˆë¡œìš´ ë‚´ìš©ì´ ì¶”ê°€ëœë‹¤ f.write('\\nwrite new text') # writeí•œ í›„ì˜ text ë‚´ìš© ì¶œë ¥ f.seek(0) print(f.read()) . first line second line write new text . +) â€˜a+â€™ ëª¨ë“œë¡œ ì—´ë©´ ì»¤ì„œ ìœ„ì¹˜ê°€ íŒŒì¼ ë§¨ ëë¶€í„° ì‹œì‘í•œë‹¤. ê·¸ëŸ¬ë¯€ë¡œ a+ëª¨ë“œë¡œ ì—´ìë§ˆì readí•´ì£¼ë ¤ë©´ seek(0)ìœ¼ë¡œ ì»¤ì„œ ìœ„ì¹˜ë¥¼ ì¡°ì •í•´ì¤˜ì•¼ í•œë‹¤ . with open('sample.txt', 'a+') as f: print(f.read()) # ì•„ë¬´ê²ƒë„ ì¶œë ¥ë˜ì§€ ì•ŠìŒ # seek(0) í•˜ê³  f.read()ë¥¼ í•´ì¤˜ì•¼ ì „ì²´ ë‚´ìš©ì„ ì¶œë ¥ ê°€ëŠ¥ . ",
    "url": "https://chaelist.github.io/docs/data_handling/file_input_output/#r-w-a",
    "relUrl": "/docs/data_handling/file_input_output/#r-w-a"
  },"62": {
    "doc": "Foreign Key & Python ì—°ê²°",
    "title": "Foreign Key &amp; Python ì—°ê²°",
    "content": ". | Foreign Key . | ì°¸ì¡° ë¬´ê²°ì„±(Referential Integrity) | Foreign Key ì„¤ì •í•˜ê¸° | Foreign Key ì‚­ì œí•˜ê¸° | . | Foreign Key ì •ì±… . | ON DELETE | ON UPDATE | . | Pythonì—ì„œ DB ì—°ê²° . | PyMySQL | . | . ",
    "url": "https://chaelist.github.io/docs/sql/foreign_key_pymysql/#foreign-key--python-%EC%97%B0%EA%B2%B0",
    "relUrl": "/docs/sql/foreign_key_pymysql/#foreign-key--python-ì—°ê²°"
  },"63": {
    "doc": "Foreign Key & Python ì—°ê²°",
    "title": "Foreign Key",
    "content": ": ë‘ ê°œì˜ í…Œì´ë¸”ì„ ì—°ê²°í•´ì£¼ëŠ” ë‹¤ë¦¬ ì—­í• ì„ í•˜ëŠ” key. | Foreign Keyê°€ ìˆëŠ” í…Œì´ë¸”: child table(ìì‹ í…Œì´ë¸”) í˜¹ì€ referencing tableì´ë¼ê³  ì§€ì¹­ | Foreign Keyì— ì˜í•´ ì°¸ì¡°ë‹¹í•˜ëŠ” í…Œì´ë¸”: parent table(ë¶€ëª¨ í…Œì´ë¸”) í˜¹ì€ referenced tableì´ë¼ê³  ì§€ì¹­ | . (ì¶œì²˜: SQLShack) . ì°¸ì¡° ë¬´ê²°ì„±(Referential Integrity) . : ë‘ í…Œì´ë¸” ê°„ì— ì°¸ì¡° ê´€ê³„ê°€ ìˆì„ ë•Œ ê° ë°ì´í„° ê°„ì— ìœ ì§€ë˜ì–´ì•¼ í•˜ëŠ” ì •í™•ì„±ê³¼ ì¼ê´€ì„±ì„ ì˜ë¯¸ . | DBMS ìƒì—ì„œ ë‘ í…Œì´ë¸” ê°„ì˜ Foreign Key ê´€ê³„ë¥¼ ì„¤ì •í•´ë‘ë©´ ì°¸ì¡° ë¬´ê²°ì„±ì„ ì§€í‚¬ ìˆ˜ ìˆë‹¤ | ex) item_id = 10ì¸ ë¦¬ë·°ë“¤ì´ ìˆëŠ”ë° ì •ì‘ item í…Œì´ë¸”ì—ëŠ” id = 10ì¸ ìƒí’ˆì´ ì—†ë‹¤ë©´ ì°¸ì¡° ë¬´ê²°ì„±ì´ ê¹¨ì§„ ê²ƒ! | . Â  . *ë…¼ë¦¬ì  Foreign Keyì™€ ë¬¼ë¦¬ì  Foreign Key . | ì‹¤ë¬´ì—ì„œëŠ” ë…¼ë¦¬ì ìœ¼ë¡œ ë‘ í…Œì´ë¸” ê°„ì— Foreign Key ê´€ê³„ê°€ ìˆì–´ë„ DBMS ìƒì—ì„œ ì„¤ì •í•´ë‘ì§€ ì•ŠëŠ” ê²½ìš°ë„ ìˆë‹¤ (ex. ì„±ëŠ¥ / Legacy data ë•Œë¬¸â€¦) | ë…¼ë¦¬ì (Logical) Foreign Key: ë…¼ë¦¬ì ìœ¼ë¡œ ì„±ë¦½í•˜ëŠ” Foreign Key | ë¬¼ë¦¬ì (Physical) Foreign Key: DBMS ìƒì—ì„œ ì‹¤ì œë¡œ Foreign Keyë¡œ ì„¤ì •í•´ì„œ ì°¸ì¡° ë¬´ê²°ì„±ì„ ë³´ì¥í•  ìˆ˜ ìˆê²Œ ëœ ê²ƒ | . Foreign Key ì„¤ì •í•˜ê¸° . | SQLë¬¸ìœ¼ë¡œ ì„¤ì •: . | í˜•ì‹: (Foreign Keyë„ ì¼ì¢…ì˜ Constraintì´ë¯€ë¡œ, ADD CONSTRAINTë¬¸ì„ ì‚¬ìš©) ALTER TABLE í…Œì´ë¸”ëª… ADD CONSTRAINT ì œì•½ì´ë¦„ FOREIGN KEY (foreign_keyë¡œ_ì„¤ì •í• _ì»¬ëŸ¼) REFERENCES ì°¸ì¡°í•˜ëŠ”_í…Œì´ë¸”ëª… (ì°¸ì¡°í• _ì»¬ëŸ¼) ON DELETE ì •ì±… ON UPDATE ì •ì±…; . | ì˜ˆì‹œ: review í…Œì´ë¸”ì˜ course_id ì»¬ëŸ¼ì´ foreign key, ì°¸ì¡°ë˜ëŠ” ì»¬ëŸ¼ì€ course í…Œì´ë¸”ì˜ id ì»¬ëŸ¼. ALTER TABLE `course_rating`.`review` ADD CONSTRAINT `fk_review_table` FOREIGN KEY (`course_id`) REFERENCES `course_rating`.`course` (`id`) ON DELETE RESTRICT ON UPDATE RESTRICT; . | ìœ„ì™€ ê°™ì´ Foreign Keyë¥¼ ì„¤ì •í•´ë‘ë©´ ì°¸ì¡° ë¬´ê²°ì„±ì´ ë³´ì¥ë˜ë„ë¡ ì œì•½ì´ ê±¸ë¦°ë‹¤. ex) course í…Œì´ë¸”ì— id = 10ì¸ rowê°€ ì—†ëŠ” ìƒí™©ì—ì„œ, review í…Œì´ë¸”ì— course_id = 10ì¸ rowë¥¼ ì‚½ì…í•˜ë ¤ê³  í•˜ë©´ error ë°œìƒ. | . Â  . | workbenchë¥¼ í†µí•´ ì§ì ‘ í´ë¦­í•´ì„œ ì„¤ì •í•˜ëŠ” ê²ƒë„ ê°€ëŠ¥: | . Â  . +) í…Œì´ë¸”ì— ë”í•´ì§„ Constraint í™•ì¸í•˜ê¸° . | SHOW CREATE TABLE í…Œì´ë¸”ëª…;: íŠ¹ì • í…Œì´ë¸”ì´ ì–´ë–»ê²Œ ìƒì„±ë˜ì—ˆëŠ”ì§€ë¥¼ ì•Œë ¤ì£¼ëŠ” SQLë¬¸ (íŠ¹ì • í…Œì´ë¸”ì„ ë‹¤ì‹œ ìƒì„±í•œë‹¤ê³  í•˜ë©´ ì–´ë–¤ CREATE TABLEë¬¸ì„ ì‘ì„±í•´ì•¼ í•˜ëŠ”ì§€ ì•Œë ¤ì¤€ë‹¤) | ì´ë¥¼ í™œìš©í•˜ë©´ í…Œì´ë¸”ì— ë”í•´ì ¸ ìˆëŠ” Constraint (Foreign Key í¬í•¨) ì¢…ë¥˜ë¥¼ íŒŒì•…í•  ìˆ˜ ìˆë‹¤ | . SHOW CREATE TABLE review; . â†’ ì•„ë˜ì™€ ê°™ì´ ê²°ê³¼ê°€ ë‚˜ì˜¤ê³ , ê·¸ ë‚´ìš©ì„ ë³µì‚¬í•´ì„œ ë‹¤ë¥¸ ê³³ì— ë¶™ì—¬ë„£ìœ¼ë©´ ì „ì²´ SQLë¬¸ì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤: . Foreign Key ì‚­ì œí•˜ê¸° . | ALTER TABLE í…Œì´ë¸”ëª… DROP FOREIGN KEY ì œì•½ì´ë¦„;ì˜ êµ¬ì¡°ë¡œ ì‘ì„± | cf) MySQLë§ê³  ë‹¤ë¥¸ DBì—ì„œëŠ” DROP CONSTRAINTë¥¼ ì‚¬ìš©í•˜ê¸°ë„ í•¨ | . -- review í…Œì´ë¸”ì— ìˆëŠ” 'fk_review_table'ì´ë¼ëŠ” Foreign Key ì¡°ê±´ì„ ì‚­ì œ ALTER TABLE review DROP FOREIGN KEY fk_review_table; . ",
    "url": "https://chaelist.github.io/docs/sql/foreign_key_pymysql/#foreign-key",
    "relUrl": "/docs/sql/foreign_key_pymysql/#foreign-key"
  },"64": {
    "doc": "Foreign Key & Python ì—°ê²°",
    "title": "Foreign Key ì •ì±…",
    "content": "ON DELETE . : ë¶€ëª¨ í…Œì´ë¸”ì˜ rowê°€ ì‚­ì œë˜ëŠ” ê²½ìš°ì— ëŒ€í•œ ì •ì±… . | RESTRICT ì •ì±… . | ìì‹ í…Œì´ë¸”ì˜ rowì— ì˜í•´ ì°¸ì¡°ë˜ê³  ìˆëŠ” ë¶€ëª¨ í…Œì´ë¸”ì˜ rowëŠ” ì‚­ì œ ë¶ˆê°€ëŠ¥ | ex) course í…Œì´ë¸”ì´ review í…Œì´ë¸”ì— ì˜í•´ ì°¸ì¡°ë˜ê³  ìˆê³ , review í…Œì´ë¸”ì— course_id = 5ì¸ rowë“¤ì´ ì¡´ì¬í•  ë•Œ, course í…Œì´ë¸”ì—ì„œ id = 5ì¸ rowë¥¼ ì‚­ì œí•˜ë ¤ê³  í•˜ë©´ error ë°œìƒ DELETE FROM course WHERE id = 5; | â€» í•´ë‹¹ rowë¥¼ ì°¸ì¡°í•˜ê³  ìˆëŠ” ìì‹ í…Œì´ë¸”ì˜ rowë“¤ì„ ëª¨ë‘ ì‚­ì œí•œ í›„ì—ì•¼ ì‚­ì œê°€ ê°€ëŠ¥í•˜ë‹¤! | +) MySQLì—ì„œ â€˜No Actionâ€™ì´ë¼ê³  ì„¤ì •í•˜ëŠ” ê²ƒë„ RESTRICTì™€ ë™ì¼í•œ ì •ì±… | . | CASCADE ì •ì±… . | ë¶€ëª¨ í…Œì´ë¸”ì˜ rowë¥¼ ì‚­ì œí•˜ë©´, ê·¸ rowë¥¼ ì°¸ì¡°í•˜ê³  ìˆë˜ ìì‹ í…Œì´ë¸”ì˜ rowë„ ëª¨ë‘ í•¨ê»˜ ì‚­ì œë¨ | ex) course í…Œì´ë¸”ì´ review í…Œì´ë¸”ì— ì˜í•´ ì°¸ì¡°ë˜ê³  ìˆê³ , review í…Œì´ë¸”ì— course_id = 5ì¸ rowë“¤ì´ ì¡´ì¬í•  ë•Œ, course í…Œì´ë¸”ì—ì„œ id = 5ì¸ rowë¥¼ ì‚­ì œí•˜ë©´ 1) ì •ìƒì ìœ¼ë¡œ ì‚­ì œë˜ê³ , 2) review í…Œì´ë¸”ì—ì„œ course_id = 5ì¸ rowë“¤ë„ í•¨ê»˜ ì‚­ì œëœë‹¤ | . | SET NULL ì •ì±… . | ë¶€ëª¨ í…Œì´ë¸”ì˜ rowë¥¼ ì‚­ì œí•˜ë©´, ê·¸ rowë¥¼ ì°¸ì¡°í•˜ê³  ìˆë˜ ìì‹ í…Œì´ë¸”ì˜ rowì˜ foreign_key ìë¦¬ê°€ ëª¨ë‘ NULLê°’ìœ¼ë¡œ ëŒ€ì²´ë¨ | ex) course í…Œì´ë¸”ì´ review í…Œì´ë¸”ì— ì˜í•´ ì°¸ì¡°ë˜ê³  ìˆê³ , review í…Œì´ë¸”ì— course_id = 5ì¸ rowë“¤ì´ ì¡´ì¬í•  ë•Œ, course í…Œì´ë¸”ì—ì„œ id = 5ì¸ rowë¥¼ ì‚­ì œí•˜ë©´ 1) ì •ìƒì ìœ¼ë¡œ ì‚­ì œë˜ê³ , 2) review í…Œì´ë¸”ì—ì„œ course_id = 5ì¸ rowë“¤ì€ course_idê°€ NULLê°’ìœ¼ë¡œ ë°”ë€ë‹¤ | . | . ON UPDATE . : ë¶€ëª¨ í…Œì´ë¸”ì˜ rowì—ì„œ ì°¸ì¡°ë˜ëŠ” ì»¬ëŸ¼ì´ ì—…ë°ì´íŠ¸ë˜ëŠ” ê²½ìš°ì— ëŒ€í•œ ì •ì±… . | RESTRICT ì •ì±… . | ë¶€ëª¨ í…Œì´ë¸”ì˜ rowì—ì„œ, ìì‹í…Œì´ë¸”ì˜ rowì— ì˜í•´ ì°¸ì¡°ë˜ê³  ìˆëŠ” ì»¬ëŸ¼ì€ ì—…ë°ì´íŠ¸ ë¶ˆê°€ëŠ¥ | ex) course í…Œì´ë¸”ì´ review í…Œì´ë¸”ì— ì˜í•´ ì°¸ì¡°ë˜ê³  ìˆê³ , review í…Œì´ë¸”ì— course_id = 1ì¸ rowë“¤ì´ ì¡´ì¬í•  ë•Œ, course í…Œì´ë¸”ì—ì„œ id = 1ì¸ rowì˜ idë¥¼ 100ìœ¼ë¡œ ì—…ë°ì´íŠ¸í•˜ë ¤ê³  í•˜ë©´ error ë°œìƒ UPDATE course SET id = 100 WHERE id = 1; | . | CASCADE ì •ì±… . | ë¶€ëª¨ í…Œì´ë¸”ì˜ rowì—ì„œ, ìì‹í…Œì´ë¸”ì˜ rowì— ì˜í•´ ì°¸ì¡°ë˜ê³  ìˆëŠ” ì»¬ëŸ¼ì„ ì—…ë°ì´íŠ¸í•˜ë©´ ì´ë¥¼ ì°¸ì¡°í•˜ë˜ ìì‹ í…Œì´ë¸” rowì˜ foreign_keyë„ í•¨ê»˜ ì—…ë°ì´íŠ¸ë¨ | ex) course í…Œì´ë¸”ì´ review í…Œì´ë¸”ì— ì˜í•´ ì°¸ì¡°ë˜ê³  ìˆê³ , review í…Œì´ë¸”ì— course_id = 1ì¸ rowë“¤ì´ ì¡´ì¬í•  ë•Œ, course í…Œì´ë¸”ì—ì„œ id = 1ì¸ rowì˜ idë¥¼ 100ìœ¼ë¡œ ì—…ë°ì´íŠ¸í•˜ë ¤ê³  í•˜ë©´ 1) ì •ìƒì ìœ¼ë¡œ ì—…ë°ì´íŠ¸ë˜ê³ , 2) review í…Œì´ë¸”ì—ì„œ course_id = 1ì¸ rowë“¤ë„ ë‹¤ course_id = 100ìœ¼ë¡œ ë³€ê²½ëœë‹¤ | . | SET NULL ì •ì±… . | ë¶€ëª¨ í…Œì´ë¸”ì˜ rowì—ì„œ, ìì‹í…Œì´ë¸”ì˜ rowì— ì˜í•´ ì°¸ì¡°ë˜ê³  ìˆëŠ” ì»¬ëŸ¼ì„ ì—…ë°ì´íŠ¸í•˜ë©´ ì´ë¥¼ ì°¸ì¡°í•˜ë˜ ìì‹ í…Œì´ë¸” rowì˜ foreign_keyëŠ” ëª¨ë‘ NULLê°’ìœ¼ë¡œ ëŒ€ì²´ë¨ | ex) course í…Œì´ë¸”ì´ review í…Œì´ë¸”ì— ì˜í•´ ì°¸ì¡°ë˜ê³  ìˆê³ , review í…Œì´ë¸”ì— course_id = 1ì¸ rowë“¤ì´ ì¡´ì¬í•  ë•Œ, course í…Œì´ë¸”ì—ì„œ id = 1ì¸ rowì˜ idë¥¼ 100ìœ¼ë¡œ ì—…ë°ì´íŠ¸í•˜ë ¤ê³  í•˜ë©´ 1) ì •ìƒì ìœ¼ë¡œ ì—…ë°ì´íŠ¸ë˜ê³ , 2) review í…Œì´ë¸”ì—ì„œ course_id = 1ì¸ rowë“¤ë„ ë‹¤ course_idê°€ NULLê°’ìœ¼ë¡œ ë°”ë€ë‹¤ | . | . Â  . +) ON DELETEì™€ ON UPDATEë¥¼ ì„œë¡œ ë‹¤ë¥¸ ì •ì±…ìœ¼ë¡œ ì„¤ì •í•˜ëŠ” ê²ƒë„ ê°€ëŠ¥! . | ex) ON DELETE RESTRICT &amp; ON UPDATE CASCADE | . ",
    "url": "https://chaelist.github.io/docs/sql/foreign_key_pymysql/#foreign-key-%EC%A0%95%EC%B1%85",
    "relUrl": "/docs/sql/foreign_key_pymysql/#foreign-key-ì •ì±…"
  },"65": {
    "doc": "Foreign Key & Python ì—°ê²°",
    "title": "Pythonì—ì„œ DB ì—°ê²°",
    "content": "PyMySQL . : pythonì—ì„œ MySQL ë°ì´í„°ë² ì´ìŠ¤ì— ì—°ê²°í•  ìˆ˜ ìˆê²Œ í•´ì£¼ëŠ” library. | pip install pymsqlë¡œ ì„¤ì¹˜í•´ì„œ ì‚¬ìš© | . | ë°ì´í„°ë² ì´ìŠ¤ì™€ ì—°ê²°í•˜ê¸° import pymysql # importí•´ì„œ ì‚¬ìš© conn = pymysql.connect(host='localhost', user='root', password='password', db='database') curs = conn.cursor(pymysql.cursors.DictCursor) . | í…Œì´ë¸” ìƒì„±í•˜ê¸° # curs.execute() ì•ˆì— ì›í•˜ëŠ” SQLë¬¸ì„ ì¨ì£¼ë©´ ëœë‹¤ curs.execute('''CREATE TABLE IF NOT EXISTS `users` ( id INT NOT NULL AUTO_INCREMENT, name VARCHAR(50) NOT NULL, follower_count INT NULL PRIMARY KEY (id));''') conn.commit() # commitì„ í•´ì•¼ DBì— ë°˜ì˜ë¨ . | ë°ì´í„° INSERT # ì•„ë˜ì™€ ê°™ì´ f stringì„ ì¨ì„œ SQLë¬¸ì„ êµ¬ì„±í•˜ëŠ” ê²ƒë„ ê°€ëŠ¥ curs.execute(f\"\"\"INSERT INTO users (name, follower_count) VALUES ('{artist_name}', {artist_follower_count});\"\"\") conn.commit() . | ê·¸ ì™¸, ë°ì´í„° ì‚­ì œ/ì—…ë°ì´íŠ¸ ë“±ë„ ê·¸ëƒ¥ cusr.execute(\"SQLë¬¸\")ìœ¼ë¡œ ì ì–´ì¤€ í›„ conn.commit()í•˜ë©´ DBì— ë°˜ì˜ëœë‹¤ | . | ë°ì´í„° SELECT . | curs.fetchall()ì„ ì‚¬ìš©í•˜ë©´ SELECTë¬¸ìœ¼ë¡œ ê°€ì ¸ì˜¨ ë°ì´í„° ì „ì²´ë¥¼ í•œ ë²ˆì— ì½ì–´ì˜¬ ìˆ˜ ìˆë‹¤ . curs.execute('SELECT * FROM users WHERE follower_count &gt; 10') curs.fetchall() . | curs.fetchone()ì„ ì‚¬ìš©í•˜ë©´ SELECTë¬¸ìœ¼ë¡œ ê°€ì ¸ì˜¨ ë°ì´í„°ë¥¼ í•œ ì¤„ í•œ ì¤„ ì½ì–´ì˜¤ê²Œ ëœë‹¤ (*ì£¼ë¡œ í•œ ì¤„ë§Œ ì½ì–´ì˜¤ë©´ ë  ë•Œ ì‚¬ìš©) . curs.execute('SELECT * FROM users ORDER BY follower_count DESC LIMIT 1') curs.fetchone() . | . | ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ë‹«ê¸° # ì›í•˜ëŠ” ì‘ì—…ì„ ëª¨ë‘ ìˆ˜í–‰í•œ í›„ì— closeë¡œ ë‹«ì•„ì£¼ë©´ ëœë‹¤ conn.close . | . ",
    "url": "https://chaelist.github.io/docs/sql/foreign_key_pymysql/#python%EC%97%90%EC%84%9C-db-%EC%97%B0%EA%B2%B0",
    "relUrl": "/docs/sql/foreign_key_pymysql/#pythonì—ì„œ-db-ì—°ê²°"
  },"66": {
    "doc": "Foreign Key & Python ì—°ê²°",
    "title": "Foreign Key & Python ì—°ê²°",
    "content": " ",
    "url": "https://chaelist.github.io/docs/sql/foreign_key_pymysql/",
    "relUrl": "/docs/sql/foreign_key_pymysql/"
  },"67": {
    "doc": "Function & Module",
    "title": "Function &amp; Module",
    "content": ". | Function (í•¨ìˆ˜) . | í•¨ìˆ˜ ë§Œë“¤ê¸° | global ë³€ìˆ˜ ì„¤ì • | lambda: í•¨ìˆ˜ì˜ ê°„ê²°í•œ ë²„ì „ | lambdaì™€ map(), filter(), reduce() | . | Module . | Module ì¤‘ ì¼ë¶€ í•¨ìˆ˜ë§Œ import | Module ì´ë¦„ì„ ë‹¤ë¥´ê²Œ import | . | . ",
    "url": "https://chaelist.github.io/docs/python_basics/function_module/#function--module",
    "relUrl": "/docs/python_basics/function_module/#function--module"
  },"68": {
    "doc": "Function & Module",
    "title": "Function (í•¨ìˆ˜)",
    "content": ": í•´ë‹¹ í•¨ìˆ˜ì— ì €ì¥ëœ íŠ¹ì • taskë¥¼ ìˆ˜í–‰í•´ì„œ ê·¸ ê²°ê³¼ë¥¼ ë°˜í™˜ . | built-in functions: ex) print(), min() ë“±â€¦ | user-created functions: create when you want to repeat a task (ì—¬ëŸ¬ ì¤„ì˜ ì½”ë“œë¥¼ ë°˜ë³µí•˜ê³  ì‹¶ì„ ë•Œ í•˜ë‚˜ì˜ functionìœ¼ë¡œ ì €ì¥í•´ë‘ê³  ì‚¬ìš©í•˜ë©´ íš¨ìœ¨ì ) | . í•¨ìˆ˜ ë§Œë“¤ê¸° . : defë¥¼ ì‚¬ìš©í•´ í•¨ìˆ˜ ìƒì„± . *í˜•ì‹: . def í•¨ìˆ˜ëª…(ë§¤ê°œë³€ìˆ˜): ìˆ˜í–‰í•  task . *ì˜ˆì‹œ: . # íŠ¹ì • ë¬¸ì¥ì˜ ë‹¨ì–´ ìˆ˜ë¥¼ êµ¬í•˜ëŠ” í•¨ìˆ˜. (ê³µë°± ê¸°ì¤€ìœ¼ë¡œ ë‚˜ëˆ„ì–´ì„œ count) def count_words(s): ## í•¨ìˆ˜ì˜ íŠ¹ì„±ì„ ì˜ ë“œëŸ¬ë‚´ëŠ” ì´ë¦„ì„ ì„¤ì •í•´ì£¼ë©´ ì¢‹ë‹¤ words = s.split() return len(words) z = 'today is a good day.' count_words(z) # ìœ„ì—ì„œ ë§Œë“  í•¨ìˆ˜ë¥¼ ì‚¬ìš© . 5 . global ë³€ìˆ˜ ì„¤ì • . | ì¼ë°˜ì ìœ¼ë¡œ, í•¨ìˆ˜ ì•ˆì˜ ë³€ìˆ˜ëŠ” í•¨ìˆ˜ ì•ˆì—ì„œë§Œ íš¨ë ¥ì„ ê°–ëŠ”ë‹¤ . a = 1 def add_one(a): a = a + 1 add_one(a) print(a) # í•¨ìˆ˜ ì•ˆì—ì„œ aë¥¼ ë³€ê²½ì‹œì¼œë„ globalí•œ aëŠ” ë³€í•¨ì´ ì—†ìœ¼ë¯€ë¡œ 1ì´ ì¶œë ¥ë¨ . 1 . cf) returnì„ í†µí•´ ë³€ê²½ëœ aë¥¼ ë‚´ë³´ë‚´ë©´ ë°–ì—ì„œ ì ‘ê·¼/ì¶œë ¥ ê°€ëŠ¥ . a = 1 def add_one(a): a = a + 1 return a a = add_one(a) # ì´ë ‡ê²Œ í•´ì•¼ aê°€ 2ë¡œ ë°”ë€ë‹¤. +1ì´ ë°˜ì˜ëœ ê²°ê³¼ë¥¼ ë‚´ë³´ë‚´ì„œ, aì— ê°’ìœ¼ë¡œ ë„£ì–´ì£¼ëŠ” ê²ƒ. print(a) # 2ê°€ ì¶œë ¥ë¨ . 2 . | global ëª…ë ¹ì–´ë¥¼ ì‚¬ìš©í•˜ë©´ í•¨ìˆ˜ ë°– ë³€ìˆ˜ì— íš¨ë ¥ì„ ë¯¸ì¹  ìˆ˜ ìˆë‹¤ . a = 1 def add_one(): global a # í•¨ìˆ˜ ë°–ì˜ a (=globalí•œ a) a = a + 1 add_one() # globalí•œ aì— 1ì„ ë”í•œ ê²ƒì´ê¸°ì—, ì‹¤ì œ í•¨ìˆ˜ ë°–ì˜ aê°€ 2ë¡œ ë³€í•¨ print(a) # 2ê°€ ì¶œë ¥ë¨ . 2 . | . â€» í•˜ì§€ë§Œ global ëª…ë ¹ì–´ëŠ” ê°€ê¸‰ì  ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ê²ƒì´ ì¢‹ë‹¤. (í•¨ìˆ˜ëŠ” ë…ë¦½ì ìœ¼ë¡œ ì¡´ì¬í•˜ëŠ” ê²ƒì´ ì¢‹ê¸° ë•Œë¬¸) Â Â Â Â  returnì„ ì‚¬ìš©í•´ì„œ ê°’ì„ ë°–ì—ì„œ ë°›ì„ ë°©ë²•ì„ ë§Œë“¤ì–´ ì£¼ëŠ” ë°©ë²•ì´ ê°€ì¥ ì¢‹ìŒ! . lambda: í•¨ìˆ˜ì˜ ê°„ê²°í•œ ë²„ì „ . : ë³´í†µ í•¨ìˆ˜ë¥¼ í•œ ì¤„ë¡œ ê°„ê²°í•˜ê²Œ í‘œí˜„í•˜ê³  ì‹¶ì„ ë•Œ def ëŒ€ì‹  lambdaë¥¼ ì‚¬ìš©í•œë‹¤ . | defë¥¼ ì‚¬ìš©í•  ì •ë„ë¡œ ë³µì¡í•˜ì§€ ì•Šê±°ë‚˜, defë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ì„ ë•Œ ì£¼ë¡œ ì‚¬ìš© | . *í˜•ì‹: . lambda ë§¤ê°œë³€ìˆ˜: í‘œí˜„ì‹ . *ì˜ˆì‹œ: . sq = lambda a: a**2 print(sq(2)) # 2**2ì˜ ê²°ê³¼ì¸ 4ê°€ ì¶œë ¥ë¨ . 4 . +) ë§¤ê°œë³€ìˆ˜ë¥¼ 2ê°œ ì´ìƒ ë„£ì–´ë„ ëœë‹¤ . add = lambda a, b: a + b print(add(1, 2)) # 1 + 2ì˜ ê²°ê³¼ì¸ 3ì´ ì¶œë ¥ë¨ . 3 . lambdaì™€ map(), filter(), reduce() . (lambdaì˜ ê°€ì¹˜ë¥¼ ê·¹ëŒ€í™”í•´ì£¼ëŠ” í•¨ìˆ˜ë“¤) . | map(í•¨ìˆ˜, ë¦¬ìŠ¤íŠ¸): ë¦¬ìŠ¤íŠ¸ì—ì„œ ì›ì†Œë¥¼ í•˜ë‚˜ì”© êº¼ë‚´ í•¨ìˆ˜ë¥¼ ì ìš©í•˜ê³ , ê·¸ ê²°ê³¼ë¥¼ ìƒˆë¡œìš´ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜ # assignëœ ë¦¬ìŠ¤íŠ¸ì˜ ê° ì›ì†Œì˜ ì œê³±ì„ ì›ì†Œë“¤ë¡œ í•˜ëŠ” ìƒˆë¡œìš´ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜ list(map(lambda x: x**2, [0, 1, 2, 3, 4])) . [0, 1, 4, 9, 16] . | filter(í•¨ìˆ˜, ë¦¬ìŠ¤íŠ¸): ë¦¬ìŠ¤íŠ¸ì˜ ê° ì›ì†Œë“¤ì— í•¨ìˆ˜ë¥¼ ì ìš©í•´, ê²°ê³¼ê°€ Trueì¸ ê°’ë“¤ë§Œ ë„£ì€ ìƒˆë¡œìš´ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜ # assignëœ ë¦¬ìŠ¤íŠ¸ì˜ ê° ì›ì†Œ ì¤‘ 2ë¡œ ë‚˜ëˆ„ì–´ ë–¨ì–´ì§€ëŠ” ê°’, ì¦‰ ì§ìˆ˜ë§Œì„ ì›ì†Œë¡œ í•˜ëŠ” ìƒˆë¡œìš´ ë¦¬ìŠ¤íŠ¸ë¥¼ ìƒì„± list(filter(lambda x: x % 2 == 0, [0, 1, 2, 3, 4])) . [0, 2, 4] . ## ë‹¤ë¥¸ ì˜ˆì‹œ: 3ë³´ë‹¤ ì‘ì€ ìˆ˜ë§Œ ê°€ì§€ê³  ë¦¬ìŠ¤íŠ¸ ìƒì„± list(filter(lambda x: x &lt; 3, [0, 1, 2, 3, 4])) . [0, 1, 2] . | reduce(í•¨ìˆ˜, ìˆœì„œí˜• ìë£Œ): ìˆœì„œí˜• ìë£Œ(list/string/tuple)ì˜ ì›ì†Œë“¤ì„ í•¨ìˆ˜ì— ëˆ„ì  ì ìš©í•´, ìµœì¢… ê²°ê³¼ë¥¼ ë°˜í™˜ . | map(), filter()ì™€ ë‹¬ë¦¬, ê¸°ë³¸ ë‚´ì¥ í•¨ìˆ˜ê°€ ì•„ë‹ˆë¼ì„œ importí•´ì£¼ì–´ì•¼ ì‚¬ìš© ê°€ëŠ¥. | . from functools import reduce ## reduceëŠ” ë‚´ì¥ í•¨ìˆ˜ê°€ ì•„ë‹ˆë¼ importí•´ì•¼ í•¨ reduce(lambda x, y: x + y, [0, 1, 2, 3, 4] # 1) 0+1ì˜ ê²°ê³¼ì¸ 1ì´ xê°€ ë˜ê³ , ë‹¤ìŒ ì›ì†Œì¸ 2ê°€ yê°€ ë˜ì–´ 1+2=3 # 2) 1+2ì˜ ê²°ê³¼ì¸ 3ì´ xê°€ ë˜ê³ , ë‹¤ìŒ ì›ì†Œì¸ 3ì´ yê°€ ë˜ì–´ 3+3=6 # 3) 3+3ì˜ ê²°ê³¼ì¸ 6ì´ xê°€ ë˜ê³ , ë‹¤ìŒ ì›ì†Œì¸ 4ê°€ yê°€ ë˜ì–´ 6+4=10 ## ê²°êµ­, &lt;ëª¨ë“  ì›ì†Œë¥¼ ë”í•œ ê²°ê³¼&gt;ì¸ 10ì´ ë°˜í™˜ë¨. 10 . | . ",
    "url": "https://chaelist.github.io/docs/python_basics/function_module/#function-%ED%95%A8%EC%88%98",
    "relUrl": "/docs/python_basics/function_module/#function-í•¨ìˆ˜"
  },"69": {
    "doc": "Function & Module",
    "title": "Module",
    "content": ": a file containing codes . | íŠ¹ì • moduleì€ íŠ¹ì • ëª©ì ì— ë§ëŠ” function, class ë“±ì„ ë‹´ê³  ìˆë‹¤ . | ex) â€˜mathâ€™ ëª¨ë“ˆì€ mathì— í•„ìš”í•œ ê¸°ëŠ¥ë“¤ì„, â€˜matplotlibâ€™ ëª¨ë“ˆì€ visualizationì— í•„ìš”í•œ ê¸°ëŠ¥ë“¤ì„ í¬í•¨ | . | importë¥¼ í†µí•´ ëª¨ë“ˆì„ ê°€ì ¸ì™€ì„œ ì‚¬ìš©í•œë‹¤ | ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ëª¨ë“ˆì„ ì‚¬ìš©í•˜ëŠ” ê²ƒ ì™¸ì—, ì§ì ‘ ëª¨ë“ˆì„ ë§Œë“¤ì–´ì„œ ì‚¬ìš©í•  ìˆ˜ë„ ìˆë‹¤ . | .py íŒŒì¼ì— ì›í•˜ëŠ” ì½”ë“œë“¤ì„ ì €ì¥í•´ë‘ê³ , ë‹¤ë¥¸ .py / .ipynb íŒŒì¼ì—ì„œ importí•´ì£¼ë©´ ëœë‹¤ | â€» import í•˜ë ¤ëŠ” ëª¨ë“ˆì´ ê°™ì€ ë””ë ‰í† ë¦¬ì— ìˆì–´ì•¼ í•œë‹¤ | â€» import filename.pyì™€ ê°™ì´ íŒŒì¼ëª…ì„ ê·¸ëŒ€ë¡œ ì“°ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, import filenameì´ë¼ê³ ë§Œ ì¨ì•¼ í•œë‹¤ | . | . import math # 'import'ë¥¼ í†µí•´ ëª¨ë“ˆì„ ê°€ì ¸ì˜´ dir(math) ## dir()ë¥¼ í†µí•´ ëª¨ë“ˆì— í¬í•¨ëœ í•¨ìˆ˜ ë“± ëª©ë¡ì„ í™•ì¸ ê°€ëŠ¥ . ['__doc__', '__loader__', '__name__', '__package__', '__spec__', 'acos', 'acosh', 'asin', 'asinh', 'atan', 'atan2', 'atanh', 'ceil', 'copysign', 'cos', 'cosh', 'degrees', 'e', 'erf', 'erfc', 'exp', 'expm1', 'fabs', 'factorial', 'floor', 'fmod', 'frexp', 'fsum', 'gamma', 'gcd', 'hypot', 'inf', 'isclose', 'isfinite', 'isinf', 'isnan', 'ldexp', 'lgamma', 'log', 'log10', 'log1p', 'log2', 'modf', 'nan', 'pi', 'pow', 'radians', 'sin', 'sinh', 'sqrt', 'tan', 'tanh', 'tau', 'trunc'] . Module ì¤‘ ì¼ë¶€ í•¨ìˆ˜ë§Œ import . from math import sqrt math.sqrt(4) # sqrt: square route. ë£¨íŠ¸ ì”Œìš´ ê°’ ì¶œë ¥ . 2.0 . +) ì—¬ëŸ¬ ê°œë¥¼ ê°€ì ¸ì˜¤ê³  ì‹¶ìœ¼ë©´ ,ë¡œ êµ¬ë¶„ . from math import sqrt, pow, trunc . Module ì´ë¦„ì„ ë‹¤ë¥´ê²Œ import . import math as ma ma.sqrt(4) . 2.0 . | import pandas as pd, import numpy as np ë“± ê´€í–‰ì ìœ¼ë¡œ ì¤„ì—¬ì„œ ì‚¬ìš©í•˜ëŠ” ì´ë¦„ì´ ì¡´ì¬í•˜ëŠ” ëª¨ë“ˆë„ ë§ìŒ | . ",
    "url": "https://chaelist.github.io/docs/python_basics/function_module/#module",
    "relUrl": "/docs/python_basics/function_module/#module"
  },"70": {
    "doc": "Function & Module",
    "title": "Function & Module",
    "content": " ",
    "url": "https://chaelist.github.io/docs/python_basics/function_module/",
    "relUrl": "/docs/python_basics/function_module/"
  },"71": {
    "doc": "Image ìˆ˜ì§‘ & API í™œìš©",
    "title": "Image ìˆ˜ì§‘ &amp; API í™œìš©",
    "content": ". | Image ìˆ˜ì§‘ | APIë¡œ ë°ì´í„° ìˆ˜ì§‘ . | KOFIC(ì˜í™”ì§„í¥ìœ„ì›íšŒ) API | . | . ",
    "url": "https://chaelist.github.io/docs/webscraping/image_api/#image-%EC%88%98%EC%A7%91--api-%ED%99%9C%EC%9A%A9",
    "relUrl": "/docs/webscraping/image_api/#image-ìˆ˜ì§‘--api-í™œìš©"
  },"72": {
    "doc": "Image ìˆ˜ì§‘ & API í™œìš©",
    "title": "Image ìˆ˜ì§‘",
    "content": "(downloading images from the web) . import requests from bs4 import BeautifulSoup import shutil # íŒŒì¼/í´í„° ê´€ë ¨ ì‘ì—…í•  ë•Œ ì“°ëŠ” í•¨ìˆ˜ ## ì´ë¯¸ì§€ ì €ì¥ í•¨ìˆ˜ ë§Œë“¤ì–´ë‘ê¸° def get_picture(picture_url, name): # nameì€ ì´ë¯¸ì§€ë¥¼ ì €ì¥í•  ì´ë¦„ with requests.get(picture_url, stream=True) as r: # stream=Trueë¡œ ì„¤ì •í•˜ë©´ connectionì´ ê³„ì† ì—´ë ¤ ìˆëŠ”ë‹¤ f = open('{}.jpg'.format(name), 'wb') # wb= write binary (0101...) shutil.copyfileobj(r.raw, f) f.close() ## ì•„ë˜ nytimes ê¸°ì‚¬ì—ì„œ ì´ë¯¸ì§€ urlì„ ê°€ì ¸ì˜¨ í›„, í•´ë‹¹ ì´ë¯¸ì§€ë¥¼ íŒŒì¼ë¡œ ì €ì¥í•˜ê¸° r = requests.get('https://www.nytimes.com/2018/05/06/us/politics/gina-haspel-cia.html') soup = BeautifulSoup(r.text, 'lxml') picture_url = soup.find('meta', attrs={'name':'image'}).get('content') # ê¸°ì‚¬ ì† ì´ë¯¸ì§€ urlì„ ê°€ì ¸ì˜¨ë‹¤ get_picture(picture_url, 'nytimes_image') ## í•¨ìˆ˜ ì‹¤í–‰ -&gt; 'nytimes_image.jpg'ë¼ëŠ” ì´ë¦„ìœ¼ë¡œ ì´ë¯¸ì§€ ì €ì¥ . | By default, when you make a request, the body of the response is downloaded immediatedly) | í•˜ì§€ë§Œ stream=Trueë¥¼ ì˜µì…˜ìœ¼ë¡œ ì¨ì£¼ë©´ ë‚´ìš©ì´ ë‹¤ìš´ë°›ì•„ì§€ëŠ” ê²ƒì´ ë¯¸ë¤„ì§€ê³ , connectionì´ ê³„ì† ì—´ë ¤ ìˆëŠ”ë‹¤. (ëª¨ë“  ë°ì´í„°ë¥¼ ë‹¤ ë°›ì•„ì˜¤ê±°ë‚˜ responseë¥¼ ë‹«ì•„ì£¼ê¸° ì „ê¹Œì§€ connectionì´ ì§€ì†) | ê·¸ë ‡ê¸°ì—, with requests.get(url, stream=True) as r: ì™€ ê°™ì€ í˜•íƒœë¡œ stream=Trueë¥¼ ì‚¬ìš©í•˜ë©´ connectionì´ ê³„ì† ë‹«í˜€ ìˆë‹¤ê°€ í•„ìš”í•  ë•Œë§Œ ì—´ë¦¬ë„ë¡ í•´ì£¼ë¯€ë¡œ íš¨ìœ¨ì . | . ",
    "url": "https://chaelist.github.io/docs/webscraping/image_api/#image-%EC%88%98%EC%A7%91",
    "relUrl": "/docs/webscraping/image_api/#image-ìˆ˜ì§‘"
  },"73": {
    "doc": "Image ìˆ˜ì§‘ & API í™œìš©",
    "title": "APIë¡œ ë°ì´í„° ìˆ˜ì§‘",
    "content": "*API (Application Program Interface): íŠ¹ì • ì‚¬ì´íŠ¸ì—ì„œ íŠ¹ì • ë°©ì‹ìœ¼ë¡œ íŠ¹ì •í•œ ì–‘ì‹ì˜ ë°ì´í„°ë¥¼ ë°›ì•„ì˜¬ ìˆ˜ ìˆëŠ” ì¼ì¢…ì˜ ë§¤ê°œì²´. (ì‚¬ì´íŠ¸ë§ˆë‹¤ ìì‚¬ APIë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ë°©ì‹ ë° ìš”ê±´ì´ ì•ˆë‚´ë˜ì–´ ìˆìŒ) . | APIë¡œ ë°›ì•„ì˜¤ëŠ” ë°ì´í„°ëŠ” ë³´í†µ XMLì´ë‚˜ JSON í˜•íƒœì¸ë°, JSON í˜•íƒœê°€ XMLë³´ë‹¤ ì‹¬í”Œí•˜ê¸° ë•Œë¬¸ì— ë” ë„ë¦¬ ì‚¬ìš©ëœë‹¤ | JSONì€ êµ¬ì¡°ê°€ python dictionaryì™€ ìœ ì‚¬í•˜ê¸° ë•Œë¬¸ì—, dictionary ì‚¬ìš©í•  ë•Œì˜ ë°©ë²•ìœ¼ë¡œ ì ‘ê·¼ ê°€ëŠ¥í•˜ë‹¤ | . +) REST ë°©ì‹ê³¼ SOAP ë°©ì‹? . | REST(Representational State Transfer): ë„¤íŠ¸ì›Œí¬ë¥¼ í†µí•´ì„œ ì»´í“¨í„°ë“¤ë¼ë¦¬ í†µì‹ í•  ìˆ˜ ìˆê²Œ í•´ì£¼ëŠ” ì•„í‚¤í…ì²˜ ìŠ¤íƒ€ì¼ë¡œ, HTTP í”„ë¡œí† ì½œë¡œ ë°ì´í„°ë¥¼ ì „ë‹¬í•œë‹¤. ëŒ€ë¶€ë¶„ì˜ public APIëŠ” REST ë°©ì‹ì„ ë”°ë¥¸ë‹¤. | SOAP(Simple Object Access Protocol): XML ê¸°ë°˜ì˜ ë©”ì‹œì§€ë¥¼ ë„¤íŠ¸ì›Œí¬ ìƒì—ì„œ êµí™˜í•˜ëŠ” í”„ë¡œí† ì½œ. ë³´ì•ˆì´ë‚˜ ë©”ì‹œì§€ ì „ì†¡ ë“±ì— ìˆì–´ì„œ RESTë³´ë‹¤ ë” ë§ì€ í‘œì¤€ë“¤ì´ ì •í•´ì ¸ìˆê¸° ë•Œë¬¸ì— ì¡°ê¸ˆ ë” ë³µì¡í•˜ë‹¤. | . KOFIC(ì˜í™”ì§„í¥ìœ„ì›íšŒ) API . : ì•„ë˜ ë§í¬ì˜ ì‚¬ì´íŠ¸ì—ì„œ keyë¥¼ ë°œê¸‰ë°›ê³ , ì‚¬ìš© ë°©ë²• ê°€ì´ë“œë¥¼ í™•ì¸í•  ìˆ˜ ìˆë‹¤. https://www.kobis.or.kr/kobisopenapi/homepg/apiservice/searchServiceInfo.do?serviceId=searchWeeklyBoxOffice . * Weekly Boxoffice ë°ì´í„° ë°›ì•„ì˜¤ê¸° . | ì›í•˜ëŠ” ì£¼ê°„ì˜ boxoffice ë°ì´í„°ë¥¼ ë°›ì•„ì˜¤ëŠ” url ë§Œë“¤ê¸° key = 'My_Key' ## ë°œê¸‰ë°›ì€ keyë¥¼ ë„£ì–´ì¤€ë‹¤ target_date = '20210104' # 20210104(ì›”)ë¶€í„° ì‹œì‘í•˜ëŠ” ì£¼ê°„ (01.04~01.10) weekchoice = '0' # 0: ì£¼ê°„(ì›”~ì¼), 1: ì£¼ë§(ê¸ˆ~ì¼)(default), 2: ì£¼ì¤‘(ì›”~ëª©) url_basic = 'http://www.kobis.or.kr/kobisopenapi/webservice/rest/boxoffice/searchWeeklyBoxOfficeList.json?key={}&amp;targetDt={}&amp;weekGb={}' url = url_basic.format(key, target_date, weekchoice) . | requestsë¡œ ë°ì´í„°ë¥¼ ë°›ì•„ json parsing ê°€ëŠ¥ í˜•íƒœë¡œ ë³€í™˜ . | json libraryì˜ â€˜loadsâ€™(=load from stringì„ ì˜ë¯¸) ì‚¬ìš© â†’ string í˜•íƒœë¡œ ë‹´ê²¨ ìˆëŠ” json dataë¥¼ json parsingì´ ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜í•´ì¤€ë‹¤. | . import requests import json # requstsë¡œ ë°ì´í„°ë¥¼ ê°€ì—¬ì˜¨ í›„,json í˜•íƒœë¡œ ë³€í™˜í•´ì„œ parsingí•  ìˆ˜ ìˆê²Œ ë§Œë“ ë‹¤ r = requests.get(url) text = r.text j = json.loads(text) ## json libraryì˜ 'loads' ì‚¬ìš©. -- loadsëŠ” load from stringì„ ì˜ë¯¸ (string í˜•íƒœë¡œ ë‹´ê²¨ ìˆëŠ” json dataë¥¼ json parsingì´ ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜) . cf) r.textì—ëŠ” ì´ëŸ° ì‹ìœ¼ë¡œ ë°ì´í„°ê°€ ë‹´ê²¨ ìˆë‹¤ . text . {\"boxOfficeResult\":{\"boxofficeType\":\"ì£¼ê°„ ë°•ìŠ¤ì˜¤í”¼ìŠ¤\",\"showRange\":\"20210104~20210110\",\"yearWeekTime\":\"202101\",\"weeklyBoxOfficeList\":[{\"rnum\":\"1\",\"rank\":\"1\",\"rankInten\":\"0\",\"rankOldAndNew\":\"OLD\",\"movieCd\":\"20192567\",\"movieNm\":\"ì›ë” ìš°ë¨¼ 1984\",\"openDt\":\"2020-12-23\",\"salesAmt\":\"428613900\",\"salesShare\":\"33.9\",\"salesInten\":\"-952680560\",\"salesChange\":\"-69.0\",\"salesAcc\":\"4573697330\",\"audiCnt\":\"45689\",\"audiInten\":\"-111904\",\"audiChange\":\"-71.0\",\"audiAcc\":\"507121\",\"scrnCnt\":\"1321\",\"showCnt\":\"14083\"},{\"rnum\":\"2\",\"rank\":\"2\",\"rankInten\":\"0\",\"rankOldAndNew\":\"OLD\",\"movieCd\":\"20040725\",\"movieNm\":\"í™”ì–‘ì—°í™”\",\"openDt\":\"2000-10-20\",\"salesAmt\":\"147512030\",\"salesShare\":\"11.7\",\"salesInten\":\"-88550420\",\"salesChange\":\"-37.5\",\"salesAcc\":\"654243280\",\"audiCnt\":\"15999\",\"audiInten\":\"-11936\",\"audiChange\":\"-42.7\",\"audiAcc\":\"75797\",\"scrnCnt\":\"447\",\"showCnt\":\"4064\"}, (ìƒëµ) . | weeklyBoxOfficeListì˜ ì²«ë²ˆì§¸ elementë§Œ ë¨¼ì € parsingí•´ë´„ j['boxOfficeResult']['weeklyBoxOfficeList'][0] . {'audiAcc': '507121', 'audiChange': '-71.0', 'audiCnt': '45689', 'audiInten': '-111904', 'movieCd': '20192567', 'movieNm': 'ì›ë” ìš°ë¨¼ 1984', 'openDt': '2020-12-23', 'rank': '1', 'rankInten': '0', 'rankOldAndNew': 'OLD', 'rnum': '1', 'salesAcc': '4573697330', 'salesAmt': '428613900', 'salesChange': '-69.0', 'salesInten': '-952680560', 'salesShare': '33.9', 'scrnCnt': '1321', 'showCnt': '14083'} . | í•´ë‹¹ ê¸°ê°„ Top10 ì˜í™”ì˜ ì´ë¦„, ê´€ê°ìˆ˜, ë§¤ì¶œì•¡, ìƒì˜íšŸìˆ˜ ë°ì´í„° ê°€ì ¸ì™€ì„œ ì •ë¦¬í•˜ê¸° . | ê´€ê°ìˆ˜, ë§¤ì¶œì•¡, ìƒì˜íšŸìˆ˜ëŠ” ëª¨ë‘ ëˆ„ì ì´ ì•„ë‹Œ, í•´ë‹¹ ê¸°ê°„ì˜ ìˆ˜ì¹˜ | . import pandas as pd import numpy as np # DataFrame ì¤€ë¹„ df = pd.DataFrame(columns=['Movie', '#_of_Tickets_Sold', 'Revenue', '#_of_Screenings']) for item in j['boxOfficeResult']['weeklyBoxOfficeList']: srs = pd.Series([item['movieNm'], int(item['audiCnt']), int(item['salesAmt']), int(item['showCnt'])], index=['Movie', '#_of_Tickets_Sold', 'Revenue', '#_of_Screenings']) df = df.append(srs, ignore_index=True) df . | Â  | Movie | #_of_Tickets_Sold | Revenue | #_of_Screenings | . | 0 | ì›ë” ìš°ë¨¼ 1984 | 45689 | 428613900 | 14083 | . | 1 | í™”ì–‘ì—°í™” | 15999 | 147512030 | 4064 | . | 2 | ì¡°ì œ | 9242 | 84387220 | 2559 | . | 3 | ë„êµ´ | 6916 | 62909780 | 1831 | . | 4 | ë±…ê°€ë“œ | 6307 | 56071400 | 2116 | . | 5 | ë¯¸ìŠ¤í„° ì¡´ìŠ¤ | 5618 | 50406750 | 1258 | . | 6 | ì™„ë²½í•œ ê°€ì¡± | 5229 | 46994840 | 1793 | . | 7 | ë¹…í’‹ ì£¼ë‹ˆì–´2: íŒ¨ë°€ë¦¬ê°€ ë–´ë‹¤ | 5193 | 44030440 | 1218 | . | 8 | ë¼ë¼ëœë“œ | 3275 | 23033000 | 368 | . | 9 | ê±¸ | 2830 | 26387710 | 1046 | . | ì¶”ê°€) ìƒì˜íšŸìˆ˜ - í‹°ì¼“íŒë§¤ëŸ‰ ê°„ì˜ ìƒê´€ê´€ê³„ë¥¼ í™•ì¸ df.plot(kind='scatter', x='#_of_Screenings', y='#_of_Tickets_Sold') . | ëŒ€ì²´ë¡œ ìƒì˜íšŸìˆ˜ê°€ ë§ì„ìˆ˜ë¡ í‹°ì¼“ íŒë§¤ëŸ‰ë„ í•¨ê»˜ ëŠ˜ì–´ë‚œë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤. | . | . ",
    "url": "https://chaelist.github.io/docs/webscraping/image_api/#api%EB%A1%9C-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%88%98%EC%A7%91",
    "relUrl": "/docs/webscraping/image_api/#apië¡œ-ë°ì´í„°-ìˆ˜ì§‘"
  },"74": {
    "doc": "Image ìˆ˜ì§‘ & API í™œìš©",
    "title": "Image ìˆ˜ì§‘ & API í™œìš©",
    "content": " ",
    "url": "https://chaelist.github.io/docs/webscraping/image_api/",
    "relUrl": "/docs/webscraping/image_api/"
  },"75": {
    "doc": "ì´ë¯¸ì§€ / ë™ì˜ìƒ ì²˜ë¦¬",
    "title": "ì´ë¯¸ì§€ / ë™ì˜ìƒ ì²˜ë¦¬",
    "content": ". | ì´ë¯¸ì§€ / ë™ì˜ìƒ ë°ì´í„° ì½ê¸° . | ì´ë¯¸ì§€ ë°ì´í„° ì½ê¸° | ë™ì˜ìƒ ë°ì´í„° ì½ê¸° | ë™ì˜ìƒì„ ì´ë¯¸ì§€ë¡œ ë‚˜ëˆ  ì €ì¥ | . | ì´ë¯¸ì§€ ì† ì‚¬ëŒ ì¸ì‹ . | ì‚¬ëŒ ê²€ì¶œ | ì‚¬ëŒ ì–¼êµ´ ê²€ì¶œ | ì‚¬ëŒ ì–¼êµ´ ë°©í–¥ í™•ì¸ | . | ì‚¬ëŒ ì¸ì‹: ì˜ìƒ ì˜ˆì‹œ . | íƒ€ì„ë©ìŠ¤ ë§Œë“¤ê¸° | ê±°ë¦¬ì˜ ì‚¬ëŒ ìˆ˜ ë³€í™” ì¸¡ì • | ì´ë™ í‰ê·  ê³„ì‚° í›„ ë¹„êµ | . | . ",
    "url": "https://chaelist.github.io/docs/ml_application/image_processing/",
    "relUrl": "/docs/ml_application/image_processing/"
  },"76": {
    "doc": "ì´ë¯¸ì§€ / ë™ì˜ìƒ ì²˜ë¦¬",
    "title": "ì´ë¯¸ì§€ / ë™ì˜ìƒ ë°ì´í„° ì½ê¸°",
    "content": "*OpenCV ì‚¬ìš© . | OpenCV: ì‹¤ì‹œê°„ ì´ë¯¸ì§€ í”„ë¡œì„¸ì‹±ì— ì¤‘ì ì„ ë‘” ë¼ì´ë¸ŒëŸ¬ë¦¬ | ì„¤ì¹˜í•´ì•¼ ì‚¬ìš© ê°€ëŠ¥: pip install opencv-python | . ì´ë¯¸ì§€ ë°ì´í„° ì½ê¸° . import cv2 img = cv2.imread(\"img/img_students.jpg\") # ì •ë³´ ì½ê¸° height, width = img.shape[:2] print(f\"ì´ë¯¸ì§€ ê°€ë¡œ: {width}\") print(f\"ì´ë¯¸ì§€ ì„¸ë¡œ: {height}\") # ì°½ì— ì´ë¯¸ì§€ ë„ìš°ê¸° cv2.imshow(\"img\", img) cv2.waitKey(0) # 0ì„ ì§€ì •í•˜ë©´ ìœˆë„ìš°ë¥¼ ë‹«ì„ ë•Œê¹Œì§€ ê³„ì†í•´ì„œ ë³´ì—¬ì¤Œ cv2.destroyAllWindows() ## ì´ë¯¸ì§€ ì°½ì´ ë³„ë„ì˜ ìœˆë„ìš°ë¡œ ì—´ë¦¬ê³ , ì•„ë¬´ í‚¤ë‚˜ ëˆ„ë¥´ë©´ ì´ë¯¸ì§€ ì°½ì´ ë‹«í˜ . ì´ë¯¸ì§€ ê°€ë¡œ: 1920 ì´ë¯¸ì§€ ì„¸ë¡œ: 1281 . (ì´ë¯¸ì§€ê°€ ë„ˆë¬´ ì»¤ì„œ PCì—ì„œ ì˜ë¦¼) . | imread: cv2.imread(â€œimg.jpgâ€, cv2.IMREAD_GRAYSCALE) ì´ëŸ°ì‹ìœ¼ë¡œ ì˜µì…˜ì„ ë„£ì–´ì¤„ ìˆ˜ë„ ìˆëŠ”ë°, ì•„ë¬´ê²ƒë„ ì•ˆë„£ìœ¼ë©´ defaultëŠ” ê·¸ëƒ¥ colorë¡œ ì½ì–´ì˜¤ëŠ” ê²ƒ. | cv2.IMREAD_COLOR: BGR ì»¬ëŸ¬ ì ìš© (3 ì±„ë„, default) | cv2.IMREAD_UNCHANGED: ì›ë³¸ ì‚¬ìš© (ì´ë¯¸ì§€ íŒŒì¼ì„ alpha channelê¹Œì§€ í¬í•¨í•˜ì—¬ ì½ì–´ ë“¤ì„) | cv2.IMREAD_GRAYSCALE: ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ì ìš© (1 ì±„ë„) | . | img.shape: (ë†’ì´, ë„ˆë¹„, ì±„ë„)ì„ return. | ì±„ë„ì€ ìƒ‰ìƒ ì •ë³´ë¡œ, 3ì´ë©´ ë‹¤ìƒ‰(BGR), 1ì´ë©´ ë‹¨ìƒ‰(grayscale) | . | waitKey(): ëª‡ ì´ˆ ë™ì•ˆ ì´ë¯¸ì§€ë¥¼ í‘œì‹œí•  ì§€ë¥¼ ë°€ë¦¬ì´ˆ(ms) ë‹¨ìœ„ë¡œ ì§€ì • . | ex. waitKey(1000)ì´ë¼ê³  í•˜ë©´ 1ì´ˆê°„ í‘œì‹œë¨ | ìœˆë„ìš°ë¥¼ ë‹«ì„ ë•Œê¹Œì§€ ê³„ì†í•´ì„œ ë³´ì—¬ì£¼ê³  ì‹¶ìœ¼ë©´ 0ì„ ì§€ì •í•˜ë©´ ë¨ | . | . Â  . â†’ image resizing (ì´ë¯¸ì§€ í¬ê¸° ë°”ê¾¸ê¸°) . # ì´ ê²½ìš°, ì´ë¯¸ì§€ê°€ ë„ˆë¬´ í¬ê¸° ë•Œë¬¸ì— ì‚¬ì´ì¦ˆë¥¼ ì¤„ì—¬ì¤€ë‹¤ resize_img = cv2.resize(img, dsize=(0, 0), fx=0.5, fy=0.5, interpolation=cv2.INTER_AREA) cv2.imshow(\"resize img\", resize_img) cv2.waitKey(0) # 0ì„ ì§€ì •í•˜ë©´ ìœˆë„ìš°ë¥¼ ë‹«ì„ ë•Œê¹Œì§€ ê³„ì†í•´ì„œ ë³´ì—¬ì¤Œ cv2.destroyAllWindows() . (ì´ì œëŠ” PCì—ì„œ ì˜ë¦¬ì§€ ì•Šê³  ì˜ ë³´ì„) . | dsize=(0, 0), fx=0.5, fx=0.5ë¼ê³  í•˜ë©´ ì›ë³¸ í¬ê¸°ì—ì„œ ê°€ë¡œ, ì„¸ë¡œë¥¼ ê°ê° 0.5ë°°ë¡œ ë³€ê²½í•˜ê² ë‹¤ëŠ” ëœ» . | +) Manual sizing: dsize=(640, 480) ì´ëŸ°ì‹ìœ¼ë¡œ ìˆ«ìë¥¼ ë„£ì–´ì£¼ë©´ ê°€ë¡œ=640, ì„¸ë¡œ=480ìœ¼ë¡œ ë³€ê²½ | . | interpolation=cv2.INTER_AREAëŠ” ì´ë¯¸ì§€ ì¶•ì†Œ ì‹œì— ì£¼ë¡œ ì‚¬ìš©í•˜ëŠ” ë³´ê°„ë²•. | +) ì´ë¯¸ì§€ë¥¼ í™•ëŒ€í•˜ëŠ” ê²½ìš°ì—ëŠ” cv2.INTER_LINEARì´ë‚˜ cv2.INTER_CUBIC ë³´ê°„ë²•ì„ ì£¼ë¡œ ì‚¬ìš© | . | . ë™ì˜ìƒ ë°ì´í„° ì½ê¸° . cap = cv2.VideoCapture(\"vid/vid01.avi\") ## ë°ì´í„° ì¶œì²˜: https://github.com/wikibook/pyda100 # ì •ë³´ ì½ê¸° width = cap.get(cv2.CAP_PROP_FRAME_WIDTH) height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT) count = cap.get(cv2.CAP_PROP_FRAME_COUNT) fps = cap.get(cv2.CAP_PROP_FPS) print(f\"ê°€ë¡œ: {width}\") print(f\"ì„¸ë¡œ: {height}\") print(f\"ì´ í”„ë ˆì„ìˆ˜: {count}\") print(f\"FPS: {fps}\") # Frame Per Second # ì°½ì— ì˜ìƒ ë„ìš°ê¸° while(cap.isOpened()): ret, frame = cap.read() # cap.read()ì˜ ê²°ê³¼ë¡œ (retvalê°’(bool), í”„ë ˆì„ì´ë¯¸ì§€)ê°€ returnë˜ëŠ”ë°, í”„ë ˆì„ì´ ì˜ ì½íˆë©´ retvalê°’ì€ True if ret: cv2.imshow(\"frame\", frame) if cv2.waitKey(1) &amp; 0xFF == ord(\"q\"): # ê° í”„ë ˆì„ì„ 1ë°€ë¦¬ì´ˆ ë™ì•ˆ í‘œì‹œí•˜ê³  ë‹¤ìŒ í”„ë ˆì„ìœ¼ë¡œ ì´ë™, 'q' í‚¤ë¥¼ í´ë¦­í•˜ë©´ ì¢…ë£Œ break else: break # ëª¨ë“  í”„ë ˆì„ì´ ì²˜ë¦¬ë˜ë©´ ì¢…ë£Œ cap.release() # íŒŒì¼ì„ ë‹«ì•„ì¤€ë‹¤ cv2.destroyAllWindows() # ìƒì„±í•œ ëª¨ë“  window ì œê±° . ê°€ë¡œ: 1920.0 ì„¸ë¡œ: 1440.0 ì´ í”„ë ˆì„ìˆ˜: 401.0 FPS: 30.0 . | Whatâ€™s 0xFF for in cv2.waitKey(1)? https://stackoverflow.com/questions/35372700/whats-0xff-for-in-cv2-waitkey1 | . ë™ì˜ìƒì„ ì´ë¯¸ì§€ë¡œ ë‚˜ëˆ  ì €ì¥ . cap = cv2.VideoCapture(\"vid/vid01.avi\") num = 0 while(cap.isOpened()): ret, frame = cap.read() if ret: cv2.imshow(\"frame\", frame) filepath = f\"snapshots/snapshot_{num}.jpg\" # ê²½ë¡œ+íŒŒì¼ëª… cv2.imwrite(filepath, frame) if cv2.waitKey(1) &amp; 0xFF == ord('q'): break else: break num = num + 1 cap.release() cv2.destroyAllWindows() . | â€˜snapshotsâ€™ í´ë”ì— â€˜snapshot_0.jpgâ€™ë¶€í„° â€˜snapshot_400.jpgâ€™ê¹Œì§€ ì €ì¥ë¨ | . ",
    "url": "https://chaelist.github.io/docs/ml_application/image_processing/#%EC%9D%B4%EB%AF%B8%EC%A7%80--%EB%8F%99%EC%98%81%EC%83%81-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%9D%BD%EA%B8%B0",
    "relUrl": "/docs/ml_application/image_processing/#ì´ë¯¸ì§€--ë™ì˜ìƒ-ë°ì´í„°-ì½ê¸°"
  },"77": {
    "doc": "ì´ë¯¸ì§€ / ë™ì˜ìƒ ì²˜ë¦¬",
    "title": "ì´ë¯¸ì§€ ì† ì‚¬ëŒ ì¸ì‹",
    "content": "ì‚¬ëŒ ê²€ì¶œ . | HOG íŠ¹ì§•ëŸ‰ì„ í™œìš© | HOG: Histogram of Oriented Gradients. ì´ë¯¸ì§€ì˜ ì§€ì—­ì ì¸ Gradient(ê¸°ìš¸ê¸° ì •ë³´)ë¥¼ íŠ¹ì§•ìœ¼ë¡œ ì‚¬ìš©í•´ ì‚¬ëŒ í˜•íƒœë¥¼ íŒë³„ (ì£¼ë¡œ ì „ì‹  íŒë³„ì— ì´ìš©) | . # ì¤€ë¹„ hog = cv2.HOGDescriptor() hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector()) # ë¯¸ë¦¬ í›ˆë ¨ëœ íŠ¹ì§• ë²¡í„°ë¥¼ ê°€ì ¸ì˜¤ëŠ” ê²ƒ hogParams = {'winStride': (8, 8), 'padding': (16, 16), 'scale': 1.05} # ê²€ì¶œ img = cv2.imread(\"img/img_pedestrians.jpg\") gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) # ì´ë¯¸ì§€ë¥¼ í‘ë°±ìœ¼ë¡œ ë³€í™˜: ì»¬ëŸ¬ë‚˜ í‘ë°±ì´ë‚˜ ì‚¬ëŒ ì¸ì‹ì— ì°¨ì´ê°€ ì—†ê¸° ë•Œë¬¸ì—, í‘ë°±ìœ¼ë¡œ ë„£ì–´ì¤Œ human, r = hog.detectMultiScale(gray, **hogParams) # ê²€ì¶œëœ ì‚¬ëŒì˜ ìœ„ì¹˜ ì •ë³´ëŠ” humanì— ì €ì¥ë¨ if len(human) &gt; 0: for (x, y, w, h) in human: color = (255, 255, 255) # BGR color - (255, 255, 255)ëŠ” white pen_w = 3 cv2.rectangle(img, (x, y, w, h), color, thickness = pen_w) # rectangle: ì´ë¯¸ì§€ì— ì‚¬ê°í˜•ì„ ê·¸ë ¤ì£¼ëŠ” í•¨ìˆ˜ cv2.namedWindow(\"img\", cv2.WINDOW_NORMAL) cv2.imshow(\"img\", img) cv2.imwrite(\"img_pedstrians_hogdetect.jpg\", img) # íŒŒì¼ë¡œ ì €ì¥ cv2.waitKey(0) cv2.destroyAllWindows() . | detectMultiScale(): input imageì—ì„œ ë‹¤ì–‘í•œ í¬ê¸°ì˜ ê°ì²´ë¥¼ ì¸ì‹ . | *ì¸ì‹ ëŒ€ìƒì´ ë˜ëŠ” imgë§Œ ì œëŒ€ë¡œ ë„£ì–´ì£¼ë©´, ë‹¤ë¥¸ parameterëŠ” ë„£ì–´ì£¼ì§€ ì•Šì•„ë„ ë¬¸ì œëŠ” ì—†ë‹¤ | winStride: ê°ì²´ ì¸ì‹ì„ ìœ„í•´ ë„¤ëª¨ë‚œ ëª¨ì–‘ì˜ sliding windowê°€ ì´ë¯¸ì§€ ìœ„ë¡œ ì§€ë‚˜ê°€ê²Œ ë˜ëŠ”ë°, ì´ íƒìƒ‰ windowì˜ í¬ê¸°ë¥¼ ê²°ì •í•˜ëŠ” ê²ƒì´ winStride ê°’. | winStrideë¥¼ í¬ê²Œ í•˜ë©´ íƒì§€ê°€ ë¹¨ë¼ì§€ì§€ë§Œ ì •í™•ë„ê°€ ë–¨ì–´ì§€ê³ , ë°˜ëŒ€ë¡œ winStrideë¥¼ ì‘ê²Œ í•˜ë©´ ì •í™•ë„ê°€ ë†’ì•„ì§€ì§€ë§Œ íƒì§€ ì†ë„ê°€ ëŠë ¤ì§„ë‹¤. | (4, 4) ì •ë„ë¡œ ì‹œì‘í•´ì„œ ì¡°ê¸ˆì”© ì˜¬ë¦¬ë©´ì„œ ì •í™•ë„ì™€ ì†ë„ ì‚¬ì´ì˜ ì ì ˆí•œ í¬ì¸íŠ¸ë¥¼ ì°¾ëŠ” ê²ƒì„ ì¶”ì²œ. | . | padding: íƒìƒ‰ windowì— ë”í•´ì§€ëŠ” padding . | (8, 8), (16, 16), (24, 24), (32, 32) ë“±ì˜ ìˆ˜ì¹˜ë¥¼ ë§ì´ ì‚¬ìš©í•œë‹¤ê³  í•¨ | . | scale: ì´ë¯¸ì§€ layerì˜ ìˆ˜ì¶• scale. ë‹¤ì–‘í•œ í¬ê¸°ì˜ ê°ì²´ë¥¼ ì¸ì‹í•˜ê¸° ìœ„í•´, ì´ë¯¸ì§€ í¬ê¸°ë¥¼ ì—¬ëŸ¬ layerë¡œ ì¤„ì—¬ ë‚˜ê°€ë©´ì„œ íƒìƒ‰ì„ ì§„í–‰í•˜ëŠ”ë° (image pyramid), ì´ ë•Œ scale ê°’ì„ ì‘ê²Œ ì„¤ì •í•˜ë©´ ë” ë§ì€ layerë¥¼ íƒìƒ‰í•´ì•¼ í•œë‹¤ . | scale ê°’ì„ ì¤„ì´ë©´ ì •í™•ë„ê°€ ë†’ì•„ì§ˆ ìˆ˜ ìˆì§€ë§Œ ê³„ì‚°í•˜ëŠ” ì‹œê°„ì´ ê¸¸ì–´ì§„ë‹¤. | defaultê°’ì€ 1.05 (5%ì”© ì‚¬ì´ì¦ˆë¥¼ ì¤„ì´ë©´ì„œ íƒìƒ‰í•œë‹¤ëŠ” ëœ») | . | hitThreshold, finalThreshold: optional. ë³´í†µì˜ ê²½ìš° ê·¸ëƒ¥ defaultê°’ìœ¼ë¡œ ë‘ì–´ë„ ê´œì°®ë‹¤ (default: hitThreshold=0, finalThreshold=2) | . | . ì‚¬ëŒ ì–¼êµ´ ê²€ì¶œ . | CascadeClassifierë¥¼ ì‚¬ìš© | . # ì¤€ë¹„ cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_alt2.xml\") # ê²€ì¶œ img = cv2.imread(\"img/img_meeting.jpg\") gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) face_list = cascade.detectMultiScale(gray, minSize=(50, 50)) # ê²€ì¶œí•œ ì–¼êµ´ í‘œì‹œí•˜ê¸° for (x, y, w, h) in face_list: color = (0, 0, 225) # BGR color - (0, 0, 255)ëŠ” red pen_w = 3 cv2.rectangle(img, (x, y, w, h), color, thickness = pen_w) cv2.namedWindow(\"img\", cv2.WINDOW_NORMAL) cv2.imshow(\"img\", img) cv2.imwrite(\"img_meeting_facedetect.jpg\", img) # íŒŒì¼ë¡œ ì €ì¥ cv2.waitKey(0) cv2.destroyAllWindows() . | ì–¼êµ´ 4ê°œ ê²€ì¶œ. (ëŒ€ì²´ë¡œ ì •ë©´ ì–¼êµ´ì— ê°€ê¹Œì›Œì•¼ ê²€ì¶œë¨) | haarcascade ì¸ì‹ ëª¨ë¸ ì¢…ë¥˜ í™•ì¸: GitHub_opencv . | frontalface_alt, frontalface_alt2, frontalface_alt_tree ë“± ì‚¬ìš© ê°€ëŠ¥ | . | . ì‚¬ëŒ ì–¼êµ´ ë°©í–¥ í™•ì¸ . | dlib ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì´ìš©: ì–¼êµ´ì„ ëˆˆ, ì½”, ì…, ì–¼êµ´ë¼ì¸ ìœ¤ê³½ì˜ 68ê°œì˜ íŠ¹ì§•ì ìœ¼ë¡œ í‘œí˜„ â†’ ì–¼êµ´ ë°©í–¥ ë“± ì„¸ì„¸í•œ ì •ë³´ë¥¼ ê²€ì¶œ ê°€ëŠ¥ | dlibì€ ì„¤ì¹˜í•´ì•¼ ì‚¬ìš© ê°€ëŠ¥: conda install -c conda-forge dlib | . import cv2 import dlib import math # ì¤€ë¹„ # predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\") # 68ê°œì˜ ì–¼êµ´ landmarkë¥¼ í‘œì‹œí•´ì£¼ëŠ” ëª¨ë¸ # https://github.com/davisking/dlib-modelsì—ì„œ ë‹¤ìš´ë°›ì•„ì„œ ê²½ë¡œì— ì €ì¥ detector = dlib.get_frontal_face_detector() # ì •ë©´ ì–¼êµ´ ê²€ì¶œ ëª¨ë¸ # ê²€ì¶œ # img = cv2.imread(\"img/img_couple_.jpg\") dets = detector(img, 1) for k, d in enumerate(dets): shape = predictor(img, d) # ì–¼êµ´ ì˜ì—­ í‘œì‹œ color_f = (0, 0, 225) # ì •ë©´ ì–¼êµ´ ê²€ì¶œ: red color_l_out = (255, 0, 0) # ì–¼êµ´ ìœ¤ê³½ í‘œì‹œ: blue color_l_in = (0, 255, 0) # ëˆˆì½”ì ìœ¤ê³½ í‘œì‹œ: green line_w = 3 # êµµê¸° circle_r = 3 # radian(ë°˜ì§€ë¦„) fontType = cv2.FONT_HERSHEY_SIMPLEX fontSize = 1 cv2.rectangle(img, (d.left(), d.top()), (d.right(), d.bottom()), color_f, line_w) # ì¤‘ì‹¬ì„ ê³„ì‚°í•  ì‚¬ê°í˜• ì¤€ë¹„ num_of_points_out = 17 num_of_points_in = shape.num_parts - num_of_points_out gx_out = 0 gy_out = 0 gx_in = 0 gy_in = 0 for shape_point_count in range(shape.num_parts): shape_point = shape.part(shape_point_count) # ì–¼êµ´ ëœë“œë§ˆí¬ë§ˆë‹¤ ê·¸ë¦¬ê¸° if shape_point_count &lt; num_of_points_out: cv2.circle(img, (shape_point.x, shape_point.y), circle_r, color_l_out, line_w) gx_out = gx_out + shape_point.x / num_of_points_out gy_out = gy_out + shape_point.y / num_of_points_out else: cv2.circle(img, (shape_point.x, shape_point.y), circle_r, color_l_in, line_w) gx_in = gx_in + shape_point.x / num_of_points_in gy_in = gy_in + shape_point.y / num_of_points_in # ì¤‘ì‹¬ ìœ„ì¹˜ í‘œì‹œ cv2.circle(img, (int(gx_out), int(gy_out)), circle_r, (0,0,255), line_w) # ì–¼êµ´ ìœ¤ê³½(blueë¡œ í‘œí˜„)ì˜ ì¤‘ì‹¬ì ì„ ì¡ì•„ì„œ red ì ìœ¼ë¡œ í‘œí˜„ cv2.circle(img, (int(gx_in), int(gy_in)), circle_r, (0,0,0), line_w) # ëˆˆì½”ì ìœ¤ê³½(greenë¡œ í‘œí˜„)ì˜ ì¤‘ì‹¬ì ì„ ì¡ì•„ì„œ black ì ìœ¼ë¡œ í‘œí˜„ # ì–¼êµ´ ë°©í–¥ ê³„ì‚° theta = math.asin(2 * (gx_in - gx_out) / (d.right() - d.left())) radian = theta * 180 / math.pi print(f\"ì–¼êµ´ ë°©í–¥: {theta} (ê°ë„: {radian}ë„)\") # ì–¼êµ´ ë°©í–¥ í‘œì‹œ (ì‚¬ì§„ ìœ„ì— ì ê¸°) if radian &lt; 0: textPrefix = \" left \" else: textPrefix = \" right \" textShow = str(k) + textPrefix + str(round(abs(radian),1)) + \" deg.\" cv2.putText(img, textShow, (d.left(), d.top()), fontType, fontSize, color_f, line_w) cv2.namedWindow(\"img\",cv2.WINDOW_NORMAL) cv2.imshow(\"img\",img) cv2.imwrite(\"09_data/temp3.jpg\",img) cv2.waitKey(0) cv2.destroyAllWindows() . ì–¼êµ´ ë°©í–¥: 0.06456096931747406 (ê°ë„: 3.6990710631648662ë„) . | cv2ë¡œ rectangle, circle ê·¸ë¦¬ê¸°: ì°¸ê³  | . ",
    "url": "https://chaelist.github.io/docs/ml_application/image_processing/#%EC%9D%B4%EB%AF%B8%EC%A7%80-%EC%86%8D-%EC%82%AC%EB%9E%8C-%EC%9D%B8%EC%8B%9D",
    "relUrl": "/docs/ml_application/image_processing/#ì´ë¯¸ì§€-ì†-ì‚¬ëŒ-ì¸ì‹"
  },"78": {
    "doc": "ì´ë¯¸ì§€ / ë™ì˜ìƒ ì²˜ë¦¬",
    "title": "ì‚¬ëŒ ì¸ì‹: ì˜ìƒ ì˜ˆì‹œ",
    "content": "íƒ€ì„ë©ìŠ¤ ë§Œë“¤ê¸° . | íƒ€ì„ë©ìŠ¤: â€œë¹ ë¥´ê²Œ ì¬ìƒí•˜ëŠ” ê²ƒâ€. ì¼ì • ì‹œê°„ì˜ í”„ë ˆì„ ì¤‘ 1 í”„ë ˆì„ì”©ë§Œ êº¼ë‚´, ê°„ë‹¨í•˜ê²Œ ê²½í–¥ì„ íŒŒì•… | íƒ€ì„ë©ìŠ¤ë¡œ ì¼ë¶€ í”„ë ˆì„ë§Œì„ êº¼ë‚´ ì‚¬ëŒì„ ì¸ì‹í•´, ì‚¬ëŒì˜ ìˆ˜ ì¦ê°ì„ ë¹ ë¥´ê²Œ íŒŒì•…í•  ìˆ˜ ìˆê²Œ í•¨ | . import cv2 print(\"íƒ€ì„ë©ìŠ¤ ìƒì„± ì‹œì‘\") # ë™ì˜ìƒ ì½ì–´ì˜¤ê¸° cap = cv2.VideoCapture(\"vid/vid01.avi\") width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)) height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)) # HOGë¡œ ì‹œê°„ë³„ ì‚¬ëŒ ìˆ˜ ê²€ì¶œ hog = cv2.HOGDescriptor() hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector()) hogParams = {'winStride': (8, 8), 'padding': (32, 32), 'scale': 1.05, 'hitThreshold': 0, 'finalThreshold': 5} # íƒ€ì„ë©ìŠ¤ ì‘ì„± movie_name = \"timelapse.avi\" fourcc = cv2.VideoWriter_fourcc(*'DIVX') # Codec ì •ë³´. OSë§ˆë‹¤ ì§€ì›ë˜ëŠ” Codecì´ ë‹¤ë¥¸ë°, WindowsëŠ” DIVX video = cv2.VideoWriter(movie_name, fourcc, 30, (width, height)) # 30: ì´ˆë‹¹ ì €ì¥ë  frame num = 0 while(cap.isOpened()): ret, frame = cap.read() if ret: if (num % 10 == 0): # FPS=30ì´ì—ˆìœ¼ë‹ˆê¹Œ, 1ì´ˆì— 3ê°œì˜ í”„ë ˆì„ë§Œ ì‚¬ìš©í•˜ëŠ” ê²ƒ # ì•½ 0.333333ì´ˆì— í•œ ë²ˆì”© í”„ë ˆì„ì„ êº¼ë‚´ ì‚¬ëŒ ì¸ì‹ í›„ videoì— ì €ì¥ gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) human, r = hog.detectMultiScale(gray, **hogParams) if (len(human) &gt; 0): for (x, y, w, h) in human: cv2.rectangle(frame, (x, y, w, h), (255,255,255), 3) video.write(frame) cv2.imshow(\"frame\", frame) cv2.waitKey(1) else: break num = num + 1 video.release() cap.release() cv2.destroyAllWindows() print(\"íƒ€ì„ë©ìŠ¤ ìƒì„± ì™„ë£Œ\") . | cv2.VideoWriterë¡œ ì˜ìƒì„ ì €ì¥: ì°¸ê³  . | VideoWriter_fourcc í•¨ìˆ˜ë¡œ ë™ì˜ìƒ ë°ì´í„° í¬ë§· ì§€ì • (ë„¤ ê°œì˜ ë™ì˜ìƒ ë°ì´í„° í¬ë§·ì„ ì§€ì •í•˜ëŠ” ê±°ë¼ FourCC) | cv2.VideoWriter(output_file_path, fourcc, frame, size)ë¡œ íŒŒì¼ì„ ìƒì„±í•œ ë‹¤ìŒ, | video.write(frame)ìœ¼ë¡œ ì €ì¥í•˜ê³  ì‹¶ì€ í”„ë ˆì„ì„ ì €ì¥ | ë§ˆì§€ë§‰ìœ¼ë¡œ video.release()ë¥¼ ì‹¤í–‰í•˜ë©´ ë™ì˜ìƒ ìƒì„± ì™„ë£Œ | . | . ê±°ë¦¬ì˜ ì‚¬ëŒ ìˆ˜ ë³€í™” ì¸¡ì • . | vid01ì— ëŒ€í•´ ì‚¬ëŒ ìˆ˜ ë³€í™”ë¥¼ ê³„ì‚° . import pandas as pd # í•¨ìˆ˜ë¡œ ìƒì„±í•´ë‘ê¸° def video_hog_detect_people(video_file): print(\"ë¶„ì„ ì‹œì‘\") # ë™ì˜ìƒ ì½ì–´ì˜¤ê¸° cap = cv2.VideoCapture(video_file) fps = cap.get(cv2.CAP_PROP_FPS) # HOGë¡œ ì‹œê°„ë³„ ì‚¬ëŒ ìˆ˜ ê²€ì¶œ hog = cv2.HOGDescriptor() hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector()) hogParams = {'winStride': (8, 8), 'padding': (32, 32), 'scale': 1.05, 'hitThreshold':0, 'finalThreshold':5} # 1ì´ˆì— 3ê°œ í”„ë ˆì„ì”© (ì•½ 0.33333ì´ˆì— í•œë²ˆì”©) êº¼ë‚´ì„œ ì‚¬ëŒì„ ì¸ì‹, dfì— ì‚¬ëŒ ìˆ˜ë¥¼ ì €ì¥ df = pd.DataFrame(columns=['time(sec.)', 'num_of_people']) num = 0 while(cap.isOpened()): ret, frame = cap.read() if ret: if (num % 10 == 0): gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) human, r = hog.detectMultiScale(gray, **hogParams) if (len(human) &gt; 0): for (x, y, w, h) in human: cv2.rectangle(frame, (x, y, w, h), (255,255,255), 3) tmp_se = pd.Series([num / fps, len(human)], index=df.columns) # num / fpsë¡œ ê²½ê³¼ ì‹œê°„ ê³„ì‚° (fps: 1ì´ˆë‹¹ í”„ë ˆì„ ìˆ˜) df = df.append(tmp_se, ignore_index=True) if cv2.waitKey(1) &amp; 0xFF == ord('q'): break else: break num = num + 1 cap.release() cv2.destroyAllWindows() print(\"ë¶„ì„ ì¢…ë£Œ\") return df # vid01ì— ëŒ€í•´ ë¶„ì„ ì§„í–‰ list_df1 = video_hog_detect_people(\"vid/vid01.avi\") ## ë°ì´í„° ì¶œì²˜: https://github.com/wikibook/pyda100 list_df1.head() . ë¶„ì„ ì‹œì‘ ë¶„ì„ ì¢…ë£Œ . | Â  | time(sec.) | num_of_people | . | 0 | 0 | 5 | . | 1 | 0.333333 | 11 | . | 2 | 0.666667 | 11 | . | 3 | 1 | 7 | . | 4 | 1.33333 | 9 | . â†’ ì‹œê°í™”í•´ì„œ ì¶”ì´ í™•ì¸ . import matplotlib.pyplot as plt import seaborn as sns sns.lineplot(data=list_df1, x='time(sec.)', y='num_of_people') plt.ylim(0, 15); . | vid02ì— ëŒ€í•´ ì‚¬ëŒ ìˆ˜ ë³€í™”ë¥¼ ê³„ì‚° . list_df2 = video_hog_detect_people(\"vid/vid02.avi\") ## ë°ì´í„° ì¶œì²˜: https://github.com/wikibook/pyda100 list_df2.head() . ë¶„ì„ ì‹œì‘ ë¶„ì„ ì¢…ë£Œ . | Â  | time(sec.) | num_of_people | . | 0 | 0 | 13 | . | 1 | 0.333333 | 14 | . | 2 | 0.666667 | 10 | . | 3 | 1 | 8 | . | 4 | 1.33333 | 10 | . â†’ ì‹œê°í™”í•´ì„œ ì¶”ì´ í™•ì¸ . import matplotlib.pyplot as plt import seaborn as sns sns.lineplot(data=list_df1, x='time(sec.)', y='num_of_people') plt.ylim(0, 15); . | . ì´ë™ í‰ê·  ê³„ì‚° í›„ ë¹„êµ . | HOGë¡œ ë¶„ì„í•œ ë°ì´í„°ì—ëŠ” ì˜¤ê²€ì¶œëœ ê°’ë„ ë“¤ì–´ê°€ ìˆê¸° ë•Œë¬¸ì—, ì˜¤ê²€ì¶œì˜ ì˜í–¥ì„ ì¤„ì—¬ì„œ ë¹„êµí•˜ëŠ” ê²ƒì´ ì¢‹ë‹¤ | ë…¸ì´ì¦ˆë¡œ ì¸í•œ fluctuationì„ ì œê±°í•˜ê³  ë³´ë‹¤ ê³µì •í•˜ê²Œ ë¹„êµí•˜ê¸° ìœ„í•´, ì´ë™í‰ê· (moving average)ì„ ê³„ì‚° | . | vid01ì— ëŒ€í•´ ì´ë™ í‰ê· ì„ ê³„ì‚° . import numpy as np # í•¨ìˆ˜ë¡œ ìƒì„±í•´ë‘ê¸° def moving_average(x, y): y_conv = np.convolve(y, np.ones(5) / float(5), mode='valid') # ë°ì´í„°ë¥¼ ìˆœì„œëŒ€ë¡œ 5ê°œì”© ì¡ì•„ì„œ í‰ê· ë‚´ì¤Œ x_dat = np.linspace(np.min(x), np.max(x), np.size(y_conv)) # y_convì˜ ìš”ì†Œ ê°œìˆ˜ë§Œí¼ x ìµœì†Ÿê°’ ~ ìµœëŒ“ê°’ ì‚¬ì´ì˜ ìˆ«ì array ë§Œë“¤ì–´ì¤Œ return x_dat, y_conv . | np.convolve(array1, array2, mode=â€™validâ€™): array2ê°€ array1 ìœ„ë¥¼ ì§€ë‚˜ê°€ë©´ì„œ ê³±í•œ ê²°ê³¼ë¥¼ ì­‰ arrayë¡œ ë§Œë“¤ì–´ì¤Œ. | ex) np.convolve([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], [0.2, 0.2, 0.2, 0.2, 0.2], mode='valid')ì´ë©´ ê²°ê³¼ë¡œ [3, 4, 5, 6, 7, 8]ì˜ arrayê°€ ë§Œë“¤ì–´ì§. (array1ì´ ì›ì†Œ 10ê°œ, array2ê°€ ì›ì†Œ 5ê°œ â†’ ê²°ê³¼ arrayëŠ” ì›ì†Œ 6ê°œ (len(array1) - len(array2) + 1) . | 1*0.2 + 2*0.2 + 3*0.2 + 4*0.2 + 5*0.2 = 3 ì´ëŸ° ì‹â€¦. (â†’ ì•ì—ì„œë¶€í„° 5ê°œì”© ë¬¶ì–´ì„œ í‰ê· ë‚¸ ê²ƒê³¼ ë™ì¼) | . | np.ones(5) / float(5)= [0.2, 0.2, 0.2, 0.2, 0.2]ì´ê¸°ì—, np.convolve(y, np.ones(5) / float(5), mode='valid')ë„ ì²«ë²ˆì§¸-5ë²ˆì§¸ í‰ê· ë‚¸ ê°’, ë‘ë²ˆì§¸-6ë²ˆì§¸ í‰ê· ë‚¸ ê°’, â€¦ ì´ëŸ°ì‹ìœ¼ë¡œ ê°’ì´ ë‚˜ì—´ëœ arrayê°€ ë°˜í™˜ëœë‹¤. | . | np.linspace(a, b, c): aë¶€í„° bê¹Œì§€ cê°œì˜ ìš”ì†Œë¥¼ ê°–ëŠ” ë°°ì—´ì„ ë§Œë“¤ë¼ëŠ” ëœ» (defaultë¡œ endpoint=Trueì´ê¸°ì— bë„ í¬í•¨) . | ex) np.linspace(0, 10, 11)ì´ë¼ê³  í•˜ë©´ [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]ì˜ arrayê°€ ë§Œë“¤ì–´ì§ | . | . plt.plot(list(list_df1[\"time(sec.)\"]), list(list_df1[\"num_of_people\"]), label=\"raw\") ma_x, ma_y = moving_average(list_df1[\"time(sec.)\"], list_df1[\"num_of_people\"]) # list_dt['num_of_people']ì˜ ì›ì†Œê°€ 41ê°œì˜€ìœ¼ë‹ˆ, ma_yì˜ ì›ì†ŒëŠ” 37ê°œ plt.plot(ma_x, ma_y, label=\"average\") plt.xlabel('time(sec.)') plt.ylabel('num_of_people') plt.ylim(0, 15) plt.legend() plt.show(); . | vid02ì— ëŒ€í•´ ì´ë™ í‰ê· ì„ ê³„ì‚° . plt.plot(list(list_df2[\"time(sec.)\"]), list(list_df2[\"num_of_people\"]), label=\"raw\") ma_x2, ma_y2 = moving_average(list_df2[\"time(sec.)\"], list_df2[\"num_of_people\"]) plt.plot(ma_x2, ma_y2, label=\"average\") plt.xlabel('time(sec.)') plt.ylabel('num_of_people') plt.ylim(0, 15) plt.legend() plt.show(); . | viod01, vid02 ë¹„êµ . plt.plot(ma_x, ma_y, label=\"1st\") plt.plot(ma_x2, ma_y2, label=\"2nd\") plt.xlabel('time(sec.)') plt.ylabel('num_of_people') plt.ylim(0, 15) plt.legend() plt.show(); . | vid01ë³´ë‹¤ vid02ê°€ ì „ë°˜ì ìœ¼ë¡œ ì‚¬ëŒ ìˆ˜ê°€ ë§ì´ ì°í˜€ ìˆë‹¤ê³  íŒë‹¨ë¨ | í•´ë‹¹ ê±°ë¦¬ì—ì„œ ê°€ê²Œë¥¼ í•œë‹¤ë©´, vid02ì˜ ì‹œê°„ëŒ€ì— ì¡°ê¸ˆ ë” ë§ì€ ì ì¬ ê³ ê°ì´ ìˆë‹¤ê³  íŒë‹¨ ê°€ëŠ¥ | . | . ",
    "url": "https://chaelist.github.io/docs/ml_application/image_processing/#%EC%82%AC%EB%9E%8C-%EC%9D%B8%EC%8B%9D-%EC%98%81%EC%83%81-%EC%98%88%EC%8B%9C",
    "relUrl": "/docs/ml_application/image_processing/#ì‚¬ëŒ-ì¸ì‹-ì˜ìƒ-ì˜ˆì‹œ"
  },"79": {
    "doc": "Home",
    "title": "Chaelist",
    "content": "Data Analytics Blog . Email LinkedIn . UK Ecommerce Dataset (EDA) . YouTube Trending Videos Dataset (EDA) . Amazon Bestselling Books Dataset (EDA) . Telco Customer Churn Dataset (EDA) . Kiva Crowdfunding Dataset (EDA) . STEM Salaries Dataset (EDA) . . Frequency Analysis (ë‹¨ì–´ ë¹ˆë„ ë¶„ì„) . ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ìˆ˜ì§‘í•´ ì „ì²˜ë¦¬ë¥¼ ê±°ì³ ë‹¨ì–´ë³„ ë¹ˆë„ë¥¼ íŒŒì•… &amp; ê¸°ì‚¬ì˜ ë‚´ìš©ì„ ì›Œë“œí´ë¼ìš°ë“œë¡œ ì••ì¶•ì ìœ¼ë¡œ í‘œí˜„ . Harry Potter Network Analysis (ì¸ë¬¼ ë„¤íŠ¸ì›Œí¬ ë¶„ì„) . ì†Œì„¤ &lt;Harry Potter&gt; ì‹œë¦¬ì¦ˆ ì† ì¸ë¬¼ë“¤ ê°„ ì—°ê²° ê´€ê³„ ë° ê¶Œë³„ ì¸ë¬¼ì˜ ì¤‘ìš”ë„ ë³€í™”ë¥¼ ë¶„ì„ . Â  . Sementic Network Analysis (ì–¸ì–´ ë„¤íŠ¸ì›Œí¬ ë¶„ì„) . ë‰´ìŠ¤ ê¸°ì‚¬ ì† ì£¼ìš” ë‹¨ì–´ë“¤ ê°„ì˜ ì—°ê²° ê´€ê³„ë¥¼ íŒŒì•…í•´, ê¸°ì‚¬ì˜ í•µì‹¬ ë‚´ìš©ì„ ìœ ì¶” . Movie Review Sentiment Analysis (ì˜í™” ë¦¬ë·° ê°ì„± ë¶„ì„) . ì˜í™” 100ê°œì˜ í‰ì -ë¦¬ë·° ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•´, ë¦¬ë·°ì˜ ê°ì„±(ê¸ì •/ë¶€ì •)ì„ ì˜ˆì¸¡í•˜ëŠ” ëª¨ë¸ì„ êµ¬ì¶• . News Clustering (ë‰´ìŠ¤ ê¸°ì‚¬ êµ°ì§‘í™”) . ì„œë¡œ ë‹¤ë¥¸ í† í”½ì˜ ë‰´ìŠ¤ ê¸°ì‚¬ë“¤ì„ Clusteringì„ í†µí•´ ìœ ì‚¬í•œ ê¸°ì‚¬ë“¤ë¼ë¦¬ ë¬¶ì–´ì¤Œ . Time Series Data Forecasting (ì‹œê³„ì—´ ë°ì´í„° ì˜ˆì¸¡) . Facebookì˜ Prophet ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í™œìš©í•´ ì‹œê³„ì—´ ë°ì´í„°ë¥¼ ì˜ˆì¸¡ &amp; íŒŒë¼ë¯¸í„° íŠœë‹ . Â  . Topic Modeling (í† í”½ëª¨ë¸ë§) . Google Play Storeì˜ â€˜Netflixâ€™ ì•± ë¦¬ë·° ë°ì´í„°ë¥¼ í™œìš©í•´ ì ì¬ ë””ë¦¬í´ë ˆ í• ë‹¹(Latent Dirichlet Allocation, LDA) ê¸°ë²•ìœ¼ë¡œ í† í”½ ëª¨ë¸ë§ êµ¬í˜„ . Logistics Optimization (ë¬¼ë¥˜ ìµœì í™”) . PuLP, ortoolpy ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í™œìš©í•´ ìµœì í™”ëœ ìš´ì†¡ ê²½ë¡œ &amp; ìƒì‚° ê³„íšì„ ê³„ì‚° . Â  . Image Processing (ì´ë¯¸ì§€ ë°ì´í„° ì²˜ë¦¬) . OpenCV ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í™œìš©í•´ ì´ë¯¸ì§€ / ë™ì˜ìƒ ë°ì´í„°ë¥¼ ì²˜ë¦¬ &amp; ì´ë¯¸ì§€ ì† ì‚¬ëŒ ê²€ì¶œ . Â  . ",
    "url": "https://chaelist.github.io/#chaelist",
    "relUrl": "/#chaelist"
  },"80": {
    "doc": "Home",
    "title": "Home",
    "content": " ",
    "url": "https://chaelist.github.io/",
    "relUrl": "/"
  },"81": {
    "doc": "ì¡°ì¸, ì„œë¸Œì¿¼ë¦¬, ë·°",
    "title": "ì¡°ì¸, ì„œë¸Œì¿¼ë¦¬, ë·°",
    "content": ". | í…Œì´ë¸” í•©ì¹˜ê¸° . | ê²°í•©ì—°ì‚° (JOIN) | ê¸°íƒ€ ë‹¤ì–‘í•œ JOINë“¤ | ì§‘í•©ì—°ì‚° (UNION ë“±) | . | ì„œë¸Œì¿¼ë¦¬ (SubQuery) . | ë¹„ìƒê´€ ì„œë¸Œì¿¼ë¦¬(Non-correlated Subquery) | ìƒê´€ ì„œë¸Œì¿¼ë¦¬(Correlated Subquery) | . | ë·° (View) | . ",
    "url": "https://chaelist.github.io/docs/sql/join_subq_view/",
    "relUrl": "/docs/sql/join_subq_view/"
  },"82": {
    "doc": "ì¡°ì¸, ì„œë¸Œì¿¼ë¦¬, ë·°",
    "title": "í…Œì´ë¸” í•©ì¹˜ê¸°",
    "content": "ê²°í•©ì—°ì‚° (JOIN) . : í…Œì´ë¸”ì„ ê°€ë¡œ ë°©í–¥ìœ¼ë¡œ ë¶™ì´ëŠ” ì—°ì‚° . | LEFT OUTER JOIN: ì™¼ìª½ í…Œì´ë¸”ì„ ê¸°ì¤€ìœ¼ë¡œ í•©ì³ì§ (ì™¼ìª½ í…Œì´ë¸”ì— ì¡´ì¬í•˜ëŠ” rowë§Œ ë³´ì—¬ì§) . | ex. ë§Œì•½ ì™¼ìª½ í…Œì´ë¸” ê°’(ex. ê½ƒë¬´ëŠ¬ ì›í”¼ìŠ¤)ì™€ ì—°ê²°ë˜ëŠ” ê°’ì´ ì˜¤ë¥¸ìª½ì— 2ê°œ ì´ìƒì´ë¼ë©´(ex. ë¦¬ë·°1, ë¦¬ë·°2) â†’ ì™¼ìª½ í…Œì´ë¸”ì—ëŠ” í•´ë‹¹ ê°’ì´ ì—¬ëŸ¬ ë²ˆ í‘œì‹œë  ìˆ˜ ìˆìŒ | . | RIGHT OUTER JOIN: ì˜¤ë¥¸ìª½ í…Œì´ë¸”ì„ ê¸°ì¤€ìœ¼ë¡œ í•©ì³ì§ (ì˜¤ë¥¸ìª½ í…Œì´ë¸”ì— ì¡´ì¬í•˜ëŠ” rowë§Œ ë³´ì—¬ì§) | INNER JOIN: ì™¼ìª½, ì˜¤ë¥¸ìª½ í…Œì´ë¸” ëª¨ë‘ì— ì¡´ì¬í•˜ëŠ” rowë§Œ ì¶”ë ¤ì„œ í•©ì³ì§ (êµì§‘í•© ê°œë…) | . -- ex) item í…Œì´ë¸”ì„ ê¸°ì¤€ìœ¼ë¡œ, stock í…Œì´ë¸”ì„ join -- item í…Œì´ë¸”ì˜ id ì¹¼ëŸ¼ê³¼ stock ë°ì´ë¸”ì˜ item_id ì¹¼ëŸ¼ì„ keyë¡œ í•´ì„œ join SELECT item.id, item.name, stock.item_id, stock.inventory_count FROM item LEFT OUTER JOIN stock ON item.id = stock.item_id; . +) í…Œì´ë¸”ì— alias ë¶™ì´ê¸° . | JOINí•  ë•ŒëŠ” í…Œì´ë¸”ì— aliasë¥¼ ë¶™ì—¬ì„œ ì‚¬ìš©í•˜ëŠ” ê²½ìš°ê°€ ë§ë‹¤. (ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ê¸° ìœ„í•¨) | â€»ì£¼ì˜: í•œ ë²ˆ í…Œì´ë¸”ì— aliasë¥¼ ë¶™ì˜€ìœ¼ë©´, ë‹¤ë¥¸ ëª¨ë“  ì ˆì—ì„œ ê·¸ í…Œì´ë¸”ì€ ê·¸ aliasë¡œë§Œ í‘œí˜„í•´ì•¼ í•¨. (ì›ë˜ì˜ í…Œì´ë¸” ì´ë¦„ì´ë‘ ì„ì–´ì„œ ë‚˜íƒ€ë‚¼ ìˆ˜ ì—†ìŒ) | . SELECT i.id, i.name, s.item_id, s.inventory_count FROM item AS i LEFT OUTER JOIN stock AS s ON i.id = s.item_id; . +) í…Œì´ë¸” 3ê°œ JOINí•˜ê¸° . SELECT i.name, i.id, r.item_id, r.star, r.comment, r.mem_id, m.id, m.email FROM item AS i LEFT OUTER JOIN review AS r ON r.item_id = i.id LEFT OUTER JOIN member AS m ON r.mem_id = m.id ORDER BY i.name ASC; . ê¸°íƒ€ ë‹¤ì–‘í•œ JOINë“¤ . | NATURAL JOIN . | ë‘ í…Œì´ë¸”ì—ì„œ ê°™ì€ ì´ë¦„ì˜ ì»¬ëŸ¼ì„ ì°¾ì•„ì„œ ìë™ìœ¼ë¡œ ê·¸ê²ƒë“¤ì„ ì¡°ì¸ ì¡°ê±´ìœ¼ë¡œ ì„¤ì •í•˜ê³ , INNER JOINì„ í•´ì¤€ë‹¤ | ë‹¤ë§Œ, NATURAL JOINì„ í•  ìˆ˜ ìˆëŠ” ê²½ìš°ë¼ë„ ì§ê´€ì ìœ¼ë¡œ í•´ì„ ê°€ëŠ¥í•œ SQLë¬¸ì„ ìœ„í•´ì„œ ONìœ¼ë¡œ ì¡°ì¸ ì¡°ê±´ì„ ëª…ì‹œí•´ì£¼ëŠ” ê²ƒì´ ë” ì¢‹ë‹¤ | . SELECT p.id, p.player_name, p.team_name, t.team_name, t.region FROM player AS p INNER JOIN team AS t ON p.team_name = t.team_name; --ìœ„ì™€ ê°™ì€ INNER JOINì„ ì•„ë˜ì™€ ê°™ì´ êµ¬í˜„í•˜ëŠ” ê²ƒë„ ê°€ëŠ¥: SELECT p.id, p.player_name, p.team_name, t.team_name, t.region FROM player AS p NATURAL JOIN team AS t . | CROSS JOIN . | ë‘ í…Œì´ë¸”ì˜ rowë“¤ì˜ ëª¨ë“  ì¡°í•©ì„ ë³´ì—¬ì£¼ëŠ” JOIN (ë‘ í…Œì´ë¸”ì˜ ì¹´ë¥´í…Œì‹œì•ˆ ê³±(Cartesian Product)ì„ êµ¬í•˜ëŠ” ê²ƒ) | . -- ê°€ëŠ¥í•œ ëª¨ë“  shirtsì™€ pantsì˜ ì¡°í•©ì„ ë³´ê³  ì‹¶ì€ ê²½ìš°: SELECT * FROM shirts CROSS JOIN pants . | Self Join . | ìê¸° ìì‹ ê³¼ JOINí•˜ëŠ” ë°©ì‹. (í•œ í…Œì´ë¸”ì„ ìê¸° ìì‹ ê³¼ ë‹¤ì‹œ JOINí•´ì£¼ëŠ” ê²ƒ) | . -- íšŒì‚¬ êµ¬ì„±ì› ì •ë³´ê°€ ë‹´ê¸´ í…Œì´ë¸” â†’ ìê¸° ìì‹ ê³¼ joiní•´ì„œ ê° ì§ì›ì˜ ìƒì‚¬ ì •ë³´ë¥¼ ì°¾ì•„ë³¸ë‹¤ SELECT employee AS e1 LEFT OUTER JOIN employee AS e2 ON e1.boss = e2.id; . | FULL OUTER JOIN . | ë‘ í…Œì´ë¸”ì˜ LEFT OUTER JOIN ê²°ê³¼ì™€ RIGHT OUTER JOIN ê²°ê³¼ë¥¼ í•©ì³ì£¼ëŠ” JOIN. ë‘ ê²°ê³¼ì— ëª¨ë‘ ì¡´ì¬í•˜ëŠ” rowë“¤(ë‘ í…Œì´ë¸”ì— ê³µí†µìœ¼ë¡œ ì¡´ì¬í•˜ë˜ rowë“¤)ì€ í•œë²ˆë§Œ í‘œí˜„í•´ì¤€ë‹¤ (í•©ì§‘í•©ì˜ ê°œë…) | . SELECT * FROM player AS p FULL OUTER JOIN team AS t ON p.team_name = t.team_name; . â€» Oracleì—ì„œëŠ” ìœ„ì™€ ê°™ì´ FULL OUTER JOIN ì—°ì‚°ìê°€ ì§€ì›ë˜ì§€ë§Œ, MySQLì—ì„œëŠ” ì§€ì›ë˜ì§€ ì•ŠëŠ”ë‹¤. â†’ LEFT OUTER JOINê³¼ RIGHT OUTER JOINì„ UNIONí•´ì„œ ê³„ì‚°í•˜ë©´ ë¨! . SELECT * FROM player AS p LEFT OUTER JOIN team AS t ON p.team_name = t.team_name UNION SELECT * FROM player AS p RIGHT OUTER JOIN team AS t ON p.team_name = t.team_name . | Non-Equi Join . | Equi: Equality Condition (ë™ë“± ì¡°ê±´) | Non-Equi Join: ë“±í˜¸(=)ê°€ ì•„ë‹Œ ë‹¤ë¥¸ ì¡°ê±´ì„ ì‚¬ìš©í•˜ëŠ” JOIN. | . -- íŠ¹ì • íšŒì›ì´ ê°€ì…í•œ ë‚ (sign_up_date)ë³´ë‹¤ ì´í›„ì— ì˜¬ë¼ì˜¨ ìƒí’ˆë“¤ì„ í™•ì¸ SELECT * FROM member AS m LEFT OUTER JOIN item AS i ON m.sign_up_date &lt; i.registration_date -- ë¶€ë“±í˜¸(&lt;)ë¡œ ë¹„êµ ORDER BY m.sign_up_date ASC; . | . ì§‘í•©ì—°ì‚° (UNION ë“±) . : í…Œì´ë¸”ì„ ì„¸ë¡œ ë°©í–¥ìœ¼ë¡œ í•©ì¹˜ëŠ” ì—°ì‚° (â€» ê°™ì€ ì¢…ë¥˜ì˜ í…Œì´ë¸”ë¼ë¦¬ë§Œ ê°€ëŠ¥) +) ì„œë¡œ ë‹¤ë¥¸ ì¢…ë¥˜ì˜ í…Œì´ë¸”ì˜ ê²½ìš°, ì¡°íšŒí•˜ëŠ” ì»¬ëŸ¼ì„ ì¼ì¹˜ì‹œí‚¤ë©´ ì§‘í•© ì—°ì‚°ì´ ê°€ëŠ¥í•´ì§„ë‹¤ . | A âˆ© B (INTERSECT): Aì™€ Bì— ëª¨ë‘ ì¡´ì¬í•˜ëŠ” rowë§Œ ì¶œë ¥ SELECT * FROM item -- ê¸°ì¡´ item í…Œì´ë¸” INTERSECT SELECT * FROM item_new -- ìƒˆë¡œ ì—…ë°ì´íŠ¸ëœ item í…Œì´ë¸” . | A - B (MINUS): Aì—ëŠ” ì¡´ì¬í•˜ì§€ë§Œ Bì—ëŠ” ì¡´ì¬í•˜ì§€ ì•ŠëŠ” rowë§Œ ì¶œë ¥ . | MINUS ì—°ì‚°ì ëŒ€ì‹  EXCEPT ì—°ì‚°ìë¥¼ ì‚¬ìš©í•´ë„ ëœë‹¤ | . SELECT * FROM item MINUS SELECT * FROM item_new . | A âˆª B (UNION): Aì™€ Bì— ì¡´ì¬í•˜ëŠ” rowë¥¼ ëª¨ë‘ ì¶œë ¥í•˜ë˜, ê³µí†µìœ¼ë¡œ ì¡´ì¬í•˜ëŠ” rowëŠ” ì¤‘ë³µì„ ì œê±°í•˜ê³  í•˜ë‚˜ë§Œ í‘œì‹œ (í•©ì§‘í•©ì˜ ê°œë…) SELECT * FROM item UNION SELECT * FROM item_new . | UNION ALL: Aì™€ Bì— ì¡´ì¬í•˜ëŠ” rowë¥¼ ëª¨ë‘ ì¶œë ¥ (ì¤‘ë³µì„ ì œê±°í•˜ì§€ ì•Šê³  ê·¸ëƒ¥ í•©ì³ì¤Œ) SELECT * FROM item UNION ALL SELECT * FROM item_new . | . â€» MySQL 8.0ì—ì„œëŠ” UNION, UNION ALL ì—°ì‚°ìë§Œ ì§€ì› (Oracleì—ì„œëŠ” ëª¨ë‘ ì§€ì›) . +) JOINìœ¼ë¡œë„ ì§‘í•©ì—°ì‚°ì„ ìœ ì‚¬í•˜ê²Œ ìˆ˜í–‰ ê°€ëŠ¥ . SELECT old.id AS old_id, old.name AS old_name, new.id AS new_id, new.name AS new_name FROM item AS old LEFT OUTER JOIN item_new AS new ON old.id = new.id WHERE new.id IS NULL; . | ì´ë ‡ê²Œ í•˜ë©´ ì™¼ìª½ í…Œì´ë¸”ì—ëŠ” ìˆì§€ë§Œ ì˜¤ë¥¸ìª½ í…Œì´ë¸”ì—ëŠ” ì—†ëŠ” rowë¥¼ í™•ì¸ ê°€ëŠ¥ (ê¸°ì¡´ item í…Œì´ë¸”ì—” ì •ë³´ê°€ ìˆì—ˆì§€ë§Œ ì˜¤ë¥¸ìª½ ìƒˆ í…Œì´ë¸”ì—ì„  ëˆ„ë½ëœ ê²ƒë“¤ í™•ì¸) | +) ìœ„ ì˜ˆì‹œì²˜ëŸ¼ JOINí•  ë•Œì˜ keyê°€ ë˜ëŠ” ë‘ í…Œì´ë¸” ë‚´ ì¹¼ëŸ¼ì˜ ì´ë¦„ì´ ê°™ìœ¼ë©´ ON ëŒ€ì‹  USINGì„ ì¨ë„ ëœë‹¤! (ON old.id = new.id ëŒ€ì‹  USING(id)ë¼ê³  ì‚¬ìš© ê°€ëŠ¥) | . ",
    "url": "https://chaelist.github.io/docs/sql/join_subq_view/#%ED%85%8C%EC%9D%B4%EB%B8%94-%ED%95%A9%EC%B9%98%EA%B8%B0",
    "relUrl": "/docs/sql/join_subq_view/#í…Œì´ë¸”-í•©ì¹˜ê¸°"
  },"83": {
    "doc": "ì¡°ì¸, ì„œë¸Œì¿¼ë¦¬, ë·°",
    "title": "ì„œë¸Œì¿¼ë¦¬ (SubQuery)",
    "content": ": SQLë¬¸ ì•ˆì— ë¶€í’ˆì²˜ëŸ¼ ë“¤ì–´ê°€ëŠ” SELECTë¬¸ (ì „ì²´ SQLë¬¸ ì•ˆì— ìˆëŠ” ë˜ ë‹¤ë¥¸ SQLë¬¸ì˜ ê°œë…) . | â€» ì„œë¸Œì¿¼ë¦¬ëŠ” ê¼­ ( ) ì•ˆì— ì¨ì¤˜ì•¼ í•¨! | . ë¹„ìƒê´€ ì„œë¸Œì¿¼ë¦¬(Non-correlated Subquery) . : ì„œë¸Œì¿¼ë¦¬ì˜ SQLì ˆë§Œ ë…ë¦½ì ìœ¼ë¡œ ì¨ë„ ì‹¤í–‰ë˜ëŠ” ì¿¼ë¦¬. | ë‹¨ì¼ê°’ì„ ë°˜í™˜í•˜ëŠ” ì„œë¸Œì¿¼ë¦¬ (ìŠ¤ì¹¼ë¼ ì„œë¸Œì¿¼ë¦¬) . | ex1) SELECTì ˆì—ì„œ ì‚¬ìš©: . -- ê° ìƒí’ˆì˜ priceì™€ ì‰½ê²Œ ë¹„êµí•  ìˆ˜ ìˆë„ë¡, ë³„ë„ë¡œ AVG(price) ê°’ì„ ê°€ì ¸ì™€ì„œ ì¶œë ¥ SELECT id, name, price, (SELECT AVG(price) FROM item) AS avg_price FROM item; . | ex2) WHEREì ˆì—ì„œ ì‚¬ìš©: . -- ê°€ì¥ ë†’ì€ ê°€ê²©ì„ ê°€ì§„ ìƒí’ˆì„ ì¶œë ¥ SELECT id, name, price FROM item WHERE price = (SELECT MAX(price) FROM item); . | ex3) HAVINGì ˆì—ì„œ ì‚¬ìš©: . -- ì „ì²´ star í‰ê· ë³´ë‹¤ ì‘ì€ avg_starë¥¼ ê°–ëŠ” groupë§Œ ì¶œë ¥ SELECT i.id, i.name, AVG(star) AS avg_star FROM item as i LEFT OUTER JOIN review AS r ON r.item_id = i.id GROUP BY i.id, i.name HAVING avg_star &lt; (SELECT AVG(star) FROM review) . | . | ë¦¬ìŠ¤íŠ¸ê°€ ê²°ê³¼ë¡œ ë‚˜ì˜¤ëŠ” ì„œë¸Œì¿¼ë¦¬ (í•˜ë‚˜ì˜ column &amp; ì—¬ëŸ¬ rowì˜ í˜•íƒœê°€ ë°˜í™˜ë˜ëŠ” ì„œë¸Œì¿¼ë¦¬) . | ë¦¬ìŠ¤íŠ¸ í˜•íƒœì˜ ê²°ê³¼ê°€ ë‚˜ì˜¤ëŠ” ì„œë¸Œì¿¼ë¦¬ëŠ” IN, ANY, ALL ë“±ê³¼ í•¨ê»˜ ì‚¬ìš© ê°€ëŠ¥ | . -- ë¦¬ë·° ìˆ˜ê°€ 3ê°œ ì´ìƒì¸ itemì˜ ì •ë³´ë§Œ ì¶œë ¥ SELECT * FROM item WHERE id IN (SELECT item_id FROM review GROUP BY item_id HAVING COUNT(*) &gt;= 3); . | í…Œì´ë¸”ì´ ê²°ê³¼ë¡œ ë‚˜ì˜¤ëŠ” ì„œë¸Œì¿¼ë¦¬ . | â€» ì´ëŸ° ì‹ìœ¼ë¡œ ì„œë¸Œì¿¼ë¦¬ë¡œ ê°€ì ¸ì˜¨ â€˜derived tableâ€˜ì„ ì‚¬ìš©í•  ë•ŒëŠ”, ë°˜ë“œì‹œ aliasë¥¼ ë¶™ì—¬ì¤˜ì•¼ í•œë‹¤! | . -- ì§€ì—­ë³„ ë¦¬ë·° ìˆ˜ë¥¼ countí•œ í›„, í‰ê· ê°’, ìµœëŒ“ê°’, ìµœì†Ÿê°’ì„ ì¶œë ¥ SELECT AVG(review_count), MAX(review_count), MIN(review_count) FROM (SELECT SUBSTRING(address, 1, 2) AS region, COUNT(*) AS review_count FROM review as r LEFT OUTER JOIN member AS m ON r.mem_id = m.id GROUP BY SUBSTRING(address, 1, 2) HAVING region IS NOT NULL) AS review_count_summary; . | . ìƒê´€ ì„œë¸Œì¿¼ë¦¬(Correlated Subquery) . : ë…ë¦½ì ìœ¼ë¡œëŠ” ì‹¤í–‰ë˜ì§€ ì•Šê³ , ë°”ê¹¥ SQLì ˆê³¼ ê´€ê³„ë¥¼ ë§ºê³  ìˆëŠ” ì„œë¸Œì¿¼ë¦¬. | EXISTSì™€ í•¨ê»˜ ì‚¬ìš© . -- ë¦¬ë·°ê°€ ë‹¬ë¦° ìƒí’ˆë“¤ë§Œ ì¡°íšŒ SELECT * FROM item WHERE EXISTS (SELECT * FROM review WHERE review.item_id = item.id); . | í•´ì„: (1) ì¼ë‹¨ item í…Œì´ë¸”ì˜ ì²« ë²ˆì§¸ rowë¥¼ ìƒê° (2) ê·¸ rowì˜ id(item.id) ê°’ê³¼ ê°™ì€ ê°’ì„ item_id(review.item_id) ì»¬ëŸ¼ì— ê°€ì§„ review í…Œì´ë¸”ì˜ rowê°€ ìˆëŠ”ì§€ ì¡°íšŒ (3) ë§Œì•½ì— ì¡´ì¬í•˜ë©´(EXISTS) (4) WHERE ì ˆì€ Trueê°€ ë˜ê³ , (1)ì—ì„œ ìƒê°í–ˆë˜ item í…Œì´ë¸”ì˜ rowëŠ” ì¡°íšŒ ëŒ€ìƒì´ ëœë‹¤ (5) item í…Œì´ë¸”ì˜ ëª¨ë“  rowì— ëŒ€í•´ ìˆœì°¨ì ìœ¼ë¡œ (2)~(4)ë¥¼ ë°˜ë³µí•œ í›„ ëŒ€ìƒ rowë“¤ë§Œ ì¶œë ¥í•´ì¤€ë‹¤ | . | NOT EXISTS ì‚¬ìš© (EXISTSì˜ ë°˜ëŒ€ ê²½ìš°) . -- ë¦¬ë·°ê°€ ë‹¬ë¦¬ì§€ ì•Šì€ ìƒí’ˆë“¤ë§Œ ì¡°íšŒ SELECT * FROM item WHERE NOT EXISTS (SELECT * FROM review WHERE review.item_id = item.id); . | EXISTS, NOT EXISTS ì—†ëŠ” ìƒê´€ ì„œë¸Œì¿¼ë¦¬ . -- member í…Œì´ë¸”ì„ ì¡°íšŒí•˜ë©´ì„œ, íŠ¹ì • íšŒì›ê³¼ ê°™ì€ í•´ì— íƒœì–´ë‚œ íšŒì›ë“¤ ì¤‘ ê°€ì¥ ì‘ì€ í‚¤ë¥¼ ê°€ì§„ íšŒì›ì˜ í‚¤ ì •ë³´ë¥¼ ë‹´ì€ ì»¬ëŸ¼ì„ ì˜¤ë¥¸ìª½ ëì— ì¶”ê°€ SELECT *, (SELECT MIN(height) FROM member AS m2 WHERE birthday IS NOT NULL AND height IS NOT NULL AND YEAR(m1.birthday) = YEAR(m2.birthday)) AS min_height_in_the_year FROM member AS m1 ORDER BY min_height_in_the_year ASC; . | *í•´ì„: (1) ì¼ë‹¨ birthday ì»¬ëŸ¼ê³¼ height ì»¬ëŸ¼ì— ë‘˜ë‹¤ ê°’ì´ ìˆëŠ” íšŒì›ë“¤ë§Œ ëŒ€ìƒìœ¼ë¡œ í•´ì•¼í•˜ê¸° ë•Œë¬¸ì— â€˜birthday IS NOT NULL AND height IS NOT NULLâ€™ ì¡°ê±´ì„ ê±¸ì–´ë‘  (2) member í…Œì´ë¸”ì˜ ì²« ë²ˆì§¸ rowë¥¼ ìƒê° (3) ê·¸ rowì— ëŒ€í•´ì„œ ê°™ì€ YEAR(birthday) ê°’ì„ ê°€ì§„ rowë“¤ì„ ì°¾ëŠ”ë‹¤ (4) ê·¸ ë‹¤ìŒ í•´ë‹¹ rowë“¤ ì¤‘ heightì˜ ìµœì†Ÿê°’ì„ êµ¬í•œë‹¤ | . | . +) ì„œë¸Œì¿¼ë¦¬ë¥¼ ì¤‘ì²©í•´ì„œ ì‚¬ìš©í•˜ëŠ” ê²ƒë„ ê°€ëŠ¥í•˜ì§€ë§Œ (ì„œë¸Œì¿¼ë¦¬ ì•ˆì— ë˜ ë‹¤ë¥¸ ì„œë¸Œì¿¼ë¦¬), ê°€ë…ì„±ì´ ë–¨ì–´ì§€ë¯€ë¡œ â€˜ë·°â€™ë¥¼ í™œìš©í•˜ëŠ” ê²ƒì´ ë‚˜ì„ ìˆ˜ ìˆë‹¤ . ",
    "url": "https://chaelist.github.io/docs/sql/join_subq_view/#%EC%84%9C%EB%B8%8C%EC%BF%BC%EB%A6%AC-subquery",
    "relUrl": "/docs/sql/join_subq_view/#ì„œë¸Œì¿¼ë¦¬-subquery"
  },"84": {
    "doc": "ì¡°ì¸, ì„œë¸Œì¿¼ë¦¬, ë·°",
    "title": "ë·° (View)",
    "content": ". | CREATE VIEW ë·°_ì´ë¦„ AS íŠ¹ì •í•œ_SQLë¬¸ì˜ êµ¬ì¡°ë¡œ â€˜ë·°â€™ë¥¼ ìƒì„±í•´ë‘ê³ , í•„ìš”í•  ë•Œë§ˆë‹¤ ì›ë˜ ìˆëŠ” í…Œì´ë¸”ì¸ ê²ƒì²˜ëŸ¼ ìì—°ìŠ¤ëŸ½ê²Œ ê°€ì ¸ë‹¤ ì“°ë©´ ëœë‹¤ | ë§¤ë²ˆ íŠ¹ì •í•œ ë°©ì‹ìœ¼ë¡œ JOIN / ì„œë¸Œì¿¼ë¦¬ë¥¼ ì‚¬ìš©í•´ì„œ êµ¬ì„±í•´ì•¼ í•˜ëŠ” í…Œì´ë¸”ì´ ìˆë‹¤ë©´, ë¯¸ë¦¬ VIEWë¡œ ì €ì¥í•´ë‘ê³  ì“°ë©´ ì¢‹ë‹¤ | . -- VIEW ìƒì„±í•´ë‘ê¸° CREATE VIEW three_tables_joined AS SELECT i.id, i.name, AVG(star) AS avg_star, COUNT(*) AS count_star FROM item AS i LEFT OUTER JOIN review AS r ON r.item_id = i.id LEFT OUTER JOIN member AS m ON r.mem_id = m.id WHERE m.gender = 'f' GROUP BY i.ithree_tables_joinedd, i.name HAVING COUNT(*) &gt;= 2 ORDER BY AVG(star) DESC, COUNT(*) DESC; . â†’ ìœ„ì™€ ê°™ì´ viewë¥¼ ìƒì„±í•´ë‘ë©´, ì„œë¸Œì¿¼ë¦¬ë¥¼ ì¤‘ì²©í•´ì„œ ì¨ì•¼ í–ˆë˜ êµ¬ë¬¸ì„ ì•„ë˜ì™€ ê°™ì´ ì§§ê²Œ ì“¸ ìˆ˜ ìˆë‹¤: . SELECT * FROM three_tables_joined WHERE avg_star = ( SELECT MAX(avg_star) FROM three_tables_joined); -- VIEWëŠ” ë‹¤ë¥¸ í‰ë²”í•œ tableì²˜ëŸ¼ ê°€ì ¸ë‹¤ ì‚¬ìš©í•˜ë©´ ëœë‹¤ . ",
    "url": "https://chaelist.github.io/docs/sql/join_subq_view/#%EB%B7%B0-view",
    "relUrl": "/docs/sql/join_subq_view/#ë·°-view"
  },"85": {
    "doc": "Kaggle Dataset EDA",
    "title": "Kaggle Dataset EDA",
    "content": " ",
    "url": "https://chaelist.github.io/docs/kaggle",
    "relUrl": "/docs/kaggle"
  },"86": {
    "doc": "Kiva Crowdfunding 1",
    "title": "Kiva Crowdfunding 1",
    "content": ". | ë°ì´í„° íŒŒì•… . | ë°ì´í„° ì •ë³´ íŒŒì•… | datetime í˜•ì‹ ì²˜ë¦¬ | . | funded amount íŒŒì•… . | funded amount ë¶„í¬ | funded amount ì¶”ì´ | funded amountì™€ loan amountì˜ ì°¨ì´ ë¹„êµ | . | ì¡°ê±´ë³„ funded amount í™•ì¸ . | êµ­ê°€ë³„ | Sectorë³„ | Activityë³„ | . | ì¡°ê±´ë³„ ë¶„í¬ í™•ì¸ . | borrower genders | term in months | repayment interval | lender count | fundingì— ê±¸ë¦¬ëŠ” ê¸°ê°„ | . | . *ë¶„ì„ ëŒ€ìƒ ë°ì´í„°ì…‹: Kiva Crowdfunding Data . | ë°ì´í„°ì…‹ ì¶œì²˜ | kiva.org: ì„¸ê³„ ê°êµ­ì˜ ê²½ì œì ìœ¼ë¡œ ì–´ë ¤ìš´ ì‚¬ëŒë“¤ì—ê²Œ ëˆì„ ë¹Œë ¤ì£¼ëŠ” online crowdfunding platform | kiva_loans.csv: 2014.01.01~2017.07.26 ì‚¬ì´ì— kivaì— ì˜¬ë¼ì˜¨ loan ì •ë³´ ë°ì´í„°. (671,205ê°œì˜ row) | . ",
    "url": "https://chaelist.github.io/docs/kaggle/kiva_crowdfunding/",
    "relUrl": "/docs/kaggle/kiva_crowdfunding/"
  },"87": {
    "doc": "Kiva Crowdfunding 1",
    "title": "ë°ì´í„° íŒŒì•…",
    "content": "# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ import import pandas as pd import numpy as np from matplotlib import pyplot as plt import seaborn as sns import scipy.stats as stats . kiva_df = pd.read_csv('data/kiva_loans.csv') kiva_df.head() . | Â  | id | funded_amount | loan_amount | activity | sector | use | country_code | country | region | currency | partner_id | posted_time | disbursed_time | funded_time | term_in_months | lender_count | tags | borrower_genders | repayment_interval | date | . | 0 | 653051 | 300 | 300 | Fruits &amp; Vegetables | Food | To buy seasonal, fresh fruits to sell. | PK | Pakistan | Lahore | PKR | 247 | 2014-01-01 06:12:39+00:00 | 2013-12-17 08:00:00+00:00 | 2014-01-02 10:06:32+00:00 | 12 | 12 | nan | female | irregular | 2014-01-01 | . | 1 | 653053 | 575 | 575 | Rickshaw | Transportation | to repair and maintain the auto rickshaw used in their business. | PK | Pakistan | Lahore | PKR | 247 | 2014-01-01 06:51:08+00:00 | 2013-12-17 08:00:00+00:00 | 2014-01-02 09:17:23+00:00 | 11 | 14 | nan | female, female | irregular | 2014-01-01 | . | 2 | 653068 | 150 | 150 | Transportation | Transportation | To repair their old cycle-van and buy another one to rent out as a source of income | IN | India | Maynaguri | INR | 334 | 2014-01-01 09:58:07+00:00 | 2013-12-17 08:00:00+00:00 | 2014-01-01 16:01:36+00:00 | 43 | 6 | user_favorite, user_favorite | female | bullet | 2014-01-01 | . | 3 | 653063 | 200 | 200 | Embroidery | Arts | to purchase an embroidery machine and a variety of new embroidery materials. | PK | Pakistan | Lahore | PKR | 247 | 2014-01-01 08:03:11+00:00 | 2013-12-24 08:00:00+00:00 | 2014-01-01 13:00:00+00:00 | 11 | 8 | nan | female | irregular | 2014-01-01 | . | 4 | 653084 | 400 | 400 | Milk Sales | Food | to purchase one buffalo. | PK | Pakistan | Abdul Hakeem | PKR | 245 | 2014-01-01 11:53:19+00:00 | 2013-12-17 08:00:00+00:00 | 2014-01-01 19:18:51+00:00 | 14 | 16 | nan | female | monthly | 2014-01-01 | . | funded_amount: amount disbursed by Kiva to the field agent (USD) | loan_amount: amount disbursed by the field agent to the borrower (USD) | country_code: ISO country code | posted_time: the time at which the loan is posted on Kiva by the field agent | disbursed_time: the time at which the loan is disbursed by the field agent to the borrower | funded_time: the time at which the loan posted to Kiva gets funded by lenders completely | term_in_months: the duration for which the loan was disbursed in months | date: posted_timeì—ì„œ dateë§Œ ì¶”ì¶œí•œ ê²ƒ. (Kivaì— ì˜¬ë¼ì˜¨ ë‚ ì§œ) | . ë°ì´í„° ì •ë³´ íŒŒì•… . | ì¤‘ë³µê°’ í™•ì¸ . # ì¤‘ë³µê°’ì´ í¬í•¨ë˜ì–´ ìˆë‚˜ í™•ì¸ (ëª¨ë“  ì—´ì˜ ë°ì´í„°ê°€ ê°™ì€ ê²½ìš°) kiva_df.duplicated().sum() . 0 . | nullê°’ ì—¬ë¶€, data type í™•ì¸ . kiva_df.info() . &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 671205 entries, 0 to 671204 Data columns (total 20 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 id 671205 non-null int64 1 funded_amount 671205 non-null float64 2 loan_amount 671205 non-null float64 3 activity 671205 non-null object 4 sector 671205 non-null object 5 use 666973 non-null object 6 country_code 671197 non-null object 7 country 671205 non-null object 8 region 614405 non-null object 9 currency 671205 non-null object 10 partner_id 657698 non-null float64 11 posted_time 671205 non-null object 12 disbursed_time 668809 non-null object 13 funded_time 622874 non-null object 14 term_in_months 671205 non-null float64 15 lender_count 671205 non-null int64 16 tags 499789 non-null object 17 borrower_genders 666984 non-null object 18 repayment_interval 671205 non-null object 19 date 671205 non-null object dtypes: float64(4), int64(2), object(14) memory usage: 102.4+ MB . | funded_amountì™€ loan_amountì˜ ê´€ê³„ë¥¼ í™•ì¸ . print('funded = loan: ', len(kiva_df[kiva_df['funded_amount'] == kiva_df['loan_amount']])) print('funded &lt; loan: ', len(kiva_df[kiva_df['funded_amount'] &lt; kiva_df['loan_amount']])) print('funded &gt; loan: ', len(kiva_df[kiva_df['funded_amount'] &gt; kiva_df['loan_amount']])) . funded = loan: 622875 funded &lt; loan: 48328 funded &gt; loan: 2 . | funded_amount: Kiva â†’ field agent // loan_amount: field_agent â†’ borrower | ë³´í†µ field agentê°€ ë¨¼ì € borrowerì—ê²Œ í•„ìš”í•œ ê¸ˆì•¡ì„ ë¹Œë ¤ì£¼ê³ , ê·¸ ë‹¤ìŒ Kivaì— postí—¤ì„œ ìê¸ˆì„ ëª¨ìœ¼ëŠ” í˜•íƒœì¸ ë“¯. | loan_amountë§Œí¼ funded_amountê°€ ì°¬ ê²½ìš°ê°€ ê°€ì¥ ë§ê³ , ì•„ì§ loan_amountë§Œí¼ ì°¨ì§€ ì•ŠëŠ” ê²½ìš°ëŠ” 48,328ê±´ | loan_amountë³´ë‹¤ funded_amountê°€ ë§ì€ ê²½ìš°ë„ ì´ë¡€ì ìœ¼ë¡œ 2ê±´ ìˆìœ¼ë‚˜, ì˜¤ê¸°ì…ì¸ì§€ ì‹¤ì œë¡œ loan_amountë¥¼ ì´ˆê³¼í•´ì„œ ìê¸ˆì„ ëª¨ì„ ìˆ˜ ìˆëŠ”ì§€ëŠ” ë¶ˆë¶„ëª…. | . | funded_timeì˜ nullê°’ì„ í™•ì¸ . funded_time_na = kiva_df[kiva_df['funded_time'].isna()] print('funded = loan: ', len(funded_time_na[funded_time_na['funded_amount'] == funded_time_na['loan_amount']])) print('funded &lt; loan: ', len(funded_time_na[funded_time_na['funded_amount'] &lt; funded_time_na['loan_amount']])) print('funded &gt; loan: ', len(funded_time_na[funded_time_na['funded_amount'] &gt; funded_time_na['loan_amount']])) . funded = loan: 1 funded &lt; loan: 48328 funded &gt; loan: 2 . | funded_time: Kivaì— postëœ loanì´ ì™„ì „íˆ ëª¨ê¸ˆëœ ì‹œê°„ì„ ê¸°ë¡ | loan_amountë§Œí¼ ì•„ì§ funded_amountê°€ ë‹¤ ì°¨ì§€ ì•Šì€ ê²½ìš°, funded_timeì´ ì•„ì§ ê¸°ë¡ë  ìˆ˜ ì—†ì–´ì„œ nullê°’ìœ¼ë¡œ ë‚¨ì•„ ìˆìŒ. | ì´ë¡€ì ìœ¼ë¡œ funded_amount &gt;= loan_amountì¸ ê²½ìš°ë„ 3ê±´ ìˆìŒ: ëª¨ê¸ˆì´ ì™„ë£Œë˜ì—ˆëŠ”ë°ë„ í•´ë‹¹ ì‹œê°„ì„ ë¯¸ì²˜ ê¸°ë¡í•˜ì§€ ëª»í•œ ë“¯. | . | . datetime í˜•ì‹ ì²˜ë¦¬ . | datetime ì •ë³´ë¥¼ ë‹´ì€ ì»¬ëŸ¼ë“¤ì„ datetime typeìœ¼ë¡œ ë³€í™˜í•´ì¤Œ kiva_df['posted_time'] = pd.to_datetime(kiva_df['posted_time']) kiva_df['disbursed_time'] = pd.to_datetime(kiva_df['disbursed_time']) kiva_df['funded_time'] = pd.to_datetime(kiva_df['funded_time']) # ì˜ ë³€í™˜ë˜ì—ˆë‚˜ í™•ì¸ kiva_df[['posted_time', 'disbursed_time', 'funded_time']].dtypes . posted_time datetime64[ns, UTC] disbursed_time datetime64[ns, UTC] funded_time datetime64[ns, UTC] dtype: object . | posted_time ì •ë³´: ë¯¸ë¦¬ ê°€ê³µí•´ë‘  . # posted_time â†’ Month, Year, Quarterë¡œë„ ë¬¶ì–´ë‘  (ì¶”ì´ë¥¼ ì‹œê°í™”í•´ì„œ í™•ì¸í•˜ê¸° í¸í•˜ë„ë¡) df['posted_month'] = df['posted_time'].dt.strftime('%Y%m') df['posted_year'] = df['posted_time'].dt.year df['posted_quarter'] = df['posted_time'].dt.to_period(\"Q\").astype('str') df[['posted_time', 'posted_month', 'posted_year', 'posted_quarter']].head() . | Â  | posted_time | posted_month | posted_year | posted_quarter | . | 0 | 2014-01-01 06:12:39+00:00 | 201401 | 2014 | 2014Q1 | . | 1 | 2014-01-01 06:51:08+00:00 | 201401 | 2014 | 2014Q1 | . | 2 | 2014-01-01 09:58:07+00:00 | 201401 | 2014 | 2014Q1 | . | 3 | 2014-01-01 08:03:11+00:00 | 201401 | 2014 | 2014Q1 | . | 4 | 2014-01-01 11:53:19+00:00 | 201401 | 2014 | 2014Q1 | . | . ",
    "url": "https://chaelist.github.io/docs/kaggle/kiva_crowdfunding/#%EB%8D%B0%EC%9D%B4%ED%84%B0-%ED%8C%8C%EC%95%85",
    "relUrl": "/docs/kaggle/kiva_crowdfunding/#ë°ì´í„°-íŒŒì•…"
  },"88": {
    "doc": "Kiva Crowdfunding 1",
    "title": "funded amount íŒŒì•…",
    "content": "# ë‹¹ì¥ í•„ìš”í•˜ì§€ëŠ” ì•Šì€ columnì„ ì œì™¸í•˜ê³  df êµ¬ì„± df = kiva_df.copy() df.drop(['country_code', 'tags', 'date'], axis='columns', inplace=True) . funded amount ë¶„í¬ . plt.figure(figsize=(12, 5)) sns.kdeplot(data=df, x='funded_amount', color='#C88686'); . â†’ outlierê°€ ë§ì•„ì„œ ì•Œì•„ë³´ê¸° ì–´ë ¤ìš°ë¯€ë¡œ íŠ¹ì • êµ¬ê°„ë§Œ í™•ëŒ€í•´ì„œ ë‹¤ì‹œ ê·¸ë ¤ë´„: . # 3ì‚¬ë¶„ìœ„ê°’ + 1.5IQR = 1875 plt.figure(figsize=(12, 5)) sns.kdeplot(data = df[df['funded_amount'] &lt;= 1875], x='funded_amount', color='#C88686'); . | ëŒ€ì²´ë¡œ 250 USD ì •ë„ì˜ fundingì´ ë§ê³ , 75%ê°€ 900 USD ì´í•˜ì˜ ê·œëª¨. | . df['funded_amount'].describe() . count 671205.000000 mean 785.995061 std 1130.398941 min 0.000000 25% 250.000000 50% 450.000000 75% 900.000000 max 100000.000000 Name: funded_amount, dtype: float64 . funded amount ì¶”ì´ . | ë¶„ê¸°ë³„ ì´ funded_amount . | ì›”ë³„ë¡œ ê·¸ë¦¬ê¸°ì—ëŠ” ê¸°ê°„ì´ ë„ˆë¬´ ê¸¸ì–´ ì‹œê°í™”í•´ì„œ ì œëŒ€ë¡œ í‘œí˜„í•˜ê¸° ì–´ë ¤ìš°ë¯€ë¡œ, ë¶„ê¸°ë³„ ì¶”ì´ë¡œ ì‹œê°í™” | 2017Q3ì€ 2017.07.26ê¹Œì§€ì˜ ê¸°ë¡ë°–ì— ì—†ìœ¼ë¯€ë¡œ ì œì™¸í•˜ê³  ì‹œê°í™” | . plt.figure(figsize=(16, 6)) sns.lineplot(data=df[df['posted_quarter'] != '2017Q3'], x='posted_quarter', y='funded_amount', color='#C88686', estimator='sum', ci=None) plt.ylim(30000000, 45000000); . | ë¶„ê¸°ë³„ ì´ postëœ fundìˆ˜ . plt.figure(figsize=(16, 6)) sns.lineplot(data=df[df['posted_quarter'] != '2017Q3'], x='posted_quarter', y='funded_amount', color='#C88686', estimator='count', ci=None) plt.ylim(30000, 60000); . | ë¶„ê¸°ë³„ í‰ê·  funded_amount . plt.figure(figsize=(16, 6)) sns.lineplot(data=df[df['posted_quarter'] != '2017Q3'], x='posted_quarter', y='funded_amount', color='#C88686') # estimator='mean'ì´ defaultê°’ plt.ylim(600, 1000); . | ë¶„ê¸°ë³„ ì´ funded_amountëŠ” ì¦ê°€í•˜ëŠ” ì¶”ì„¸ | í‰ê·  funded_amountëŠ” ê°ì†Œì„¸ì´ì§€ë§Œ, postë˜ëŠ” fundì˜ ìˆ˜ê°€ ì¦ê°€ ì¶”ì„¸ì—¬ì„œ ì´ amountê°€ í•¨ê»˜ ì¦ê°€. | ë‹¤ë§Œ, 2017Q2ì—ëŠ” postëœ fundì˜ ìˆ˜ê°€ ì¦ê°€í–ˆìŒì—ë„ í‰ê·  funded_amountê°€ í¬ê²Œ ê°ì†Œí•´ ì´ funded_amountë„ í¬ê²Œ ê°ì†Œ. | . | . funded amountì™€ loan amountì˜ ì°¨ì´ ë¹„êµ . | funded_amountì™€ loan_amountì˜ ë³€í™” ì¶”ì´ë¥¼ ë¹„êµí•´ì„œ ì‹œê°í™” . plt.figure(figsize=(16, 6)) sns.lineplot(data=df[df['posted_quarter'] != '2017Q3'], x='posted_quarter', y='funded_amount', color='#C88686', estimator='sum', ci=None) sns.lineplot(data=df[df['posted_quarter'] != '2017Q3'], x='posted_quarter', y='loan_amount', color='#D7CCAD', estimator='sum', ci=None) plt.ylim(30000000, 50000000); . | loan_amount - funded_amount ì°¨ì•¡ ë³€í™”ë¥¼ í™•ì¸ . | loan_amountì—ì„œ funded_amountë¥¼ ëº€ ê¸ˆì•¡ì´ ì–´ë–»ê²Œ ë³€í™”í•˜ëŠ”ì§€ ì‹œê°í™” | . quarterly_amount = df.groupby(['posted_quarter'])[['funded_amount', 'loan_amount']].sum().reset_index() quarterly_amount['loan-funded'] = quarterly_amount['loan_amount'] - quarterly_amount['funded_amount'] plt.figure(figsize=(16, 6)) sns.barplot(data=quarterly_amount[quarterly_amount['posted_quarter'] != '2017Q3'], x='posted_quarter', y='loan-funded', color='#C88686'); . | 2017Q2ì—ëŠ” loan_amount ìì²´ë„ ì¡°ê¸ˆ ê°ì†Œí•˜ê¸´ í–ˆìœ¼ë‚˜ ë³„ ë¬¸ì œëŠ” ì—†ì–´ ë³´ì„. | 2017Q2ì— funded_amount ì´ì•¡ì´ í¬ê²Œ ê°ì†Œí•œ ê²ƒì€ ì•„ì§ loan_amountë§Œí¼ ë‹¤ ì±„ì›Œì§€ì§€ ì•Šì€ ê±´ë“¤ì´ ë‚¨ì•„ ìˆê¸° ë•Œë¬¸ì¸ ë“¯. ì¡°ê¸ˆ ë” ì‹œê°„ì´ ìˆìœ¼ë©´ fundingì´ ëŠ˜ì–´ë‚  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì‹œê°„ì„ ë‘ê³  ì¶”ì´ë¥¼ í™•ì¸í•´ë³¼ í•„ìš”ê°€ ìˆë‹¤ê³  íŒë‹¨ë¨. | íŠ¹ì´í•˜ê²Œë„ ì—°ë„ë³„ Q1(1ë¶„ê¸°)ì—ëŠ” loan_amountì™€ funded_amount ì‚¬ì´ì˜ ì°¨ì´ê°€ ìœ ë… ì‘ê²Œ ë‚˜íƒ€ë‚¨. ì£¼ë¡œ ì—°ì´ˆì— crowdfunding ë“± ìƒˆë¡œìš´ ê³„íšì„ ì‹¤ì²œí•˜ëŠ” ì‚¬ëŒì´ ë§ì´ ë•Œë¬¸? | . | . ",
    "url": "https://chaelist.github.io/docs/kaggle/kiva_crowdfunding/#funded-amount-%ED%8C%8C%EC%95%85",
    "relUrl": "/docs/kaggle/kiva_crowdfunding/#funded-amount-íŒŒì•…"
  },"89": {
    "doc": "Kiva Crowdfunding 1",
    "title": "ì¡°ê±´ë³„ funded amount í™•ì¸",
    "content": "êµ­ê°€ë³„ . | funded amountê°€ ê°€ì¥ ë†’ì€ êµ­ê°€ Top 10 í™•ì¸ . # funded_amount í•©ì‚° ê¸ˆì•¡ ë‚´ë¦¼ì°¨ìˆœìœ¼ë¡œ Top 10 êµ­ê°€ë§Œ ì‹œê°í™” loans_country = df.groupby('country')[['funded_amount']].sum().reset_index() loans_country.sort_values(by='funded_amount', ascending=False, inplace=True) loans_country.head(10) plt.figure(figsize=(12, 5)) sns.barplot(data=loans_country.head(10), x='country', y='funded_amount', palette='pink'); . | kivaì—ì„œ fund ë°›ì€ ê¸ˆì•¡ì´ ê°€ì¥ ë§ì€ ë‚˜ë¼ëŠ” Philippines | . | ë¶„ê¸°ë³„ êµ­ê°€ë³„ funded amount . quarterly_loans_country = df.groupby(['posted_quarter', 'country'])[['funded_amount']].sum().reset_index() # 2017Q3ì€ 7ì›”ë°–ì— ì—†ê³ , ê·¸ë‚˜ë§ˆ 7ì›”ë„ 26ì¼ê¹Œì§€ë°–ì— ì—†ìœ¼ë¯€ë¡œ ì œì™¸ quarterly_loans_country = quarterly_loans_country[quarterly_loans_country['posted_quarter'] != '2017Q3'] # ê° ê¸°ê°„ë³„ë¡œ, funded amount top 5ì— í¬í•¨ë˜ëŠ” êµ­ê°€ë“¤ë§Œ ë”°ë¡œ ì €ì¥ quarters = quarterly_loans_country['posted_quarter'].unique() top_countries_set = set() for q in quarters: temp = quarterly_loans_country[quarterly_loans_country['posted_quarter'] == q] temp_set = set(temp.sort_values(by='funded_amount', ascending=False).head()['country'].to_list()) top_countries_set.update(temp_set) # ìœ„ì—ì„œ ì •ë¦¬í•œ top_countries_setì— í¬í•¨ëœ êµ­ê°€ì— í•œí•´ì„œ ì‹œê°í™” plt.figure(figsize=(16, 7)) sns.lineplot(data=quarterly_loans_country[quarterly_loans_country['country'].isin(top_countries_set)], x='posted_quarter', y='funded_amount', hue='country', palette='Set2'); # legendë¥¼ box ë°–ìœ¼ë¡œ ë¹¼ ì¤Œ plt.legend(bbox_to_anchor=(1.01, 1), borderaxespad=0); . | 2014Q1ì„ ì œì™¸í•œ ëª¨ë“  ê¸°ê°„ì— Philippinesì´ fund ë°›ì€ ê¸ˆì•¡ì´ ê°€ì¥ ë§ìŒ | Kenyaì˜ ê²½ìš°, íŠ¹íˆ Q1(1ë¶„ê¸°)ë¥¼ ìœ„ì£¼ë¡œ funded amountê°€ ì¦ê°€í•˜ëŠ” íŠ¸ë Œë“œë¥¼ ë³´ì„ | Cambodiaì˜ ê²½ìš° 2017Q1, 2017Q2ì— í° funded amount ìƒìŠ¹ì„¸ë¥¼ ë³´ì—¬, ì¶”í›„ ì§€ì¼œë´ì•¼ í•  ë“¯. | . +) Kenyaì˜ sectorë³„ funded_amount í™•ì¸ . kenya_sector = df[df['country'] == 'Kenya'].groupby(['sector'])[['funded_amount']].sum().sort_values(by='funded_amount', ascending=False) kenya_sector['percentage(%)'] = kenya_sector['funded_amount'] / kenya_sector['funded_amount'].sum() * 100 kenya_sector.head() . | sector | funded_amount | percentage(%) | . | Agriculture | 16484185 | 51.12 | . | Food | 4603345 | 14.27 | . | Retail | 4232640 | 13.13 | . | Services | 1896535 | 5.88 | . | Clothing | 1547850 | 4.80 | . | Kenyaì˜ ê²½ìš° funded amount ì¤‘ ì•½ 51%ê°€ Agriculture(ë†ì—…) ê´€ë ¨ì´ì—¬ì„œ 1ë¶„ê¸°ì— íŠ¹íˆ fundê°€ ì¦ê°€í•˜ëŠ” ì¶”ì„¸ê°€ ë‚˜íƒ€ë‚˜ëŠ” ë“¯. | . | . Sectorë³„ . | funded amountê°€ ê°€ì¥ ë†’ì€ sector Top 10 í™•ì¸ . # funded_amount í•©ì‚° ê¸ˆì•¡ ë‚´ë¦¼ì°¨ìˆœìœ¼ë¡œ ì •ë ¬ í›„ ì‹œê°í™” loans_sector = df.groupby('sector')[['funded_amount']].sum().reset_index() loans_sector.sort_values(by='funded_amount', ascending=False, inplace=True) plt.figure(figsize=(12, 5)) sns.barplot(data=loans_sector, x='sector', y='funded_amount', palette='pink') plt.xticks(rotation=40); . | ê°€ì¥ ë§ì€ funded amountë¥¼ ë³´ì´ëŠ” sectorëŠ” Agriculture, Food, Retail. | . | ë¶„ê¸°ë³„ sectorë³„ funded amount . quarterly_loans_sector = df.groupby(['posted_quarter', 'sector'])[['funded_amount']].sum().reset_index() # 2017Q3ì€ 7ì›”ë°–ì— ì—†ê³ , ê·¸ë‚˜ë§ˆ 7ì›”ë„ 26ì¼ê¹Œì§€ë°–ì— ì—†ìœ¼ë¯€ë¡œ ì œì™¸ quarterly_loans_sector = quarterly_loans_sector[quarterly_loans_sector['posted_quarter'] != '2017Q3'] # ê° ê¸°ê°„ë³„ë¡œ, funded amount top 5ì— í¬í•¨ë˜ëŠ” sectorë“¤ë§Œ ë”°ë¡œ ì €ì¥ quarters = quarterly_loans_sector['posted_quarter'].unique() top_sectors_set = set() for q in quarters: temp = quarterly_loans_sector[quarterly_loans_sector['posted_quarter'] == q] temp_set = set(temp.sort_values(by='funded_amount', ascending=False).head()['sector'].to_list()) top_sectors_set.update(temp_set) # ìœ„ì—ì„œ ì •ë¦¬í•œ top_sectors_setì— í¬í•¨ëœ sectorì— í•œí•´ì„œ ì‹œê°í™” plt.figure(figsize=(16, 7)) sns.lineplot(data=quarterly_loans_sector[quarterly_loans_sector['sector'].isin(top_sectors_set)], x='posted_quarter', y='funded_amount', hue='sector', palette='Set2'); # legendë¥¼ box ë°–ìœ¼ë¡œ ë¹¼ ì¤Œ plt.legend(bbox_to_anchor=(1.01, 1), borderaxespad=0); . | Agriculture ì„¹í„°ì˜ ê²½ìš°, ì£¼ë¡œ Q1 ~ Q2ì˜ ê¸°ê°„ì— íŠ¹íˆ funded amountê°€ ì¦ê°€í•˜ëŠ” íŠ¸ë Œë“œë¥¼ ë³´ì„ | Food, Retail ì„¹í„°ëŠ” ê¾¸ì¤€íˆ ë§ì€ funded amountë¥¼ ë³´ì„ | . | êµ­ê°€ë³„ sectorë³„ funded amount . # funded amount Top 10 countryë§Œ ë”°ë¡œ ì €ì¥ top_countries = df.groupby('country')[['funded_amount']].sum().sort_values(by='funded_amount', ascending=False).head(10).index # funded amount ë‚´ë¦¼ì°¨ìˆœìœ¼ë¡œ sector ìˆœì„œ ì •ë ¬ sectors_order = df.groupby('sector')[['funded_amount']].sum().sort_values(by='funded_amount', ascending=False).index loans_country_sector = pd.pivot_table(df, index='country', columns='sector', values='funded_amount', fill_value=0, aggfunc='sum') loans_country_sector = loans_country_sector[loans_country_sector.index.isin(top_countries)] # top 10 countryë§Œ ëŒ€ìƒìœ¼ë¡œ ìë¦„ loans_country_sector = loans_country_sector[sectors_order] # ì „ì²´ loan amountê°€ ê°€ì¥ ë†’ì€ sectorë¶€í„° ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬ loans_country_sector = loans_country_sector.T loans_country_sector = loans_country_sector[top_countries] # ì „ì²´ loan amountê°€ ê°€ì¥ ë†’ì€ countryë¶€í„° ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬ # ê°’ì„ normalize(ì •ê·œí™”)í•´ì„œ ë¹„êµ (heatmapìœ¼ë¡œ ë³´ë‹¤ ëª…í™•í•˜ê²Œ ë¹„êµí•  ìˆ˜ ìˆê²Œ í‘œí˜„í•˜ê¸° ìœ„í•¨) from sklearn import preprocessing scaler = preprocessing.MinMaxScaler() normalized_data = scaler.fit_transform(loans_country_sector) loans_cntry_sctr_normalized = pd.DataFrame(normalized_data, columns=loans_country_sector.columns, index=loans_country_sector.index) # heatmapìœ¼ë¡œ ì‹œê°í™” plt.figure(figsize=(14, 9)) sns.heatmap(loans_cntry_sctr_normalized, annot=True, cmap='pink_r', fmt='.2f'); . | funded amount Top 10 êµ­ê°€ ì¤‘ Kenya, El Salvador, Cambodia, EcuadorëŠ” Agriculture ì„¹í„°ë¡œ funding ë°›ì€ ê¸ˆì•¡ì´ ê°€ì¥ ë§ìŒ | funded amount Top 10 êµ­ê°€ ì¤‘ Peru, Paraguay, Bolivia, RwandaëŠ” Food ì„¹í„°ë¡œ funding ë°›ì€ ê¸ˆì•¡ì´ ê°€ì¥ ë§ìŒ | ê°€ì¥ funded amountê°€ ë§ì€ êµ­ê°€ì¸ Philippinesì€ Retail ì„¹í„°ë¡œ ë°›ì€ ê¸ˆì•¡ì´ ê°€ì¥ ë§ìŒ | United StatesëŠ” ì´ë¡€ì ìœ¼ë¡œ Services ì„¹í„°ë¡œ ë°›ì€ ê¸ˆì•¡ì´ ê°€ì¥ ë§ìŒ | . | . Activityë³„ . | funded amountê°€ ê°€ì¥ ë†’ì€ activity Top 10 í™•ì¸ . # funded_amount í•©ì‚° ê¸ˆì•¡ ë‚´ë¦¼ì°¨ìˆœìœ¼ë¡œ Top 10 activityë§Œ í™•ì¸ loans_activity = df.groupby(['sector', 'activity'])[['funded_amount']].sum().reset_index() loans_activity.sort_values(by='funded_amount', ascending=False, inplace=True) loans_activity.head(10) . | Â  | sector | activity | funded_amount | . | 7 | Agriculture | Farming | 47900500 | . | 94 | Retail | General Store | 35555135 | . | 0 | Agriculture | Agriculture | 25530735 | . | 54 | Food | Food Production/Sales | 24326140 | . | 112 | Retail | Retail | 23911950 | . | 24 | Clothing | Clothing Sales | 22569725 | . | 68 | Housing | Personal Housing Expenses | 20166525 | . | 37 | Education | Higher education costs | 19012400 | . | 57 | Food | Grocery Store | 14299895 | . | 10 | Agriculture | Livestock | 13699275 | . â†’ ì‹œê°í™” . plt.figure(figsize=(14, 5)) sns.barplot(data=loans_activity.head(10), x='activity', y='funded_amount', palette='pink') plt.xticks(rotation=20); . | Agriculture ì„¹í„°ì— ì†í•˜ëŠ” â€˜Farmingâ€™ activityê°€ ê°€ì¥ ë§ì€ funded amountë¥¼ ë³´ì„ | . | ë¶„ê¸°ë³„ activityë³„ funded amount . quarterly_loans_activity = df.groupby(['posted_quarter', 'activity'])[['funded_amount']].sum().reset_index() # 2017Q3ì€ 7ì›”ë°–ì— ì—†ê³ , ê·¸ë‚˜ë§ˆ 7ì›”ë„ 26ì¼ê¹Œì§€ë°–ì— ì—†ìœ¼ë¯€ë¡œ ì œì™¸ quarterly_loans_activity = quarterly_loans_activity[quarterly_loans_activity['posted_quarter'] != '2017Q3'] # ê° ê¸°ê°„ë³„ë¡œ, funded amount top 5ì— í¬í•¨ë˜ëŠ” activityë“¤ë§Œ ë”°ë¡œ ì €ì¥ quarters = quarterly_loans_activity['posted_quarter'].unique() top_activities_set = set() for q in quarters: temp = quarterly_loans_activity[quarterly_loans_activity['posted_quarter'] == q] temp_set = set(temp.sort_values(by='funded_amount', ascending=False).head()['activity'].to_list()) top_activities_set.update(temp_set) # ìœ„ì—ì„œ ì •ë¦¬í•œ top_sectors_setì— í¬í•¨ëœ activityì— í•œí•´ì„œ ì‹œê°í™” plt.figure(figsize=(16, 7)) sns.lineplot(data=quarterly_loans_activity[quarterly_loans_activity['activity'].isin(top_activities_set)], x='posted_quarter', y='funded_amount', hue='activity', palette='Set2'); # legendë¥¼ box ë°–ìœ¼ë¡œ ë¹¼ ì¤Œ plt.legend(bbox_to_anchor=(1.01, 1), borderaxespad=0); . | Farming activityì˜ funded amountëŠ” Q4ì— ê°€ì¥ ì ê³  ì£¼ë¡œ Q1 ~ Q2 ê¸°ê°„ì— ì¦ê°€í•˜ëŠ” ì¶”ì„¸ë¥¼ ë³´ì„ (Seasonality ì¡´ì¬) | General Store activityì˜ ê¾¸ì¤€íˆ ë†’ì€ funded amountë¥¼ ë³´ì´ëŠ” í¸ | . | êµ­ê°€ë³„ activityë³„ funded amount . # funded amount Top 10 countryë§Œ ë”°ë¡œ ì €ì¥ top_countries = df.groupby('country')[['funded_amount']].sum().sort_values(by='funded_amount', ascending=False).head(10).index # funded amount ë‚´ë¦¼ì°¨ìˆœìœ¼ë¡œ activity ìˆœì„œ ì •ë ¬ activity_order = df.groupby('activity')[['funded_amount']].sum().sort_values(by='funded_amount', ascending=False).index # Top 10 country ê°ê°ì˜ Top 3 activityë§Œ ë”°ë¡œ ì €ì¥ temp_df = df.groupby(['country', 'activity'])[['funded_amount']].sum().reset_index() top_activities = set() for country in top_countries: temp = temp_df[temp_df['country'] == country].sort_values(by='funded_amount', ascending=False).head(3) top_activities.update(set(temp['activity'].to_list())) loans_country_activity = pd.pivot_table(df, index='country', columns='activity', values='funded_amount', fill_value=0, aggfunc='sum') loans_country_activity = loans_country_activity[loans_country_activity.index.isin(top_countries)] # top 10 countryë§Œ ëŒ€ìƒìœ¼ë¡œ ìë¦„ loans_country_activity = loans_country_activity[activity_order] # ì „ì²´ loan amountê°€ ê°€ì¥ ë†’ì€ activityë¶€í„° ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬ loans_country_activity = loans_country_activity.T loans_country_activity = loans_country_activity[loans_country_activity.index.isin(top_activities)] # top_activitiesë¡œ í•œì • loans_country_activity = loans_country_activity[top_countries] # ì „ì²´ loan amountê°€ ê°€ì¥ ë†’ì€ countryë¶€í„° ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬ # ê°’ì„ normalize(ì •ê·œí™”)í•´ì„œ ë¹„êµ (heatmapìœ¼ë¡œ ë³´ë‹¤ ëª…í™•í•˜ê²Œ ë¹„êµí•  ìˆ˜ ìˆê²Œ í‘œí˜„í•˜ê¸° ìœ„í•¨) scaler = preprocessing.MinMaxScaler() normalized_data = scaler.fit_transform(loans_country_activity) loans_country_activity_normalized = pd.DataFrame(normalized_data, columns=loans_country_activity.columns, index=loans_country_activity.index) # heatmapìœ¼ë¡œ ì‹œê°í™” plt.figure(figsize=(14, 9)) sns.heatmap(loans_country_activity_normalized, annot=True, cmap='pink_r', fmt='.2f'); . | Kenyaì™€ CambodiaëŠ” Farmingìœ¼ë¡œ ë°›ì€ ê¸ˆì•¡ì´ ê°€ì¥ ë§ìŒ | PhilipinesëŠ” General Store ê´€ë ¨ ê¸ˆì•¡ì´ ê°€ì¥ ë§ìŒ | . | . ",
    "url": "https://chaelist.github.io/docs/kaggle/kiva_crowdfunding/#%EC%A1%B0%EA%B1%B4%EB%B3%84-funded-amount-%ED%99%95%EC%9D%B8",
    "relUrl": "/docs/kaggle/kiva_crowdfunding/#ì¡°ê±´ë³„-funded-amount-í™•ì¸"
  },"90": {
    "doc": "Kiva Crowdfunding 1",
    "title": "ì¡°ê±´ë³„ ë¶„í¬ í™•ì¸",
    "content": "borrower genders . | ê²°ì¸¡ì¹˜ëŠ” ë¬´ì‹œí•˜ê³  ê³„ì‚° | . | borrower_genders ì¹¼ëŸ¼ì„ typeë³„ë¡œ ë¬¶ì–´ì„œ ì €ì¥ . | female ë˜ëŠ” male í•œ ëª…ì¸ ê²½ìš° ê·¸ëŒ€ë¡œ ì ê³ , female ì—¬ëŸ¬ ëª…ì¼ ê²½ìš° female_group, male ì—¬ëŸ¬ ëª…ì¼ ê²½ìš° male_group, ê·¸ë¦¬ê³  femaleê³¼ maleì´ ì„ì—¬ ìˆëŠ” ê·¸ë£¹ì¼ ê²½ìš° mixed_groupìœ¼ë¡œ ê¸°ì¬ | . def borrower_type(x): if type(x) != str: borrower = 'N/A' elif x == 'female': borrower = 'female' elif x == 'male': borrower = 'male' else: borr_set = set(x.split(', ')) if 'female' in borr_set: if 'male' in borr_set: borrower = 'mixed_group' else: borrower = 'female_group' else: borrower = 'male_group' return borrower df['borrower_type'] = df['borrower_genders'].apply(lambda x: borrower_type(x)) df[['borrower_genders', 'borrower_type']].head() . | Â  | borrower_genders | borrower_type | . | 0 | female | female | . | 1 | female, female | female_group | . | 2 | female | female | . | 3 | female | female | . | 4 | female | female | . | borrower typeë³„ ì´ funded_amount . plt.figure(figsize=(9, 5)) sns.barplot(data=df, x='borrower_type', y='funded_amount', palette='pink', estimator=np.sum, order = ['female', 'female_group', 'male', 'male_group', 'mixed_group', 'N/A']); . | borrower typeë³„ fund count . plt.figure(figsize=(9, 5)) sns.countplot(data=df, x='borrower_type', palette='pink', order = ['female', 'female_group', 'male', 'male_group', 'mixed_group', 'N/A']); . | borrower typeë³„ í‰ê·  funded_amount . plt.figure(figsize=(9, 5)) sns.barplot(data=df, x='borrower_type', y='funded_amount', palette='pink', order = ['female', 'female_group', 'male', 'male_group', 'mixed_group', 'N/A']); . | female í˜¼ì ë¹Œë¦¬ëŠ” ê²½ìš°ëŠ” í‰ê·  funded_amountëŠ” ì ì§€ë§Œ ìˆ˜ ìì²´ê°€ ê°€ì¥ ë§ê¸° ë•Œë¬¸ì— ì´ funded_amountê°€ ê°€ì¥ ë§ìŒ | groupìœ¼ë¡œ ë¹Œë¦¬ëŠ” ê²½ìš°ëŠ” ìˆ˜ ìì²´ëŠ” ì ì§€ë§Œ í‰ê·  funded_amountê°€ ë§ë‹¤ | . | ë¶„ê¸°ë³„ borrower typeë³„ funded amount . borrower_loan = df.groupby(['posted_quarter', 'borrower_type'])[['funded_amount']].sum().reset_index() borrower_loan = borrower_loan[borrower_loan['posted_quarter'] != '2017Q3'] # 2017Q3ì€ 7ì›”ë°–ì— ì—†ê³ , ê·¸ë‚˜ë§ˆ 7ì›”ë„ 26ì¼ê¹Œì§€ë°–ì— ì—†ìœ¼ë¯€ë¡œ ì œì™¸ plt.figure(figsize=(16, 6)) sns.lineplot(data=borrower_loan, x='posted_quarter', y='funded_amount', hue='borrower_type', palette='pink', hue_order=['female', 'female_group', 'male', 'male_group', 'mixed_group', 'N/A']) # legendë¥¼ box ë°–ìœ¼ë¡œ ë¹¼ ì¤Œ plt.legend(bbox_to_anchor=(1.01, 1), borderaxespad=0); . | ëª¨ë“  ê¸°ê°„ì— female í˜¼ì ë¹Œë¦¬ëŠ” í˜•íƒœì˜ funded_amount ì´ì•¡ì´ ê°€ì¥ ë§ë‹¤ | . | borrower typeë³„ activityë³„ funded amount . # borrower_type ì¤‘ 'N/A'ë§Œ ì œì™¸ borrower_type_list = list(df['borrower_type'].unique()) borrower_type_list.remove('N/A') # funded amount ë‚´ë¦¼ì°¨ìˆœìœ¼ë¡œ activity ìˆœì„œ ì •ë ¬ activity_order = df.groupby('activity')[['funded_amount']].sum().sort_values(by='funded_amount', ascending=False).index # Top 10 country ê°ê°ì˜ Top 5 activityë§Œ ë”°ë¡œ ì €ì¥ temp_df = df.groupby(['borrower_type', 'activity'])[['funded_amount']].sum().reset_index() top_activities = set() for borrower in borrower_type_list: temp = temp_df[temp_df['borrower_type'] == borrower].sort_values(by='funded_amount', ascending=False).head(3) top_activities.update(set(temp['activity'].to_list())) loans_borrower_activity = pd.pivot_table(df, index='borrower_type', columns='activity', values='funded_amount', fill_value=0, aggfunc='sum') loans_borrower_activity = loans_borrower_activity[activity_order] # ì „ì²´ loan amountê°€ ê°€ì¥ ë†’ì€ activityë¶€í„° ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬ loans_borrower_activity = loans_borrower_activity[loans_borrower_activity.index.isin(borrower_type_list)] loans_borrower_activity = loans_borrower_activity.T loans_borrower_activity = loans_borrower_activity[loans_borrower_activity.index.isin(top_activities)] # top_activitiesë¡œ í•œì • # ê°’ì„ normalize(ì •ê·œí™”)í•´ì„œ ë¹„êµ (heatmapìœ¼ë¡œ ë³´ë‹¤ ëª…í™•í•˜ê²Œ ë¹„êµí•  ìˆ˜ ìˆê²Œ í‘œí˜„í•˜ê¸° ìœ„í•¨) scaler = preprocessing.MinMaxScaler() normalized_data = scaler.fit_transform(loans_borrower_activity) loans_borrower_activity_normalized = pd.DataFrame(normalized_data, columns=loans_borrower_activity.columns, index=loans_borrower_activity.index) # heatmapìœ¼ë¡œ ì‹œê°í™” plt.figure(figsize=(14, 9)) sns.heatmap(loans_borrower_activity_normalized, annot=True, cmap='pink_r', fmt='.2f'); . | ë‹¤ë¥¸ íƒ€ì…ì€ ëª¨ë‘ â€˜Farmingâ€™ ê´€ë ¨ funded amountê°€ ê°€ì¥ ë§ì€ ë°˜ë©´, femaleì€ â€˜General Storeâ€™ ê´€ë ¨ funded amountê°€ ê°€ì¥ ë§ìŒ | maleê³¼ male_groupì€ Farmingê³¼ Agriculture ê´€ë ¨ funded amountê°€ ë§ìŒ | female_groupì˜ ê²½ìš°, ê°€ì¥ funded amountê°€ ë§ì€ ë‘ activityëŠ” Farmingê³¼ Retail | . | . term in months . | term_in_months: ëˆì„ ëª‡ ê°œì›”ì— ê±¸ì³ ë‚˜ëˆ ì„œ ì§€ê¸‰í•˜ëŠ”ì§€ ë‚˜íƒ€ë‚´ëŠ” ì§€í‘œ (The duration for which the loan was disbursed in months) | . | term_in_monthsì™€ funded_amount ì‚¬ì´ì˜ ìƒê´€ê´€ê³„ í™•ì¸ . plt.figure(figsize=(12, 6)) sns.scatterplot(data = df[df['posted_quarter'] != '2017Q3'], x='funded_amount', y='term_in_months', color='#C88686'); . +) ìƒê´€ê³„ìˆ˜ ê³„ì‚°: . # í”¼ì–´ìŠ¨ ìƒê´€ê³„ìˆ˜ ê²€ì • corr = stats.pearsonr(df['term_in_months'], df['funded_amount']) print('Corr_Coefficient : %.3f \\np-value : %.3f' % (corr)) . Corr_Coefficient : 0.149 p-value : 0.000 . | funded_amountì™€ term_in_months ì‚¬ì´ì— í° ê´€ê³„ëŠ” ë³´ì´ì§€ ì•ŠìŒ | . | funded amountë¥¼ êµ¬ê°„ë³„ë¡œ ë‚˜ëˆ ì„œ ë¹„êµ . # funded_amountë¥¼ ê¸°ì¤€ìœ¼ë¡œ 1ì‚¬ë¶„ìœ„ ~ 4ì‚¬ë¶„ìœ„ë¡œ ë‚˜ëˆ ì„œ flagë¥¼ ë¶™ì„ q1, q2, q3 = np.percentile(df['funded_amount'], [25, 50, 75]) def get_flag(x): if x&lt; q1: quarter = '1st_q' elif x &lt; q2: quarter = '2nd_q' elif x&lt; q3: quarter = '3rd_q' else: quarter = '4th_q' return quarter df['funded_amount_flag'] = df['funded_amount'].apply(lambda x: get_flag(x)) df[['funded_amount', 'funded_amount_flag']].head() . | Â  | funded_amount | funded_amount_flag | . | 0 | 300 | 2nd_q | . | 1 | 575 | 3rd_q | . | 2 | 150 | 1st_q | . | 3 | 200 | 1st_q | . | 4 | 400 | 2nd_q | . â†’ funded_amount_flagë³„ í‰ê·  term_in_monthsë¥¼ ë¹„êµ . sns.barplot(data=df, x='funded_amount_flag', y='term_in_months', palette='pink', order=['4th_q', '3rd_q', '2nd_q', '1st_q']); . | loan_amountê°€ ê°€ì¥ ë§ì€ ê·¸ë£¹(4ì‚¬ë¶„ìœ„)ì´ ì§€ê¸‰ ê¸°ê°„ë„ í‰ê· ì ìœ¼ë¡œ ê¸´ í¸ | . | ë¶„ê¸°ë³„ ê·¸ë£¹ë³„ í‰ê·  term in months ë¹„êµ . plt.figure(figsize=(16, 6)) sns.lineplot(data=df[df['posted_quarter'] != '2017Q3'], x='posted_quarter', y='term_in_months', hue='funded_amount_flag', palette='pink', hue_order=['4th_q', '3rd_q', '2nd_q', '1st_q']) # legendë¥¼ box ë°–ìœ¼ë¡œ ë¹¼ ì¤Œ plt.legend(bbox_to_anchor=(1.01, 1), borderaxespad=0); . | ëŒ€ì²´ë¡œ loan_amountê°€ ë§ì€ ê·¸ë£¹(3~4ì‚¬ë¶„ìœ„)ì´ ì§€ê¸‰ ê¸°ê°„ë„ í‰ê· ì ìœ¼ë¡œ ê¸´ í¸ | . | ë¶„ê¸°ë³„ í‰ê·  term in months ì¶”ì´ í™•ì¸ . plt.figure(figsize=(16, 6)) sns.lineplot(data=df[df['posted_quarter'] != '2017Q3'], x='posted_quarter', y='term_in_months', color='#C88686') plt.ylim(12, 15); . | íŠ¹íˆ Q1 ~ Q2 ê¸°ê°„ì— í‰ê·  term_in_monthsê°€ ì§§ì€ í¸ìœ¼ë¡œ ë‚˜íƒ€ë‚˜ëŠ” ê²½í–¥ì´ ìˆëŠ” ë“¯ | ì „ì²´ì ìœ¼ë¡œ ë´¤ì„ ë•Œ, í‰ê·  term_in_monthsê°€ ê°ì†Œí•˜ëŠ” ì¶”ì„¸ë¼ê³  íŒë‹¨ë¨. (ì¶”í›„ ìˆ˜ì¹˜ íŠ¸ë˜í‚¹ í•„ìš”) | . | sectorë³„ í‰ê·  term in months ë¹„êµ . # term_in_months ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœìœ¼ë¡œ index ìˆœì„œë¥¼ ì €ì¥í•´ë‘  index_order = df.groupby('sector')[['term_in_months']].mean().sort_values(by='term_in_months', ascending=False).index plt.figure(figsize=(16, 7)) sns.barplot(data = df, x='sector', y='term_in_months', palette='pink', order=index_order) plt.xticks(rotation=40); . | ê° sectorì˜ í‰ê·  funded_amountì™€ëŠ” í° ê´€ê³„ê°€ ì—†ìŒ | í‰ê·  term_in_monthsê°€ í° sectorëŠ” Education, Housing, Health | . | . repayment interval . | repayment intervalë³„ ì´ funded_amount . sns.barplot(data=df, x='repayment_interval', y='funded_amount', estimator=np.sum, ci=None, palette='pink', order=['bullet', 'irregular', 'monthly', 'weekly']); . | repayment intervalë³„ í‰ê·  funded_amount . sns.barplot(data=df, x='repayment_interval', y='funded_amount', palette='pink', order=['bullet', 'irregular', 'monthly', 'weekly']); . | repayment intervalë³„ loan ìˆ˜ . sns.countplot(data=df, x='repayment_interval', palette='pink', order=['bullet', 'irregular', 'monthly', 'weekly']); . | â€˜monthlyâ€™ë¡œ ìƒí™˜í•˜ëŠ” loanì´ ê°€ì¥ ë§ìŒ | monthlyë¡œ ìƒí™˜í•˜ëŠ” loan ìˆ˜ê°€ ë§ê³ , í‰ê·  funded_amountë„ ë†’ì€ í¸ì´ê¸° ë•Œë¬¸ì— monthlyë¡œ ìƒí™˜ë˜ëŠ” ì´ funded_amountê°€ ê°€ì¥ ë§ìŒ | . | ë¶„ê¸°ë³„ repayment intervalë³„ funded amount . quarter_interval = df.groupby(['posted_quarter', 'repayment_interval'])[['funded_amount']].sum().reset_index() quarter_interval = quarter_interval[quarter_interval['posted_quarter'] != '2017Q3'] plt.figure(figsize=(16, 6)) sns.lineplot(data=quarter_interval, x='posted_quarter', y='funded_amount', hue='repayment_interval', palette='pink') # legendë¥¼ box ë°–ìœ¼ë¡œ ë¹¼ ì¤Œ plt.legend(bbox_to_anchor=(1.01, 1), borderaxespad=0); . | 2015Q3 ì´í›„ë¡œëŠ” weekly ìƒí™˜ ë°©ì‹ì€ ì—†ì–´ì§ | monthly í˜¹ì€ bullet ìƒí™˜ ë°©ì‹ì´ ë” ì•ˆì •ì ì¸ ìê¸ˆ ê³„íšì— ìœ ë¦¬í•˜ê² ì§€ë§Œ, irregular ìƒí™˜ ë°©ì‹ë„ ì§€ë‚˜ì¹˜ê²Œ ì¦ê°€í•˜ê³  ìˆëŠ” ì¶”ì„¸ëŠ” ì•„ë‹ˆë¯€ë¡œ ê´œì°®ë‹¤ê³  ìƒê°ë¨ | ë‹¤ë§Œ, irregular ìƒí™˜ ë°©ì‹ì˜ ë¹„ì¤‘ì´ ë„ˆë¬´ ë†’ì•„ì§€ë©´ ì¢‹ì§€ ì•Šì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ í–¥í›„ ê¾¸ì¤€íˆ íŠ¸ë˜í‚¹í•  í•„ìš”ê°€ ìˆë‹¤ê³  ìƒê°ë¨ | . print('*2014Q1 ëŒ€ë¹„ 2017Q1 funded_amount ì¦ê°*') for interval in ['monthly', 'irregular', 'bullet']: temp = quarter_interval[quarter_interval['repayment_interval'] == interval] loan_2014 = int(temp[temp['posted_quarter'] == '2014Q1']['funded_amount']) loan_2017 = int(temp[temp['posted_quarter'] == '2017Q1']['funded_amount']) growth_rate = (loan_2017 / loan_2014 - 1) * 100 print(f'{interval}: {growth_rate :.0f}% ({loan_2014} -&gt; {loan_2017})') . *2014Q1 ëŒ€ë¹„ 2017Q1 funded_amount ì¦ê°* monthly: 15% (21201250 -&gt; 24314025) irregular: 30% (10370950 -&gt; 13451700) bullet: 41% (3173205 -&gt; 4460625) . | . lender count . | lender_countì™€ funded_amount ì‚¬ì´ì˜ ìƒê´€ê´€ê³„ í™•ì¸ . plt.figure(figsize=(12, 6)) sns.scatterplot(data=df, x='funded_amount', y='lender_count', color='#C88686'); . +) ìƒê´€ê³„ìˆ˜ ê³„ì‚°: . # í”¼ì–´ìŠ¨ ìƒê´€ê³„ìˆ˜ ê²€ì • corr = stats.pearsonr(df['funded_amount'], df['lender_count']) print('Corr_Coefficient : %.3f \\np-value : %.3f' % (corr)) . Corr_Coefficient : 0.849 p-value : 0.000 . | lender_countì™€ funded_amountëŠ” ê½¤ ê°•í•œ ìƒê´€ê´€ê³„ë¥¼ ë³´ì„ | . | lender_countì˜ ë¶„í¬ í™•ì¸ . plt.figure(figsize=(16, 6)) sns.kdeplot(data = df, x='lender_count', color='#C88686'); . â†’ outlierë¡œ ì¸í•´ ê°’ì´ ëª°ë ¤ ìˆëŠ” ë¶€ë¶„ì„ êµ¬ì²´ì ìœ¼ë¡œ í™•ì¸í•˜ê¸° ì–´ë ¤ìš°ë¯€ë¡œ, ì£¼ìš” êµ¬ê°„ì„ í™•ëŒ€í•´ì„œ í™•ì¸: . # Q3 + 1.5IQR = 49.5 plt.figure(figsize=(16, 6)) sns.countplot(data = df[df['lender_count'] &lt; 50], x='lender_count', color='#C88686'); . | ì£¼ë¡œ 1ëª… í˜¹ì€ 5~9ëª… ì •ë„ê°€ í•˜ë‚˜ì˜ fundingì— ì°¸ì—¬í•˜ëŠ” ê²½ìš°ê°€ ë§ë‹¤ | . | ë¶„ê¸°ë³„ í‰ê·  lender_count . plt.figure(figsize=(16, 6)) sns.barplot(data = df[df['posted_quarter'] != '2017Q3'], x='posted_quarter', y='lender_count', palette='pink'); . | í‰ê· ì ì¸ lender ìˆ˜ê°€ ì ì  ê°ì†Œí•˜ëŠ” ì¶”ì„¸ë¥¼ ë³´ì„ | ì ì  postë˜ëŠ” loanì˜ ìˆ˜ê°€ ë§ì•„ì§€ë©´ì„œ ê°œë‹¹ lenderìˆ˜, funded_amountê°€ ê°ì†Œí•˜ëŠ” ê²ƒì¸ë¼ê³  ì¶”ì •ë¨ | . | . fundingì— ê±¸ë¦¬ëŠ” ê¸°ê°„ . | posted_time ~ funded_time ì‚¬ì´ ê¸°ê°„ (postí•œ í›„ ì™„ì „íˆ funding ì™„ë£Œë˜ê¸°ê¹Œì§€) | . | funding ë¯¸ì™„ë£Œ postì˜ ë¶„í¬ë¥¼ í™•ì¸ . # funded_timeì´ nullê°’ì¸ postë¥¼ funding ë¯¸ì™„ë£Œë¼ê³  ë¶„ë¥˜ funding_incomplete = df[df['funded_time'].isna()] plt.figure(figsize=(16, 6)) sns.countplot(data=funding_incomplete, x='posted_quarter', color='#C88686'); . | posted_time ~ funded_time ì‚¬ì´ ê¸°ê°„ì„ ê³„ì‚°í•´ì„œ ì €ì¥ . # day(ì¼) ë‹¨ìœ„ë¡œ ì €ì¥ df['post_to_funded'] = (df['funded_time'] - df['posted_time']).dt.days df[['funded_time', 'posted_time', 'post_to_funded']].head() . | Â  | funded_time | posted_time | post_to_funded | . | 0 | 2014-01-02 10:06:32+00:00 | 2014-01-01 06:12:39+00:00 | 1 | . | 1 | 2014-01-02 09:17:23+00:00 | 2014-01-01 06:51:08+00:00 | 1 | . | 2 | 2014-01-01 16:01:36+00:00 | 2014-01-01 09:58:07+00:00 | 0 | . | 3 | 2014-01-01 13:00:00+00:00 | 2014-01-01 08:03:11+00:00 | 0 | . | 4 | 2014-01-01 19:18:51+00:00 | 2014-01-01 11:53:19+00:00 | 0 | . +) nullê°’ ìˆ˜ íŒŒì•… . df['post_to_funded'].isnull().sum() . 48331 . +) ì˜ëª»ëœ ê°’ íŒŒì•… . df[df['funded_time'] &lt; df['posted_time']][['funded_time', 'posted_time']] . | Â  | posted_time | funded_time | . | 636606 | 2017-05-15 00:00:00+00:00 | 2017-04-27 11:52:24+00:00 | . | posted_timeì€ kivaì— loanì´ postëœ ì‹œê°ì´ê³ , funded_timeì€ kivaì— postëœ loanì´ ì™„ì „íˆ fundedëœ ì‹œê°ì´ë¯€ë¡œ posted_timeì´ funded_timeë³´ë‹¤ ë’¤ì¼ ìˆ˜ëŠ” ì—†ë‹¤ | posted_time &lt; posted_timeìœ¼ë¡œ ë‚˜ì˜¤ëŠ” rowëŠ” ì˜ëª» ê¸°ì…ëœ ê²ƒì´ë¼ê³  ìƒê°ë¨ | . â†’ ë§ˆì´ë„ˆìŠ¤ ê°’ì„ ë³´ì´ëŠ” ì¹¼ëŸ¼ &amp; N/Aê°’(ì•„ì§ funding ì™„ë£Œ ì•ˆë¨)ì„ ì œì™¸í•œ ë°ì´í„°ë¥¼ ë³„ë„ë¡œ ì €ì¥ . post_to_fund_notna = df[df['post_to_funded'] &gt;= 0] . | postëœ í›„ funding ì™„ë£Œê¹Œì§€ì˜ ê¸°ê°„ ë¶„í¬ í™•ì¸ . plt.figure(figsize=(16, 6)) sns.kdeplot(data = post_to_fund_notna, x=\"post_to_funded\", color='#C88686'); . â†’ outlierë¡œ ì¸í•´ ê°’ì´ ëª°ë ¤ ìˆëŠ” ë¶€ë¶„ì„ êµ¬ì²´ì ìœ¼ë¡œ í™•ì¸í•˜ê¸° ì–´ë ¤ìš°ë¯€ë¡œ, ì£¼ìš” êµ¬ê°„ì„ í™•ëŒ€í•´ì„œ í™•ì¸: . # Q3 + 1.5IQR = 47.5 plt.figure(figsize=(20, 6)) sns.countplot(data = post_to_fund_notna[post_to_fund_notna['post_to_funded'] &lt; 50], x=\"post_to_funded\", palette='pink'); . | ì£¼ë¡œ funding ì™„ë£Œê¹Œì§€ 5ì¼ ì •ë„ ê±¸ë¦¬ëŠ” ê²½ìš°ê°€ ë§ìœ¼ë©°, ì „ì²´ì˜ 50% ì´ìƒì´ 9ì¼ ì•ˆì— fundingì´ ì™„ë£Œë¨ | . | ë¶„ê¸°ë³„ í‰ê·  posted ~ funded ê¸°ê°„ . plt.figure(figsize=(16, 6)) sns.barplot(data = post_to_fund_notna[post_to_fund_notna['posted_quarter'] != '2017Q3'], x='posted_quarter', y='post_to_funded', palette='pink'); . | íŠ¹íˆ Q1ì— post í›„ funding ì™„ë£Œê¹Œì§€ì˜ ê¸°ê°„ì´ í‰ê· ì ìœ¼ë¡œ ì§§ì€ ê²½í–¥ì„ ë³´ì„ | . | sectorë³„ í‰ê·  posted ~ funded ê¸°ê°„ . plt.figure(figsize=(16, 6)) sns.barplot(data = post_to_fund_notna, x='sector', y='post_to_funded', palette='pink') plt.xticks(rotation=40); . | Arts, Manufacturing, Education, Personal Use ì„¹í„°ê°€ íŠ¹íˆ post í›„ funding ì™„ë£Œê¹Œì§€ì˜ ê¸°ê°„ì´ ë¹„êµì  ì§§ì€ ê²ƒìœ¼ë¡œ ë³´ì„ | . | funded_amountì™€ posted ~ funded ê¸°ê°„ì˜ ìƒê´€ê´€ê³„ í™•ì¸ . plt.figure(figsize=(12, 6)) sns.scatterplot(data=post_to_fund_notna, x='funded_amount', y='post_to_funded', color='#C88686'); . +) ìƒê´€ê³„ìˆ˜ ê³„ì‚°: . # í”¼ì–´ìŠ¨ ìƒê´€ê³„ìˆ˜ ê²€ì • corr = stats.pearsonr(post_to_fund_notna['post_to_funded'], post_to_fund_notna['funded_amount']) print('Corr_Coefficient : %.3f \\np-value : %.3f' % (corr)) . Corr_Coefficient : 0.140 p-value : 0.000 . | funded_amountì™€ funding ì™„ë£Œë˜ê¸°ê¹Œì§€ì˜ ê¸°ê°„ ì‚¬ì´ì— ê´€ê³„ê°€ ìˆì„ ìˆ˜ ìˆë‹¤ê³  ìƒê°í–ˆìœ¼ë‚˜, ë³„ë¡œ ê´€ê³„ê°€ ì—†ëŠ” ê²ƒìœ¼ë¡œ í™•ì¸ë¨ | . | . ",
    "url": "https://chaelist.github.io/docs/kaggle/kiva_crowdfunding/#%EC%A1%B0%EA%B1%B4%EB%B3%84-%EB%B6%84%ED%8F%AC-%ED%99%95%EC%9D%B8",
    "relUrl": "/docs/kaggle/kiva_crowdfunding/#ì¡°ê±´ë³„-ë¶„í¬-í™•ì¸"
  },"91": {
    "doc": "Kiva Crowdfunding 2",
    "title": "Kiva Crowdfunding 2",
    "content": ". | ë°ì´í„° ì •ë¦¬ | Funds for Philippines . | ë¶„ê¸°ë³„ ì¶”ì´ | borrower ì„±ë³„ | borrower ì§€ì—­ | repayment interval | ì£¼ìš” sector, activity, use | â€˜useâ€™ ë‹¨ì–´ ë¹ˆë„ ë¶„ì„ | . | ë°ì´í„° ê²°í•©í•´ ë¶„ì„ . | ë°ì´í„° ì •ë¦¬ | kivaë§Œì„ ìœ„í•œ loan ë¹„ì¤‘ í™•ì¸ | country, sectorë³„ forkiva ë¹„ì¤‘ | Field Partnerì™€ì˜ í˜‘ì—… ê´€ê³„ í™•ì¸ | êµ­ê°€ë³„ funded amountì™€ MPI | . | . *ë¶„ì„ ëŒ€ìƒ ë°ì´í„°ì…‹: Kiva Crowdfunding Data . | ë°ì´í„°ì…‹ ì¶œì²˜ | kiva.org: ì„¸ê³„ ê°êµ­ì˜ ê²½ì œì ìœ¼ë¡œ ì–´ë ¤ìš´ ì‚¬ëŒë“¤ì—ê²Œ ëˆì„ ë¹Œë ¤ì£¼ëŠ” online crowdfunding platform | kiva_loans.csv: 2014.01.01~2017.07.26 ì‚¬ì´ì— kivaì— ì˜¬ë¼ì˜¨ loan ì •ë³´ ë°ì´í„°. (671,205ê°œì˜ row) | kiva_mpi_region_locations.csv: 2,772ê°œì˜ row. ê° ì§€ì—­ì˜ geolocation &amp; poverty level ì •ë³´ | loan_theme_ids.csv: 779,092ê°œì˜ row. ê° loanì˜ themeì— ëŒ€í•œ ì •ë³´. | loan_theme_by_region.csv: 15,736ê°œì˜ row. ê° loan themeê³¼ ì—°ê²°ëœ regional ì •ë³´ë“¤ | . ",
    "url": "https://chaelist.github.io/docs/kaggle/kiva_crowdfunding2/",
    "relUrl": "/docs/kaggle/kiva_crowdfunding2/"
  },"92": {
    "doc": "Kiva Crowdfunding 2",
    "title": "ë°ì´í„° ì •ë¦¬",
    "content": "# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ import import pandas as pd import numpy as np from matplotlib import pyplot as plt import seaborn as sns import scipy.stats as stats . kiva_df = pd.read_csv('data/kiva_loans.csv') kiva_df.head(3) . | Â  | id | funded_amount | loan_amount | activity | sector | use | country_code | country | region | currency | partner_id | posted_time | disbursed_time | funded_time | term_in_months | lender_count | tags | borrower_genders | repayment_interval | date | . | 0 | 653051 | 300 | 300 | Fruits &amp; Vegetables | Food | To buy seasonal, fresh fruits to sell. | PK | Pakistan | Lahore | PKR | 247 | 2014-01-01 06:12:39+00:00 | 2013-12-17 08:00:00+00:00 | 2014-01-02 10:06:32+00:00 | 12 | 12 | nan | female | irregular | 2014-01-01 | . | 1 | 653053 | 575 | 575 | Rickshaw | Transportation | to repair and maintain the auto rickshaw used in their business. | PK | Pakistan | Lahore | PKR | 247 | 2014-01-01 06:51:08+00:00 | 2013-12-17 08:00:00+00:00 | 2014-01-02 09:17:23+00:00 | 11 | 14 | nan | female, female | irregular | 2014-01-01 | . | 2 | 653068 | 150 | 150 | Transportation | Transportation | To repair their old cycle-van and buy another one to rent out as a source of income | IN | India | Maynaguri | INR | 334 | 2014-01-01 09:58:07+00:00 | 2013-12-17 08:00:00+00:00 | 2014-01-01 16:01:36+00:00 | 43 | 6 | user_favorite, user_favorite | female | bullet | 2014-01-01 | . | 3 | 653063 | 200 | 200 | Embroidery | Arts | to purchase an embroidery machine and a variety of new embroidery materials. | PK | Pakistan | Lahore | PKR | 247 | 2014-01-01 08:03:11+00:00 | 2013-12-24 08:00:00+00:00 | 2014-01-01 13:00:00+00:00 | 11 | 8 | nan | female | irregular | 2014-01-01 | . | 4 | 653084 | 400 | 400 | Milk Sales | Food | to purchase one buffalo. | PK | Pakistan | Abdul Hakeem | PKR | 245 | 2014-01-01 11:53:19+00:00 | 2013-12-17 08:00:00+00:00 | 2014-01-01 19:18:51+00:00 | 14 | 16 | nan | female | monthly | 2014-01-01 | . | funded_amount: amount disbursed by Kiva to the field agent (USD) | loan_amount: amount disbursed by the field agent to the borrower . | funded_amount &lt; loan_amountì¸ ê²½ìš°: 48328ê±´ | funded_amount &gt; loan_amountì¸ ê²½ìš°: 2ê±´ | . | country_code: ISO country code | posted_time: the time at which the loan is posted on Kiva by the field agent | disbursed_time: the time at which the loan is disbursed by the field agent to the borrower | funded_time: the time at which the loan posted to Kiva gets funded by lenders completely . | nullê°’: 48331ê°œ (funded_amount &gt;= loan_amountì¸ ê²½ìš° ì¤‘ 3ê±´ë„ funded_timeì´ nullë¡œ ë‚˜ì˜´â€¦) | . | term_in_months: the duration for which the loan was disbursed in months | date: posted_timeì—ì„œ dateë§Œ ì¶”ì¶œí•œ ê²ƒ. (Kivaì— ì˜¬ë¼ì˜¨ ë‚ ì§œ) | . | datetime ë°ì´í„° ì •ë¦¬ . # datetime í˜•ì‹ ë³€í™˜ kiva_df['posted_time'] = pd.to_datetime(kiva_df['posted_time']) kiva_df['disbursed_time'] = pd.to_datetime(kiva_df['disbursed_time']) kiva_df['funded_time'] = pd.to_datetime(kiva_df['funded_time']) # posted_time -&gt; Month, Quarter ë‹¨ìœ„ë¡œ ì €ì¥í•´ë‘  kiva_df['posted_month'] = kiva_df['posted_time'].dt.strftime('%Y%m') kiva_df['posted_quarter'] = kiva_df['posted_time'].dt.to_period(\"Q\").astype('str') . | borrower type ì •ë¦¬ . ## borrowerë¥¼ ì„±ë³„ / ë‹¨ì²´ ìœ ë¬´ë¡œ ì •ë¦¬ def borrower_type(x): if type(x) != str: borrower = 'N/A' elif x == 'female': borrower = 'female' elif x == 'male': borrower = 'male' else: borr_set = set(x.split(', ')) if 'female' in borr_set: if 'male' in borr_set: borrower = 'mixed_group' else: borrower = 'female_group' else: borrower = 'male_group' return borrower kiva_df['borrower_type'] = kiva_df['borrower_genders'].apply(lambda x: borrower_type(x)) . | . ",
    "url": "https://chaelist.github.io/docs/kaggle/kiva_crowdfunding2/#%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%A0%95%EB%A6%AC",
    "relUrl": "/docs/kaggle/kiva_crowdfunding2/#ë°ì´í„°-ì •ë¦¬"
  },"93": {
    "doc": "Kiva Crowdfunding 2",
    "title": "Funds for Philippines",
    "content": ": kivaë¥¼ í†µí•´ ë°›ì€ funding ê¸ˆì•¡ì´ ê°€ì¥ ë§ì€ êµ­ê°€ì¸ Philippinesì— ëŒ€í•´ ì§‘ì¤‘ íƒêµ¬ . # philippines ë°ì´í„°ë§Œ ë”°ë¡œ ì¶”ì¶œ ph_df = kiva_df[kiva_df['country'] == 'Philippines'] ph_df.info() . &lt;class 'pandas.core.frame.DataFrame'&gt; Int64Index: 160441 entries, 51 to 670445 Data columns (total 23 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 id 160441 non-null int64 1 funded_amount 160441 non-null float64 2 loan_amount 160441 non-null float64 3 activity 160441 non-null object 4 sector 160441 non-null object 5 use 160361 non-null object 6 country_code 160441 non-null object 7 country 160441 non-null object 8 region 160360 non-null object 9 currency 160441 non-null object 10 partner_id 160441 non-null float64 11 posted_time 160441 non-null datetime64[ns, UTC] 12 disbursed_time 160441 non-null datetime64[ns, UTC] 13 funded_time 157607 non-null datetime64[ns, UTC] 14 term_in_months 160441 non-null float64 15 lender_count 160441 non-null int64 16 tags 94094 non-null object 17 borrower_genders 160361 non-null object 18 repayment_interval 160441 non-null object 19 date 160441 non-null object 20 posted_quarter 160441 non-null object 21 borrower_type 160441 non-null object 22 posted_month 160441 non-null object dtypes: datetime64[ns, UTC](3), float64(4), int64(2), object(14) memory usage: 29.4+ MB . ë¶„ê¸°ë³„ ì¶”ì´ . | ë¶„ê¸°ë³„ loan count . plt.figure(figsize=(16, 6)) # 2017Q3ì€ 2017.07.26ê¹Œì§€ì˜ ê¸°ë¡ë°–ì— ì—†ìœ¼ë¯€ë¡œ ì œì™¸í•˜ê³  ì‹œê°í™” sns.lineplot(data=ph_df[ph_df['posted_quarter'] != '2017Q3'], x='posted_quarter', y='funded_amount', estimator='count', ci=None, color='#C88686') plt.ylim(5000, 18000); . | ë¶„ê¸°ë³„ í‰ê·  funded_amount . plt.figure(figsize=(16, 6)) sns.lineplot(data=ph_df[ph_df['posted_quarter'] != '2017Q3'], x='posted_quarter', y='funded_amount', color='#C88686') plt.ylim(250, 450); . | ë¶„ê¸°ë³„ ì´ funded_amount . plt.figure(figsize=(16, 6)) sns.lineplot(data=ph_df[ph_df['posted_quarter'] != '2017Q3'], x='posted_quarter', y='funded_amount', estimator='sum', ci=None, color='#C88686') plt.ylim(1500000, 5500000); . | 2014ë…„ ì´í›„ postë‹¹ í‰ê·  funded_amountëŠ” ì¤„ê³  ìˆì§€ë§Œ, ì˜¬ë¼ì˜¨ loan post ìˆ˜ ìì²´ê°€ ê³„ì† ì¦ê°€ì„¸ë¥¼ ë³´ì„ì— ë”°ë¼ ì´ funded_amountë„ ì¦ê°€ ì¶”ì„¸. | . | . borrower ì„±ë³„ . | borrower typeë³„ loan count . sns.countplot(data=ph_df, x='borrower_type', palette='pink'); . | ì—¬ëŸ¬ borrowerê°€ ê·¸ë£¹ìœ¼ë¡œ ë¹Œë¦¬ëŠ” ê²½ìš°ëŠ” ì—†ìŒ (female_group, male_group, mixed_group) | female í˜¼ì ë¹Œë¦¬ëŠ” ê²½ìš°ê°€ ì••ë„ì  (ì „ì²´ loanì˜ ì•½ 95%) | . ph_df.groupby('borrower_type')['funded_amount'].count() . borrower_type N/A 80 female 151984 male 8377 Name: funded_amount, dtype: int64 . | borrower typeë³„ í‰ê·  funded amount . sns.barplot(data=ph_df, x='borrower_type', y='funded_amount', palette='pink'); . | í‰ê·  funded amountëŠ” ì˜¤íˆë ¤ femaleì´ ë§ì€ í¸ì€ ì•„ë‹˜ | . | ë¶„ê¸°ë³„ ì´ funded amount ë¹„êµ . plt.figure(figsize=(16, 6)) sns.lineplot(data=ph_df[ph_df['posted_quarter'] != '2017Q3'], x='posted_quarter', y='funded_amount', hue='borrower_type', estimator='sum', ci=None, palette='pink'); . | ì´ funded amount ìì²´ë„ femaleì´ ëª¨ë“  ê¸°ê°„ì— ê°€ì¥ ì••ë„ì . | female borrowerë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ funded_amountê°€ í¬ê²Œ ì¦ê°€ì„¸ | . | . borrower ì§€ì—­ . | funded_amount Top 10 ì§€ì—­ . ph_region = ph_df.groupby('region')[['funded_amount']].sum().reset_index() ph_region.sort_values(by='funded_amount', ascending=False, inplace=True) # Top 10ê¹Œì§€ë§Œ ì‹œê°í™” plt.figure(figsize=(7, 5)) sns.barplot(data=ph_region.head(10), x='funded_amount', y='region', palette='pink'); . | Palawanê³¼ Negros Occidentalì˜ ì§€ì—­ë“¤ì´ ìƒìœ„ê¶Œì— ë§ì´ í¬ì§„ë˜ì–´ ìˆìŒ | . Â  . | regionì˜ ë’·ë¶€ë¶„ì„ ë”°ë¡œ â€˜provinceâ€™ë¡œ ì €ì¥ . | regionì— ì íŒ êµ¬ì¡°ê°€ ëŒ€ì²´ë¡œ â€˜Narra, Palawanâ€™ì²˜ëŸ¼ â€˜municipality, provinceâ€™ì˜ êµ¬ì¡°ë¡œ ë˜ì–´ ìˆë‹¤ê³  ìƒê°í•´, ì‰¼í‘œ(,) ë’·ë¶€ë¶„ì„ ë¶„ë¦¬í•´ì„œ ì €ì¥ | . # ì‰¼í‘œê°€ ì—†ëŠ” regionëª…ì´ ëª‡ ê°œì¸ì§€ í™•ì¸ print(len(ph_region)) print(len(ph_region[ph_region['region'].str.contains(',')])) . 3638 3249 . â†’ ì‰¼í‘œë¡œ ë¶„ë¦¬ëœ regionëª…ì´ ëŒ€ë¶€ë¶„ì´ë¯€ë¡œ, ì‰¼í‘œë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë‚˜ëˆ„ê¸°ë¡œ í•¨. # ì‰¼í‘œ(,)ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë’·ë¶€ë¶„ ë°˜ì„ provinceë¡œ ì €ì¥í•˜ê³ , ì‰¼í‘œ ì—†ì´ í•˜ë‚˜ê°€ í†µì§¸ë¡œ regionëª…ì¸ ê²½ìš° ê·¸ëŒ€ë¡œ provinceì— ì €ì¥. ph_region_temp = ph_region['region'].str.rsplit(',', 1, expand=True) for i in ph_region_temp.index: if ph_region_temp.loc[i, 1]: ## 1ì´ Noneì´ ì•„ë‹ˆë©´ ì‹¤í–‰ë¨ (Noneì´ë©´ elseë¡œ) ph_region.loc[i, 'province'] = ph_region_temp.loc[i, 1] else: ph_region.loc[i, 'province'] = ph_region_temp.loc[i, 0] ph_region.head() . | Â  | region | funded_amount | province | . | 2000 | Narra, Palawan | 1234625.0 | Palawan | . | 2556 | Quezon, Palawan | 1210875.0 | Palawan | . | 583 | Brookes Point, Palawan | 1180100.0 | Palawan | . | 1381 | Kabankalan, Negros Occidental | 1040700.0 | Negros Occidental | . | 1282 | Hinigaran, Negros Occidental | 956950.0 | Negros Occidental | . | provinceë³„ funded_amount ì •ë¦¬, ë¹„êµ . ph_province = ph_region.groupby('province')[['funded_amount']].sum().reset_index() ph_province.sort_values(by='funded_amount', ascending=False, inplace=True) # Top 10ê¹Œì§€ë§Œ ì‹œê°í™” plt.figure(figsize=(7, 5)) sns.barplot(data=ph_province.head(10), x='funded_amount', y='province', palette='pink'); . | Negros Occidentalì´ ê°€ì¥ ì••ë„ì  | . | ê° provinceë³„ë¡œ, ì–´ë–¤ sectorì˜ funded_amountê°€ ë§ì€ì§€ í™•ì¸ . # ph_dfì— 'province' ì¹¼ëŸ¼ë¥¼ ë”í•´ì¤Œ ph_df = pd.merge(ph_df, ph_region[['region', 'province']], how='left', on='region') . # funded amount Top 10 provinceë§Œ ë”°ë¡œ ì €ì¥ top_countries = ph_df.groupby('province')[['funded_amount']].sum().sort_values(by='funded_amount', ascending=False).head(10).index # funded amount ë‚´ë¦¼ì°¨ìˆœìœ¼ë¡œ sector ìˆœì„œ ì •ë ¬ sectors_order = ph_df.groupby('sector')[['funded_amount']].sum().sort_values(by='funded_amount', ascending=False).index loans_province_sector = pd.pivot_table(ph_df, index='province', columns='sector', values='funded_amount', fill_value=0, aggfunc='sum') loans_province_sector = loans_province_sector[loans_province_sector.index.isin(top_countries)] # top 10 provinceë§Œ ëŒ€ìƒìœ¼ë¡œ ìë¦„ loans_province_sector = loans_province_sector[sectors_order] # ì „ì²´ loan amountê°€ ê°€ì¥ ë†’ì€ sectorë¶€í„° ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬ loans_province_sector = loans_province_sector.T loans_province_sector = loans_province_sector[top_countries] # ì „ì²´ loan amountê°€ ê°€ì¥ ë†’ì€ provinceë¶€í„° ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬ # heatmapìœ¼ë¡œ ì‹œê°í™” plt.figure(figsize=(14, 9)) sns.heatmap(loans_province_sector, annot=True, cmap='pink_r', fmt='.0f') plt.xticks(rotation=40); . | Negros Occidentalì€ Retail sectorë¡œ ë¹Œë¦° ê¸ˆì•¡ì´ ê°€ì¥ ë§ìŒ | Palawanì€ Retailê³¼ Food sectorê°€ ë¹„ìŠ·í•˜ê²Œ ë§ìŒ | Negros Occidentalê³¼ Negros Orientalì€ Agriculture sectorë¡œ ë¹Œë¦° ê¸ˆì•¡ë„ ë§ì€ í¸ | . | . repayment interval . | repayment intervalë³„ ì´ funded_amount . sns.barplot(data=ph_df, x='repayment_interval', y='funded_amount', estimator=np.sum, ci=None, palette='pink'); . | repayment intervalë³„ í‰ê·  funded_amount . sns.barplot(data=ph_df, x='repayment_interval', y='funded_amount', palette='pink'); . | repayment intervalë³„ loan ìˆ˜ . sns.countplot(data=ph_df, x='repayment_interval', palette='pink'); . | irregular ìƒí™˜ ë°©ì‹ì˜ loanì´ ê°€ì¥ ë§ìŒ | irregular ìƒí™˜ ë°©ì‹ì˜ loanì´ í‰ê·  funded amountê°€ ë§ì€ í¸ì€ ì•„ë‹ˆì§€ë§Œ, ìˆ˜ ìì²´ê°€ ì••ë„ì ì´ê¸° ë•Œë¬¸ì— ì „ì²´ funded amount ì¤‘ irregular ìƒí™˜ ë°©ì‹ì¸ ê¸ˆì•¡ì´ ì••ë„ì  | ì „ì²´ kiva loan ì¤‘ì—ì„œëŠ” monthlyë¡œ ìƒí™˜ë˜ëŠ” ê¸ˆì•¡ì´ ê°€ì¥ ë§ì€ ê²ƒì„ ìƒê°í•˜ë©´, irregular ìƒí™˜ ê¸ˆì•¡ì´ ë§ë‹¤ëŠ” ê²ƒì€ Philippinesì˜ íŠ¹ì§•ì¸ ë“¯ | . | ë¶„ê¸°ë³„ repayment intervalë³„ funded amount . quarter_interval = ph_df.groupby(['posted_quarter', 'repayment_interval'])[['funded_amount']].sum().reset_index() quarter_interval = quarter_interval[quarter_interval['posted_quarter'] != '2017Q3'] plt.figure(figsize=(16, 6)) sns.lineplot(data=quarter_interval, x='posted_quarter', y='funded_amount', hue='repayment_interval', palette='pink', hue_order=['irregular', 'monthly', 'bullet']) # legendë¥¼ box ë°–ìœ¼ë¡œ ë¹¼ ì¤Œ plt.legend(bbox_to_anchor=(1.01, 1), borderaxespad=0); . | irregular ìƒí™˜ ë°©ì‹ì˜ ê¸ˆì•¡ì´ ê°€ì¥ ë§ì´ ì¦ê°€í•˜ëŠ” ì¶”ì„¸ | irregular ìƒí™˜ë³´ë‹¤ëŠ” monthly ìƒí™˜ì´ ë” ì•ˆì •ì ì¸ ìê¸ˆ ê³„íšì— ìœ ë¦¬í•˜ë¯€ë¡œ, irregular ìƒí™˜ ë°©ì‹ë³´ë‹¤ëŠ” monthly ìƒí™˜ ë°©ì‹ì„ ëŠ˜ë¦¬ëŠ” ê²ƒì´ ì¢‹ë‹¤ê³  ìƒê°ë¨ | . | . ì£¼ìš” sector, activity, use . | sectorë³„ funded amount . # funded amount ìˆœìœ¼ë¡œ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬í•´ ì‹œê°í™” ph_sectors = ph_df.groupby('sector')[['funded_amount']].sum().sort_values(by='funded_amount', ascending=False).reset_index() plt.figure(figsize=(12, 5)) sns.barplot(data=ph_sectors, x='sector', y='funded_amount', palette='pink') plt.xticks(rotation=40); . | Retail, Food, Agriculture ê´€ë ¨ funded amountê°€ ê°€ì¥ ë§ìŒ | . | funded amount Top 10 activities . ph_activities = ph_df.groupby(['activity', 'sector'])[['funded_amount']].sum().sort_values(by='funded_amount', ascending=False).reset_index() ph_activities.head(10) . | Â  | activity | sector | funded_amount | . | 0 | General Store | Retail | 15025550.0 | . | 1 | Pigs | Agriculture | 6039975.0 | . | 2 | Farming | Agriculture | 4063150.0 | . | 3 | Fishing | Food | 3129675.0 | . | 4 | Fish Selling | Food | 2969850.0 | . | 5 | Food Production/Sales | Food | 2659425.0 | . | 6 | Personal Housing Expenses | Housing | 1710425.0 | . | 7 | Fruits &amp; Vegetables | Food | 1563675.0 | . | 8 | Retail | Retail | 1366075.0 | . | 9 | Motorcycle Transport | Transportation | 1067450.0 | . â†’ Top 10 activities ì‹œê°í™” . plt.figure(figsize=(7, 5)) sns.barplot(data=ph_activities.head(10), x='funded_amount', y='activity', palette='pink'); . | â€˜Retailâ€™ sectorì˜ â€˜General Storeâ€™ ëª©ì ì˜ funded amountê°€ ì••ë„ì ìœ¼ë¡œ ë§ê³ , ê·¸ ë‹¤ìŒì€ ì˜¤íˆë ¤ â€˜Agricultureâ€™ sectorì˜ â€˜Pigsâ€™ì™€ â€˜Farmingâ€™ ëª©ì ì˜ funded amountê°€ ë§ìŒ | ì—¬ëŸ¬ ì„¬ìœ¼ë¡œ êµ¬ì„±ëœ êµ­ê°€ì¸ë§Œí¼, â€˜Foodâ€™ sector ì¤‘ì—ì„œëŠ” â€˜Fishingâ€™ê³¼ â€˜Fish Sellingâ€™ ëª©ì ì˜ funded amountê°€ ë§ìŒ | . | funded amount Top 10 use . ph_use = ph_df.groupby(['use'])[['funded_amount']].sum().sort_values(by='funded_amount', ascending=False).reset_index() plt.figure(figsize=(7, 5)) sns.barplot(data=ph_use.head(10), x='funded_amount', y='use', palette='pink'); . | ê°™ì€ ë‚´ìš©ì´ë¼ë„ ì‚¬ìš©í•œ ë‹¨ì–´ì— ë”°ë¼ ë‹¤ë¥´ê²Œ ì§‘ê³„ë˜ë¯€ë¡œ ì •í™•íˆ íŒŒì•…í•˜ê¸° ì–´ë µì§€ë§Œ, â€˜groceryâ€™, â€˜pigâ€™, â€˜food production businessâ€™ ë“±ì˜ ë‹¨ì–´ë“¤ì´ ëˆˆì— ë” | . | â€˜General Storeâ€™ ê´€ë ¨ Top 10 use . # ìš°ì„ , activity, sector, useë³„ funded amountë¥¼ ì •ë¦¬ ph_use_activity = ph_df.groupby(['use', 'activity', 'sector'])[['funded_amount']].sum().sort_values(by='funded_amount', ascending=False).reset_index() ph_use_activity.head() . | Â  | use | activity | sector | funded_amount | . | 0 | to purchase more groceries to sell. | General Store | Retail | 501400.0 | . | 1 | to buy feed and other supplies to raise her pigs. | Pigs | Agriculture | 431850.0 | . | 2 | to buy fertilizers and other farm supplies. | Farming | Agriculture | 427725.0 | . | 3 | to buy feed and vitamins for her pigs. | Pigs | Agriculture | 374800.0 | . | 4 | to buy ingredients for her food production business | Food Production/Sales | Food | 344125.0 | . â†’ â€˜General Storeâ€™ Top 10 use ì‹œê°í™” . plt.figure(figsize=(7, 5)) sns.barplot(data=ph_use_activity[ph_use_activity['activity'] == 'General Store'].head(10), x='funded_amount', y='use', palette='pink'); . | ê°™ì€ ë‚´ìš©ì´ë¼ë„ ì‚¬ìš©í•œ ë‹¨ì–´ì— ë”°ë¼ ë‹¤ë¥´ê²Œ ì§‘ê³„ë˜ë¯€ë¡œ ì •í™•íˆ íŒŒì•…í•˜ê¸° ì–´ë µì§€ë§Œ, General Storeë¥¼ ìœ„í•œ fundë¡œëŠ” â€˜grocery êµ¬ë§¤â€™ ê´€ë ¨ ëª©ì ì´ ë§ë‹¤ê³  ìƒê°ë¨ | . | . â€˜useâ€™ ë‹¨ì–´ ë¹ˆë„ ë¶„ì„ . | â€˜useâ€™ëŠ” ê°™ì€ ë‚´ìš©ì´ë¼ë„ ë‹¨ì–´ ì‚¬ìš© ë“±ì— ë”°ë¼ ë‹¤ë¥´ê²Œ ì§‘ê³„ë˜ë¯€ë¡œ, ë‹¨ìˆœ ì§‘ê³„ í›„ ì‹œê°í™”ë³´ë‹¤ ë‹¨ì–´ë³„ë¡œ ìª¼ê°œì„œ íŒŒì•…í•˜ëŠ” ê²ƒì´ ë„ì›€ì´ ë  ìˆ˜ ìˆë‹¤ê³  ìƒê° | ì‚¬ìœ ë¥¼ ë‹¨ì–´ë³„ë¡œ ë‚˜ëˆ ì„œ ë¶„ì„í•´, ì–´ë–¤ ë‹¨ì–´ê°€ ë¹ˆë²ˆí•˜ê²Œ ë“±ì¥í•˜ëŠ”ì§€ í™•ì¸í•˜ê³ , ì´ë¡œë¶€í„° í•„ë¦¬í•€ì˜ ì „ë°˜ì ì¸ loan needsë¥¼ íŒŒì•… | . ## ë‹¨ì–´ ë¹ˆë„ ê³„ì‚° &amp; wordcloud ìƒì„± &amp; ë¹ˆë²ˆí•˜ê²Œ ì‚¬ìš©ëœ ë‹¨ì–´ bar graphë¡œ ì‹œê°í™”í•˜ëŠ” ì½”ë“œë¥¼ ë¯¸ë¦¬ í•¨ìˆ˜ë¡œ ë‹¤ ë§Œë“¤ì–´ ë‘  ## ê° loanì˜ funded amountëŠ” ê³ ë ¤í•˜ì§€ ì•Šê³ , countë§Œ ê³ ë ¤ import nltk from wordcloud import WordCloud from collections import Counter # wordcloud mask x, y = np.ogrid[:1200, :1200] mask = (x - 600) ** 2 + (y - 600) ** 2 &gt; 520 ** 2 mask = 255 * mask.astype(int) def count_words(sector_name): # Text Cleaning &amp; Case Conversion &amp; Tokenization words = [] for content in ph_df[ph_df['sector'] == sector_name]['use']: content_cleaned = str(content).replace('.', '').replace(',','').strip().lower() # ., ì •ë„ë§Œ ì œê±°, strip, lower words.extend(content_cleaned.split()) # ê°„ë‹¨í•˜ê²Œ ê³µë°± ê¸°ì¤€ìœ¼ë¡œë§Œ ìª¼ê°œì¤Œ # POS Tagging tokens_pos = nltk.pos_tag(words) # Noun, Verbë§Œ ì¶”ì¶œ &amp; Lemmatization wlem = nltk.WordNetLemmatizer() lemm_words = [] for word, pos in tokens_pos: if ('NN' in pos) | ('VB' in pos): lemm_words.append(wlem.lemmatize(word)) # Counterë¡œ ë¹ˆë„ ê³„ì‚° c = Counter(lemm_words) return c def generate_wordcloud(c): # generate wordcloud wordcloud = WordCloud(relative_scaling=.2, background_color='white', colormap='pink', mask=mask).generate_from_frequencies(c) plt.figure(figsize=(7, 7)) plt.imshow(wordcloud) plt.axis('off') def visualize_top10_words(c): word_count = pd.DataFrame.from_dict(c, orient='index', columns=['count']).reset_index().rename(columns={'index':'word'}) word_count.sort_values(by='count', ascending=False, inplace=True) plt.figure(figsize=(6, 4)) sns.barplot(data=word_count.head(10), x='count', y='word', palette='pink') . | Retail sectorì˜ use . retail_c = count_words('Retail') generate_wordcloud(retail_c) . | canned goods, grocery ë“±ì˜ itemë“¤ì„ ì‚¬ê¸° ìœ„í•œ ëª©ì ì´ ë§ë‹¤ê³  íŒŒì•…ë¨ | . visualize_top10_words(retail_c) . | Food sectorì˜ use . food_c = count_words('Food') generate_wordcloud(food_c) . | â€˜foodâ€™ sectorë„ ë‹¨ìˆœíˆ ìì‹ ì˜ ì‹ëŸ‰ì„ ì‚¬ê¸° ìœ„í•œ ê²ƒë³´ë‹¤, â€˜food production businessâ€™ì˜ ì¬ë£Œ êµ¬ë§¤ &amp; íŒë§¤í•  ì‹ë£Œí’ˆ êµ¬ë§¤ ëª©ì ì´ ë§ë‹¤ê³  íŒë‹¨ë¨ | . visualize_top10_words(food_c) . | Agriculture sectorì˜ use . agri_c = count_words('Agriculture') generate_wordcloud(agri_c) . | pigë¥¼ ìœ„í•œ ì‹ëŸ‰ ë“±ì„ êµ¬ë§¤í•˜ëŠ” ëª©ì ì´ ë§ê³ , ê·¸ ë‹¤ìŒìœ¼ë¡œëŠ” ë†ì—…ì„ ìœ„í•œ fertilizer ë“±ì˜ êµ¬ë§¤ ëª©ì ë„ ë§ë‹¤ê³  íŒë‹¨ë¨ | ë¼ì§€ ë†ì¥ì´ í•„ë¦¬í•€ì˜ ì£¼ìš” business ì¤‘ í•˜ë‚˜ë¼ê³  ì¶”ì •ë¨ | . visualize_top10_words(agri_c) . | . ",
    "url": "https://chaelist.github.io/docs/kaggle/kiva_crowdfunding2/#funds-for-philippines",
    "relUrl": "/docs/kaggle/kiva_crowdfunding2/#funds-for-philippines"
  },"94": {
    "doc": "Kiva Crowdfunding 2",
    "title": "ë°ì´í„° ê²°í•©í•´ ë¶„ì„",
    "content": "ë°ì´í„° ì •ë¦¬ . | kiva_mpi_region_locations.csv . location_df = pd.read_csv('data/kiva_mpi_region_locations.csv') location_df.head(3) . | Â  | LocationName | ISO | country | region | world_region | MPI | geo | lat | lon | . | 0 | Badakhshan, Afghanistan | AFG | Afghanistan | Badakhshan | South Asia | 0.387 | (36.7347725, 70.81199529999999) | 36.7348 | 70.812 | . | 1 | Badghis, Afghanistan | AFG | Afghanistan | Badghis | South Asia | 0.466 | (35.1671339, 63.7695384) | 35.1671 | 63.7695 | . | 2 | Baghlan, Afghanistan | AFG | Afghanistan | Baghlan | South Asia | 0.3 | (35.8042947, 69.2877535) | 35.8043 | 69.2878 | . | ISO: unique ID for country | MPI: multidimensional poverty index â€“ ìˆ˜ì¹˜ê°€ ë†’ì„ìˆ˜ë¡ poverty levelì´ ë†’ì€ ê²ƒ | . â†’ ì¤‘ë³µê°’ ì œê±° . print('ì¤‘ë³µ ì œê±° ì´ì „: ', len(location_df)) location_df.drop_duplicates(inplace=True, ignore_index=True) print('ì¤‘ë³µ ì œê±° ì´í›„: ', len(location_df)) . ì¤‘ë³µ ì œê±° ì´ì „: 2772 ì¤‘ë³µ ì œê±° ì´í›„: 1009 . â†’ LocationNameì´ nullê°’ì¸ í–‰ ì œì™¸ (LocationNameì´ nullì´ë©´ region, MPIë„ ë‹¤ nullì„) . print('ê²°ì¸¡ì¹˜ ì œê±° ì´ì „: ', len(location_df)) location_df = location_df[~location_df['LocationName'].isna()] location_df.reset_index(inplace=True, drop=True) print('ê²°ì¸¡ì¹˜ ì œê±° ì´í›„: ', len(location_df)) . ê²°ì¸¡ì¹˜ ì œê±° ì´ì „: 1009 ê²°ì¸¡ì¹˜ ì œê±° ì´í›„: 984 . | loan_theme_ids.csv . loan_themes_df = pd.read_csv('data/loan_theme_ids.csv') loan_themes_df.head(3) . | Â  | id | Loan Theme ID | Loan Theme Type | Partner ID | . | 0 | 638631 | a1050000000skGl | General | 151 | . | 1 | 640322 | a1050000000skGl | General | 151 | . | 2 | 641006 | a1050000002X1ij | Higher Education | 160 | . â†’ id ì œì™¸í•˜ê³  ë‹¤ nullê°’ì¸ ê²½ìš° ì‚­ì œ (Loan Theme IDê°€ nullì´ë©´ Loan Theme Type, Partner IDë„ ë‹¤ null) . print('ê²°ì¸¡ì¹˜ ì œê±° ì´ì „: ', len(loan_themes_df)) loan_themes_df = loan_themes_df[~loan_themes_df['Loan Theme ID'].isna()] loan_themes_df.reset_index(inplace=True, drop=True) print('ê²°ì¸¡ì¹˜ ì œê±° ì´í›„: ', len(loan_themes_df)) . | loan_themes_by_region.csv . themes_region_df = pd.read_csv('data/loan_themes_by_region.csv') themes_region_df.head(3) . | Â  | Partner ID | Field Partner Name | sector | Loan Theme ID | Loan Theme Type | country | forkiva | region | geocode_old | ISO | number | amount | LocationName | geocode | names | geo | lat | lon | mpi_region | mpi_geo | rural_pct | . | 0 | 9 | KREDIT Microfinance Institution | General Financial Inclusion | a1050000000slfi | Higher Education | Cambodia | No | Banteay Meanchey | (13.75, 103.0) | KHM | 1 | 450 | Banteay Meanchey, Cambodia | [(13.6672596, 102.8975098)] | Banteay Meanchey Province; Cambodia | (13.6672596, 102.8975098) | 13.6673 | 102.898 | Banteay Mean Chey, Cambodia | (13.6672596, 102.8975098) | 90 | . | 1 | 9 | KREDIT Microfinance Institution | General Financial Inclusion | a10500000068jPe | Vulnerable Populations | Cambodia | No | Battambang Province | nan | KHM | 58 | 20275 | Battambang Province, Cambodia | [(13.0286971, 102.989615)] | Battambang Province; Cambodia | (13.0286971, 102.989615) | 13.0287 | 102.99 | Banteay Mean Chey, Cambodia | (13.6672596, 102.8975098) | 90 | . | 2 | 9 | KREDIT Microfinance Institution | General Financial Inclusion | a1050000000slfi | Higher Education | Cambodia | No | Battambang Province | nan | KHM | 7 | 9150 | Battambang Province, Cambodia | [(13.0286971, 102.989615)] | Battambang Province; Cambodia | (13.0286971, 102.989615) | 13.0287 | 102.99 | Banteay Mean Chey, Cambodia | (13.6672596, 102.8975098) | 90 | . | forkiva: Was this loan theme created specifically for Kiva? | geocode_old: ì˜›ë‚  geocoding system | ISO: unique ID for country | number: Number of loans funded in this LocationName and this loan theme | amount: Dollar value of loans funded in this LocationName and this loan theme | LocationName: â€œ{region}, {country}â€ | names: All placenames that the Gmaps API associates with LocationName | mpi_region: MPI Region where we think this loan theme is located | mpi_geo: Lat-Lon pair where we think this MPI region is located | rural_pct: The percentage of this field partnersâ€™ borrowers that are in rural areas | . | . kivaë§Œì„ ìœ„í•œ loan ë¹„ì¤‘ í™•ì¸ . | kiva_dfì™€ loan_themes_dfë¥¼ merge . kiva_loan_themes = kiva_df.copy() kiva_loan_themes.drop(['loan_amount', 'country_code', 'disbursed_time', 'tags', 'date', 'borrower_genders'], axis='columns', inplace=True) kiva_loan_themes = pd.merge(kiva_loan_themes, loan_themes_df, how='left', on='id') kiva_loan_themes.head(3) . | Â  | id | funded_amount | activity | sector | use | country | region | currency | partner_id | posted_time | funded_time | term_in_months | lender_count | repayment_interval | posted_month | posted_quarter | borrower_type | Loan Theme ID | Loan Theme Type | Partner ID | . | 0 | 653051 | 300 | Fruits &amp; Vegetables | Food | To buy seasonal, fresh fruits to sell. | Pakistan | Lahore | PKR | 247 | 2014-01-01 06:12:39+00:00 | 2014-01-02 10:06:32+00:00 | 12 | 12 | irregular | 201401 | 2014Q1 | female | nan | nan | nan | . | 1 | 653053 | 575 | Rickshaw | Transportation | to repair and maintain the auto rickshaw used in their business. | Pakistan | Lahore | PKR | 247 | 2014-01-01 06:51:08+00:00 | 2014-01-02 09:17:23+00:00 | 11 | 14 | irregular | 201401 | 2014Q1 | female_group | a1050000000sjEC | Underserved | 247 | . | 2 | 653068 | 150 | Transportation | Transportation | To repair their old cycle-van and buy another one to rent out as a source of income | India | Maynaguri | INR | 334 | 2014-01-01 09:58:07+00:00 | 2014-01-01 16:01:36+00:00 | 43 | 6 | bullet | 201401 | 2014Q1 | female | a1050000002VkWz | Underserved | 334 | . â†’ ê²°í•© í›„ì˜ nullê°’ íŒŒì•… . kiva_loan_themes.info() . &lt;class 'pandas.core.frame.DataFrame'&gt; Int64Index: 671205 entries, 0 to 671204 Data columns (total 17 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 id 671205 non-null int64 1 funded_amount 671205 non-null float64 2 activity 671205 non-null object 3 sector 671205 non-null object 4 country 671205 non-null object 5 region 614405 non-null object 6 currency 671205 non-null object 7 partner_id 657698 non-null float64 8 posted_time 671205 non-null datetime64[ns, UTC] 9 funded_time 622874 non-null datetime64[ns, UTC] 10 term_in_months 671205 non-null float64 11 lender_count 671205 non-null int64 12 borrower_genders 666984 non-null object 13 repayment_interval 671205 non-null object 14 Loan Theme ID 657692 non-null object 15 Loan Theme Type 657692 non-null object 16 Partner ID 657692 non-null float64 dtypes: datetime64[ns, UTC](2), float64(4), int64(2), object(9) memory usage: 92.2+ MB . | ìœ„ì—ì„œ ë§Œë“  kiva_loan_themesì— themes_region_dfë¥¼ merge . temp1 = kiva_loan_themes[['funded_amount', 'posted_quarter', 'Loan Theme ID', 'borrower_type', 'activity', 'sector', 'use', 'country']] temp2 = themes_region_df[['Loan Theme ID', 'Loan Theme Type', 'forkiva', 'Partner ID', 'Field Partner Name']] temp2.drop_duplicates(inplace=True) kiva_themes_df = pd.merge(temp1, temp2, how='left', on='Loan Theme ID') kiva_themes_df.head(3) . | Â  | funded_amount | posted_quarter | Loan Theme ID | borrower_type | activity | sector | use | country | Loan Theme Type | forkiva | Partner ID | Field Partner Name | . | 0 | 300 | 2014Q1 | nan | female | Fruits &amp; Vegetables | Food | To buy seasonal, fresh fruits to sell. | Pakistan | nan | nan | nan | nan | . | 1 | 575 | 2014Q1 | a1050000000sjEC | female_group | Rickshaw | Transportation | to repair and maintain the auto rickshaw used in their business. | Pakistan | Underserved | No | 247 | BRAC Pakistan | . | 2 | 150 | 2014Q1 | a1050000002VkWz | female | Transportation | Transportation | To repair their old cycle-van and buy another one to rent out as a source of income | India | Underserved | Yes | 334 | Belghoria Janakalyan Samity | . â†’ ê²°í•© í›„ì˜ nullê°’ íŒŒì•… . kiva_themes_df.info() . &lt;class 'pandas.core.frame.DataFrame'&gt; Int64Index: 671205 entries, 0 to 671204 Data columns (total 11 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 funded_amount 671205 non-null float64 1 posted_quarter 671205 non-null object 2 Loan Theme ID 657692 non-null object 3 borrower_type 671205 non-null object 4 activity 671205 non-null object 5 sector 671205 non-null object 6 country 671205 non-null object 7 Loan Theme Type 629021 non-null object 8 forkiva 629021 non-null object 9 Partner ID 629021 non-null float64 10 Field Partner Name 629021 non-null object dtypes: float64(2), object(9) memory usage: 61.5+ MB . | kiva loan ì¤‘ kivaë§Œì„ ìœ„í•œ loanì´ ì–´ëŠ ì •ë„ì¸ì§€ íŒŒì•… . | forkiva=Yesë©´ kivaë§Œì„ ìœ„í•´ êµ¬ì¶•í•œ loanì´ë¼ëŠ” ì˜ë¯¸ | . sns.countplot(data=kiva_themes_df, x='forkiva', palette='pink'); . | kivaë§Œì„ ìœ„í•œ loanê³¼ ì•„ë‹Œ loanì˜ í‰ê·  funded amount ë¹„êµ . sns.barplot(data=kiva_themes_df, x='forkiva', y='funded_amount', palette='pink'); . | forkiva=Noì¸ loanì´ ì••ë„ì ìœ¼ë¡œ ë§ì§€ë§Œ, í‰ê·  funded amount ìì²´ëŠ” ë³„ ì°¨ì´ê°€ ì—†ìŒ | . | kivaë§Œì„ ìœ„í•œ loanê³¼ ì•„ë‹Œ loanì˜ ë¶„ê¸°ë³„ ì´ funded amount ì¶”ì´ . temp = kiva_themes_df.groupby(['posted_quarter', 'forkiva'])[['funded_amount']].sum().reset_index() plt.figure(figsize=(16, 6)) sns.lineplot(data = temp[temp['posted_quarter'] != '2017Q3'], x='posted_quarter', y='funded_amount', hue='forkiva', palette='pink'); . | kivaë§Œì„ ìœ„í•œ loanê³¼ ê·¸ë ‡ì§€ ì•Šì€ loan ëª¨ë‘ funded amountê°€ ì¦ê°€ ì¶”ì„¸ì— ìˆë‹¤ | . | . country, sectorë³„ forkiva ë¹„ì¤‘ . | kivaë§Œì„ ìœ„í•œ loan ë¹„ì¤‘ì´ ë§ì€ country í™•ì¸ . # countryë³„ forkiva=yesì¸ ë¹„ì¤‘: ì´ funded amountë¥¼ ê¸°ì¤€ìœ¼ë¡œ ê³„ì‚° country_forkiva = pd.pivot_table(df, index='country', columns='forkiva', values='funded_amount', fill_value=0, aggfunc='sum') country_forkiva['forkiva_ratio(%)'] = (country_forkiva['Yes'] / (country_forkiva['No'] + country_forkiva['Yes'])) * 100 country_forkiva.sort_values(by='forkiva_ratio(%)', ascending=False, inplace=True) country_forkiva.head(10) . | country | No | Yes | forkiva_ratio(%) | . | Belize | 0 | 63150 | 100.00 | . | Chile | 0 | 38525 | 100.00 | . | Georgia | 105875 | 3263625 | 96.86 | . | Lao Peopleâ€™s Democratic Republic | 59000 | 1089050 | 94.86 | . | Moldova | 58825 | 628025 | 91.44 | . | Myanmar (Burma) | 292925 | 2742925 | 90.35 | . | Egypt | 428750 | 656175 | 60.48 | . | Mongolia | 532425 | 676950 | 55.98 | . | Zimbabwe | 1690075 | 1365500 | 44.69 | . | India | 2932575 | 2312850 | 44.09 | . â†’ forkiva=yesì¸ ë¹„ì¤‘ì´ 0ë³´ë‹¤ í° êµ­ê°€ë§Œ ì‹œê°í™” . plt.figure(figsize=(9, 9)) sns.barplot(data=country_forkiva[country_forkiva['forkiva_ratio(%)'] &gt; 0].reset_index(), x='forkiva_ratio(%)', y='country', palette='pink'); . | Belize, ChileëŠ” ì§€ì›ë°›ì€ ê¸ˆì•¡ì˜ 100%ê°€ kivaë§Œì„ ìœ„í•´ êµ¬ì¶•ëœ loan | Georgia, Lao Peopleâ€™s Democratic Republic, Moldova, MyanmarëŠ” 90% ì´ìƒì˜ ê¸ˆì•¡ì´ kivaë§Œì„ ìœ„í•œ loanìœ¼ë¡œ ì§€ì›ë°›ì€ ê²ƒ | . | kivaë§Œì„ ìœ„í•œ loan ë¹„ì¤‘ì´ ë§ì€ sector í™•ì¸ . sector_forkiva = pd.pivot_table(kiva_themes_df, index='sector', columns='forkiva', values='funded_amount', fill_value=0, aggfunc='sum') sector_forkiva['forkiva_ratio(%)'] = (sector_forkiva['Yes'] / (sector_forkiva['No'] + sector_forkiva['Yes'])) * 100 sector_forkiva.sort_values(by='forkiva_ratio(%)', ascending=False, inplace=True) # Sectorë³„ forkiva ë¹„ì¤‘ì„ ì‹œê°í™” plt.figure(figsize=(9, 6)) sns.barplot(data=sector_forkiva.reset_index(), x='forkiva_ratio(%)', y='sector', palette='pink'); . | â€˜Personal Useâ€™ sectorëŠ” funded amountì˜ ì•½ 43%ê°€ kivaë§Œì„ ìœ„í•œ ë‹¨ë… loanìœ¼ë¡œ ì§€ì›ë°›ì€ ê²ƒ | . | kivaë§Œì„ ìœ„í•œ loan ì¤‘, â€˜Personal Useâ€™ sectorì˜ êµ¬ì²´ì ì¸ useë¥¼ íŒŒì•… . temp = kiva_themes_df[(kiva_themes_df['forkiva'] == 'Yes') &amp; (kiva_themes_df['sector'] == 'Personal Use')] temp.groupby('use')[['funded_amount']].sum().sort_values(by='funded_amount', ascending=False).head() . | use | funded_amount | . | to buy a water filter to provide safe drinking water for their family. | 805125 | . | to buy a water filter to provide safe drinking water for her family. | 628150 | . | to purchase TerraClear water filters so they can have access to safe drinking water. | 566900 | . | To buy a water filter to provide safe drinking water for their family. | 253300 | . | to buy a water filter to provide safe drinking water for his family. | 200375 | . â†’ useì— ë“±ì¥í•˜ëŠ” ë‹¨ì–´ë¥¼ wordcloudë¡œ ì‹œê°í™” . # Text Cleaning &amp; Case Conversion &amp; Tokenization words = [] for content in kiva_themes_df[(kiva_themes_df['forkiva'] == 'Yes') &amp; (kiva_themes_df['sector'] == 'Personal Use')]['use']: content_cleaned = str(content).replace('.', '').replace(',','').strip().lower() # ., ì •ë„ë§Œ ì œê±°, strip, lower words.extend(content_cleaned.split()) # ê°„ë‹¨í•˜ê²Œ ê³µë°± ê¸°ì¤€ìœ¼ë¡œë§Œ ìª¼ê°œì¤Œ # POS Tagging tokens_pos = nltk.pos_tag(words) # Noun, Verbë§Œ ì¶”ì¶œ &amp; Lemmatization wlem = nltk.WordNetLemmatizer() lemm_words = [] for word, pos in tokens_pos: if ('NN' in pos) | ('VB' in pos): lemm_words.append(wlem.lemmatize(word)) # Counterë¡œ ë¹ˆë„ ê³„ì‚° c = Counter(lemm_words) generate_wordcloud(c) # ìœ„ì—ì„œ ë§Œë“¤ì–´ë‘” ì›Œë“œí´ë¼ìš°ë“œ ë§Œë“œëŠ” í•¨ìˆ˜ . +) Top 10 ë‹¨ì–´ bar graphë¡œ ì‹œê°í™” . visualize_top10_words(c) # ìœ„ì—ì„œ ë§Œë“¤ì–´ë‘” top 10 ë‹¨ì–´ bargraph ë§Œë“œëŠ” í•¨ìˆ˜ . | drinking water ê´€ë ¨ loanì´ ë§ë‹¤ê³  íŒë‹¨ë¨ | ì‹ìˆ˜ë„ ê´€ë ¨ íƒ€ loan / ì‚¬ì—…ì„ ëŒì–´ì˜¤ë©´ kiva 100% loanì„ êµ¬ì¶•í•˜ì§€ ì•Šê³ ë„ ë‹¤ë¥¸ ë°©ë²•ìœ¼ë¡œ ê¸ˆì „ì  ì§€ì›ì´ ê°€ëŠ¥í•  ìˆ˜ ìˆë‹¤ê³  ìƒê°ë¨ | . | . Field Partnerì™€ì˜ í˜‘ì—… ê´€ê³„ í™•ì¸ . # funded amountìˆœìœ¼ë¡œ Field Partner Name í™•ì¸ loans_partner = kiva_themes_df.groupby('Field Partner Name')[['funded_amount']].sum().reset_index() loans_partner.sort_values(by='funded_amount', ascending=False, inplace=True) plt.figure(figsize=(9, 6)) sns.barplot(data=loans_partner.head(10), x='funded_amount', y='Field Partner Name', palette='pink') plt.xticks(rotation=60); . | Negros Women for Tomorrow Foundation (NWTF)ê°€ ê°€ì¥ ë§ì€ funded amountì˜ ìê¸ˆ ìœµí†µì„ ë‹´ë‹¹í•˜ëŠ” field partner | . +) NWTFê°€ ì£¼ë¡œ ë‹´ë‹¹í•˜ëŠ” ì§€ì—­ í™•ì¸: . temp = kiva_themes_df[kiva_themes_df['Field Partner Name'] == 'Negros Women for Tomorrow Foundation (NWTF)'] temp.groupby('country')[['funded_amount']].sum().sort_values(by='funded_amount', ascending=False).head(3) . | country | funded_amount | . | Philippines | 33831500.0 | . | Kenya | 25.0 | . | Bolivia | 0.0 | . | NWTFëŠ” Philippinesì—ì„œ ì£¼ë¡œ í™œë™í•˜ëŠ” field partnerì¸ ê²ƒìœ¼ë¡œ í™•ì¸ë¨ | . êµ­ê°€ë³„ funded amountì™€ MPI . | êµ­ê°€ë³„ ì´ funded amountì™€ í‰ê·  MPI ì§€ìˆ˜ë¥¼ ë¹„êµ | . | region ê¸°ì¤€ìœ¼ë¡œ mergeê°€ ê°€ëŠ¥í• ì§€ íƒìƒ‰ . kiva_df_region_set = set(kiva_df['region']) location_df_region_set = set(location_df['region']) print('kiva_df: ', len(kiva_df_region_set)) print('location_df: ', len(location_df_region_set)) print('ê²¹ì¹˜ëŠ” region: ', len(kiva_df_region_set &amp; location_df_region_set)) . kiva_df: 12696 location_df: 838 ê²¹ì¹˜ëŠ” region: 129 . | ê²¹ì¹˜ëŠ” regionì´ ë„ˆë¬´ ì ì–´ì„œ ë‹¨ìˆœ mergeëŠ” ë¶ˆê°€â€¦ | . | country ê¸°ì¤€ìœ¼ë¡œ ì´ funded amount, í‰ê·  MPI ì§€ìˆ˜ë¥¼ ê°ê° groupby í›„ merge . kiva_df_groupby = kiva_df.groupby('country')[['funded_amount', 'loan_amount']].sum().reset_index() location_df_groupby = location_df.groupby(['country', 'world_region'])[['MPI']].mean().reset_index() groupby_merged = pd.merge(kiva_df_groupby, location_df_groupby, how='inner', on='country') print(len(groupby_merged)) groupby_merged.head() . | Â  | country | funded_amount | loan_amount | world_region | MPI | . | 0 | Afghanistan | 14000 | 14000 | South Asia | 0.309853 | . | 1 | Belize | 114025 | 114025 | Latin America and Caribbean | 0.0201429 | . | 2 | Benin | 516825 | 518950 | Sub-Saharan Africa | 0.320333 | . | 3 | Bhutan | 15625 | 20000 | South Asia | 0.123474 | . | 4 | Brazil | 661025 | 662200 | Latin America and Caribbean | 0.0272593 | . â†’ êµ­ê°€ë³„ ì´ funded_amountì™€ í‰ê·  MPI ê°„ì˜ ê´€ê³„ ì‹œê°í™” . plt.figure(figsize=(12, 8)) sns.scatterplot(data=groupby_merged, x='funded_amount', y='MPI', hue='world_region', style='world_region', palette='Set2', s=100); . | í‰ê·  MPIê°€ ë†’ì€ êµ­ê°€ì¼ìˆ˜ë¡ funded amountê°€ ë§ì„ ê±°ë¼ê³  ì˜ˆìƒí–ˆì§€ë§Œ, ê·¸ëŸ° ê´€ê³„ëŠ” ë°œê²¬ë˜ì§€ ì•ŠìŒ | ì˜¤íˆë ¤ í‰ê·  MPIê°€ ë†’ì€ (= poverty levelì´ ë†’ì€) Sub-Shararan Africaì˜ êµ­ê°€ë“¤ ì¤‘ ìƒìœ„ 8ê°œ êµ­ê°€ëŠ” funded amountê°€ ë§ì§€ ì•Šì€ í¸ | . | . ",
    "url": "https://chaelist.github.io/docs/kaggle/kiva_crowdfunding2/#%EB%8D%B0%EC%9D%B4%ED%84%B0-%EA%B2%B0%ED%95%A9%ED%95%B4-%EB%B6%84%EC%84%9D",
    "relUrl": "/docs/kaggle/kiva_crowdfunding2/#ë°ì´í„°-ê²°í•©í•´-ë¶„ì„"
  },"95": {
    "doc": "ë¹ˆë„ ë¶„ì„ (í•œê¸€)",
    "title": "ë¹ˆë„ ë¶„ì„ (í•œê¸€)",
    "content": ". | ì‚¬ìš©í•  text ìˆ˜ì§‘ | ì „ì²˜ë¦¬ (Preprocessing) . | Text Cleaning | Tokenization + Lemmatization + POS tagging | Stopwords ì œê±° | . | ë¹ˆë„ ë¶„ì„ . | Counter | WordCloud | . | . ",
    "url": "https://chaelist.github.io/docs/text_analysis/korean_text/",
    "relUrl": "/docs/text_analysis/korean_text/"
  },"96": {
    "doc": "ë¹ˆë„ ë¶„ì„ (í•œê¸€)",
    "title": "ì‚¬ìš©í•  text ìˆ˜ì§‘",
    "content": "import requests from bs4 import BeautifulSoup url = 'https://news.joins.com/article/23904235' r = requests.get(url) soup = BeautifulSoup(r.text, 'lxml') title = soup.select_one('#article_title').text.strip() content = soup.select_one('#article_body').text.strip() print(title, '\\n') print(content) . [ê°•ì°¬ìˆ˜ì˜ ì—ì½”ì‚¬ì´ì–¸ìŠ¤] í•´ì–‘ í”Œë¼ìŠ¤í‹± ì“°ë ˆê¸°ì— ë¶™ì€ ì—„ì²­ë‚œ ì„¸ê· ì´ ê±´ê°• ìœ„í˜‘í•œë‹¤ ê°•ì°¬ìˆ˜ í™˜ê²½ì „ë¬¸ê¸°ì íƒë°° ë…¸ë™ìë“¤ì´ ì‡ë”°ë¼ ìˆ¨ì§€ê³  ìˆë‹¤. ì „êµ­ íƒë°°ì—°ëŒ€ ë…¸ì¡° ë“±ì— ë”°ë¥´ë©´ ì´ë‹¬ì—ë§Œ CJëŒ€í•œí†µìš´ ì„œìš¸ ê°•ë¶ì§€ì , í•œì§„íƒë°° ì„œìš¸ ë™ëŒ€ë¬¸ì§€ì‚¬, ê²½ë¶ ì¹ ê³¡ ì¿ íŒ¡ ë¬¼ë¥˜ì„¼í„°ì—ì„œ ì¼í–ˆë˜ ë…¸ë™ìê°€ ì—°ë‹¬ì•„ ìˆ¨ì¡Œë‹¤. ë°°ë‹¬ ë¬¼ëŸ‰ ê¸‰ì¦ìœ¼ë¡œ ì¸í•œ ê³¼ë¡œì‚¬ë¼ëŠ” ì§€ì ì´ ìŸì•„ì§„ë‹¤. ì‹¤ì œë¡œ êµ­í† êµí†µë¶€ê°€ êµ­íšŒì— ì œì¶œí•œ ìë£Œì— ë”°ë¥´ë©´, 2020ë…„ 6ì›” ìƒí™œë¬¼ë¥˜ íƒë°° ë¬¼ë™ëŸ‰ì€ 2ì–µ 9340ë§Œ ê°œë¡œ ì§€ë‚œí•´ 6ì›”ë³´ë‹¤ 36.3% ëŠ˜ì—ˆë‹¤. (ìƒëµ) . ",
    "url": "https://chaelist.github.io/docs/text_analysis/korean_text/#%EC%82%AC%EC%9A%A9%ED%95%A0-text-%EC%88%98%EC%A7%91",
    "relUrl": "/docs/text_analysis/korean_text/#ì‚¬ìš©í• -text-ìˆ˜ì§‘"
  },"97": {
    "doc": "ë¹ˆë„ ë¶„ì„ (í•œê¸€)",
    "title": "ì „ì²˜ë¦¬ (Preprocessing)",
    "content": "Â  . *ì „ì²˜ë¦¬ ê³¼ì • (í•œê¸€): (ì˜ì–´ì™€ ë¹„êµ) . | Text Cleaning . | case conversionì€ í•„ìš” ì—†ë‹¤ | . | Tokenization + Lemmatization + POS tagging . | í•œê¸€ì˜ ê²½ìš°, KoNLPyì˜ í˜•íƒœì†Œ ë¶„ì„ê¸°ë¥¼ í†µí•´ 3ê°€ì§€ ì‘ì—…ì„ í•œë²ˆì— ìˆ˜í–‰ (.pos() í•¨ìˆ˜ ì´ìš©) | . | ì›í•˜ëŠ” í’ˆì‚¬ì˜ ë‹¨ì–´ë“¤ë§Œ ì„ íƒ | Stopwords Removal | . Text Cleaning . # ë¶ˆí•„ìš”í•œ ê¸°í˜¸ ì—†ì• ê¸° - ì •ê·œì‹ ì‚¬ìš© import re cleaned_content = re.sub('[^,.?!\\w\\s]','', content) ## ,.?!ì™€ ë¬¸ì+ìˆ«ì+_(\\w)ì™€ ê³µë°±(\\s)ë§Œ ë‚¨ê¹€ # ì¶”ê°€ë¡œ .replaceë¥¼ í™œìš©í•´ ë¶ˆí•„ìš”í•œ ë‹¨ì–´/ê¸°í˜¸ë¥¼ ì—†ì• ì¤€ë‹¤ cleaned_content = cleaned_content.replace('\\xa0', '').replace('ê°•ì°¬ìˆ˜', '').replace('ì—°í•©ë‰´ìŠ¤', '').replace('í™˜ê²½ì „ë¬¸ê¸°ì', '') cleaned_content . íƒë°° ë…¸ë™ìë“¤ì´ ì‡ë”°ë¼ ìˆ¨ì§€ê³  ìˆë‹¤. ì „êµ­ íƒë°°ì—°ëŒ€ ë…¸ì¡° ë“±ì— ë”°ë¥´ë©´ ì´ë‹¬ì—ë§Œ CJëŒ€í•œí†µìš´ ì„œìš¸ ê°•ë¶ì§€ì , í•œì§„íƒë°° ì„œìš¸ ë™ëŒ€ë¬¸ì§€ì‚¬, ê²½ë¶ ì¹ ê³¡ ì¿ íŒ¡ ë¬¼ë¥˜ì„¼í„°ì—ì„œ ì¼í–ˆë˜ ë…¸ë™ìê°€ ì—°ë‹¬ì•„ ìˆ¨ì¡Œë‹¤. ë°°ë‹¬ ë¬¼ëŸ‰ ê¸‰ì¦ìœ¼ë¡œ ì¸í•œ ê³¼ë¡œì‚¬ë¼ëŠ” ì§€ì ì´ ìŸì•„ì§„ë‹¤. ì‹¤ì œë¡œ êµ­í† êµí†µë¶€ê°€ êµ­íšŒì— ì œì¶œí•œ ìë£Œì— ë”°ë¥´ë©´, 2020ë…„ 6ì›” ìƒí™œë¬¼ë¥˜ íƒë°° ë¬¼ë™ëŸ‰ì€ 2ì–µ 9340ë§Œ ê°œë¡œ ì§€ë‚œí•´ 6ì›”ë³´ë‹¤ 36.3 ëŠ˜ì—ˆë‹¤. ì‹ ì¢… ì½”ë¡œë‚˜ë°”ì´ëŸ¬ìŠ¤ ê°ì—¼ì¦ì½”ë¡œë‚˜19 íƒ“ì— ì‹œë¯¼ë“¤ì´ ì™¸ì¶œê³¼ ë§¤ì¥ ì‡¼í•‘ì„ êº¼ë¦° íƒ“ì´ë‹¤. 1íšŒìš© ë§ˆìŠ¤í¬ ì¬ì§ˆì´ í”Œë¼ìŠ¤í‹±ë¨¼ì§€ ì¹˜ì†Ÿìœ¼ë©´ ìŒì‹ ë°°ë‹¬ë„ ëŠ˜ì–´ë‚™ë™ê°• ë¬¼ê³ ê¸°ì—ë„ ë¯¸ì„¸í”Œë¼ìŠ¤í‹±ë‹¤íšŒìš©ê¸° ì‚¬ìš© ë“± ëŒ€ì•ˆëª¨ë¸ í•„ìš” ë°°ë‹¬ìŒì‹ ì£¼ë¬¸ë„ ëŠ˜ì—ˆë‹¤. (ìƒëµ) . Tokenization + Lemmatization + POS tagging . *â€˜KoNLPyâ€™ì˜ â€˜Kkmaâ€™ Class í™œìš© . | KoNLPy: í•œêµ­ì–´ NLPë¥¼ ìœ„í•œ Python íŒ¨í‚¤ì§€. | ì„¤ì¹˜í•´ì•¼ ì‚¬ìš© ê°€ëŠ¥: https://konlpy-ko.readthedocs.io/ko/v0.4.3/install/ | . | Kkma: KoNLPyì—ì„œ ì œê³µí•˜ëŠ” í•œê¸€ í˜•íƒœì†Œë¶„ì„ê¸° ì¤‘ í•˜ë‚˜. | ë‹¤ë¥¸ Classì™€ì˜ ë¹„êµ: https://konlpy.org/ko/v0.5.2/morph/ | . | . from konlpy.tag import Kkma # importí•´ì„œ ì‚¬ìš© kkma = Kkma() NN_words = [] kkma_pos = kkma.pos(cleaned_content) ## í•˜ë‚˜ì˜ stringì„ inputìœ¼ë¡œ ë°›ëŠ”ë‹¤ for word, pos in kkma_pos: if 'NN' in pos: ## ëª…ì‚¬ë§Œ ê³¨ë¼ëƒ„ NN_words.append(word) print(NN_words) . ['íƒë°°', 'ë…¸ë™ì', 'ì „êµ­', 'íƒë°°', 'ì—°ëŒ€', 'ë…¸ì¡°', 'ë“±', 'ì´ë‹¬', 'í†µìš´', 'ì„œìš¸', 'ê°•ë¶', 'ì§€ì ', 'í•œì§„', 'íƒë°°', 'ì„œìš¸', 'ë™ëŒ€ë¬¸', 'ì§€ì‚¬', 'ê²½ë¶', 'ê³¡', 'ë¬¼ë¥˜', 'ì„¼í„°', 'ë…¸ë™ì', 'ë°°ë‹¬', 'ë¬¼ëŸ‰', 'ê¸‰ì¦', 'ê³¼ë¡œ', 'ì‚¬', 'ì§€ì ', 'êµ­í† ', 'êµí†µë¶€', 'êµ­íšŒ', 'ì œì¶œ', (ìƒëµ)] . Stopwords ì œê±° . # ì§ì ‘ ë§Œë“  ë¶ˆìš©ì–´ ì‚¬ì „ í™œìš© customized_stopwords = ['ê²ƒ', 'ë“±', 'íƒ“', 'ë°”', 'ìš©', 'ë…„', 'ê°œ', 'ë‹¹', 'ë©´', 'ë§'] unique_NN_words = set(NN_words) for word in unique_NN_words: if word in customized_stopwords: while word in NN_words: NN_words.remove(word) . ",
    "url": "https://chaelist.github.io/docs/text_analysis/korean_text/#%EC%A0%84%EC%B2%98%EB%A6%AC-preprocessing",
    "relUrl": "/docs/text_analysis/korean_text/#ì „ì²˜ë¦¬-preprocessing"
  },"98": {
    "doc": "ë¹ˆë„ ë¶„ì„ (í•œê¸€)",
    "title": "ë¹ˆë„ ë¶„ì„",
    "content": "Counter . from collections import Counter c = Counter(NN_words) ## ë‹¨ì–´ ê°œìˆ˜ë¥¼ ì„¸ì–´ì¤€ë‹¤ print(c) . Counter({'í”Œë¼ìŠ¤í‹±': 28, 'ë¯¸ì„¸': 13, 'ì“°ë ˆê¸°': 12, 'ë°°ë‹¬': 8, 'ì—°êµ¬': 8, 'ìŒì‹': 7, 'í†¤': 7, 'íƒë°°': 6, 'íŒ€': 6, 'ì½”ë¡œë‚˜': 5, 'ìƒì‚°': 5, 'ë°œí‘œ': 5, 'ë…¼ë¬¸': 5, 'ë…¸ë™ì': 4, 'ë§ˆìŠ¤í¬': 4, 'ë¨¼ì§€': 4, 'ë¬¼ê³ ê¸°': 4, 'ìµœê·¼': 4, 'ì¤‘': 4, 'ë°”ë‹¤': 4, 'ì§€ë‚œí•´': 3, 'êµ­ë‚´': 3, 'ìƒí™©': 3, 'í™˜ê²½': 3, (ìƒëµ)}) . â†’ ê°€ì¥ ë§ì´ ë‚˜ì˜¨ ë‹¨ì–´ 10ê°œ ì¶”ì¶œ . print(c.most_common(10)) . [('í”Œë¼ìŠ¤í‹±', 28), ('ë¯¸ì„¸', 13), ('ì“°ë ˆê¸°', 12), ('ë°°ë‹¬', 8), ('ì—°êµ¬', 8), ('ìŒì‹', 7), ('í†¤', 7), ('íƒë°°', 6), ('íŒ€', 6), ('ì½”ë¡œë‚˜', 5)] . WordCloud . | í•œê¸€ë¡œ ì›Œë“œí´ë¼ìš°ë“œë¥¼ ê·¸ë¦´ ë•ŒëŠ” font pathë¥¼ ì§€ì •í•´ì¤˜ì•¼ í•œë‹¤. | . from wordcloud import WordCloud import matplotlib.pyplot as plt total_words = '' for word in NN_words: total_words = total_words+' '+word wordcloud = WordCloud(font_path='c:/Windows/Fonts/malgun.ttf', # í•œê¸€ì„ ì¶œë ¥í•˜ë ¤ë©´ font_pathë¥¼ ì§€ì •í•´ì¤˜ì•¼ í•œë‹¤ max_words=200, relative_scaling = 0.5, background_color='white', colormap='summer').generate(total_words) plt.imshow(wordcloud) plt.axis('off') plt.show() . +) ì›í•˜ëŠ” ì´ë¯¸ì§€ë¡œ ì›Œë“œí´ë¼ìš°ë“œ ë§Œë“¤ê¸° . | maskë¡œ ì‚¬ìš©í•  ì´ë¯¸ì§€ ë¶ˆëŸ¬ì˜¤ê¸° from PIL import Image import matplotlib.pyplot as plt import numpy as np icon = Image.open('green.png') # maskë¡œ ì‚¬ìš©í•  ì´ë¯¸ì§€ ë¶ˆëŸ¬ì˜¤ê¸° plt.imshow(icon) mask = Image.new(\"RGB\", icon.size, (255,255,255)) mask.paste(icon,icon) mask = np.array(mask) . | mask=mask ì˜µì…˜ ì¶”ê°€í•´ì„œ ì›Œë“œí´ë¼ìš°ë“œ ê·¸ë¦¬ê¸° from wordcloud import WordCloud, ImageColorGenerator import matplotlib.pyplot as plt total_words = '' for word in NN_words: total_words = total_words+' '+word wordcloud = WordCloud(font_path='c:/Windows/Fonts/malgun.ttf', max_words=200, background_color='white', mask=mask).generate(total_words) plt.figure(figsize=(7,7)) #ì•¡ìì‚¬ì´ì¦ˆì„¤ì • plt.axis('off') #í…Œë‘ë¦¬ ì„  ì—†ì• ê¸° ## maskì˜ ìƒ‰ìœ¼ë¡œ ë‹¨ì–´ë“¤ì˜ ìƒ‰ì„ ì¹ í•´ì¤Œ - ì´ ë¶€ë¶„ì„ ìƒëµí•˜ë©´ ë§ˆìŠ¤í¬ ëª¨ì–‘ëŒ€ë¡œ ì›Œë“œí´ë¼ìš°ë“œë¥¼ ë§Œë“¤ì–´ì£¼ë˜, ìƒ‰ì€ ê¸°ë³¸ìƒ‰ìœ¼ë¡œ ë‚˜ì˜¨ë‹¤. image_colors = ImageColorGenerator(mask) plt.imshow(wordcloud.recolor(color_func=image_colors),interpolation=\"bilinear\") . | . ",
    "url": "https://chaelist.github.io/docs/text_analysis/korean_text/#%EB%B9%88%EB%8F%84-%EB%B6%84%EC%84%9D",
    "relUrl": "/docs/text_analysis/korean_text/#ë¹ˆë„-ë¶„ì„"
  },"99": {
    "doc": "ê¸°ì´ˆ & Linear Regression",
    "title": "ê¸°ì´ˆ &amp; Linear Regression",
    "content": ". | Machine Learningì´ë€? . | ê´€ë ¨ ìš©ì–´ë“¤ | Maching Learningì˜ ì¢…ë¥˜ | . | Linear Regression (ì„ í˜•íšŒê·€) . | ê¸°ë³¸ ê°œë… | scikit-learnìœ¼ë¡œ êµ¬í˜„í•˜ê¸° | . | Polynomial Regression (ë‹¤í•­íšŒê·€) . | ê¸°ë³¸ ê°œë… | scikit-learnìœ¼ë¡œ êµ¬í˜„í•˜ê¸° | . | . ",
    "url": "https://chaelist.github.io/docs/ml_basics/linear_regression/#%EA%B8%B0%EC%B4%88--linear-regression",
    "relUrl": "/docs/ml_basics/linear_regression/#ê¸°ì´ˆ--linear-regression"
  },"100": {
    "doc": "ê¸°ì´ˆ & Linear Regression",
    "title": "Machine Learningì´ë€?",
    "content": ": ë°ì´í„°ì…‹ìœ¼ë¡œ ëª¨ë¸ì„ í•™ìŠµì‹œì¼œ ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìƒˆë¡œìš´ ë°ì´í„°ì…‹ì„ ì˜ˆì¸¡í•  ìˆ˜ ìˆê²Œ í•˜ëŠ” ë°©ì‹. | ì¸ê³µì§€ëŠ¥(AI)ì˜ í•œ ë¶„ì•¼ì´ë©°, ëª…ì‹œì ìœ¼ë¡œ í”„ë¡œê·¸ë˜ë°í•˜ì§€ ì•Šì•„ë„ ê¸°ê³„ ìŠ¤ìŠ¤ë¡œì˜ í•™ìŠµì„ í†µí•œ ì˜ˆì¸¡ì´ ê°€ëŠ¥. | . ê´€ë ¨ ìš©ì–´ë“¤ . | ì¢…ì†ë³€ìˆ˜ì™€ ë…ë¦½ë³€ìˆ˜ . | y = f(x)ì—ì„œì˜ yê°€ ì¢…ì†ë³€ìˆ˜, xê°€ ë…ë¦½ë³€ìˆ˜. | Independent variable (ë…ë¦½ë³€ìˆ˜): MLì—ì„œ í•™ìŠµì˜ ë‹¨ì„œê°€ ë˜ëŠ” ë³€ìˆ˜ë“¤. (ex. ê·€ì˜ ëª¨ì–‘, í„¸ì˜ ìƒ‰, ìˆ˜ì—¼ì˜ ê¸¸ì´) . | feature ë˜ëŠ” ì…ë ¥ë³€ìˆ˜ë¼ê³ ë„ í•¨. | . | Dependent variable (ì¢…ì†ë³€ìˆ˜): MLì—ì„œ ì˜ˆì¸¡í•´ì•¼ í•˜ëŠ” ì •ë‹µ (ex. ê°œ / ê³ ì–‘ì´) . | ëª©í‘œë³€ìˆ˜ë¼ê³ ë„ í•¨. | . | IVì™€ DV ê°„ì˜ ê´€ê³„ëŠ” â€˜íŒŒë¼ë¯¸í„°â€™ì˜ ê°’ì— ë”°ë¼ ë‹¬ë¼ì§„ë‹¤ . | y = a + bxì—ì„œì˜ a, bì²˜ëŸ¼ ëª¨ë¸ì˜ ëª¨ì–‘ì„ ê²°ì •ì§“ëŠ” ê°’ë“¤ì„ Parameter(íŒŒë¼ë¯¸í„°)ë¼ê³  í•œë‹¤ | . | . Â  . | ì—°ì†ë³€ìˆ˜ì™€ ì´ì‚°ë³€ìˆ˜ . | Continuous variable (ì—°ì†ë³€ìˆ˜): ë¬´í•œí•œ ê°’ì„ ì·¨í•  ìˆ˜ ìˆëŠ” ë³€ìˆ˜ (ex. ì²´ì¤‘, ìˆ˜ì…) | Discrete variable (ì´ì‚°ë³€ìˆ˜): ì·¨í•  ìˆ˜ ìˆëŠ” ê°’ì´ ìœ í•œí•œ ë³€ìˆ˜. (ex. ì„±ë³„, ìë…€ ìˆ˜) . | Categorical varialbe (ë²”ì£¼í˜•ë³€ìˆ˜): ì·¨í•˜ëŠ” ê°’ì´ ìˆ«ìì˜ í¬ê¸°ê°€ ì•„ë‹Œ ê·¸ë£¹ì˜ ì´ë¦„ì„ ì§€ì¹­í•˜ëŠ” ë³€ìˆ˜. (ex. ì„±ë³„) | . | . | . Maching Learningì˜ ì¢…ë¥˜ . | Supervised Learning (ì§€ë„ í•™ìŠµ) . | ì •ë‹µ(=DV)ì´ ìˆëŠ” training dataë¡œ í•™ìŠµ â†’ ìƒˆë¡œìš´ ë°ì´í„°(test data)ì— ì ìš©í•´ì„œ ì •ë‹µì„ ì˜ˆì¸¡í•œë‹¤. | Machine Learningì˜ ëŒ€ë¶€ë¶„ì€ Supervised Learning. | . *ì˜ˆì‹œ: . | Regression problems (íšŒê·€): DVê°€ ì—°ì† ë³€ìˆ˜ì¸ ê²½ìš°. (ex. ì•„íŒŒíŠ¸ ê°€ê²© ì˜ˆì¸¡, ì—°ë´‰ ì˜ˆì¸¡) | Classifiaction problems (ë¶„ë¥˜): DVê°€ ë²”ì£¼í˜• ë³€ìˆ˜ì¸ ê²½ìš°. (ex. ê³ ì–‘ì´ì¸ì§€ ê°•ì•„ì§€ì¸ì§€ ë¶„ë¥˜) | . Â  . | Unsupervised Learning (ë¹„ì§€ë„ í•™ìŠµ) . | training dataê°€ í•„ìš” ì—†ìŒ (=ì •ë‹µì´ ë”°ë¡œ ì—†ìŒ). ë¹„êµë¥¼ í†µí•´ ë¹„ìŠ·í•œ data pointë¼ë¦¬ ë¬¶ì–´ì¤€ë‹¤. | ë¹„ì§€ë„ í•™ìŠµì€ ëŒ€ë¶€ë¶„ íƒìƒ‰ì  ë°ì´í„° ë¶„ì„ì˜ ì¼ë¶€ë¡œ ìˆ˜í–‰ëœë‹¤. | . *ì˜ˆì‹œ: . | Clustering (êµ°ì§‘í™”): ê±°ë¦¬ë¥¼ ê³„ì‚°í•´ ìœ ì‚¬í•œ data pointë¼ë¦¬ ê°™ì€ Clusterë¡œ ë¬¶ì–´ì¤€ë‹¤. | Dimension reduction (ì°¨ì› ì¶•ì†Œ): ë§ì€ featureë¥¼ ê°–ê³  ìˆëŠ” ë‹¤ì°¨ì› ë°ì´í„°ì…‹ì˜ ì°¨ì›ì„ ì¶•ì†Œí•´ ìƒˆë¡œìš´ ë°ì´í„°ì…‹ì„ ìƒì„±í•´ì¤€ë‹¤. | . Â  . | Reinforcement learning (ê°•í™”í•™ìŠµ) . | action &amp; feedback í”„ë¡œì„¸ìŠ¤ë¥¼ í†µí•´ ë¬¸ì œë¥¼ í’€ì–´ë‚˜ê°€ëŠ” ë°©ì‹. | í˜„ì¬ ìƒíƒœì—ì„œ ë†’ì€ rewardë¥¼ ë°›ëŠ” ë°©ë²•ì„ ì°¾ì•„ê°€ë©° actionì„ ì·¨í•˜ëŠ” ê²ƒì„ ë°˜ë³µí•˜ë‹¤ë³´ë©´ ë†’ì€ rewardë¥¼ ë°›ì„ ìˆ˜ ìˆëŠ” ì „ëµì„ í„°ë“í•˜ê²Œ ë¨ | . | . +. Deep Learning: Â Â  ë¨¸ì‹ ëŸ¬ë‹ ì¤‘ ì¸ê³µì‹ ê²½ë§ ëª¨ë¸ì„ ì§‘ì¤‘ ì—°êµ¬í•˜ëŠ” ë¶„ì•¼ë¡œ, ì§€ë„í•™ìŠµ, ë¹„ì§€ë„í•™ìŠµ, ê°•í™”í•™ìŠµê³¼ ëª¨ë‘ ì—°ê²°ì´ ê°€ëŠ¥í•˜ë‹¤ . ",
    "url": "https://chaelist.github.io/docs/ml_basics/linear_regression/#machine-learning%EC%9D%B4%EB%9E%80",
    "relUrl": "/docs/ml_basics/linear_regression/#machine-learningì´ë€"
  },"101": {
    "doc": "ê¸°ì´ˆ & Linear Regression",
    "title": "Linear Regression (ì„ í˜•íšŒê·€)",
    "content": "ê¸°ë³¸ ê°œë… . | ê°€ì„¤ í•¨ìˆ˜: ì¼ì°¨ í•¨ìˆ˜ (í•™ìŠµ = ë°ì´í„°ì— ê°€ì¥ ì˜ ë§ëŠ” ì¼ì°¨ í•¨ìˆ˜ë¥¼ ì°¾ëŠ” ê²ƒ!) . | featureê°€ í•˜ë‚˜ì¸ ê²½ìš°: hÎ¸(x) = Î¸0 + Î¸1x1 | featureê°€ ì—¬ëŸ¬ ê°œì¸ ê²½ìš°: hÎ¸(x) = Î¸0 + Î¸1x1 + Î¸2x2 + â€¦ + Î¸nxn . | featureê°€ ì—¬ëŸ¬ ê°œì¸ ê²½ìš°ë„ í•­ì´ ë§ê¸´ í•˜ì§€ë§Œ ì¼ì°¨í•¨ìˆ˜. | . | . | ì†ì‹¤í•¨ìˆ˜: MSE(í‰ê· ì œê³±ì˜¤ì°¨) . | ì†ì‹¤ í•¨ìˆ˜(loss function): ê°€ì„¤ í•¨ìˆ˜ë¥¼ í‰ê°€í•˜ê¸° ìœ„í•œ í•¨ìˆ˜. ì†ì‹¤ í•¨ìˆ˜ì˜ ê²°ê³¼ê°’ì´ ì‘ì„ìˆ˜ë¡ ì†ì‹¤ì´ ì‘ì€ ê²ƒ. = ì¢‹ì€ ê°€ì„¤ í•¨ìˆ˜. | MSE: ê° ë°ì´í„°í¬ì¸íŠ¸ë“¤ì´ ê°€ì„¤ í•¨ìˆ˜ë¡œë¶€í„° ì–¼ë§ˆë‚˜ ë–¨ì–´ì ¸ ìˆëŠ”ì§€, ê·¸ ì˜¤ì°¨ë¥¼ ëª¨ë‘ ì œê³±í•´ì„œ í•©í•˜ê³  í‰ê· ì„ ë‚¸ ê²ƒ. | ì†ì‹¤ í•¨ìˆ˜ì˜ ìµœì €ì , ì¦‰ MSEê°€ ê°€ì¥ ìµœì†Œê°€ ë˜ëŠ” ì§€ì ì„ ì°¾ìœ¼ë©´ ë°ì´í„°ì— ê°€ì¥ ì˜ ë§ëŠ” ê°€ì„¤í•¨ìˆ˜ë¥¼ ì°¾ì„ ìˆ˜ ìˆë‹¤. | . | ì†ì‹¤í•¨ìˆ˜ì˜ ìµœì €ì ì„ ì°¾ëŠ” ë°©ë²•: . | Normal Equation: derivative(ë¯¸ë¶„ê°’)ì´ 0ì´ ë˜ëŠ” ì§€ì ì„ ì§ì ‘ ë°”ë¡œ ê³„ì‚°í•´ì„œ ì°¾ëŠ”ë‹¤ | Gradient Descent (ê²½ì‚¬í•˜ê°•ë²•): íŠ¹ì • ì§€ì ì—ì„œ ì‹œì‘, ì ì  ê°’ì„ updateí•´ê°€ë©´ì„œ ìµœì €ì ì„ ì°¾ëŠ”ë‹¤ | . | . â€» sklearnì˜ Linear Regression ëª¨ë¸ì€ scipy.linalg.lstsqë¥¼ ì‚¬ìš©í•´ì„œ ìµœì €ì ì„ ê³„ì‚°. (Normal Equation ë°©ì‹) . scikit-learnìœ¼ë¡œ êµ¬í˜„í•˜ê¸° . â€» Anacondaë¥¼ ì‚¬ìš©í•˜ê³  ìˆë‹¤ë©´ scikit-learnì€ ì´ë¯¸ ë‚´ì¥ë˜ì–´ ìˆìŒ. ê·¸ ì™¸ ê²½ìš°ì—ëŠ” pip install scikit-learnë¡œ ì„¤ì¹˜. | sklearnì—ì„œ ê¸°ë³¸ ì œê³µí•˜ëŠ” í•™ìŠµìš© dataset ì¤‘ â€˜boston ì§‘ê°’ ë°ì´í„°â€™ë¥¼ í™œìš© | IVì´ 2ê°œ ì´ìƒì¸ â€˜Multiple Linear Regression (ë‹¤ì¤‘ì„ í˜•íšŒê·€)â€™ ë¬¸ì œ. | . from sklearn.datasets import load_boston # boston ì§‘ê°’ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° from sklearn.model_selection import train_test_split # ë°ì´í„°ì…‹ì„ training set / test set ë‚˜ëˆ„ê¸° ìœ„í•œ í•¨ìˆ˜ from sklearn.linear_model import LinearRegression # ì„ í˜•íšŒê·€ë¥¼ êµ¬í˜„í•´ì£¼ëŠ” í•¨ìˆ˜ from sklearn.metrics import mean_squared_error # mean squared error(í‰ê· ì œê³±ì˜¤ì°¨)ë¥¼ êµ¬í•´ì£¼ëŠ” í•¨ìˆ˜ import pandas as pd . 1. ë°ì´í„° ì¤€ë¹„ . boston_dataset = load_boston() # ë°ì´í„°ì…‹ì„ ê°€ì ¸ì™€ì¤€ë‹¤ . +) print(boston_dataset.DESCR)ë¥¼ í•´ì£¼ë©´ ë°ì´í„°ì…‹ì— ëŒ€í•œ ì •ë³´ë¥¼ ì‚´í´ë³¼ ìˆ˜ ìˆìŒ . | Number of Instances: 506 (506ê°œì˜ ì§‘ê°’ ë°ì´í„°ê°€ ë“¤ì–´ìˆìŒ) | Number of Attributes: 13 (ì…ë ¥ë³€ìˆ˜ 13ê°œ) | 14ë²ˆì§¸ ì†ì„±ì´ ëŒ€ì²´ë¡œ ëª©í‘œë³€ìˆ˜ì¸ â€˜ì§‘ê°’â€™ | . Â  . â†’ ë°ì´í„° ì •ë¦¬ . # boston datasetì˜ ì…ë ¥ë³€ìˆ˜ë“¤ì„ dataframeìœ¼ë¡œ ì •ë¦¬ X = pd.DataFrame(boston_dataset.data, columns=boston_dataset.feature_names) # ëª©í‘œë³€ìˆ˜ë„ dataframeìœ¼ë¡œ ì •ë¦¬ y = pd.DataFrame(boston_dataset.target, columns=['MEDV']) . Â  . X.head() . | Â  | CRIM | ZN | INDUS | CHAS | NOX | RM | AGE | DIS | RAD | TAX | PTRATIO | B | LSTAT | . | 0 | 0.00632 | 18 | 2.31 | 0 | 0.538 | 6.575 | 65.2 | 4.09 | 1 | 296 | 15.3 | 396.9 | 4.98 | . | 1 | 0.02731 | 0 | 7.07 | 0 | 0.469 | 6.421 | 78.9 | 4.9671 | 2 | 242 | 17.8 | 396.9 | 9.14 | . | 2 | 0.02729 | 0 | 7.07 | 0 | 0.469 | 7.185 | 61.1 | 4.9671 | 2 | 242 | 17.8 | 392.83 | 4.03 | . | 3 | 0.03237 | 0 | 2.18 | 0 | 0.458 | 6.998 | 45.8 | 6.0622 | 3 | 222 | 18.7 | 394.63 | 2.94 | . | 4 | 0.06905 | 0 | 2.18 | 0 | 0.458 | 7.147 | 54.2 | 6.0622 | 3 | 222 | 18.7 | 396.9 | 5.33 | . Â  . y.head() . | Â  | MEDV | . | 0 | 24.0 | . | 1 | 21.6 | . | 2 | 34.7 | . | 3 | 33.4 | . | 4 | 36.2 | . 2. train_test_split . # training set / test set ë¶„ë¦¬ X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=5) . # ì•½ 4:1ë¡œ ì˜ ë‚˜ë‰˜ì—ˆìŒì„ í™•ì¸í•  ìˆ˜ ìˆìŒ print(X_train.shape) print(X_test.shape) print(y_train.shape) print(y_test.shape) . (404, 13) (102, 13) (404, 1) (102, 1) . 3. ëª¨ë¸ í•™ìŠµì‹œí‚¤ê¸° . model = LinearRegression() model.fit(X_train, y_train) # í•™ìŠµ . LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False) . *thetaê°’ í™•ì¸ (ë³€ìˆ˜ë³„ ê°€ì¤‘ì¹˜ë¥¼ í™•ì¸í•  ìˆ˜ ìˆë‹¤) . # theta_1ë¶€í„°ì˜ thetaê°’ë“¤. (ê° ë³€ìˆ˜ë³„ ê°€ì¤‘ì¹˜) model.coef_ . array([[ -0.13, 0.05, 0. , 2.71, -15.96, 3.41, 0. , -1.49, 0.36, -0.01, -0.95, 0.01, -0.59]]) . # theta_0. (ì ˆí¸) model.intercept_ . array([37.91248701]) . 4. test dataë¡œ ì„±ëŠ¥ ì²´í¬ . | í‰ê· ì œê³±ê·¼ì˜¤ì°¨ í™•ì¸ . # X_testë¡œ ì˜ˆì¸¡ y_test_prediction = model.predict(X_test) ## í‰ê· ì œê³±ê·¼ì˜¤ì°¨ (RMSE: Root Mean Square Error) mean_squared_error(y_test, y_test_prediction) ** 0.5 . 4.568292042303217 . | R ìŠ¤í€˜ì–´ ê°’ í™•ì¸ . | model.score(): R square (R2) ê°’ì„ ê³„ì‚°í•´ì£¼ëŠ” í•¨ìˆ˜ | R square: 0 ~ 1 ì‚¬ì´ì˜ ê°’ì„ ê°€ì§€ë©°, 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ëª¨ë¸ì˜ ì„±ëŠ¥ì´ ì¢‹ì€ ê²ƒ. (ì„¤ëª…ë ¥ì´ ì¢‹ì€ ê²ƒ) | . print(model.score(X_train, y_train)) print(model.score(X_test, y_test)) . 0.738339392059052 0.7334492147453064 . training dataëŠ” ì•½ 74%, test dataëŠ” ì•½ 73%ë¥¼ ì˜ ì˜ˆì¸¡í•œë‹¤ëŠ” ëœ» . | . ",
    "url": "https://chaelist.github.io/docs/ml_basics/linear_regression/#linear-regression-%EC%84%A0%ED%98%95%ED%9A%8C%EA%B7%80",
    "relUrl": "/docs/ml_basics/linear_regression/#linear-regression-ì„ í˜•íšŒê·€"
  },"102": {
    "doc": "ê¸°ì´ˆ & Linear Regression",
    "title": "Polynomial Regression (ë‹¤í•­íšŒê·€)",
    "content": ". | IVì™€ DV ê°„ì˜ ê´€ê³„ë¥¼ ê°€ì¥ ì˜ í‘œí˜„í•˜ëŠ” ê²Œ ì¼ì°¨í•¨ìˆ˜ê°€ ì•„ë‹ˆë¼ ë‹¤í•­ì‹(ê³¡ì„  ëª¨ì–‘)ì¸ ê²½ìš°, ë‹¤í•­ íšŒê·€ë¥¼ ì‚¬ìš© | . ê¸°ë³¸ ê°œë… . | ê°€ì„¤ í•¨ìˆ˜: ë‹¤í•­ì‹ (ì›í•˜ëŠ” ëª¨ì–‘ì— ë”°ë¼ 2ì°¨ í•¨ìˆ˜ ~ nì°¨ í•¨ìˆ˜) . | ex) featureê°€ í•˜ë‚˜ì´ê³ , ê°€ì„¤í•¨ìˆ˜ê°€ 2ì°¨ í•¨ìˆ˜: hÎ¸(x) = Î¸0 + Î¸1x1 + Î¸2x2 | ex) featureê°€ í•˜ë‚˜ì´ê³ , ê°€ì„¤í•¨ìˆ˜ê°€ 3ì°¨ í•¨ìˆ˜: hÎ¸(x) = Î¸0 + Î¸1x1 + Î¸2x2 + + Î¸3x3 | ex) featureê°€ 2ê°œì´ê³ , ê°€ì„¤í•¨ìˆ˜ê°€ 2ì°¨ í•¨ìˆ˜: hÎ¸(x) = Î¸0 + Î¸1x1 + Î¸2x2 + Î¸3x1x2 + Î¸4x12 + Î¸5x22 | . | ì„ í˜•íšŒê·€ì™€ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ ë¬¸ì œë¥¼ í•´ê²° . | ex) hÎ¸(x) = Î¸0 + Î¸1x1 + Î¸2x2: ì…ë ¥ë³€ìˆ˜ê°€ 2ê°œì¸ ì„ í˜•íšŒê·€ë¡œ ì·¨ê¸‰ | ex) hÎ¸(x) = Î¸0 + Î¸1x1 + Î¸2x2 + Î¸3x1x2 + Î¸4x12 + Î¸5x22: ì…ë ¥ë³€ìˆ˜ê°€ 5ê°œì¸ ì„ í˜•íšŒê·€ë¡œ ì·¨ê¸‰ | . | ë‹¤í•­íšŒê·€ì˜ ì¥ì : ì†ì„± ê°„ì— ìˆì„ ìˆ˜ ìˆëŠ” ë³µì¡í•œ ê´€ê³„ë“¤ì´ í•™ìŠµì— ë°˜ì˜ëœë‹¤ . | ex) ì§‘ ê°’ ì˜ˆì¸¡ ë¬¸ì œì—ì„œ IVê°€ â€˜ë„ˆë¹„â€™, â€˜ë†’ì´â€™ 2ê°œì¼ ë•Œ, ë‹¤í•­íšŒê·€ë¥¼ ì‚¬ìš©í•˜ë©´ â€˜ë„ˆë¹„xë†’ì´â€™ë¼ëŠ” ìƒˆë¡œìš´ ë³€ìˆ˜ê°€ ì…ë ¥ë³€ìˆ˜ë¡œ ë“¤ì–´ê°€ëŠ” ì…ˆì´ê¸°ì—, ì˜ˆì¸¡ë ¥ì´ ì»¤ì§ (ë‹¨ìˆœì„ í˜•íšŒê·€ì˜ ê²½ìš° ë‘ ë³€ìˆ˜ê°€ ë…ë¦½ì ì´ë¼, â€˜ë†’ì´ì™€ ë„ˆë¹„ê°€ ëª¨ë‘ ì»¤ì•¼ ì§‘ ê°’ì´ ë†’ë‹¤â€™ëŠ” ê´€ê³„ëŠ” í•™ìŠµ ë¶ˆê°€) | . | ë‹¤í•­íšŒê·€ì˜ ë‹¨ì : ë„ˆë¬´ ì°¨ìˆ˜ë¥¼ ë†’ì—¬ì„œ training dataì— ê¼­ ë§ëŠ” ê°€ì„¤ í•¨ìˆ˜ë¥¼ ë§Œë“œëŠ” ê²½ìš°, training dataëŠ” ì˜ ì„¤ëª…í•˜ì§€ë§Œ ìƒˆë¡œìš´ ë°ì´í„°ëŠ” ì˜ ì„¤ëª…í•˜ì§€ ëª»í•˜ëŠ” overfitting(ê³¼ì í•©) ë¬¸ì œê°€ ë°œìƒí•  ìˆ˜ ìˆë‹¤ | . scikit-learnìœ¼ë¡œ êµ¬í˜„í•˜ê¸° . | sklearnì—ì„œ ê¸°ë³¸ ì œê³µí•˜ëŠ” í•™ìŠµìš© dataset ì¤‘ â€˜boston ì§‘ê°’ ë°ì´í„°â€™ë¥¼ í™œìš©í•´ 2ì°¨ í•¨ìˆ˜ë¥¼ ë§Œë“¤ì–´ì¤„ ì˜ˆì •. | IVì´ 2ê°œ ì´ìƒì¸ â€˜ë‹¤ì¤‘ ë‹¤í•­ íšŒê·€â€™ ë¬¸ì œ. | LinearRegression ëª¨ë¸ì„ ì‚¬ìš©í•˜ë˜, ë‹¤í•­ ì†ì„±ì„ ë§Œë“¤ì–´ì„œ IVë¡œ ë„£ì–´ì¤€ë‹¤ | . from sklearn.datasets import load_boston from sklearn.preprocessing import PolynomialFeatures # ë‹¤í•­ì†ì„±ì„ ë§Œë“¤ì–´ì£¼ëŠ” íˆ´ from sklearn.model_selection import train_test_split from sklearn.linear_model import LinearRegression # ë˜‘ê°™ì´ LinearRegression ëª¨ë¸ì„ ì‚¬ìš© from sklearn.metrics import mean_squared_error import pandas as pd . 1. ë‹¤í•­ ì†ì„± ë§Œë“¤ì–´ì£¼ê¸° . # boston dataset ê°€ì ¸ì˜´ boston_dataset = load_boston() . polynomial_transformer = PolynomialFeatures(2) # () ì•ˆì— ëª‡ì°¨ í•¨ìˆ˜ë¡œ í•  ê±´ì§€ ì¨ì¤Œ - ì—¬ê¸°ì„  2ì°¨í•¨ìˆ˜ë¡œ ì§€ì •. polynomial_data = polynomial_transformer.fit_transform(boston_dataset.data) polynomial_data.shape # ì—´ì´ 13ê°œì—ì„œ 105ê°œë¡œ ëŠ˜ì–´ë‚¬ìŒì„ í™•ì¸ ê°€ëŠ¥. (506, 105) . +) ì–´ë–¤ featureë“¤ì´ 105ê°œ ì•ˆì— ë“¤ì–´ê°”ë‚˜ í™•ì¸ . polynomial_feature_names = polynomial_transformer.get_feature_names(boston_dataset.feature_names) print(polynomial_feature_names) # ê°€ëŠ¥í•œ ëª¨ë“  2ì°¨ ì¡°í•©ì´ ë‹¤ ë“¤ì–´ìˆìŒì„ í™•ì¸ ê°€ëŠ¥ . ['1', 'CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'CRIM^2', 'CRIM ZN', 'CRIM INDUS', 'CRIM CHAS', 'CRIM NOX', 'CRIM RM', 'CRIM AGE', 'CRIM DIS', 'CRIM RAD', 'CRIM TAX', 'CRIM PTRATIO', 'CRIM B', 'CRIM LSTAT', 'ZN^2', 'ZN INDUS', 'ZN CHAS', 'ZN NOX', 'ZN RM', 'ZN AGE', 'ZN DIS', 'ZN RAD', 'ZN TAX', 'ZN PTRATIO', 'ZN B', 'ZN LSTAT', 'INDUS^2', 'INDUS CHAS', 'INDUS NOX', 'INDUS RM', 'INDUS AGE', 'INDUS DIS', 'INDUS RAD', 'INDUS TAX', 'INDUS PTRATIO', 'INDUS B', 'INDUS LSTAT', 'CHAS^2', 'CHAS NOX', 'CHAS RM', 'CHAS AGE', 'CHAS DIS', 'CHAS RAD', 'CHAS TAX', 'CHAS PTRATIO', 'CHAS B', 'CHAS LSTAT', 'NOX^2', 'NOX RM', 'NOX AGE', 'NOX DIS', 'NOX RAD', 'NOX TAX', 'NOX PTRATIO', 'NOX B', 'NOX LSTAT', 'RM^2', 'RM AGE', 'RM DIS', 'RM RAD', 'RM TAX', 'RM PTRATIO', 'RM B', 'RM LSTAT', 'AGE^2', 'AGE DIS', 'AGE RAD', 'AGE TAX', 'AGE PTRATIO', 'AGE B', 'AGE LSTAT', 'DIS^2', 'DIS RAD', 'DIS TAX', 'DIS PTRATIO', 'DIS B', 'DIS LSTAT', 'RAD^2', 'RAD TAX', 'RAD PTRATIO', 'RAD B', 'RAD LSTAT', 'TAX^2', 'TAX PTRATIO', 'TAX B', 'TAX LSTAT', 'PTRATIO^2', 'PTRATIO B', 'PTRATIO LSTAT', 'B^2', 'B LSTAT', 'LSTAT^2'] . 2. X, yê°’ ì •ë¦¬ . X = pd.DataFrame(polynomial_data, columns=polynomial_feature_names) X.head() . | Â  | 1 | CRIM | ZN | INDUS | CHAS | NOX | RM | AGE | DIS | RAD | TAX | PTRATIO | B | LSTAT | CRIM^2 | CRIM ZN | CRIM INDUS | CRIM CHAS | CRIM NOX | CRIM RM | CRIM AGE | CRIM DIS | CRIM RAD | CRIM TAX | CRIM PTRATIO | CRIM B | CRIM LSTAT | ZN^2 | ZN INDUS | ZN CHAS | ZN NOX | ZN RM | ZN AGE | ZN DIS | ZN RAD | ZN TAX | ZN PTRATIO | ZN B | ZN LSTAT | INDUS^2 | INDUS CHAS | INDUS NOX | INDUS RM | INDUS AGE | INDUS DIS | INDUS RAD | INDUS TAX | INDUS PTRATIO | INDUS B | INDUS LSTAT | CHAS^2 | CHAS NOX | CHAS RM | CHAS AGE | CHAS DIS | CHAS RAD | CHAS TAX | CHAS PTRATIO | CHAS B | CHAS LSTAT | NOX^2 | NOX RM | NOX AGE | NOX DIS | NOX RAD | NOX TAX | NOX PTRATIO | NOX B | NOX LSTAT | RM^2 | RM AGE | RM DIS | RM RAD | RM TAX | RM PTRATIO | RM B | RM LSTAT | AGE^2 | AGE DIS | AGE RAD | AGE TAX | AGE PTRATIO | AGE B | AGE LSTAT | DIS^2 | DIS RAD | DIS TAX | DIS PTRATIO | DIS B | DIS LSTAT | RAD^2 | RAD TAX | RAD PTRATIO | RAD B | RAD LSTAT | TAX^2 | TAX PTRATIO | TAX B | TAX LSTAT | PTRATIO^2 | PTRATIO B | PTRATIO LSTAT | B^2 | B LSTAT | LSTAT^2 | . | 0 | 1 | 0.00632 | 18 | 2.31 | 0 | 0.538 | 6.575 | 65.2 | 4.09 | 1 | 296 | 15.3 | 396.9 | 4.98 | 3.99424e-05 | 0.11376 | 0.0145992 | 0 | 0.00340016 | 0.041554 | 0.412064 | 0.0258488 | 0.00632 | 1.87072 | 0.096696 | 2.50841 | 0.0314736 | 324 | 41.58 | 0 | 9.684 | 118.35 | 1173.6 | 73.62 | 18 | 5328 | 275.4 | 7144.2 | 89.64 | 5.3361 | 0 | 1.24278 | 15.1883 | 150.612 | 9.4479 | 2.31 | 683.76 | 35.343 | 916.839 | 11.5038 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0.289444 | 3.53735 | 35.0776 | 2.20042 | 0.538 | 159.248 | 8.2314 | 213.532 | 2.67924 | 43.2306 | 428.69 | 26.8917 | 6.575 | 1946.2 | 100.598 | 2609.62 | 32.7435 | 4251.04 | 266.668 | 65.2 | 19299.2 | 997.56 | 25877.9 | 324.696 | 16.7281 | 4.09 | 1210.64 | 62.577 | 1623.32 | 20.3682 | 1 | 296 | 15.3 | 396.9 | 4.98 | 87616 | 4528.8 | 117482 | 1474.08 | 234.09 | 6072.57 | 76.194 | 157530 | 1976.56 | 24.8004 | . | 1 | 1 | 0.02731 | 0 | 7.07 | 0 | 0.469 | 6.421 | 78.9 | 4.9671 | 2 | 242 | 17.8 | 396.9 | 9.14 | 0.000745836 | 0 | 0.193082 | 0 | 0.0128084 | 0.175358 | 2.15476 | 0.135652 | 0.05462 | 6.60902 | 0.486118 | 10.8393 | 0.249613 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 49.9849 | 0 | 3.31583 | 45.3965 | 557.823 | 35.1174 | 14.14 | 1710.94 | 125.846 | 2806.08 | 64.6198 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0.219961 | 3.01145 | 37.0041 | 2.32957 | 0.938 | 113.498 | 8.3482 | 186.146 | 4.28666 | 41.2292 | 506.617 | 31.8937 | 12.842 | 1553.88 | 114.294 | 2548.49 | 58.6879 | 6225.21 | 391.904 | 157.8 | 19093.8 | 1404.42 | 31315.4 | 721.146 | 24.6721 | 9.9342 | 1202.04 | 88.4144 | 1971.44 | 45.3993 | 4 | 484 | 35.6 | 793.8 | 18.28 | 58564 | 4307.6 | 96049.8 | 2211.88 | 316.84 | 7064.82 | 162.692 | 157530 | 3627.67 | 83.5396 | . | 2 | 1 | 0.02729 | 0 | 7.07 | 0 | 0.469 | 7.185 | 61.1 | 4.9671 | 2 | 242 | 17.8 | 392.83 | 4.03 | 0.000744744 | 0 | 0.19294 | 0 | 0.012799 | 0.196079 | 1.66742 | 0.135552 | 0.05458 | 6.60418 | 0.485762 | 10.7203 | 0.109979 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 49.9849 | 0 | 3.31583 | 50.798 | 431.977 | 35.1174 | 14.14 | 1710.94 | 125.846 | 2777.31 | 28.4921 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0.219961 | 3.36976 | 28.6559 | 2.32957 | 0.938 | 113.498 | 8.3482 | 184.237 | 1.89007 | 51.6242 | 439.003 | 35.6886 | 14.37 | 1738.77 | 127.893 | 2822.48 | 28.9555 | 3733.21 | 303.49 | 122.2 | 14786.2 | 1087.58 | 24001.9 | 246.233 | 24.6721 | 9.9342 | 1202.04 | 88.4144 | 1951.23 | 20.0174 | 4 | 484 | 35.6 | 785.66 | 8.06 | 58564 | 4307.6 | 95064.9 | 975.26 | 316.84 | 6992.37 | 71.734 | 154315 | 1583.1 | 16.2409 | . | 3 | 1 | 0.03237 | 0 | 2.18 | 0 | 0.458 | 6.998 | 45.8 | 6.0622 | 3 | 222 | 18.7 | 394.63 | 2.94 | 0.00104782 | 0 | 0.0705666 | 0 | 0.0148255 | 0.226525 | 1.48255 | 0.196233 | 0.09711 | 7.18614 | 0.605319 | 12.7742 | 0.0951678 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 4.7524 | 0 | 0.99844 | 15.2556 | 99.844 | 13.2156 | 6.54 | 483.96 | 40.766 | 860.293 | 6.4092 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0.209764 | 3.20508 | 20.9764 | 2.77649 | 1.374 | 101.676 | 8.5646 | 180.741 | 1.34652 | 48.972 | 320.508 | 42.4233 | 20.994 | 1553.56 | 130.863 | 2761.62 | 20.5741 | 2097.64 | 277.649 | 137.4 | 10167.6 | 856.46 | 18074.1 | 134.652 | 36.7503 | 18.1866 | 1345.81 | 113.363 | 2392.33 | 17.8229 | 9 | 666 | 56.1 | 1183.89 | 8.82 | 49284 | 4151.4 | 87607.9 | 652.68 | 349.69 | 7379.58 | 54.978 | 155733 | 1160.21 | 8.6436 | . | 4 | 1 | 0.06905 | 0 | 2.18 | 0 | 0.458 | 7.147 | 54.2 | 6.0622 | 3 | 222 | 18.7 | 396.9 | 5.33 | 0.0047679 | 0 | 0.150529 | 0 | 0.0316249 | 0.4935 | 3.74251 | 0.418595 | 0.20715 | 15.3291 | 1.29123 | 27.4059 | 0.368036 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 4.7524 | 0 | 0.99844 | 15.5805 | 118.156 | 13.2156 | 6.54 | 483.96 | 40.766 | 865.242 | 11.6194 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0.209764 | 3.27333 | 24.8236 | 2.77649 | 1.374 | 101.676 | 8.5646 | 181.78 | 2.44114 | 51.0796 | 387.367 | 43.3265 | 21.441 | 1586.63 | 133.649 | 2836.64 | 38.0935 | 2937.64 | 328.571 | 162.6 | 12032.4 | 1013.54 | 21512 | 288.886 | 36.7503 | 18.1866 | 1345.81 | 113.363 | 2406.09 | 32.3115 | 9 | 666 | 56.1 | 1190.7 | 15.99 | 49284 | 4151.4 | 88111.8 | 1183.26 | 349.69 | 7422.03 | 99.671 | 157530 | 2115.48 | 28.4089 | . Â  . y = pd.DataFrame(boston_dataset.target, columns=['MEDV']) y.head() . | Â  | MEDV | . | 0 | 24.0 | . | 1 | 21.6 | . | 2 | 34.7 | . | 3 | 33.4 | . | 4 | 36.2 | . 3. train_test_split &amp; í•™ìŠµ . X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=5) . model = LinearRegression() model.fit(X_train, y_train) #í•™ìŠµ . LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False) . *thetaê°’ í™•ì¸ . model.coef_ # ë³€ìˆ˜ë³„ ê°€ì¤‘ì¹˜ . array([[ -0. , -5.09, -0.17, -5.97, 24.32, 165.18, 21.99, 1.03, -5.67, 3.22, -0.01, 5.35, -0.05, 0.75, 0. , 0.27, 0.59, 2.42, -0.03, 0.09, -0.01, -0.06, 0.36, -0.04, 0.54, -0. , 0.02, -0. , -0.01, -0.11, -1.28, 0.03, 0. , -0.01, -0. , 0. , -0.01, 0. , -0. , 0.03, -0.21, 1.3 , 0.31, 0. , 0.08, -0.01, 0. , -0.01, 0.01, -0.01, 24.32, -18.48, -6.89, 0.04, 3.05, -0.41, 0.02, -0.85, 0.03, -0.47, -46.79, 3.65, -0.6 , 15.93, -0.99, 0.13, -11.92, -0.04, 1.5 , 0.16, -0.06, -0.02, -0.15, -0.01, -0.54, -0. , -0.22, 0. , -0.01, 0.02, -0. , 0. , -0. , -0.01, 0.51, -0.1 , -0.01, -0.19, -0.01, 0.1 , -0.14, 0.01, -0.12, -0. , -0.05, -0. , 0.01, -0. , -0. , -0.01, 0.01, -0. , -0. , -0. , 0.01]]) . model.intercept_ # ì ˆí¸ . array([-141.9]) . 4. test dataë¡œ ì„±ëŠ¥ ì²´í¬ . | í‰ê· ì œê³±ê·¼ì˜¤ì°¨ í™•ì¸ y_test_prediction = model.predict(X_test) ## í‰ê· ì œê³±ê·¼ì˜¤ì°¨ (RMSE: Root Mean Square Error) mean_squared_error(y_test, y_test_prediction) ** 0.5 . 3.1965276513308156 . | R ìŠ¤í€˜ì–´ ê°’ í™•ì¸ print(model.score(X_train, y_train)) print(model.score(X_test, y_test)) . 0.9315569004651908 0.8694943908787136 . training dataëŠ” ì•½ 93%, test dataëŠ” ì•½ 87%ë¥¼ ì˜ ì˜ˆì¸¡í•œë‹¤ëŠ” ëœ» . | . ",
    "url": "https://chaelist.github.io/docs/ml_basics/linear_regression/#polynomial-regression-%EB%8B%A4%ED%95%AD%ED%9A%8C%EA%B7%80",
    "relUrl": "/docs/ml_basics/linear_regression/#polynomial-regression-ë‹¤í•­íšŒê·€"
  },"103": {
    "doc": "ê¸°ì´ˆ & Linear Regression",
    "title": "ê¸°ì´ˆ & Linear Regression",
    "content": " ",
    "url": "https://chaelist.github.io/docs/ml_basics/linear_regression/",
    "relUrl": "/docs/ml_basics/linear_regression/"
  },"104": {
    "doc": "ë¬¼ë¥˜ ìµœì í™”",
    "title": "ë¬¼ë¥˜ ìµœì í™”",
    "content": ". | ìš´ì†¡ ê²½ë¡œ ìµœì í™” . | ìš´ì†¡ ë¹„ìš© ìµœì†Œí™” ëª¨ë¸ ê³„ì‚° | ìµœì  ìš´ì†¡ ê²½ë¡œ ê°€ì‹œí™” | ì œì•½ ì¡°ê±´ ë§Œì¡± ì—¬ë¶€ í™•ì¸ | . | ìƒì‚° ê³„íš ìµœì í™” . | ì´ìµ ê·¹ëŒ€í™” ëª¨ë¸ ê³„ì‚° | ì œì•½ ì¡°ê±´ ë§Œì¡± ì—¬ë¶€ í™•ì¸ | . | ë¬¼ë¥˜ ë„¤íŠ¸ì›Œí¬ ìµœì í™” . | ë¬¼ë¥˜ ë„¤íŠ¸ì›Œí¬ ìµœì í™” ê³„ì‚° | ìˆ˜ìš” ë§Œì¡± ì—¬ë¶€ í™•ì¸ | ìµœì  ìš´ì†¡ ê²½ë¡œ í™•ì¸ | ìµœì  ìƒì‚°ëŸ‰ í™•ì¸ | . | . ",
    "url": "https://chaelist.github.io/docs/ml_application/logistics/",
    "relUrl": "/docs/ml_application/logistics/"
  },"105": {
    "doc": "ë¬¼ë¥˜ ìµœì í™”",
    "title": "ìš´ì†¡ ê²½ë¡œ ìµœì í™”",
    "content": "# í•„ìš”í•œ libraryë¥¼ import import numpy as np import pandas as pd from itertools import product from pulp import LpVariable, lpSum, value from ortoolpy import model_min, addvars, addvals . | itertools: íš¨ìœ¨ì ì¸ ë°˜ë³µì„ ìœ„í•œ í•¨ìˆ˜. - ex) itertools.product(AB, xy) â†’ Ax, Ay, Bx, By | PuLP: ìµœì í™” ëª¨ë¸ì„ ì‘ì„±í•˜ëŠ” ì—­í• . | ortoolpy: ëª©ì  í•¨ìˆ˜ë¥¼ ìƒì„±í•´ì„œ ìµœì í™” ë¬¸ì œë¥¼ í‘¸ëŠ” ì—­í• . | pupl, ortoolpyëŠ” ë³„ë„ë¡œ ì„¤ì¹˜í•´ì•¼ ì‚¬ìš© ê°€ëŠ¥ (pip installë¡œ ì„¤ì¹˜) | . # ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° ## ë°ì´í„° ì¶œì²˜: https://github.com/wikibook/pyda100 df_tc = pd.read_csv('data/trans_cost.csv', index_col='ê³µì¥') df_tr = pd.read_csv('data/trans_route.csv', index_col=\"ê³µì¥\") df_demand = pd.read_csv('data/demand.csv') df_supply = pd.read_csv('data/supply.csv') . | trans_cost.csv: ì°½ê³ ì™€ ê³µì¥ ê°„ì˜ ìš´ì†¡ ë¹„ìš© | trans_route.csv: ìš´ì†¡ ê²½ë¡œ | demand.csv: ê³µì¥ì˜ ì œí’ˆ ìƒì‚°ëŸ‰ì— ëŒ€í•œ ìˆ˜ìš” | supply.csv: ì°½ê³ ê°€ ê³µê¸‰ ê°€ëŠ¥í•œ ìµœëŒ€ ë¶€í’ˆ ìˆ˜ | . print(df_tc, '\\n') print(df_tr, '\\n') print(df_demand, '\\n') print(df_supply) . F1 F2 F3 F4 ê³µì¥ W1 10 10 11 27 W2 18 21 12 14 W3 15 12 14 12 F1 F2 F3 F4 ê³µì¥ W1 15 15 0 5 W2 5 0 30 5 W3 10 15 2 15 F1 F2 F3 F4 0 28 29 31 25 W1 W2 W3 0 35 41 42 . ìš´ì†¡ ë¹„ìš© ìµœì†Œí™” ëª¨ë¸ ê³„ì‚° . | í˜„ì¬ì˜ ìš´ì†¡ ë¹„ìš© ê³„ì‚° . total_cost = 0 for i in df_tc.index: for c in df_tc.columns: total_cost += df_tr.loc[i, c] * df_tc.loc[i, c] print(f\"í˜„ì¬ ì´ ìš´ì†¡ ë¹„ìš©: {total_cost}ë§Œì›\") . í˜„ì¬ ì´ ìš´ì†¡ ë¹„ìš©: 1493ë§Œì› . | ìš´ì†¡ ë¹„ìš©ì„ ìµœì†Œí™”í•˜ëŠ” ìš´ì†¡ ê²½ë¡œë¥¼ ê³„ì‚° . warehouses = len(df_tc.index) # ì°½ê³  ìˆ˜: 3 factories = len(df_tc.columns) # ê³µì¥ ìˆ˜: 4 iter_pr = list(product(range(warehouses), range(factories))) # [(0, 0), (0, 1), (0, 2), (0, 3), (1, 0), (1, 1), (1, 2), (1, 3), (2, 0), (2, 1), (2, 2), (2, 3)] ## ìˆ˜ë¦¬ ëª¨ë¸ ì‘ì„± model = model_min() # ëª©ì  í•¨ìˆ˜ë¥¼ ì œì•½ ì¡°ê±´ í•˜ì—ì„œ 'ìµœì†Œí™”'í•˜ëŠ” ëª¨ë¸ì„ ì •ì˜ v1 = {(i,j): LpVariable(f'v{i}_{j}', lowBound=0) for i,j in pr} # prì˜ i,jë¥¼ dict í˜•ì‹ìœ¼ë¡œ ì €ì¥ = ex) 0, 0 â†’ (0, 0): v0_0 # ëª©ì  í•¨ìˆ˜ ì¶”ê°€ model += lpSum(df_tc.iloc[i, j] * v1[i, j] for i, j in iter_pr) # ì œì•½ ì¡°ê±´ ì¶”ê°€ for i in range(nw): model += lpSum(v1[i, j] for j in range(factories)) &lt;= df_supply.iloc[0, i] # ê° ì°½ê³ ê°€ ê³µê¸‰í•  ë¶€í’ˆì´ ìµœëŒ€ ê³µê¸‰ ê°€ëŠ¥í•œ í•œê³„ë¥¼ ë„˜ì§€ ì•Šë„ë¡ for j in range(nf): model += lpSum(v1[i, j] for i in range(warehouses)) &gt;= df_demand.iloc[0, j] # ê° ê³µì¥ì´ ì œì¡°í•  ì œí’ˆ ìˆ˜ìš”ëŸ‰ì„ ë§Œì¡±ì‹œí‚¤ë„ë¡ model.solve() # ìƒì„±í•œ ìˆ˜ë¦¬ ëª¨ë¸ì„ ê³„ì‚° . 1 . +) ì‘ì„±ëœ ìˆ˜ë¦¬ ëª¨ë¸ í™•ì¸í•´ë³´ê¸° . model . NoName: MINIMIZE 10*v0_0 + 10*v0_1 + 11*v0_2 + 27*v0_3 + 18*v1_0 + 21*v1_1 + 12*v1_2 + 14*v1_3 + 15*v2_0 + 12*v2_1 + 14*v2_2 + 12*v2_3 + 0 SUBJECT TO _C1: v0_0 + v0_1 + v0_2 + v0_3 &lt;= 35 _C2: v1_0 + v1_1 + v1_2 + v1_3 &lt;= 41 _C3: v2_0 + v2_1 + v2_2 + v2_3 &lt;= 42 _C4: v0_0 + v1_0 + v2_0 &gt;= 28 _C5: v0_1 + v1_1 + v2_1 &gt;= 29 _C6: v0_2 + v1_2 + v2_2 &gt;= 31 _C7: v0_3 + v1_3 + v2_3 &gt;= 25 VARIABLES v0_0 Continuous v0_1 Continuous v0_2 Continuous v0_3 Continuous v1_0 Continuous v1_1 Continuous v1_2 Continuous v1_3 Continuous v2_0 Continuous v2_1 Continuous v2_2 Continuous v2_3 Continuous . | ìµœì í™”ëœ ìš´ì†¡ ê²½ë¡œ í™•ì¸ ë° ìš´ì†¡ ë¹„ìš© ê³„ì‚° . df_tr_solved = df_tr.copy() # df_trê³¼ ê°™ì€ index, columnì„ ê°–ëŠ” dfë¥¼ ìƒì„±í•˜ê¸° ìœ„í•´ ë³µì œ for k, x in v1.items(): # k, x ì˜ˆì‹œ: ((0, 0), v0_0) i, j = k[0], k[1] df_tr_solved.iloc[i, j] = value(x) # pulp.value: ê³„ì‚°ëœ LpVariableì˜ ê°’ì„ ëŒë ¤ì¤Œ print(df_tr_solved) print(f\"\\nìµœì í™”ëœ ì´ ìš´ì†¡ ë¹„ìš©: {value(model.objective) :.0f}ë§Œì›\") # pulp.value(model.objective): ìµœì í™”ëœ ê²°ê³¼ê°’ì„ í™•ì¸ ê°€ëŠ¥ . F1 F2 F3 F4 ê³µì¥ W1 28 7 0 0 W2 0 0 31 5 W3 0 22 0 20 ìµœì í™”ëœ ì´ ìš´ì†¡ ë¹„ìš©: 1296ë§Œì› . | ìœ„ì—ì„œ ê³„ì‚°í–ˆë˜ 1493ë§Œì›ë³´ë‹¤ ë¹„ìš©ì´ í¬ê²Œ ì ˆê°ë¨! | . | . ìµœì  ìš´ì†¡ ê²½ë¡œ ê°€ì‹œí™” . import matplotlib.pyplot as plt import networkx as nx import seaborn as sns . | ì°½ê³  - ê³µì¥ ê°„ ê²½ë¡œë¥¼ ë„¤íŠ¸ì›Œí¬ë¡œ ë§Œë“¤ì–´ ì‹œê°í™” . # ê°ì²´ ìƒì„± G = nx.Graph() # ë…¸ë“œ ì„¤ì • G.add_nodes_from(df_tr_solved.columns) G.add_nodes_from(df_tr_solved.index) # edge ì„¤ì • for index in df_tr_solved.index: for column in df_tr_solved.columns: if df_tr_solved.loc[index, column] != 0: # 0ì´ ì•„ë‹Œ ê²½ìš°ë§Œ ì¶”ê°€ G.add_edge(index, column) G[index][column]['weight'] = df_tr_solved.loc[index, column] # edge ë° weight í™•ì¸ G.edges(data=True) . EdgeDataView([('F1', 'W1', {'weight': 28}), ('F2', 'W1', {'weight': 7}), ('F2', 'W3', {'weight': 22}), ('F3', 'W2', {'weight': 31}), ('F4', 'W2', {'weight': 5}), ('F4', 'W3', {'weight': 20})]) . # ì¢Œí‘œ ì„¤ì •: ì˜ ë³´ì´ë„ë¡ ìœ„ì¹˜ë¥¼ ì„ì˜ë¡œ ì„¤ì • pos = {'W1':(0,1), 'W2':(0,2), 'W3':(0,3), 'F1':(4, 0.5), 'F2':(4,1.5), 'F3':(4,2.5), 'F4':(4,3.5)} # ê·¸ë¦¬ê¸° ## node &amp; ë„¤íŠ¸ì›Œí¬ ê·¸ë¦¬ê¸° weights = [G[i][c]['weight']*0.1 for i,c in G.edges] nx.draw_networkx(G, pos, font_size=14, node_size = 500, node_color='k', font_color='w', width=weights) # weightì— ë”°ë¼ ì„ ì˜ êµµê¸°ë¥¼ ë‹¤ë¥´ê²Œ í‘œì‹œ ## weight ê·¸ë ¤ë„£ê¸° labels = nx.get_edge_attributes(G, 'weight') nx.draw_networkx_edge_labels(G, pos, font_size=12, edge_labels=labels) plt.axis('off') # turn off axis plt.show() . | heatmapìœ¼ë¡œë„ ì‹œê°í™”í•´ë³´ê¸° . df_tr_copy = df_tr_solved.copy() df_tr_copy.index.name = None # ê·¸ë˜í”„ì—ì„œ index nameì„ ì•ˆë³´ì´ê²Œ í•˜ë ¤ê³  ì§€ì›€ sns.heatmap(df_tr_copy, annot=True, cmap=\"YlGnBu\"); . | W1 â†’ F1, W2 â†’ F3, W3 â†’ F4 &amp; F2ë¡œì˜ ê³µê¸‰ì´ ëŒ€ë¶€ë¶„ì´ë©°, ê·¸ ì™¸ì˜ ê³µê¸‰ì€ ìµœì†Œí•œìœ¼ë¡œ ì œí•œë¨ | ìš´ì†¡ ê²½ë¡œê°€ ì–´ëŠ ì •ë„ ì§‘ì¤‘ë˜ì–´ì•¼ ë¹„ìš©ì´ ì ˆê°ë˜ëŠ” ê²ƒâ€¦ | . | . ì œì•½ ì¡°ê±´ ë§Œì¡± ì—¬ë¶€ í™•ì¸ . : ìœ„ì—ì„œ ê³„ì‚°í•œ ìµœì  ìš´ì†¡ ê²½ë¡œê°€ ì œì•½ ì¡°ê±´ì„ ì˜ ë§Œì¡±í•˜ê³  ìˆëŠ”ì§€ í™•ì¸í•´ë³´ê¸° . print(df_demand, '\\n') print(df_supply, '\\n') print(df_tr_solved) . F1 F2 F3 F4 0 28 29 31 25 W1 W2 W3 0 35 41 42 F1 F2 F3 F4 ê³µì¥ W1 28 7 0 0 W2 0 0 31 5 W3 0 22 0 20 . # ê³µê¸‰ì¸¡ flag = np.zeros(len(df_supply.columns)) for i in range(len(df_supply.columns)): supply_limit = df_supply.iloc[0, i] supply = df_tr_sol.iloc[i, :].sum() print(f'{df_supply.columns[i]} ê³µê¸‰ ê°€ëŠ¥ í•œê³„: {supply_limit}, ìˆ˜ìš”: {supply}') if supply &lt;= supply_limit: flag[i] = 1 print(f'&gt;&gt; ê³µê¸‰ ì¡°ê±´ ê³„ì‚° ê²°ê³¼: {flag}\\n') # ìˆ˜ìš”ì¸¡ flag = np.zeros(len(df_demand.columns)) for i in range(len(df_demand.columns)): demand_limit = df_demand.iloc[0, i] demand = df_tr_sol.iloc[:, i].sum() print(f'{df_demand.columns[i]} ìµœì†Œ í•„ìš”ëŸ‰: {demand_limit}, ê³µê¸‰: {demand}') if demand &gt;= demand_limit: flag[i] = 1 print(f'&gt;&gt; ìˆ˜ìš” ì¡°ê±´ ê³„ì‚° ê²°ê³¼: {flag}') . W1 ê³µê¸‰ ê°€ëŠ¥ í•œê³„: 35, ìˆ˜ìš”: 35 W2 ê³µê¸‰ ê°€ëŠ¥ í•œê³„: 41, ìˆ˜ìš”: 36 W3 ê³µê¸‰ ê°€ëŠ¥ í•œê³„: 42, ìˆ˜ìš”: 42 &gt;&gt; ê³µê¸‰ ì¡°ê±´ ê³„ì‚° ê²°ê³¼: [1. 1. 1.] F1 ìµœì†Œ í•„ìš”ëŸ‰: 28, ê³µê¸‰: 28 F2 ìµœì†Œ í•„ìš”ëŸ‰: 29, ê³µê¸‰: 29 F3 ìµœì†Œ í•„ìš”ëŸ‰: 31, ê³µê¸‰: 31 F4 ìµœì†Œ í•„ìš”ëŸ‰: 25, ê³µê¸‰: 25 &gt;&gt; ìˆ˜ìš” ì¡°ê±´ ê³„ì‚° ê²°ê³¼: [1. 1. 1. 1.] . ",
    "url": "https://chaelist.github.io/docs/ml_application/logistics/#%EC%9A%B4%EC%86%A1-%EA%B2%BD%EB%A1%9C-%EC%B5%9C%EC%A0%81%ED%99%94",
    "relUrl": "/docs/ml_application/logistics/#ìš´ì†¡-ê²½ë¡œ-ìµœì í™”"
  },"106": {
    "doc": "ë¬¼ë¥˜ ìµœì í™”",
    "title": "ìƒì‚° ê³„íš ìµœì í™”",
    "content": ": ì–´ë–¤ ì œí’ˆì„ ì–¼ë§ˆë‚˜ ìƒì‚°í•˜ëŠ” ê²ƒì´ ì´ìµì„ ê·¹ëŒ€í™”í•˜ëŠ”ì§€ ê³„ì‚° . ## ë°ì´í„° ì¶œì²˜: https://github.com/wikibook/pyda100 df_material = pd.read_csv('data/product_plan_material.csv', index_col=\"ì œí’ˆ\") df_profit = pd.read_csv('data/product_plan_profit.csv', index_col=\"ì œí’ˆ\") df_stock = pd.read_csv('data/product_plan_stock.csv', index_col=\"í•­ëª©\") df_plan = pd.read_csv('data/product_plan.csv', index_col=\"ì œí’ˆ\") . | product_plan_material.csv: ì œí’ˆ ì œì¡°ì— í•„ìš”í•œ ì›ë£Œ ë¹„ìœ¨ | product_plan_profit.csv: ì œí’ˆ ì´ìµ | product_plan_stock.csv: ì›ë£Œ ì¬ê³  | product_plan.csv: ì œí’ˆ ìƒì‚°ëŸ‰ | . print(df_material, '\\n') print(df_profit, '\\n') print(df_stock, '\\n') print(df_plan) . ì›ë£Œ1 ì›ë£Œ2 ì›ë£Œ3 ì œí’ˆ ì œí’ˆ1 1 4 3 ì œí’ˆ2 2 4 1 ì´ìµ ì œí’ˆ ì œí’ˆ1 5.0 ì œí’ˆ2 4.0 ì›ë£Œ1 ì›ë£Œ2 ì›ë£Œ3 í•­ëª© ì¬ê³  40 80 50 ìƒì‚°ëŸ‰ ì œí’ˆ ì œí’ˆ1 16 ì œí’ˆ2 0 . | í˜„ì¬ëŠ” ë” ì´ìµì´ ë†’ì€ ì œí’ˆ1ë§Œ ìƒì‚°ë˜ê³  ìˆìŒ. (ì›ë£Œê°€ íš¨ìœ¨ì ìœ¼ë¡œ ì‚¬ìš©ë˜ê³  ìˆì§€ ì•ŠìŒ) | . ì´ìµ ê·¹ëŒ€í™” ëª¨ë¸ ê³„ì‚° . | í˜„ì¬ì˜ ì´ ì´ìµ ê³„ì‚° . profit = 0 for i in range(len(df_profit.index)): for j in range(len(df_plan.columns)): profit += df_profit.iloc[i, j] * df_plan.iloc[i, j] # ê° ì œí’ˆì˜ ì´ìµ * ìƒì‚°ëŸ‰ print(f'í˜„ì¬ ì´ ì´ìµ: {profit :.0f}ë§Œì›') . í˜„ì¬ ì´ ì´ìµ: 80ë§Œì› . | ì´ìµì„ ê·¹ëŒ€í™”í•˜ëŠ” ìƒì‚°ëŸ‰ì„ ê³„ì‚° . from pulp import LpVariable, lpSum, value from ortoolpy import model_max, addvars, addvals # ìš´ì†¡ ë¹„ìš© ìµœì†Œí™” ë¬¸ì œì™€ ë‹¤ë¥´ê²Œ model_maxë¥¼ ì‚¬ìš© ## ìˆ˜ë¦¬ ëª¨ë¸ ì‘ì„± model = model_max() # ëª©ì  í•¨ìˆ˜ë¥¼ ì œì•½ì¡°ê±´ í•˜ì—ì„œ 'ìµœëŒ€í™”'í•˜ëŠ” ëª¨ë¸ì„ ì •ì˜ v1 = {i: LpVariable(f'v{i}', lowBound=0) for i in range(len(df_profit))} # {0: v0, 1: v1} # ëª©ì  í•¨ìˆ˜ ì¶”ê°€ model += lpSum(df_profit.iloc[i] * v1[i] for i in range(len(df_profit))) # ì œí’ˆë³„ ì´ìµ * ìƒì‚°ëŸ‰ # ì œì•½ ì¡°ê±´ ì¶”ê°€ for i in range(len(df_material.columns)): model += lpSum(df_material.iloc[j, i] * v1[j] for j in range(len(df_profit)) ) &lt;= df_stock.iloc[:, i] # ì œí’ˆì— ì‚¬ìš©ë˜ëŠ” ì›ë£Œì˜ ì´ í•©ì´ ì¬ê³  ìˆ˜ë¥¼ ë„˜ì§€ ì•Šë„ë¡ model.solve() # ìƒì„±í•œ ìˆ˜ë¦¬ ëª¨ë¸ì„ ê³„ì‚° . 1 . +) ì‘ì„±ëœ ìˆ˜ë¦¬ ëª¨ë¸ í™•ì¸í•´ë³´ê¸° . model . NoName: MAXIMIZE 5.0*v0 + 4.0*v1 + 0.0 SUBJECT TO _C1: v0 + 2 v1 &lt;= 40 _C2: 4 v0 + 4 v1 &lt;= 80 _C3: 3 v0 + v1 &lt;= 50 VARIABLES v0 Continuous v1 Continuous . | ìµœì í™”ëœ ìƒì‚° ê³„íš ë° ì´ìµ ê³„ì‚° . df_plan_solved = df_plan.copy() for k, x in v1.items(): # k, x ì˜ˆì‹œ: (0, v0) df_plan_solved.iloc[k] = value(x) print(df_plan_solved) print(f'\\nìµœì í™”ëœ ì´ ì´ìµ: {value(m.objective) :.0f}ë§Œì›') . ìƒì‚°ëŸ‰ ì œí’ˆ ì œí’ˆ1 15 ì œí’ˆ2 5 ì´ ì´ìµ: 95ë§Œì› . | ìœ„ì—ì„œ ê³„ì‚°í–ˆë˜ ì´ìµ 80ë§Œì›ë³´ë‹¤ ì¦ê°€í•¨! (íš¨ìœ¨ì ìœ¼ë¡œ ì‚¬ìš©ë˜ì§€ ì•Šê³  ìˆë˜ ì›ë£Œë“¤ì„ í™œìš©í•´ ì œí’ˆ2 ìƒì‚°ì„ ëŠ˜ë¦¬ëŠ” ê²ƒì´ ì´ìµ ì¦ê°€ë¡œ ì´ì–´ì§) | . | . ì œì•½ ì¡°ê±´ ë§Œì¡± ì—¬ë¶€ í™•ì¸ . print(df_plan_solved, '\\n') print(df_material, '\\n') print(df_stock) . ìƒì‚°ëŸ‰ ì œí’ˆ ì œí’ˆ1 15 ì œí’ˆ2 5 ì›ë£Œ1 ì›ë£Œ2 ì›ë£Œ3 ì œí’ˆ ì œí’ˆ1 1 4 3 ì œí’ˆ2 2 4 1 ì›ë£Œ1 ì›ë£Œ2 ì›ë£Œ3 í•­ëª© ì¬ê³  40 80 50 . # ì œì•½ ì¡°ê±´ ê³„ì‚° flag = np.zeros(len(df_material.columns)) for i in range(len(df_material.columns)): usage = 0 for j in range(len(df_material.index)): usage += df_plan_solved.iloc[j, 0] * df_material.iloc[j, i] stock = df_stock.iloc[0, i] print(f'{df_material.columns[i]} ì‚¬ìš©ëŸ‰: {usage}, ì¬ê³ : {stock}') if usage &lt;= stock: flag[i] = 1 print(f'&gt;&gt; ì œì•½ ì¡°ê±´ ê³„ì‚° ê²°ê³¼: {flag}') . ì›ë£Œ1 ì‚¬ìš©ëŸ‰: 25, ì¬ê³ : 40 ì›ë£Œ2 ì‚¬ìš©ëŸ‰: 80, ì¬ê³ : 80 ì›ë£Œ3 ì‚¬ìš©ëŸ‰: 50, ì¬ê³ : 50 &gt;&gt; ì œì•½ ì¡°ê±´ ê³„ì‚° ê²°ê³¼: [1. 1. 1.] . | ì›ë£Œ2ì™€ ì›ë£Œ3ì€ ì¬ê³ ë¥¼ ëª¨ë‘ ì‚¬ìš©. ì›ë£Œ1ì€ ì¡°ê¸ˆ ë‚¨ì§€ë§Œ, ìµœì í™” ê³„ì‚° ì „ë³´ë‹¤ ì‚¬ìš© íš¨ìœ¨ì´ í¬ê²Œ ê°œì„ ë¨. | . ",
    "url": "https://chaelist.github.io/docs/ml_application/logistics/#%EC%83%9D%EC%82%B0-%EA%B3%84%ED%9A%8D-%EC%B5%9C%EC%A0%81%ED%99%94",
    "relUrl": "/docs/ml_application/logistics/#ìƒì‚°-ê³„íš-ìµœì í™”"
  },"107": {
    "doc": "ë¬¼ë¥˜ ìµœì í™”",
    "title": "ë¬¼ë¥˜ ë„¤íŠ¸ì›Œí¬ ìµœì í™”",
    "content": ": ìš´ì†¡ ê²½ë¡œ &amp; ìƒì‚° ê³„íš ìµœì í™”ë¥¼ ë™ì‹œì— (ì‹¤ì œ ë¬¼ë¥˜ ë„¤íŠ¸ì›Œí¬ ì„¤ê³„ì—ì„œëŠ” ë‘ ê°€ì§€ë¥¼ ëª¨ë‘ ê³ ë ¤í•´ì•¼ í•œë‹¤) . | ìˆ˜ìš”ë¥¼ ë§Œì¡±ì‹œí‚¤ëŠ” ì„ ì—ì„œ ìš´ì†¡ ë¹„ìš©ê³¼ ì œì¡° ë¹„ìš©ì´ ìµœì†Œê°€ ë˜ì–´ì•¼ í•¨ | ëª©ì  í•¨ìˆ˜: ìš´ì†¡ ë¹„ìš© + ì œì¡° ë¹„ìš© | ì œì•½ ì¡°ê±´: ê° ëŒ€ë¦¬ì ì˜ íŒë§¤ ìˆ˜ &gt;= ìˆ˜ìš” ìˆ˜ (ìˆ˜ìš”ë¥¼ ì¶©ì¡±í•˜ê³  ë‚¨ì„ ë§Œí¼ ìƒí’ˆì´ ì¤€ë¹„ë˜ì–´ì•¼ í•¨) | . ë¬¼ë¥˜ ë„¤íŠ¸ì›Œí¬ ìµœì í™” ê³„ì‚° . # ë°ì´í„° ì§ì ‘ ìƒì„± ## ì°¸ê³ : https://github.com/wikibook/pyda100 item = ['A', 'B'] # ì œí’ˆ: A, B 2ê°œ store = ['P', 'Q'] # ëŒ€ë¦¬ì : P, Q 2ê°œ factory = ['X', 'Y'] # ê³µì¥: X, Y 2ê°œ prod_line = [0, 1] # ê° ê³µì¥ë§ˆë‹¤ ìƒì‚°ë¼ì¸ 2ê°œ (0, 1) # ëŒ€ë¦¬ì ë³„ ì œí’ˆë³„ ìˆ˜ìš” df_demand = pd.DataFrame(list(product(store, item)), columns=['ëŒ€ë¦¬ì ', 'ì œí’ˆ']) df_demand['ìˆ˜ìš”'] = [10, 10, 20, 20] print(df_demand, '\\n') # ëŒ€ë¦¬ì  â†’ ê³µì¥ ìš´ì†¡ë¹„ df_trans_fee = pd.DataFrame(list(product(store, factory)), columns=['ëŒ€ë¦¬ì ', 'ê³µì¥']) df_trans_fee['ìš´ì†¡ë¹„'] = [1, 2, 3, 1] print(df_trans_fee, '\\n') # ê³µì¥ ë¼ì¸ë³„ ì œí’ˆë³„ ìƒì‚°ê°€ëŠ¥ëŸ‰, ìƒì‚°ë¹„ df_prod = pd.DataFrame(list(product(factory, prod_line, item)), columns=['ê³µì¥', 'ìƒì‚°ë¼ì¸', 'ì œí’ˆ']) df_prod['í•˜í•œ'] = 0 df_prod['ìƒí•œ'] = np.inf df_prod.loc[4,'ìƒí•œ'] = 10 df_prod['ìƒì‚°ë¹„'] = [1, np.nan, np.nan, 1, 3, np.nan, 5, 3] # ëª‡ ê°œ ì¡°í•©ì€ ì¼ë¶€ë¡œ nullë¡œ ë§Œë“¤ì–´ì„œ ì‚­ì œ (í•´ë‹¹ ìƒì‚°ë¼ì¸ì—ì„œ ìƒì‚°X) df_prod.dropna(inplace=True) print(df_prod) . ëŒ€ë¦¬ì  ì œí’ˆ ìˆ˜ìš” 0 P A 10 1 P B 10 2 Q A 20 3 Q B 20 ëŒ€ë¦¬ì  ê³µì¥ ìš´ì†¡ë¹„ 0 P X 1 1 P Y 2 2 Q X 3 3 Q Y 1 ê³µì¥ ìƒì‚°ë¼ì¸ ì œí’ˆ í•˜í•œ ìƒí•œ ìƒì‚°ë¹„ 0 X 0 A 0 inf 1.0 3 X 1 B 0 inf 1.0 4 Y 0 A 0 10.0 3.0 6 Y 1 A 0 inf 5.0 7 Y 1 B 0 inf 3.0 . â†’ ìµœì  ìƒì‚°ëŸ‰ &amp; ìµœì  ìš´ì†¡ëŸ‰ ê³„ì‚° (ìš´ì†¡ë¹„, ìƒì‚°ë¹„ë¥¼ ìµœì†Œí™”í•˜ëŠ” ì œí’ˆë³„ ìƒì‚°ëŸ‰ &amp; ìš´ì†¡ ê²½ë¡œ ë°°ì¹˜ ê³„ì‚°) . from ortoolpy import logistics_network _, df_trans, _ = logistics_network(df_demand, df_trans_fee, df_prod, dep = 'ëŒ€ë¦¬ì ', dem = 'ìˆ˜ìš”', fac = 'ê³µì¥', prd = 'ì œí’ˆ', tcs = 'ìš´ì†¡ë¹„', pcs = 'ìƒì‚°ë¹„', lwb = 'í•˜í•œ', upb = 'ìƒí•œ') print(df_prod, '\\n') # ValY: ê³„ì‚°ëœ ìµœì  ìƒì‚°ëŸ‰ print(df_trans) # ValX: ê³„ì‚°ëœ ìµœì  ìš´ì†¡ëŸ‰ . ê³µì¥ ìƒì‚°ë¼ì¸ ì œí’ˆ í•˜í•œ ìƒí•œ ìƒì‚°ë¹„ VarY ValY 0 X 0 A 0 inf 1.0 v000009 20.0 3 X 1 B 0 inf 1.0 v000010 10.0 4 Y 0 A 0 10.0 3.0 v000011 10.0 6 Y 1 A 0 inf 5.0 v000012 0.0 7 Y 1 B 0 inf 3.0 v000013 20.0 ëŒ€ë¦¬ì  ê³µì¥ ìš´ì†¡ë¹„ ì œí’ˆ VarX ValX 0 P X 1 A v000001 10.0 1 P X 1 B v000002 10.0 2 Q X 3 A v000003 10.0 3 Q X 3 B v000004 0.0 4 P Y 2 A v000005 0.0 5 P Y 2 B v000006 0.0 6 Q Y 1 A v000007 10.0 7 Q Y 1 B v000008 20.0 . +) ortoolpy.logistics_network ì„¤ëª…: . | input: . | ìˆœì„œëŒ€ë¡œ df1, df2, df3ê°€ í•„ìš”: . | df1: ìˆ˜ìš”ì§€(ì œí’ˆì„ ìš´ì†¡ë°›ì„ íŒë§¤ì²˜), ì œí’ˆ, ìˆ˜ìš” 3ê°œì˜ ì •ë³´ë¥¼ ë‹´ì€ dataframe | df2: ìˆ˜ìš”ì§€, ê³µì¥, ìš´ì†¡ë¹„ 3ê°œì˜ ì •ë³´ë¥¼ ë‹´ì€ dataframe | df3: ê³µì¥, ì œí’ˆ, ìƒì‚°ë¹„ 3ê°œì˜ ì •ë³´ë¥¼ ë‹´ì€ dataframe (+ í•˜í•œ, ìƒí•œ ì •ë³´ëŠ” optional) | . | dep, dem, fac, prd, tcs, pcs, lwb, upb ë³€ìˆ˜ì— ê°ê° í•´ë‹¹ë˜ëŠ” ì»¬ëŸ¼ëª…ì„ ëŒ€ì‘ì‹œì¼œì¤Œ | . | output: . | í•´ì˜ ìœ ë¬´ (ê³„ì‚°ì´ ì˜ ë˜ì—ˆëŠ”ì§€), ìš´ì†¡í‘œ (ìš´ì†¡ëŸ‰ì´ ê³„ì‚°ëœ dataframe), ìƒì‚°í‘œ(ìƒì‚°ëŸ‰ì´ ê³„ì‚°ëœ dataframe) | í•´ì˜ ìœ ë¬´: ê³„ì‚°ì´ ê°€ëŠ¥í•˜ë©´ Trueë¥¼ return | ìš´ì†¡í‘œ: df1, df2ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê³„ì‚°ëœ ìƒˆë¡œìš´ dataframe | ìƒì‚°í‘œ: df3ì— ê³„ì‚°ê°’ì´ ì¶”ê°€ëœ ê²ƒ (ìƒì‚°í‘œëŠ” df3 ìì²´ê°€ ë³€í˜•ë˜ë¯€ë¡œ ë³„ë„ë¡œ ë°›ì•„ì„œ ì €ì¥í•  í•„ìš”X) | . | . ìˆ˜ìš” ë§Œì¡± ì—¬ë¶€ í™•ì¸ . calculated = df_trans.groupby(['ëŒ€ë¦¬ì ', 'ì œí’ˆ'])[['ValX']].sum().reset_index() calculated.rename(columns={'ValX':'ê³„ì‚°ëœ ìµœì  ìš´ì†¡ëŸ‰'}, inplace=True) calculated = pd.merge(tbde, calculated, on=['ëŒ€ë¦¬ì ', 'ì œí’ˆ']) calculated['ìˆ˜ìš” ë§Œì¡± ì—¬ë¶€'] = np.where(calculated['ìˆ˜ìš”'] &lt;= calculated['ê³„ì‚°ëœ ìµœì  ìš´ì†¡ëŸ‰'], 'O', 'X') # ìˆ˜ìš” &lt;= ê³„ì‚°ëœ ìµœì  ìš´ì†¡ëŸ‰ì´ë©´ O calculated . | Â  | ëŒ€ë¦¬ì  | ì œí’ˆ | ìˆ˜ìš” | ê³„ì‚°ëœ ìµœì  ìš´ì†¡ëŸ‰ | ìˆ˜ìš” ë§Œì¡± ì—¬ë¶€ | . | 0 | P | A | 10 | 10 | O | . | 1 | P | B | 10 | 10 | O | . | 2 | Q | A | 20 | 20 | O | . | 3 | Q | B | 20 | 20 | O | . | ìš´ì†¡ëŸ‰ì´ ëª¨ë‘ ëŒ€ë¦¬ì ë³„ë¡œ í•„ìš”í•œ ì œí’ˆ ìˆ˜ìš”ë¥¼ ì™„ë²½íˆ ë§Œì¡±í•˜ê³  ìˆìŒì„ í™•ì¸ ê°€ëŠ¥ | . ìµœì  ìš´ì†¡ ê²½ë¡œ í™•ì¸ . | ë³´ê¸° í¸í•˜ê²Œ ì¹¼ëŸ¼ ìˆœì„œ ì •ë ¬ . # ì–´ëŠ ê³µì¥ì—ì„œ ì–´ëŠ ëŒ€ë¦¬ì ìœ¼ë¡œ ìš´ì†¡í•˜ëŠ”ì§€, ë³´ê¸° í¸í•˜ê²Œ ì¹¼ëŸ¼ ìˆœì„œ ì •ë ¬ df_trans = df_trans[['ê³µì¥', 'ëŒ€ë¦¬ì ', 'ìš´ì†¡ë¹„', 'ì œí’ˆ', 'VarX', 'ValX']] df_trans . | Â  | ê³µì¥ | ëŒ€ë¦¬ì  | ìš´ì†¡ë¹„ | ì œí’ˆ | VarX | ValX | . | 0 | X | P | 1 | A | v000001 | 10 | . | 1 | X | P | 1 | B | v000002 | 10 | . | 2 | X | Q | 3 | A | v000003 | 10 | . | 3 | X | Q | 3 | B | v000004 | 0 | . | 4 | Y | P | 2 | A | v000005 | 0 | . | 5 | Y | P | 2 | B | v000006 | 0 | . | 6 | Y | Q | 1 | A | v000007 | 10 | . | 7 | Y | Q | 1 | B | v000008 | 20 | . | ì´ ìš´ì†¡ë¹„ ê³„ì‚° . trans_cost = sum(df_trans['ìš´ì†¡ë¹„'] * df_trans['ValX']) print(f'ì´ ìš´ì†¡ë¹„: {trans_cost :.0f}ë§Œì›') . ì´ ìš´ì†¡ë¹„: 80ë§Œì› . | ìš´ì†¡ ê²½ë¡œ heatmapìœ¼ë¡œ ì‹œê°í™” . plt.rcParams['font.family'] = 'Malgun Gothic' # í•œê¸€ í°íŠ¸ ì„¤ì • (ê¹¨ì§ ë°©ì§€) plt.rc('font', size=12) # ê¸€ì í¬ê¸° ì¡°ì • plt.figure(figsize=(6, 4)) trans_pivot = pd.pivot_table(tbdi2, index=['ê³µì¥', 'ì œí’ˆ'], columns='ëŒ€ë¦¬ì ', values='ValX', aggfunc='sum') sns.heatmap(trans_pivot, annot=True, cmap=\"YlGnBu\"); . | ìš´ì†¡ë¹„ê°€ ì ì€ â€˜ê³µì¥ X â†’ ëŒ€ë¦¬ì  Pâ€™, â€˜ê³µì¥ Y â†’ ëŒ€ë¦¬ì  Qâ€™ì˜ ê²½ë¡œë¥¼ ì£¼ë¡œ ì´ìš©í•˜ê³ , ëŒ€ë¦¬ì  Qì˜ ìƒí’ˆ A ìˆ˜ìš”ë¥¼ ì¶©ì¡±ì‹œí‚¤ê¸° ìœ„í•´ ê³µì¥ Xì—ì„œ ëŒ€ë¦¬ì  Që¡œ Aë§Œ 10ë§Œí¼ ì¶”ê°€ë¡œ ìš´ì†¡ (ê³µì¥ Y 0 ìƒì‚°ë¼ì¸ì˜ ìƒì‚° í•œê³„ë¥¼ ê³ ë ¤) | . | . ìµœì  ìƒì‚°ëŸ‰ í™•ì¸ . | ìµœì  ìƒì‚°ëŸ‰ ê³„ì‚°ê°’ í™•ì¸ . df_prod . | Â  | ê³µì¥ | ë ˆì¸ | ì œí’ˆ | í•˜í•œ | ìƒí•œ | ìƒì‚°ë¹„ | VarY | ValY | . | 0 | X | 0 | A | 0 | inf | 1 | v000009 | 20 | . | 3 | X | 1 | B | 0 | inf | 1 | v000010 | 10 | . | 4 | Y | 0 | A | 0 | 10 | 3 | v000011 | 10 | . | 6 | Y | 1 | A | 0 | inf | 5 | v000012 | 0 | . | 7 | Y | 1 | B | 0 | inf | 3 | v000013 | 20 | . | ì´ ìƒì‚°ë¹„ ê³„ì‚° . ## +) (ë‚´ê°€ ë§ë¶™ì„) prod_cost = sum(df_prod['ìƒì‚°ë¹„'] * df_prod['ValY']) print(f'ì´ ìƒì‚°ë¹„: {prod_cost :.0f}ë§Œì›') . ì´ ìƒì‚°ë¹„: 120ë§Œì› . | ì œí’ˆë³„ ìƒì‚°ëŸ‰ heatmapìœ¼ë¡œ ì‹œê°í™” . prod_pivot = pd.pivot_table(tbfa, index=['ê³µì¥', 'ë ˆì¸'], columns='ì œí’ˆ', values='ValY', aggfunc='sum') sns.heatmap(prod_pivot, annot=True, cmap=\"YlGnBu\"); . | ìƒì‚° ë¹„ìš©ì´ ë‚®ì€ ê³µì¥ Xì—ì„œì˜ ìƒì‚°ëŸ‰ì„ ëŠ˜ë¦¬ê¸° ìœ„í•´ ì œí’ˆ AëŠ” 20ë§Œí¼, ì œí’ˆ BëŠ” 10ë§Œí¼ ìƒì‚° | ìƒì‚° ë¹„ìš©ë§Œì„ ìƒê°í•˜ë©´ ê³µì¥ Xì—ì„œ ëª¨ë“  ì œí’ˆì„ ì œì¡°í•´ë„ ë˜ì§€ë§Œ, ìš´ì†¡ ë¹„ìš©ê¹Œì§€ ê³ ë ¤í–ˆì„ ë•Œ, ìˆ˜ìš”ê°€ ë§ì€ ëŒ€ë¦¬ì  Qê¹Œì§€ì˜ ìš´ì†¡ ë¹„ìš©ì´ ì ì€ ê³µì¥ Yì—ì„œë„ ì¼ì •ëŸ‰ì„ ìƒì‚°í•˜ëŠ” ê²ƒì´ ìœ ë¦¬ | ìš´ì†¡ë¹„ì™€ ìƒì‚°ë¹„ì˜ ê· í˜•ì„ ìƒê°í•˜ë©´ ëŒ€ì²´ë¡œ íƒ€ë‹¹í•œ ê³„ì‚°ì´ë¼ê³  ìƒê°ë¨. | . | . ",
    "url": "https://chaelist.github.io/docs/ml_application/logistics/#%EB%AC%BC%EB%A5%98-%EB%84%A4%ED%8A%B8%EC%9B%8C%ED%81%AC-%EC%B5%9C%EC%A0%81%ED%99%94",
    "relUrl": "/docs/ml_application/logistics/#ë¬¼ë¥˜-ë„¤íŠ¸ì›Œí¬-ìµœì í™”"
  },"108": {
    "doc": "Matplotlib",
    "title": "Matplotlib",
    "content": ". | plt.plot(): ì„  ê·¸ë¦¬ê¸° | plt.scatter(): ì  ì°ê¸° | data labeling . | plt.annotate() | plt.text() | . | . *Matplotlib: ê¸°ì´ˆì ì¸ ë°ì´í„° ì‹œê°í™” library â†’ ì„œë¸Œëª¨ë“ˆ matplotlib.pyplotìœ¼ë¡œ ë‹¤ì–‘í•œ ê·¸ë˜í”„ë¥¼ ì§ì ‘ ê·¸ë ¤ë³¼ ìˆ˜ ìˆë‹¤ . ",
    "url": "https://chaelist.github.io/docs/visualization/matplotlib/",
    "relUrl": "/docs/visualization/matplotlib/"
  },"109": {
    "doc": "Matplotlib",
    "title": "plt.plot(): ì„  ê·¸ë¦¬ê¸°",
    "content": "import matplotlib.pyplot as plt ## ë³´í†µ pyplotë§Œ ë³„ë„ë¡œ pltë¡œ ì¤„ì—¬ì„œ import %matplotlib inline ## jupyter notebookì—ì„œ ì‹œê°í™” ê²°ê³¼ê°€ í‘œì‹œë˜ë„ë¡ í•˜ëŠ” ì„¤ì • . | ì§ì ‘ ì ì„ ì°ì–´ì„œ ì„  ê·¸ë¦¬ê¸° plt.plot([1,2,3,4,5,6,7,8,9,8,7,6,5,4,3,2,1]) # xê°’ì€ ìë™ìœ¼ë¡œ 0, 1, 2, ..., 16ìœ¼ë¡œ ê·¸ë ¤ì§ . | xê°’, yê°’ì„ ê°€ì§€ê³  ì„  ê·¸ë¦¬ê¸° import numpy as np x = np.arange(0, 12, 0.01) # 0ì—ì„œ 12ê¹Œì§€ 0.01 ê°„ê²©ìœ¼ë¡œ ë°ì´í„°ë¥¼ ë§Œë“¦ y = np.sin(x) # xì˜ sinê°’ (ì‚¬ì¸í•¨ìˆ˜) plt.figure(figsize=(6,4)) # figsizeë¡œ ì–¼ë§ˆë‚˜ í¬ê²Œ ê·¸ë˜í”„ë¥¼ ê·¸ë¦´ ì§€ ì§€ì • ê°€ëŠ¥ plt.plot(x, y) . | grid, label, title ì¶”ê°€ plt.plot(x, y) plt.grid() # ê²©ìë¬´ëŠ¬ ìƒì„± plt.xlabel('time') # xì¶• ë¼ë²¨ plt.ylabel('Amplitude') # yì¶• ë¼ë²¨ plt.title('Example of sinewave'); # ì œëª© . | í•œ í™”ë©´ì— ê·¸ë˜í”„ ë‘ ê°œ ê·¸ë¦¬ê¸° ## plot ë©”ì†Œë“œë¥¼ 2ë²ˆ ì¨ì„œ í•œ í™”ë©´ì— ê·¸ë˜í”„ë¥¼ ë‘ ê°œ ê·¸ë¦¬ê¸° plt.plot(x, np.sin(x), label='sin') ## plt.legend()ë¥¼ í•˜ë ¤ë©´ labelì„ ë¶™ì—¬ì¤˜ì•¼ í•œë‹¤ plt.plot(x, np.cos(x), label='cos') plt.grid() # ê²©ìë¬´ëŠ¬ ìƒì„± plt.legend() # legend(ë²”ë¡€) ìƒì„± plt.xlabel('time') # xì¶• ë¼ë²¨ plt.ylabel('Amplitude') # yì¶• ë¼ë²¨ plt.title('Example of sinewave'); # ì œëª© . | ì„  êµµê¸°, ìƒ‰ìƒ ì¡°ì • plt.plot(x, np.sin(x), linewidth=3) # linewidth (í˜¹ì€ lw): ì„  êµµê¸°ë¥¼ ì¡°ì • plt.plot(x, np.cos(x), color='r') # color (í˜¹ì€ c): ìƒ‰ì„ ì¡°ì • ('r'ì€ redë¡œ í•˜ê² ë‹¤ëŠ” ëœ») plt.grid() # ê²©ìë¬´ëŠ¬ ìƒì„± plt.xlabel('time') # xì¶• ë¼ë²¨ plt.ylabel('Amplitude') # yì¶• ë¼ë²¨ plt.title('Example of sinewave'); # ì œëª© . | ì„  ìŠ¤íƒ€ì¼ ì¡°ì • x = [0, 1, 2, 3, 4, 5, 6] y = [1, 4, 5, 8, 9, 5, 3] plt.plot(x, y, color='green', linestyle='dashed'); # ì„  ìŠ¤íƒ€ì¼ì„ ì ì„ ìœ¼ë¡œ ì§€ì • . | marker ìƒì„± . | marker ì¢…ë¥˜ëŠ” 'o', '.', 'v', '^', '&lt;', '&gt;', '1', 's' ë“±â€¦ ë‹¤ì–‘ | ë‹¤ì–‘í•œ ìŠ¤íƒ€ì¼ì€ https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.plot.html ì°¸ì¡° | . plt.plot(x, y, color='green', linestyle='dashed', marker='*', # ë³„ ëª¨ì–‘ ë§ˆì»¤ markerfacecolor='blue', markersize=12); # markerfacecolorë¡œ ë§ˆì»¤ ìƒ‰ ì§€ì •, markersizeë¡œ ë§ˆì»¤ì˜ í¬ê¸° ì§€ì • . | . ",
    "url": "https://chaelist.github.io/docs/visualization/matplotlib/#pltplot-%EC%84%A0-%EA%B7%B8%EB%A6%AC%EA%B8%B0",
    "relUrl": "/docs/visualization/matplotlib/#pltplot-ì„ -ê·¸ë¦¬ê¸°"
  },"110": {
    "doc": "Matplotlib",
    "title": "plt.scatter(): ì  ì°ê¸°",
    "content": ". | ê¸°ë³¸ scatter plot x = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) y = np.array([9, 8, 7, 9, 8, 3, 2, 4, 3, 4]) plt.scatter(x, y) . | colormap ì˜µì…˜ ì§€ì • plt.scatter(x, y, s = 50, c = x, marker='&gt;') # c = xë¼ê³  í•˜ë©´ xê°’ì— ë”°ë¼ ìƒ‰ìƒì„ ë°”ê¾¸ëŠ” colormapìœ¼ë¡œ í‘œí˜„í•˜ê² ë‹¤ëŠ” ëœ». # markerë¡œ ë§ˆì»¤ì˜ ëª¨ì–‘ì„ ì§€ì •, së¡œ ë§ˆì»¤ í¬ê¸°ë¥¼ ì§€ì • plt.colorbar(); # ì˜¤ë¥¸ìª½ì— colorbar í‘œì‹œ . | . ",
    "url": "https://chaelist.github.io/docs/visualization/matplotlib/#pltscatter-%EC%A0%90-%EC%B0%8D%EA%B8%B0",
    "relUrl": "/docs/visualization/matplotlib/#pltscatter-ì -ì°ê¸°"
  },"111": {
    "doc": "Matplotlib",
    "title": "data labeling",
    "content": "plt.annotate() . import pandas as pd score_df = pd.read_excel('data/math_reading_scores.xlsx') score_df . | Â  | Name | Math | Reading | . | 0 | Annie | 75 | 95 | . | 1 | Bella | 78 | 85 | . | 2 | Chloe | 98 | 75 | . | 3 | Diana | 65 | 65 | . | 4 | Emily | 78 | 75 | . | 5 | George | 84 | 85 | . | 6 | Serene | 82 | 95 | . | 7 | Jakob | 95 | 85 | . | 8 | Kim | 75 | 75 | . â†’ ê° ì ì— â€˜Nameâ€™ ë¼ë²¨ ë¶™ì´ê¸° . plt.figure(figsize=(9, 6)) # ê·¸ë˜í”„ í¬ê¸° ê²°ì • x = score_df['Math'] y = score_df['Reading'] plt.scatter(x = x, y = y) # ì œëª©, xì¶• ì´ë¦„, yì¶• ì´ë¦„ ë¶™ì´ê¸° plt.title('&lt;Score&gt;', pad=20, size=20) plt.xlabel('Math', size=16) plt.ylabel('Reading', size=16) # ê° ì ì— label ë¶™ì—¬ì£¼ê¸°: annotate() ë©”ì†Œë“œ ì‚¬ìš© for i in range(len(score_df)): label = score_df['Name'][i] plt.annotate(label, xy=(x[i],y[i]), xytext=(x[i]+0.3,y[i]+0.3), fontsize=10) ## xyëŠ” (x,y)ë¡œ ë¼ë²¨ì´ ë¶™ì„ ì ì„ ì•Œë ¤ì£¼ëŠ” ì—­í•  ## xytextëŠ” textê°€ ì–´ëŠ ìœ„ì¹˜ì— ë“¤ì–´ê°ˆì§€ë¥¼ ê²°ì •í•˜ëŠ” ì—­í• . (x[i]+0.3,y[i]+0.3)ëŠ” ê° (x,y) ì ì—ì„œ 0.3ì”© ë–¨ì–´ì§„ ê³³ì— ê¸€ì„ ì“°ê² ë‹¤ëŠ” ëœ» plt.grid() # ê²©ìë¬´ëŠ¬ ë§Œë“¤ê¸° . cf) pandas.plot()ì— plt.annotateë¥¼ ì¡°í•©í•´ì„œ ì•„ë˜ì™€ ê°™ì´ í‘œí˜„í•˜ëŠ” ê²ƒë„ ê°€ëŠ¥ . score_df.plot(kind='scatter', x='Math', y='Reading', figsize=(9, 6), grid=True) # xì¶•, yì¶• ì´ë¦„ë„ ìë™ìœ¼ë¡œ ë¶™ìŒ x = score_df['Math'] y = score_df['Reading'] for i in range(len(score_df)): list1 = score_df.loc[i].values.tolist() label = score_df['Name'][i] plt.annotate(label, xy=(x[i],y[i]), xytext=(x[i]+0.3,y[i]+0.3), fontsize=10) . plt.text() . | annotateì™€ ê±°ì˜ ë™ì¼í•˜ê²Œ í‘œí˜„ ê°€ëŠ¥ | . plt.figure(figsize=(9, 6)) x = score_df['Math'] y = score_df['Reading'] plt.scatter(x = x, y = y) # ì œëª©, xì¶• ì´ë¦„, yì¶• ì´ë¦„ ë¶™ì´ê¸° plt.title('&lt;Score&gt;', pad=20, size=20) plt.xlabel('Math', size=16) plt.ylabel('Reading', size=16) # ê° ì ì— ì´ë¦„ ë¶™ì—¬ì£¼ê¸°: plt.text() ë©”ì†Œë“œ ì‚¬ìš© for i in range(len(score_df)): plt.text(score_df['Math'][i]*1.01, score_df['Reading'][i], score_df['Name'][i], fontsize=10) ## plt.text(xê°’, yê°’, labelë¶™ì¼ ê°’) ì´ë ‡ê²Œ ê°’ì„ assign. ## score_df['Math'][i]*1.01ì´ë¼ê³  í•œ ê±´ ì¡°ê¸ˆ ì˜¤ë¥¸ìª½ë¡œ ë–¨ì–´ì ¸ì„œ labelì´ ë³´ì´ë„ë¡ í•œ ê²ƒ. plt.grid() . ",
    "url": "https://chaelist.github.io/docs/visualization/matplotlib/#data-labeling",
    "relUrl": "/docs/visualization/matplotlib/#data-labeling"
  },"112": {
    "doc": "Machine Learning ì‹¬í™”",
    "title": "Machine Learning ì‹¬í™”",
    "content": " ",
    "url": "https://chaelist.github.io/docs/ml_advanced",
    "relUrl": "/docs/ml_advanced"
  },"113": {
    "doc": "Machine Learning ì‘ìš©",
    "title": "Machine Learning ì‘ìš©",
    "content": " ",
    "url": "https://chaelist.github.io/docs/ml_application",
    "relUrl": "/docs/ml_application"
  },"114": {
    "doc": "Machine Learning ê¸°ì´ˆ",
    "title": "Machine Learning ê¸°ì´ˆ",
    "content": " ",
    "url": "https://chaelist.github.io/docs/ml_basics",
    "relUrl": "/docs/ml_basics"
  },"115": {
    "doc": "Model Selection",
    "title": "Model Selection",
    "content": ". | k-ê²¹ êµì°¨ ê²€ì¦ (k-fold cross validation) . | k-ê²¹ êµì°¨ ê²€ì¦ ë°©ë²• | scikit-learnìœ¼ë¡œ êµ¬í˜„ | . | ê·¸ë¦¬ë“œ ì„œì¹˜ (Grid Search) . | scikit-learnìœ¼ë¡œ êµ¬í˜„ | Randomized Search | . | . ",
    "url": "https://chaelist.github.io/docs/ml_advanced/model_selection/",
    "relUrl": "/docs/ml_advanced/model_selection/"
  },"116": {
    "doc": "Model Selection",
    "title": "k-ê²¹ êµì°¨ ê²€ì¦ (k-fold cross validation)",
    "content": ": ë¨¸ì‹  ëŸ¬ë‹ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ë³´ë‹¤ ì •í™•í•˜ê²Œ í‰ê°€í•  ìˆ˜ ìˆëŠ” ë°©ë²•. | test setê³¼ training setì„ ì„ì˜ë¡œ ë‚˜ëˆŒ ë•Œ, ì–´ë–»ê²Œ ë‚˜ë‰˜ëŠ”ì§€ì— ë”°ë¼ ë”± ì´ test setì—ì„œë§Œ ì„±ëŠ¥ì´ ì¢‹ê²Œ ë‚˜ì˜¬ ìˆ˜ë„ ìˆê³ , ì•ˆì¢‹ê²Œ ë‚˜ì˜¬ ìˆ˜ë„ ìˆë‹¤. ì´ëŸ° ë¬¸ì œë¥¼ í•´ê²°í•´ì£¼ëŠ” ê²ƒì´ êµì°¨ ê²€ì¦! | kê°’ì€ ë°ì´í„°ë¥¼ ëª‡ ì„¸íŠ¸ë¡œ ë‚˜ëˆŒì§€ë¥¼ ì˜ë¯¸. ë°ì´í„° ìˆ˜ì— ë”°ë¼ ë‹¤ë¥´ì§€ë§Œ, ê°€ì¥ ì¼ë°˜ì ìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” ìˆ«ìëŠ” 5 | +) ë°ì´í„°ê°€ ë§ì„ìˆ˜ë¡ ìš°ì—°íˆ test setì—ì„œë§Œ ì„±ëŠ¥ì´ ë‹¤ë¥´ê²Œ ë‚˜ì˜¬ í™•ë¥ ì´ ì ê¸° ë•Œë¬¸ì— ì‘ì€ kë¥¼ ì‚¬ìš©í•´ë„ ëœë‹¤ | . k-ê²¹ êµì°¨ ê²€ì¦ ë°©ë²• . | ë¨¼ì € ì „ì²´ ë°ì´í„°ë¥¼ k ê°œì˜ ê°™ì€ ì‚¬ì´ì¦ˆë¡œ ë‚˜ëˆˆë‹¤. | ex) k=5, ë°ì´í„°ê°€ ì´ 1000ê°œê°€ ìˆë‹¤ë©´ ì´ ë°ì´í„°ë¥¼ 200ê°œì”© 5ê°œì˜ ì…‹ìœ¼ë¡œ ë‚˜ëˆˆë‹¤ | . | ë§¨ ì²˜ìŒì—ëŠ” ì²« ë²ˆì§¸ ë°ì´í„° ì…‹ì„ test testìœ¼ë¡œ ì‚¬ìš©í•˜ê³ , ë‚˜ë¨¸ì§€ë¥¼ training setìœ¼ë¡œ ì‚¬ìš© | ê·¸ ë‹¤ìŒì—ëŠ” ë‘ ë²ˆì§¸ ë°ì´í„° ì…‹ì„ test setìœ¼ë¡œ ì‚¬ìš©í•˜ê³ , ë‚˜ë¨¸ì§€ 4ê°œë¥¼ training setìœ¼ë¡œ ì‚¬ìš©í•´ì„œ ë‹¤ì‹œ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ê³ , ì„±ëŠ¥ì„ íŒŒì•… | ì´ ê³¼ì •ì„ ëª¨ë“  ë°ì´í„° ì…‹ì— ë°˜ë³µ â†’ 5ê°œì˜ í…ŒìŠ¤íŠ¸ ì…‹ì— ëŒ€í•œ ê° ì„±ëŠ¥ 5ê°œì˜ í‰ê· ì„ ëª¨ë¸ ì„±ëŠ¥ìœ¼ë¡œ ë³¸ë‹¤ (ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ì—¬ëŸ¬ ë²ˆ ë‹¤ë¥¸ ë°ì´í„°ë¡œ ê²€ì¦í•˜ê¸° ë•Œë¬¸ì— í‰ê°€ì— ëŒ€í•œ ì‹ ë¢°ê°€ ì˜¬ë¼ê°€ëŠ” ê²ƒ!) | . scikit-learnìœ¼ë¡œ êµ¬í˜„ . | sklearn.model_selection.cross_val_scoreë¥¼ ì‚¬ìš© | . from sklearn import datasets from sklearn.model_selection import cross_val_score # êµì°¨ ê²€ì¦ì„ í•´ì£¼ëŠ” í•¨ìˆ˜ from sklearn.linear_model import LogisticRegression import numpy as np import pandas as pd # ê²½ê³  ë©”ì‹œì§€ ì¶œë ¥ì„ ë§‰ëŠ” ì½”ë“œ ì¶”ê°€ import warnings warnings.simplefilter(action='ignore', category=FutureWarning) . *iris dataset ì¤€ë¹„ . iris_data = datasets.load_iris() X = pd.DataFrame(iris_data.data, columns=iris_data.feature_names) y = pd.DataFrame(iris_data.target, columns=['Class']) X.head() . | Â  | sepal length (cm) | sepal width (cm) | petal length (cm) | petal width (cm) | . | 0 | 5.1 | 3.5 | 1.4 | 0.2 | . | 1 | 4.9 | 3 | 1.4 | 0.2 | . | 2 | 4.7 | 3.2 | 1.3 | 0.2 | . | 3 | 4.6 | 3.1 | 1.5 | 0.2 | . | 4 | 5 | 3.6 | 1.4 | 0.2 | . Â  . y.head() . | Â  | Class | . | 0 | 0 | . | 1 | 0 | . | 2 | 0 | . | 3 | 0 | . | 4 | 0 | . *LogisticRegression ëª¨ë¸ë¡œ êµì°¨ ê²€ì¦ . logistic_model = LogisticRegression(max_iter=2000) cross_val_score(logistic_model, X, y.values.ravel(), cv=5) # ravel(): ë‹¤ì°¨ì› arrayë¥¼ 1ì°¨ì› arrayë¡œ í‰í‰í•˜ê²Œ í´ì£¼ëŠ” í•¨ìˆ˜. array([0.96666667, 1. , 0.93333333, 0.96666667, 1. ]) . | cross_val_score: ì•Œì•„ì„œ ëª¨ë¸ì„ í•™ìŠµì‹œì¼œì„œ kê²¹ êµì°¨ê²€ì¦ì„ í•´ì£¼ê³ , ê° kê°œì˜ ì„±ëŠ¥ì´ returnëœë‹¤ . | íŒŒë¼ë¯¸í„°: k-ê²¹ êµì°¨ê²€ì¦ì„ í•´ì¤„ ëª¨ë¸ëª…, Xê°’, yê°’, cv = kê°’ì„ ê²°ì •. | ì—¬ê¸°ì„œëŠ” cv=5ë‹ˆê¹Œ 5ê²¹ êµì°¨ê²€ì¦ì„ í•œë‹¤ëŠ” ëœ». | . | . â†’ cross_val_scoreì˜ ê²°ê³¼ë¡œ returnëœ ê°’ë“¤ì„ í‰ê· ë‚´ì¤€ë‹¤ . # ì´ë ‡ê²Œ í‰ê· ë‚¸ ê°’ì„ í•´ë‹¹ ëª¨ë¸ì˜ ì„±ëŠ¥ìœ¼ë¡œ ìƒê° np.average(cross_val_score(logistic_model, X, y.values.ravel(), cv=5)) . 0.9733333333333334 . ",
    "url": "https://chaelist.github.io/docs/ml_advanced/model_selection/#k-%EA%B2%B9-%EA%B5%90%EC%B0%A8-%EA%B2%80%EC%A6%9D-k-fold-cross-validation",
    "relUrl": "/docs/ml_advanced/model_selection/#k-ê²¹-êµì°¨-ê²€ì¦-k-fold-cross-validation"
  },"117": {
    "doc": "Model Selection",
    "title": "ê·¸ë¦¬ë“œ ì„œì¹˜ (Grid Search)",
    "content": ": ì¢‹ì€ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ê³ ë¥´ëŠ” ë°©ë²• ì¤‘ í•˜ë‚˜ . | ìš°ì„  ì •í•´ì¤˜ì•¼ í•˜ëŠ” ê° í•˜ì´í¼íŒŒë¼ë¯¸í„°ì— ë„£ì–´ë³´ê³  ì‹¶ì€ í›„ë³´ ê°’ì„ ëª‡ ê°œì”© ì •í•´ì¤€ë‹¤ | ì„ íƒí•œ í›„ë³´ ê°’ë“¤ì˜ ëª¨ë“  ì¡°í•©ì„ ì¨ì„œ í•™ìŠµì„ ì‹œí‚¤ê³ , ê·¸ì¤‘ ì„±ëŠ¥ì´ ê°€ì¥ ì¢‹ê²Œ ë‚˜ì˜¤ëŠ” í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°í•©ì„ ê³ ë¥¸ë‹¤ | . scikit-learnìœ¼ë¡œ êµ¬í˜„ . | sklearn.model_selection.GridSearchCVë¥¼ ì‚¬ìš© | . from sklearn import datasets from sklearn.linear_model import Lasso from sklearn.preprocessing import PolynomialFeatures from sklearn.model_selection import GridSearchCV # Grid Searchë¥¼ ì‰½ê²Œ í•´ì£¼ëŠ” í•¨ìˆ˜ import numpy as np import pandas as pd . 1. boston housing ë°ì´í„° ì¤€ë¹„ . boston_dataset = datasets.load_boston() X = pd.DataFrame(boston_dataset.data, columns=boston_dataset.feature_names) # ì…ë ¥ë³€ìˆ˜ë¥¼ 6ì°¨í•­ìœ¼ë¡œ ë°”ê¾¸ê¸° polynomial_transformer = PolynomialFeatures(2) # 2ì°¨í•­ ë³€í™˜ê¸° ì¤€ë¹„ polynomial_features = polynomial_transformer.fit_transform(X.values) features = polynomial_transformer.get_feature_names(X.columns) # ë°”ê¾¼ ê°’ë“¤ì„ ë‹¤ì‹œ Xë¡œ ì €ì¥ X = pd.DataFrame(polynomial_features, columns=features) X.head() . | Â  | 1 | CRIM | ZN | INDUS | CHAS | NOX | RM | AGE | DIS | RAD | TAX | PTRATIO | B | LSTAT | CRIM^2 | CRIM ZN | CRIM INDUS | CRIM CHAS | CRIM NOX | CRIM RM | CRIM AGE | CRIM DIS | CRIM RAD | CRIM TAX | CRIM PTRATIO | CRIM B | CRIM LSTAT | ZN^2 | ZN INDUS | ZN CHAS | ZN NOX | ZN RM | ZN AGE | ZN DIS | ZN RAD | ZN TAX | ZN PTRATIO | ZN B | ZN LSTAT | INDUS^2 | INDUS CHAS | INDUS NOX | INDUS RM | INDUS AGE | INDUS DIS | INDUS RAD | INDUS TAX | INDUS PTRATIO | INDUS B | INDUS LSTAT | CHAS^2 | CHAS NOX | CHAS RM | CHAS AGE | CHAS DIS | CHAS RAD | CHAS TAX | CHAS PTRATIO | CHAS B | CHAS LSTAT | NOX^2 | NOX RM | NOX AGE | NOX DIS | NOX RAD | NOX TAX | NOX PTRATIO | NOX B | NOX LSTAT | RM^2 | RM AGE | RM DIS | RM RAD | RM TAX | RM PTRATIO | RM B | RM LSTAT | AGE^2 | AGE DIS | AGE RAD | AGE TAX | AGE PTRATIO | AGE B | AGE LSTAT | DIS^2 | DIS RAD | DIS TAX | DIS PTRATIO | DIS B | DIS LSTAT | RAD^2 | RAD TAX | RAD PTRATIO | RAD B | RAD LSTAT | TAX^2 | TAX PTRATIO | TAX B | TAX LSTAT | PTRATIO^2 | PTRATIO B | PTRATIO LSTAT | B^2 | B LSTAT | LSTAT^2 | . | 0 | 1 | 0.00632 | 18 | 2.31 | 0 | 0.538 | 6.575 | 65.2 | 4.09 | 1 | 296 | 15.3 | 396.9 | 4.98 | 3.99424e-05 | 0.11376 | 0.0145992 | 0 | 0.00340016 | 0.041554 | 0.412064 | 0.0258488 | 0.00632 | 1.87072 | 0.096696 | 2.50841 | 0.0314736 | 324 | 41.58 | 0 | 9.684 | 118.35 | 1173.6 | 73.62 | 18 | 5328 | 275.4 | 7144.2 | 89.64 | 5.3361 | 0 | 1.24278 | 15.1883 | 150.612 | 9.4479 | 2.31 | 683.76 | 35.343 | 916.839 | 11.5038 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0.289444 | 3.53735 | 35.0776 | 2.20042 | 0.538 | 159.248 | 8.2314 | 213.532 | 2.67924 | 43.2306 | 428.69 | 26.8917 | 6.575 | 1946.2 | 100.598 | 2609.62 | 32.7435 | 4251.04 | 266.668 | 65.2 | 19299.2 | 997.56 | 25877.9 | 324.696 | 16.7281 | 4.09 | 1210.64 | 62.577 | 1623.32 | 20.3682 | 1 | 296 | 15.3 | 396.9 | 4.98 | 87616 | 4528.8 | 117482 | 1474.08 | 234.09 | 6072.57 | 76.194 | 157530 | 1976.56 | 24.8004 | . | 1 | 1 | 0.02731 | 0 | 7.07 | 0 | 0.469 | 6.421 | 78.9 | 4.9671 | 2 | 242 | 17.8 | 396.9 | 9.14 | 0.000745836 | 0 | 0.193082 | 0 | 0.0128084 | 0.175358 | 2.15476 | 0.135652 | 0.05462 | 6.60902 | 0.486118 | 10.8393 | 0.249613 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 49.9849 | 0 | 3.31583 | 45.3965 | 557.823 | 35.1174 | 14.14 | 1710.94 | 125.846 | 2806.08 | 64.6198 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0.219961 | 3.01145 | 37.0041 | 2.32957 | 0.938 | 113.498 | 8.3482 | 186.146 | 4.28666 | 41.2292 | 506.617 | 31.8937 | 12.842 | 1553.88 | 114.294 | 2548.49 | 58.6879 | 6225.21 | 391.904 | 157.8 | 19093.8 | 1404.42 | 31315.4 | 721.146 | 24.6721 | 9.9342 | 1202.04 | 88.4144 | 1971.44 | 45.3993 | 4 | 484 | 35.6 | 793.8 | 18.28 | 58564 | 4307.6 | 96049.8 | 2211.88 | 316.84 | 7064.82 | 162.692 | 157530 | 3627.67 | 83.5396 | . | 2 | 1 | 0.02729 | 0 | 7.07 | 0 | 0.469 | 7.185 | 61.1 | 4.9671 | 2 | 242 | 17.8 | 392.83 | 4.03 | 0.000744744 | 0 | 0.19294 | 0 | 0.012799 | 0.196079 | 1.66742 | 0.135552 | 0.05458 | 6.60418 | 0.485762 | 10.7203 | 0.109979 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 49.9849 | 0 | 3.31583 | 50.798 | 431.977 | 35.1174 | 14.14 | 1710.94 | 125.846 | 2777.31 | 28.4921 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0.219961 | 3.36976 | 28.6559 | 2.32957 | 0.938 | 113.498 | 8.3482 | 184.237 | 1.89007 | 51.6242 | 439.003 | 35.6886 | 14.37 | 1738.77 | 127.893 | 2822.48 | 28.9555 | 3733.21 | 303.49 | 122.2 | 14786.2 | 1087.58 | 24001.9 | 246.233 | 24.6721 | 9.9342 | 1202.04 | 88.4144 | 1951.23 | 20.0174 | 4 | 484 | 35.6 | 785.66 | 8.06 | 58564 | 4307.6 | 95064.9 | 975.26 | 316.84 | 6992.37 | 71.734 | 154315 | 1583.1 | 16.2409 | . | 3 | 1 | 0.03237 | 0 | 2.18 | 0 | 0.458 | 6.998 | 45.8 | 6.0622 | 3 | 222 | 18.7 | 394.63 | 2.94 | 0.00104782 | 0 | 0.0705666 | 0 | 0.0148255 | 0.226525 | 1.48255 | 0.196233 | 0.09711 | 7.18614 | 0.605319 | 12.7742 | 0.0951678 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 4.7524 | 0 | 0.99844 | 15.2556 | 99.844 | 13.2156 | 6.54 | 483.96 | 40.766 | 860.293 | 6.4092 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0.209764 | 3.20508 | 20.9764 | 2.77649 | 1.374 | 101.676 | 8.5646 | 180.741 | 1.34652 | 48.972 | 320.508 | 42.4233 | 20.994 | 1553.56 | 130.863 | 2761.62 | 20.5741 | 2097.64 | 277.649 | 137.4 | 10167.6 | 856.46 | 18074.1 | 134.652 | 36.7503 | 18.1866 | 1345.81 | 113.363 | 2392.33 | 17.8229 | 9 | 666 | 56.1 | 1183.89 | 8.82 | 49284 | 4151.4 | 87607.9 | 652.68 | 349.69 | 7379.58 | 54.978 | 155733 | 1160.21 | 8.6436 | . | 4 | 1 | 0.06905 | 0 | 2.18 | 0 | 0.458 | 7.147 | 54.2 | 6.0622 | 3 | 222 | 18.7 | 396.9 | 5.33 | 0.0047679 | 0 | 0.150529 | 0 | 0.0316249 | 0.4935 | 3.74251 | 0.418595 | 0.20715 | 15.3291 | 1.29123 | 27.4059 | 0.368036 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 4.7524 | 0 | 0.99844 | 15.5805 | 118.156 | 13.2156 | 6.54 | 483.96 | 40.766 | 865.242 | 11.6194 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0.209764 | 3.27333 | 24.8236 | 2.77649 | 1.374 | 101.676 | 8.5646 | 181.78 | 2.44114 | 51.0796 | 387.367 | 43.3265 | 21.441 | 1586.63 | 133.649 | 2836.64 | 38.0935 | 2937.64 | 328.571 | 162.6 | 12032.4 | 1013.54 | 21512 | 288.886 | 36.7503 | 18.1866 | 1345.81 | 113.363 | 2406.09 | 32.3115 | 9 | 666 | 56.1 | 1190.7 | 15.99 | 49284 | 4151.4 | 88111.8 | 1183.26 | 349.69 | 7422.03 | 99.671 | 157530 | 2115.48 | 28.4089 | . Â  . y = pd.DataFrame(boston_dataset.target, columns=['MEDV']) y.head() . | Â  | MEDV | . | 0 | 24.0 | . | 1 | 21.6 | . | 2 | 34.7 | . | 3 | 33.4 | . | 4 | 36.2 | . Â  . +) ë°ì´í„° ì…”í”Œ . # boston housing ë°ì´í„°ëŠ” ë¯¸ë¦¬ ì…”í”Œí•˜ì§€ ì•Šìœ¼ë©´ êµì°¨ê²€ì¦í•  ë•Œ scoreê°’ì´ ë§¤ìš° ë‚®ê²Œ ë‚˜ì˜´ # ì´ì²˜ëŸ¼, ë°ì´í„°ê°€ íŠ¹ì • ìˆœì„œì— ë”°ë¼ ì •ë ¬ë˜ì–´ ìˆëŠ” ê²½ìš° ì…”í”Œí•´ì„œ ì‚¬ìš©í•˜ëŠ” ê²Œ ì¢‹ë‹¤ joined_data = X.join(y) shuffled_data = joined_data.sample(frac=1) shuffled_X = shuffled_data.iloc[:, :-1] shuffled_y = shuffled_data.iloc[:, -1] . 2. hyperparameter í›„ë³´ ì •ë¦¬ . # Grid Searchë¥¼ í†µí•´ ì„±ëŠ¥ì„ í…ŒìŠ¤íŠ¸í•´ë³´ê³  ì‹¶ì€ hyperparameterë¥¼ dictionary í˜•íƒœë¡œ ì ì–´ì¤€ë‹¤ # key: parameterëª…, value: í…ŒìŠ¤íŠ¸í•´ë³¼ ê°’ë“¤ì˜ ë¦¬ìŠ¤íŠ¸ hyper_parameter = { 'alpha': [0.001, 0.001, 0.01, 0.1, 1], 'max_iter': [500, 1000, 1500, 2000, 3000] } . 3. Grid Search . lasso_model = Lasso() ## hyperparameterë¥¼ ìš°ì„  í•˜ë‚˜ë„ ë„£ì§€ ì•Šê³  ëª¨ë¸ì„ ë§Œë“ ë‹¤ grid_search = GridSearchCV(lasso_model, hyper_parameter, cv=5) # ê²€ì¦í•  ëª¨ë¸, ê²€ì¦í•  hyperparameterë“¤ì„ ì ì–´ì¤€ dictionaryë¥¼ ì ì–´ì¤Œ # cvëŠ” ì„±ëŠ¥ì„ ëª‡ ê²¹ êµì°¨ê²€ì¦ìœ¼ë¡œ í‰ê°€í• ì§€ ì •í•˜ëŠ” íŒŒë¼ë¯¸í„°. cv=5ë©´ 5ê²¹ êµì°¨ê²€ì¦ì„ í•˜ê² ë‹¤ëŠ” ì˜ë¯¸. (cv=5ê°€ default) grid_search.fit(shuffled_X, shuffled_y) # ì¼ë°˜ì ì¸ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ë“¯ì´ fit()ì— X, yë¥¼ ì¨ì£¼ë©´ ëœë‹¤ . GridSearchCV(cv=5, error_score=nan, estimator=Lasso(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=1000, normalize=False, positive=False, precompute=False, random_state=None, selection='cyclic', tol=0.0001, warm_start=False), iid='deprecated', n_jobs=None, param_grid={'alpha': [0.001, 0.001, 0.01, 0.1, 1], 'max_iter': [500, 1000, 1500, 2000, 3000]}, pre_dispatch='2*n_jobs', refit=True, return_train_score=False, scoring=None, verbose=0) . | refit=Trueê°€ default settingì´ë¼ ì—¬ëŸ¬ hyperparameter ì¡°í•©ì„ ëª¨ë‘ í•™ìŠµì‹œì¼œë³¸ í›„ ëª¨ë¸ì„ ìµœì ì˜ ì¡°í•©ìœ¼ë¡œ ë‹¤ì‹œ í•™ìŠµì‹œì¼œì¤€ë‹¤. | . 4. ìµœì ì˜ hyperparameter ì¡°í•© í™•ì¸ . grid_search.best_params_ ## ìµœì ì˜ hyperparameterë¥¼ ì°¾ì•„ì¤Œ . {'alpha': 0.1, 'max_iter': 3000} . +) best ì¡°í•©ì˜ scoreë¥¼ í™•ì¸ . grid_search.best_score_ . 0.8425712188051439 . +) scoreê°€ ë†’ì€ Top 5 ì¡°í•©ê³¼ ê·¸ í‰ê· ì ìˆ˜ í™•ì¸ . | cv_results_ë¡œ í…ŒìŠ¤íŠ¸ëœ ê° ì¡°í•©ë³„ë¡œ, ê° êµì°¨ê²€ì¦ íšŒë³„ ì ìˆ˜ì™€ í‰ê· ì ìˆ˜ ë“±ì„ ëª¨ë‘ í™•ì¸í•  ìˆ˜ ìˆë‹¤ | . scores_df = pd.DataFrame(grid_search.cv_results_) scores_df[scores_df['rank_test_score'] &lt; 6][['params', 'mean_test_score', 'rank_test_score']] . | Â  | params | mean_test_score | rank_test_score | . | 16 | {â€˜alphaâ€™: 0.1, â€˜max_iterâ€™: 1000} | 0.836727 | 4 | . | 17 | {â€˜alphaâ€™: 0.1, â€˜max_iterâ€™: 1500} | 0.838901 | 3 | . | 18 | {â€˜alphaâ€™: 0.1, â€˜max_iterâ€™: 2000} | 0.840577 | 2 | . | 19 | {â€˜alphaâ€™: 0.1, â€˜max_iterâ€™: 3000} | 0.842571 | 1 | . | 24 | {â€˜alphaâ€™: 1, â€˜max_iterâ€™: 3000} | 0.834124 | 5 | . Randomized Search . | hyperparameter gridê°€ ì»¤ì§ˆìˆ˜ë¡ Grid SearchëŠ” ëŠë ¤ì§„ë‹¤ | ê·¸ë ‡ê¸°ì—, ê°€ëŠ¥í•œ ëª¨ë“  ì¡°í•©ì„ ì‹œë„í•˜ëŠ” ëŒ€ì‹ , randomí•˜ê²Œ grid ì•ˆ ì—¬ëŸ¬ ì¡°í•©ì„ ì ë‹¹íˆ ê±´ë„ˆë›°ë©´ì„œ ì‹œë„í•˜ë©´ ì‹œê°„ì„ í¬ê²Œ ë‹¨ì¶•í•  ìˆ˜ ìˆë‹¤ | Randomized Searchë¥¼ í™œìš©í•˜ë©´ ìµœì ì˜ ì¡°í•©ì„ ê±´ë„ˆë›¸ í™•ë¥ ë„ ì•½ê°„ ì¡´ì¬í•˜ì§€ë§Œ, ëŒ€ì²´ë¡œ ë¹ ë¥¸ ì‹œê°„ ë‚´ì— ìµœì ì— ê°€ê¹Œìš´ ì¡°í•©ì„ ì°¾ì•„ë‚¼ ìˆ˜ ìˆë‹¤. (ê°™ì€ ì‹œê°„ ë‚´ì— ë” ë§ì€ hyperparameterë¥¼ ì¡°ì • ê°€ëŠ¥) | . # Import RandomizedSearchCV from sklearn.model_selection import RandomizedSearchCV # GridSearchì—ì„œ í™œìš©í–ˆë˜ lasso_modelì™€ hyper_parameterë¥¼ ê·¸ëŒ€ë¡œ í™œìš© lasso_model = Lasso() hyper_parameter = { 'alpha': [0.001, 0.001, 0.01, 0.1, 1], 'max_iter': [500, 1000, 1500, 2000, 3000] } # Randomized Search ëª¨ë¸ì„ ë§Œë“¤ê³  fit random_search = RandomizedSearchCV(lasso_model, hyper_parameter, cv=5) random_search.fit(shuffled_X, shuffled_y) . RandomizedSearchCV(cv=5, error_score=nan, estimator=Lasso(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=1000, normalize=False, positive=False, precompute=False, random_state=None, selection='cyclic', tol=0.0001, warm_start=False), iid='deprecated', n_iter=10, n_jobs=None, param_distributions={'alpha': [0.001, 0.001, 0.01, 0.1, 1], 'max_iter': [500, 1000, 1500, 2000, 3000]}, pre_dispatch='2*n_jobs', random_state=None, refit=True, return_train_score=False, scoring=None, verbose=0) . *ìµœì ì˜ hyperparameter ì¡°í•© í™•ì¸ . print(random_search.best_params_) . {'max_iter': 3000, 'alpha': 0.1} . +) best ì¡°í•©ì˜ scoreë¥¼ í™•ì¸ . random_search.best_score_ . 0.8425712188051439 . +) scoreê°€ ë†’ì€ Top 5 ì¡°í•©ê³¼ ê·¸ í‰ê· ì ìˆ˜ í™•ì¸ . scores_df = pd.DataFrame(random_search.cv_results_) scores_df[scores_df['rank_test_score'] &lt; 6][['params', 'mean_test_score', 'rank_test_score']] . | Â  | params | mean_test_score | rank_test_score | . | 0 | {â€˜max_iterâ€™: 1000, â€˜alphaâ€™: 0.001} | 0.820117 | 5 | . | 1 | {â€˜max_iterâ€™: 500, â€˜alphaâ€™: 0.01} | 0.824949 | 2 | . | 2 | {â€˜max_iterâ€™: 3000, â€˜alphaâ€™: 0.1} | 0.842571 | 1 | . | 6 | {â€˜max_iterâ€™: 1000, â€˜alphaâ€™: 0.001} | 0.820117 | 5 | . | 7 | {â€˜max_iterâ€™: 3000, â€˜alphaâ€™: 0.001} | 0.820963 | 3 | . | 8 | {â€˜max_iterâ€™: 3000, â€˜alphaâ€™: 0.001} | 0.820963 | 3 | . ",
    "url": "https://chaelist.github.io/docs/ml_advanced/model_selection/#%EA%B7%B8%EB%A6%AC%EB%93%9C-%EC%84%9C%EC%B9%98-grid-search",
    "relUrl": "/docs/ml_advanced/model_selection/#ê·¸ë¦¬ë“œ-ì„œì¹˜-grid-search"
  },"118": {
    "doc": "í…Œì´ë¸” ê°€ê³µ",
    "title": "í…Œì´ë¸” ê°€ê³µ",
    "content": ". | ì»¬ëŸ¼ ì¶”ê°€/ë³€ê²½/ì‚­ì œ . | ì»¬ëŸ¼ ê´€ë ¨ ê¸°íƒ€ ì‘ì—…ë“¤ | . | ì»¬ëŸ¼ì— ì†ì„± ì¶”ê°€ . | NOT NULL, DEFAULT, UNIQUE | CURRENT_TIMESTAMP ì†ì„± | CONSTRAINT ì†ì„± | . | í…Œì´ë¸” ë³€ê²½/ë³µì‚¬/ì‚­ì œ | . ",
    "url": "https://chaelist.github.io/docs/sql/modify_table/",
    "relUrl": "/docs/sql/modify_table/"
  },"119": {
    "doc": "í…Œì´ë¸” ê°€ê³µ",
    "title": "ì»¬ëŸ¼ ì¶”ê°€/ë³€ê²½/ì‚­ì œ",
    "content": ". | ì»¬ëŸ¼ ì¶”ê°€ . | ALTER TABLE í…Œì´ë¸”ëª… ADD ì¹¼ëŸ¼ ì†ì„±;ì˜ êµ¬ì¡°ë¡œ ì‘ì„± | . -- studentë¼ëŠ” í…Œì´ë¸”ì— CHAR(1) íƒ€ì… &amp; NULLì´ ê°€ëŠ¥í•œ gender ì¹¼ëŸ¼ì„ ì¶”ê°€ ALTER TABLE student ADD gender CHAR(1) NULL; . | ì»¬ëŸ¼ëª… ë³€ê²½ . | ALTER TABLE í…Œì´ë¸”ëª… RENAME COLUMN ì¹¼ëŸ¼ëª…1 TO ì¹¼ëŸ¼ëª…2;ì˜ êµ¬ì¡°ë¡œ ì‘ì„± | . -- student í…Œì´ë¸”ì˜ student_number ì»¬ëŸ¼ì„ registration_numberë¼ëŠ” ì´ë¦„ìœ¼ë¡œ ë³€ê²½ ALTER TABLE student RENAME COLUMN student_number TO registration_number; . | ì»¬ëŸ¼ ì‚­ì œ . | ALTER TABLE í…Œì´ë¸”ëª… DROP COLUMN ì»¬ëŸ¼;ì˜ êµ¬ì¡°ë¡œ ì‘ì„± | . -- student í…Œì´ë¸”ì—ì„œ admission_date ì»¬ëŸ¼ì„ ì‚­ì œ ALTER TABLE student DROP COLUMN admission_date; . | ì»¬ëŸ¼ì˜ ë°ì´í„° íƒ€ì… ë³€ê²½ . | ALTER TABLE í…Œì´ë¸”ëª… MODIFY ì»¬ëŸ¼ ì†ì„±;ì˜ êµ¬ì¡°ë¡œ ì‘ì„± | â€» ìœ ì˜: ë°ì´í„° íƒ€ì…ì„ ë³€ê²½í•˜ë ¤ë©´ ì»¬ëŸ¼ ë‚´ ê°’ë“¤ì´ í•´ë‹¹ íƒ€ì…ì´ ìˆ˜ìš©í•  ìˆ˜ ìˆëŠ” í˜•íƒœê°€ ë˜ì–´ì•¼ í•œë‹¤ (ex. INT íƒ€ì…ìœ¼ë¡œ ë³€ê²½í•˜ë ¤ë©´ ìš°ì„  ê°’ë“¤ì„ ë‹¤ ìˆ«ìë¡œ ë°”ê¿”ì¤˜ì•¼ í•œë‹¤) | . -- ì „ê³µëª…ì´ ì €ì¥ëœ VARCHAR(15) íƒ€ì…ì˜ ì»¬ëŸ¼ 'major'ë¥¼ ì „ê³µì½”ë“œë¥¼ ì €ì¥í•˜ëŠ” INT íƒ€ì…ìœ¼ë¡œ ë³€ê²½: --- UPDATEë¬¸ìœ¼ë¡œ ê° ì „ê³µëª…ì„ ì „ê³µì½”ë“œë¡œ ë³€ê²½í•´ì¤€ í›„, major ì»¬ëŸ¼ì˜ ë°ì´í„° íƒ€ì…ì„ INTë¡œ ë°”ê¿”ì¤Œ UPDATE student SET major = 10 WHERE major = 'ì–¸ë¡ í™ë³´ì˜ìƒí•™ê³¼'; UPDATE student SET major = 12 WHERE major = 'ê²½ì˜í•™ê³¼'; ALTER TABLE student MODIFY major INT; . | . ì»¬ëŸ¼ ê´€ë ¨ ê¸°íƒ€ ì‘ì—…ë“¤ . | ì»¬ëŸ¼ì„ ê°€ì¥ ì•ìœ¼ë¡œ ë³´ë‚´ê¸° . | ì†ì„±ìœ¼ë¡œ â€˜FIRSTâ€™ë¥¼ ì¶”ê°€: ALTER TABLE í…Œì´ë¸”ëª… MODIFY ì»¬ëŸ¼ ì†ì„± FIRST;ì˜ êµ¬ì¡° | . -- id ì»¬ëŸ¼ì„ ê°€ì¥ ì•ìœ¼ë¡œ ë³´ë‚´ì£¼ê¸° (ì£¼ë¡œ primary keyë¥¼ ê°€ì¥ ì•ì— ë‘”ë‹¤) ALTER TABLE student MODIFY id INT NOT NULL AUTO_INCREMENT FIRST; . | ì»¬ëŸ¼ ê°„ ìˆœì„œ ë°”ê¾¸ê¸° . | â€˜AFTERâ€™ë¥¼ ì‚¬ìš©: ALTER TABLE í…Œì´ë¸”ëª… MODIFY ì»¬ëŸ¼ ì†ì„± AFTER ì»¬ëŸ¼2;ì˜ êµ¬ì¡° | . -- gender ì»¬ëŸ¼ì„ name ì»¬ëŸ¼ ë’¤ë¡œ ë³´ë‚´ê¸° ALTER TABLE student MODIFY gender CHAR(1) NULL AFTER name; . | ì»¬ëŸ¼ì˜ ì´ë¦„ê³¼ ë°ì´í„° íƒ€ì… ë° ì†ì„± ë™ì‹œì— ìˆ˜ì •í•˜ê¸° . | â€˜CHANGEâ€™ì ˆì„ ì´ìš©í•˜ë©´ ì»¬ëŸ¼ëª…ê³¼ ë°ì´í„° íƒ€ì…, ì†ì„±ì„ ë™ì‹œì— ìˆ˜ì •í•  ìˆ˜ ìˆë‹¤ | ALTER TABLE í…Œì´ë¸”ëª… CHANGE ì»¬ëŸ¼ëª…1 ì»¬ëŸ¼ëª…2 ì†ì„±ì˜ êµ¬ì¡° | . -- role ì»¬ëŸ¼ì˜ ì´ë¦„ì„ 'position'ìœ¼ë¡œ ë°”ê¾¸ê³ , ë°ì´í„° íƒ€ì…ì€ VARCHAR(5), ì†ì„±ì€ NOT NULLë¡œ ë°”ê¿”ì¤Œ ALTER TABLE player CHANGE role position VARCHAR(5) NOT NULL; . | í•˜ë‚˜ì˜ ALTER TABLEë¬¸ìœ¼ë¡œ ì—¬ëŸ¬ ê°œì˜ ì‘ì—…ì„ í•˜ëŠ” ê²ƒë„ ê°€ëŠ¥ . | ex) ì•„ë˜ 4ê°€ì§€ ì‘ì—…ì„ ë™ì‹œì— ìˆ˜í–‰: 1) id ì»¬ëŸ¼ì˜ ì´ë¦„ì„ numberë¡œ ìˆ˜ì • 2) name ì»¬ëŸ¼ì˜ ë°ì´í„° íƒ€ì…ì„ VARCHAR(20)ë¡œ, ì†ì„±ì„ NOT NULLë¡œ ìˆ˜ì • 3) position ì»¬ëŸ¼ì„ í…Œì´ë¸”ì—ì„œ ì‚­ì œ 4) ìƒˆë¡œìš´ ì»¬ëŸ¼ heightë¥¼ ì¶”ê°€ | . ALTER TABLE player RENAME COLUMN id TO number, MODIFY name VARCHAR(20) NOT NULL, DROP COLUMN position, ADD height DOUBLE NOT NULL; . | . ",
    "url": "https://chaelist.github.io/docs/sql/modify_table/#%EC%BB%AC%EB%9F%BC-%EC%B6%94%EA%B0%80%EB%B3%80%EA%B2%BD%EC%82%AD%EC%A0%9C",
    "relUrl": "/docs/sql/modify_table/#ì»¬ëŸ¼-ì¶”ê°€ë³€ê²½ì‚­ì œ"
  },"120": {
    "doc": "í…Œì´ë¸” ê°€ê³µ",
    "title": "ì»¬ëŸ¼ì— ì†ì„± ì¶”ê°€",
    "content": "NOT NULL, DEFAULT, UNIQUE . | NOT NULL ì†ì„± ì¶”ê°€ . | ALTER ~ MODIFY ë¬¸ì„ ì‚¬ìš©í•´ì„œ NOT NULL ì†ì„± ì¶”ê°€: ALTER TABLE í…Œì´ë¸”ëª… MODIFY ì»¬ëŸ¼ ì†ì„± NOT NULLì˜ êµ¬ì¡° | MODIFY ë’¤ì—ëŠ” ì»¬ëŸ¼ì˜ ê¸°ì¡´ ë°ì´í„° ì†ì„±ë„ í•¨ê»˜ ì¨ì¤€ í›„ì— NOT NULLì„ ì¶”ê°€í•´ì•¼ í•œë‹¤. í˜¹ì€, ë³€ê²½í•˜ë ¤ê³  í•˜ëŠ” ë°ì´í„° ì†ì„±ì„ ì¨ì¤˜ë„ ëœë‹¤ (ë°ì´í„° íƒ€ì… ë³€ê²½ &amp; NOT NULL ì¶”ê°€ ë™ì‹œì—) | NOT NULL ì†ì„±ì„ ì¶”ê°€í•œ ì»¬ëŸ¼ì— ê°’ì´ ì—†ëŠ” rowë¥¼ ì¶”ê°€í•˜ë ¤ê³  í•˜ë©´ errorê°€ ë‚œë‹¤ | . -- student í…Œì´ë¸”ì— NOT NULL ì†ì„± ì¶”ê°€ ALTER TABLE student MODIFY name VARCHAR(20) NOT NULL; . | DEFAULT ì†ì„± ì¶”ê°€ . | ALTER ~ MODIFY ë¬¸ì„ ì‚¬ìš©í•´ì„œ DEFAULT ì†ì„± ì¶”ê°€ | DEFAULT ê°’ì„ ì¶”ê°€í•´ë‘ë©´, í•´ë‹¹ ì»¬ëŸ¼ì— ê°’ì´ ì—†ëŠ” rowë¥¼ ì¶”ê°€í•  ë•Œ ìë™ìœ¼ë¡œ DEFAULT ê°’ì´ ì…ë ¥ë¨ | NOT NULL ì†ì„±ì„ ê°–ê³  ìˆì§€ ì•Šì€ ì»¬ëŸ¼ë“¤ì€ Defaultê°’ì´ ìë™ìœ¼ë¡œ â€˜NULLâ€™ë¡œ ì„¤ì •ë˜ì–´ ìˆìŒ: í•´ë‹¹ ì»¬ëŸ¼ì— ê°’ì´ ì—†ëŠ” rowë¥¼ ì¶”ê°€í•˜ë©´, ìë™ìœ¼ë¡œ NULLê°’ì´ ë“¤ì–´ê°„ë‹¤ | . -- major ì»¬ëŸ¼ì˜ defaultê°’ì„ 101ë¡œ ì„¤ì • -- ì•ìœ¼ë¡œ major ê°’ì´ ì—†ëŠ” rowë¥¼ ì¶”ê°€í•˜ë©´ ìë™ìœ¼ë¡œ 101ì´ë¼ëŠ” ê°’ì´ ì €ì¥ëœë‹¤ëŠ” ì˜ë¯¸ ALTER TABLE student MODIFY major INT NOT NULL DEFAULT 101; . | UNIQUE ì†ì„± ì¶”ê°€ . | ALTER ~ MODIFY ë¬¸ì„ ì‚¬ìš©í•´ì„œ UNIQUE ì†ì„± ì¶”ê°€ | UNIQUE ì†ì„±ì„ ì¶”ê°€í•´ë‘ë©´, ì»¬ëŸ¼ì— ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ê°’ê³¼ ì¤‘ë³µë˜ëŠ” ê°’ì„ ê°€ì§„ ìƒˆ rowë¥¼ ì…ë ¥í•˜ë ¤ê³  í•˜ë©´ errorê°€ ë‚œë‹¤ | â€˜í•™ë²ˆâ€™ì²˜ëŸ¼, ê° ê°’ì´ ë°˜ë“œì‹œ ê³ ìœ í•œ ê°’ì„ ê°€ì ¸ì•¼ í•˜ëŠ” ì»¬ëŸ¼ì˜ ê²½ìš° UNIQUE ì†ì„±ì„ ì£¼ë©´ ì¢‹ë‹¤ | cf) UNIQUE ì†ì„±ì´ ìˆëŠ” ì»¬ëŸ¼ì´ë¼ë„, NULLê°’ì€ í—ˆìš©ë  ìˆ˜ ìˆë‹¤. (Primary Keyì˜ ê²½ìš° ë°˜ë“œì‹œ NOT NULLì´ì—¬ì•¼ í•˜ëŠ” ê²ƒê³¼ ì¡°ê¸ˆ ë‹¤ë¥¸ ê°œë…â€¦) | . -- student_number ì»¬ëŸ¼ì— UNIQUE ì†ì„± ì¶”ê°€ ALTER TABLE student MODIRY student_number INT NOT NULL UNIQUE; . | . CURRENT_TIMESTAMP ì†ì„± . : DATETIME, TIMESTAMP íƒ€ì…ì˜ ì»¬ëŸ¼ì— ì¶”ê°€í•  ìˆ˜ ìˆëŠ” ì†ì„±. rowê°€ ì¶”ê°€/ê°±ì‹ ë  ë•Œë§ˆë‹¤ í˜„ì¬ ì‹œê°ì„ ì €ì¥í•´ì¤˜ì•¼ í•˜ëŠ” ê²½ìš° ìœ ìš©í•˜ë‹¤. (ex. ê²Œì‹œê¸€ ì—…ë¡œë“œ ì‹œê°, ë§ˆì§€ë§‰ ìˆ˜ì • ì‹œê°) . | DEFAULT CURRENT_TIMESTAMP ì†ì„± . | í…Œì´ë¸”ì— ìƒˆ rowë¥¼ ì¶”ê°€í•  ë•Œ ë³„ë„ë¡œ NOW()ê°’ì„ ì£¼ì§€ ì•Šì•„ë„ í˜„ì¬ ì‹œê°„ì´ ìë™ ì €ì¥ë˜ë„ë¡ í•˜ëŠ” ì†ì„± | . | ON UPDATE CURRENT_TIMESTAMP ì†ì„± . | ê¸°ì¡´ rowê°€ ë‹¨ í•˜ë‚˜ì˜ ì»¬ëŸ¼ì´ë¼ë„ ìˆ˜ì •ë˜ë©´, ì—…ë°ì´íŠ¸ë  ë•Œì˜ ì‹œê°„ì´ ìë™ìœ¼ë¡œ ì €ì¥ë˜ë„ë¡ í•˜ëŠ” ì†ì„±. | . | . -- upload_time ì»¬ëŸ¼ì—ëŠ” DEFAULT CURRENT_TIMESTAMP ì†ì„±ì„, -- recent_modified_time ì»¬ëŸ¼ì—ëŠ” DEFAULT CURRENT_TIMESTAMP ì†ì„±ê³¼ ON UPDATE CURRENT_TIMESTAMP ì†ì„±ì„ ëª¨ë‘ ì¶”ê°€ ALTER TABLE post MODIFY upload_time DATETIME DEFAULT CURRENT_TIMESTAMP, MODIFY recent_modified_time DATETIME DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP; . | ì´ë ‡ê²Œ í•´ë‘ë©´ 1) ìƒˆë¡œìš´ ê²Œì‹œê¸€ì„ ì¶”ê°€í•  ë•Œ ë‹¤ë¥¸ ì»¬ëŸ¼ë“¤ì—ë§Œ ê°’ì„ ì ì–´ì¤˜ë„ upload_time, recent_modified_time ì»¬ëŸ¼ì— ìë™ìœ¼ë¡œ í˜„ì¬ ì‹œê°„ì´ ë“¤ì–´ê°€ê³ , 2) ê²Œì‹œê¸€ ë‚´ìš©ì„ ìˆ˜ì •í•˜ë©´ recent_modified_timeì˜ ê°’ì´ ìë™ìœ¼ë¡œ í•´ë‹¹ ì‹œê°ìœ¼ë¡œ ì—…ë°ì´íŠ¸ëœë‹¤ | . Â  . +) NOW() í•¨ìˆ˜: í˜„ì¬ ì‹œê°ì„ ì¶”ê°€í•´ì£¼ëŠ” í•¨ìˆ˜ . | NOW() í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´ë„ ìœ„ì™€ ê°™ì€ ê²°ê³¼ë¥¼ ë‚¼ ìˆ˜ ìˆìŒ (ë‹¤ë§Œ, ìˆ˜ë™ìœ¼ë¡œ NOW()ë¥¼ ë§¤ë²• ì…ë ¥í•´ì¤˜ì•¼ í•œë‹¤) . INSERT INTO post (title, content, upload_time, recent_modified_time) VALUES (\"ì œëª©\", \"ë³¸ë¬¸\", NOW(), NOW()); -- upload_time, recent_modified_time ëª¨ë‘ í˜„ì¬ ì‹œê°ì´ ì¶”ê°€ë¨ . UPDATE post SET content = 'ë³¸ë¬¸ ìˆ˜ì •', recent_modified_time = NOW() WHERE id = 1; -- recent_modified_timeì´ í˜„ì¬ ì‹œê°ìœ¼ë¡œ ë³€ê²½ë¨ . | . CONSTRAINT ì†ì„± . : ì»¬ëŸ¼ì— ì ì ˆí•œ constraint(ì œì•½ ì‚¬í•­)ì„ ê±¸ì–´ë‘ë©´ ì˜ëª»ëœ ë°ì´í„°ê°€ ì…ë ¥ë˜ëŠ” ê²ƒì„ ë§‰ì•„ì„œ ë°ì´í„° í€„ë¦¬í‹°ë¥¼ ê´€ë¦¬í•  ìˆ˜ ìˆë‹¤ . | ADD CONSTRAINT . | ì»¬ëŸ¼ì— ì œì•½ ì‚¬í•­ì„ ì¶”ê°€í•´ì£¼ëŠ” ê²ƒ | ALTER TABLE í…Œì´ë¸”ëª… ADD CONSTRAINT ì œì•½ì´ë¦„ CHECK (ì¡°ê±´);ì˜ êµ¬ì¡°ë¡œ ì‘ì„± | ì œì•½ ì¡°ê±´ì— ìœ„ë°°ë˜ëŠ” ë°ì´í„°ë¥¼ ë„£ìœ¼ë ¤ê³  í•˜ë©´ errorê°€ ë°œìƒí•œë‹¤ | . -- student í…Œì´ë¸”ì— 'student_numberëŠ” 3ì²œë§Œë³´ë‹¤ ì‘ì•„ì•¼ í•œë‹¤'ëŠ” ì œì•½ ê±¸ê¸° ALTER TABLE student ADD CONSTRAINT st_rule CHECK (student_number &lt; 30000000); -- (ì—¬ê¸°ì„œ st_ruleì€ ì´ ì œì•½ ì‚¬í•­ì˜ ì´ë¦„) -- ìœ„ì™€ ê°™ì€ ì œì•½ì„ ê±´ í›„ì— student_numberì— 30000000ì´ ë“¤ì–´ê°€ëŠ” rowë¥¼ ì‚½ì…í•˜ë ¤ í•˜ë©´ errorê°€ ë°œìƒ . | DROP CONSTRAINT . | ê±¸ì–´ë‘” CONSTRAINT ì‚­ì œí•˜ê¸° | ALTER TABLE í…Œì´ë¸”ëª… DROP CONSTRAINT ì œì•½ì´ë¦„;ì˜ êµ¬ì¡° | . ALTER TABLE student DROP CONSTRAINT st_rule; . | ë‘ ê°œ ì´ìƒì˜ ì¡°ê±´ì´ ë‹´ê¸´ CONSTRAINT ë§Œë“¤ê¸° . | ì—¬ëŸ¬ ì¡°ê±´ì„ ANDë¡œ ì—°ê²°í•´ì£¼ë©´ ëœë‹¤ | . -- email ì»¬ëŸ¼ì— ë“¤ì–´ê°ˆ ê°’ì—ëŠ” '@'ê°€ ë“¤ì–´ê°€ì•¼ í•˜ê³ , gender ì»¬ëŸ¼ì— ë“¤ì–´ê°ˆ ê°’ì€ 'm'ì´ë‚˜ 'f' ë‘˜ ì¤‘ í•˜ë‚˜ì—¬ì•¼ í•œë‹¤ëŠ” ì¡°ê±´ ê±¸ê¸°: ALTER TABLE student ADD CONSTRAINT st_rule CHECK (email LIKE '%@%' AND gender IN ('m', 'f')); . | . ",
    "url": "https://chaelist.github.io/docs/sql/modify_table/#%EC%BB%AC%EB%9F%BC%EC%97%90-%EC%86%8D%EC%84%B1-%EC%B6%94%EA%B0%80",
    "relUrl": "/docs/sql/modify_table/#ì»¬ëŸ¼ì—-ì†ì„±-ì¶”ê°€"
  },"121": {
    "doc": "í…Œì´ë¸” ê°€ê³µ",
    "title": "í…Œì´ë¸” ë³€ê²½/ë³µì‚¬/ì‚­ì œ",
    "content": ". | í…Œì´ë¸” ì´ë¦„ ë³€ê²½ . | RENAME TABLE í…Œì´ë¸”ëª… TO í…Œì´ë¸”ëª…2ì˜ êµ¬ì¡°ë¡œ ì‘ì„± | . RENAME TABLE student TO undergraduate; . | í…Œì´ë¸” ë³µì‚¬ . | â€˜ASâ€™ë¥¼ ì‚¬ìš©í•´ SELECTë¬¸ìœ¼ë¡œ ê°€ì ¸ì˜¨ ë°ì´í„°ë¥¼ ë³µì‚¬í•œ í…Œì´ë¸”ì„ ìƒì„± | CREATE TABLE í…Œì´ë¸”ëª… AS SELECTë¬¸;ì˜ êµ¬ì¡°ë¡œ ì‘ì„± | . CREATE TABLE undergraduate_copy AS SELECT * FROM undergraduate; . +) â€˜WHEREâ€™ì ˆì„ ì‚¬ìš©í•´ íŠ¹ì • ì¡°ê±´ì˜ ë°ì´í„°ë§Œ ë³µì‚¬í•œ í…Œì´ë¸”ì„ ìƒì„±í•˜ëŠ” ê²ƒë„ ê°€ëŠ¥: . -- genderê°€ 'u'ì¸ item í…Œì´ë¸”ì˜ rowë“¤ë§Œ ìƒˆë¡œìš´ í…Œì´ë¸”ë¡œ ë³µì‚¬ CREATE TABLE item_copy AS SELECT * FROM item WHERE gender = 'u'; . | í…Œì´ë¸” ì‚­ì œ . | DROP TABLE í…Œì´ë¸”ëª…;ì˜ êµ¬ì¡°ë¡œ ì‘ì„± | . DROP TABLE undergraduate_copy; . | í…Œì´ë¸”ì˜ ì»¬ëŸ¼ êµ¬ì¡° ë³µì‚¬ . | í…Œì´ë¸” ë‚´ì˜ ë°ì´í„°ë¥¼ ì œì™¸í•˜ê³ , ì»¬ëŸ¼ êµ¬ì¡°ë§Œ ë³µì‚¬í•´ì˜¤ëŠ” ê²ƒ | CREATE TABLE í…Œì´ë¸”ëª… LIKE ê¸°ì¡´_í…Œì´ë¸”;ì˜ êµ¬ì¡°ë¡œ ì‘ì„± | . CREATE TABLE undergraduate_copy LIKE undergraduate; . +) ì»¬ëŸ¼ êµ¬ì¡°ë§Œ ë³µì‚¬í•´ë‘” í…Œì´ë¸”ì—, ë‹¤ì‹œ ë°ì´í„°ê¹Œì§€ ë³µì‚¬í•´ì„œ ë¶™ì—¬ë„£ìœ¼ë ¤ë©´?: . -- SELECTë¬¸ìœ¼ë¡œ ê°€ì ¸ì˜¨ ë°ì´í„°ë¥¼ INSERT INTOë¬¸ìœ¼ë¡œ ë„£ì–´ì£¼ê¸° INSERT INTO undergraduate_copy SELECT * FROM undergraduate; . | í…Œì´ë¸” ë‚´ ë°ì´í„°ë§Œ ì‚­ì œ . | í…Œì´ë¸”ì˜ êµ¬ì¡°ëŠ” ë‚¨ê¸°ê³ , ë°ì´í„°ë§Œ ëª¨ë‘ ì‚­ì œí•˜ê¸° (cf. DROP TABLEì€ í…Œì´ë¸” ìì²´ë¥¼ ì‚­ì œ) | TRUNCATE í…Œì´ë¸”ëª…;ì˜ êµ¬ì¡°ë¡œ ì‘ì„± | . TRUNCATE exam_result; . +) DELETE FROMì„ ì‚¬ìš©í•´ë„ ë°ì´í„°ë¥¼ ëª¨ë‘ ì‚­ì œí•˜ëŠ” ê²°ê³¼ë¥¼ ë‚¼ ìˆ˜ ìˆë‹¤: . DELETE FROM exam_result; . | ë‹¤ë§Œ, DELETE FROMìœ¼ë¡œ ì‚­ì œí•˜ë©´ ë°ì´í„°ê°€ í•œì¤„í•œì¤„ ì œê±°ë˜ëŠ” ê³¼ì •ì„ ê±°ì¹˜ê²Œ ë˜ë©°, ë°ì´í„°ëŠ” ì§€ì›Œì§€ë˜ ì‚¬ìš©í•˜ë˜ ê³µê°„ì€ ê·¸ëŒ€ë¡œ ë‚¨ì•„ ìˆìŒ (TRUNCATEì™€ DELETEëŠ” ë‚´ë¶€ì ìœ¼ë¡œ êµ¬í˜„ë˜ëŠ” ë°©ì‹ì´ ë‹¤ë¦„) | . | . ",
    "url": "https://chaelist.github.io/docs/sql/modify_table/#%ED%85%8C%EC%9D%B4%EB%B8%94-%EB%B3%80%EA%B2%BD%EB%B3%B5%EC%82%AC%EC%82%AD%EC%A0%9C",
    "relUrl": "/docs/sql/modify_table/#í…Œì´ë¸”-ë³€ê²½ë³µì‚¬ì‚­ì œ"
  },"122": {
    "doc": "Network Analysis",
    "title": "Network Analysis",
    "content": " ",
    "url": "https://chaelist.github.io/docs/network_analysis",
    "relUrl": "/docs/network_analysis"
  },"123": {
    "doc": "Network Analysis ê¸°ì´ˆ",
    "title": "Network Analysis ê¸°ì´ˆ",
    "content": ". | Network ê¸°ë³¸ ê°œë… . | Nodeì™€ Edge | Tie(ê´€ê³„)ì˜ ì¢…ë¥˜ | . | ê°„ë‹¨í•œ ë„¤íŠ¸ì›Œí¬ ê·¸ë ¤ë³´ê¸° . | ê·¸ë˜í”„ ìƒì„± | Basic Calculations | ë„¤íŠ¸ì›Œí¬ ì‹œê°í™” | node, edgeì— attribute ë”í•˜ê¸° | attribute í¬í•¨í•´ì„œ ì‹œê°í™” | íŠ¹ì • ì¡°ê±´ì˜ node, edge ì°¾ê¸° | DiGraph ê·¸ë¦¬ê¸° &amp; self-loop | . | Network êµ¬ì¡° íŒŒì•…í•˜ê¸° . | Neighbors &amp; Degree | Shortest path | Centrality | Cliques | Subgraphs | . | Visualization with nxviz . | nxviz example 1 | nxviz example 2: complex network | . | . ",
    "url": "https://chaelist.github.io/docs/network_analysis/network_basics/",
    "relUrl": "/docs/network_analysis/network_basics/"
  },"124": {
    "doc": "Network Analysis ê¸°ì´ˆ",
    "title": "Network ê¸°ë³¸ ê°œë…",
    "content": "Nodeì™€ Edge . | Network: Nodeì™€ Edgeë¡œ ì´ë£¨ì–´ì§„ ìë£Œ êµ¬ì¡°. | Node: vertexë¼ê³ ë„ ë¶€ë¥´ë©°, networkë¥¼ ì´ë£¨ëŠ” ê° ì ì„ ì˜ë¯¸í•œë‹¤. ex) twitter user networkë¥¼ ë§Œë“ ë‹¤ë©´, userë“¤ì´ ê°ê°ì˜ node. | Edge: link, ë˜ëŠ” tieë¼ê³ ë„ ë¶€ë¥´ë©°, ê° node ê°„ì˜ ê´€ê³„ë¥¼ ì˜ë¯¸í•œë‹¤. ex) twitter user networkë¥¼ ë§Œë“ ë‹¤ë©´, user(=node)ê°„ follow ê´€ê³„ë¥¼ edgeë¡œ í‘œí˜„í•  ìˆ˜ ìˆë‹¤. | . Tie(ê´€ê³„)ì˜ ì¢…ë¥˜ . | ë¹„ëŒ€ì¹­ ê´€ê³„(asymmetric tie): ê´€ê³„ì—ì„œ ë‘ nodeê°€ ê°–ëŠ” íŠ¹ì„±ì´ ë‹¤ë¥¸ ê²½ìš°. | ê´€ê³„ì— ë°©í–¥ì„±ì´ ìˆìœ¼ë¯€ë¡œ, directed tieë¼ê³ ë„ í•œë‹¤. | ex) SNSì—ì„œì˜ follower - followee ê´€ê³„ (Aê°€ Bë¥¼ followí•˜ë©´ A â†’ Bì™€ ê°™ì´ ê´€ê³„ì˜ ë°©í–¥ì´ ì¡´ì¬) | directed tieë¡œ êµ¬ì„±ëœ ë„¤íŠ¸ì›Œí¬ë¥¼ directed networkë¼ê³  í•¨ | . | ëŒ€ì¹­ ê´€ê³„ (symmetric tie): ê´€ê³„ì—ì„œ ë‘ nodeê°€ ê°–ëŠ” íŠ¹ì„±ì´ ë¹„ìŠ·í•œ ê²½ìš° . | ê´€ê³„ì— ë°©í–¥ì„±ì´ ì—†ìœ¼ë¯€ë¡œ, undirexted tieë¼ê³ ë„ í•œë‹¤. | ex) íŠ¹ì • ì§‘ë‹¨ ë‚´ ì‚¬ëŒë“¤ ê°„ì˜ ì¹œë¶„ ê´€ê³„. (ì„œë¡œ ì•„ëŠ” ì‚¬ì´ì¸ì§€) | undirected tieë¡œ êµ¬ì„±ëœ ë„¤íŠ¸ì›Œí¬ë¥¼ undirected networkë¼ê³  í•¨ | . | . ",
    "url": "https://chaelist.github.io/docs/network_analysis/network_basics/#network-%EA%B8%B0%EB%B3%B8-%EA%B0%9C%EB%85%90",
    "relUrl": "/docs/network_analysis/network_basics/#network-ê¸°ë³¸-ê°œë…"
  },"125": {
    "doc": "Network Analysis ê¸°ì´ˆ",
    "title": "ê°„ë‹¨í•œ ë„¤íŠ¸ì›Œí¬ ê·¸ë ¤ë³´ê¸°",
    "content": ". | network ë¶„ì„ì— íŠ¹í™”ëœ python libraryì¸ networkxë¥¼ ì‚¬ìš© | . import networkx as nx # importí•´ì•¼ ì‚¬ìš© ê°€ëŠ¥; ë³´í†µ nxë¡œ ì¤„ì—¬ì„œ import import matplotlib.pyplot as plt # ì‹œê°í™”ë¥¼ ìœ„í•´ ë¯¸ë¦¬ í•¨ê»˜ import . ê·¸ë˜í”„ ìƒì„± . | ë¨¼ì €, ë¹„ì–´ìˆëŠ” graphë¥¼ ìƒì„± # Create an empy undirected graph g = nx.Graph() . | directed graphë¥¼ ê·¸ë¦¬ë ¤ë©´: nx.DiGraph() | +) nx.MultiGraph(), nx.MultiDiGraph() ì˜µì…˜ë„ ì¡´ì¬ (multi-edge graph) . | ê°™ì€ node 2ê°œë¥¼ ì—°ê²°í•˜ëŠ” edgeê°€ ì—¬ëŸ¬ ê°œì¼ ìˆ˜ ìˆëŠ” ê·¸ë˜í”„. | ex) ì •ë¥˜ì¥ ê°„ tripì„ í˜•ìƒí™” â†’ A ì •ë¥˜ì¥ì—ì„œ B ì •ë¥˜ì¥ì„ ê±°ì³ ê°€ëŠ” routeê°€ 3ì¢…ë¥˜ë©´ edgeê°€ 3ê°œ | í•˜ì§€ë§Œ multi edgeë¥¼ ê·¸ë¦¬ë ¤ë©´ ë¦¬ì†ŒìŠ¤ê°€ ë§ì´ ë“¤ì–´ì„œ, ë³´í†µ ê·¸ëƒ¥ edge í•˜ë‚˜ë¡œ collapseí•´ì£¼ê³  â€˜weightâ€™ metadataë¡œ edgeì˜ ê°•ë„ë¥¼ í‘œí˜„í•´ì¤€ë‹¤. (Graphë‚˜ DiGraphë¥¼ ê·¸ë¦¬ê³ , weightë¥¼ ì§€ì •) | . | . +) type í™•ì¸: â€˜Graphâ€™ type . type(g) . networkx.classes.graph.Graph . | directed graphì˜ typeì€ networkx.classes.digraph.DiGraph | . | node ì¶”ê°€í•˜ê¸° g.add_nodes_from([1,2,3,4,5,6]) # Add nodes from a list . | cf) nodeë¥¼ í•˜ë‚˜ì”© ì¶”ê°€í•˜ë ¤ë©´: g.add_node(1) ì´ë ‡ê²Œ í•˜ë‚˜ì”© ì¨ì£¼ë©´ ëœë‹¤ | . | edge ì¶”ê°€í•˜ê¸° g.add_edges_from([(1,3), (2,4), (2,5), (2,6), (3,4), (4,6), (5,6)]) # Add edges from a list . | cf) edgeë¥¼ í•˜ë‚˜ì”© ì¶”ê°€í•˜ë ¤ë©´: g.add_edge(1,3) ì´ë ‡ê²Œ í•˜ë‚˜ì”© ì¨ì£¼ë©´ ëœë‹¤ | ë§Œì•½ ëª¨ë“  nodeê°€ edgeê°€ ì—°ê²°ë˜ì–´ ìˆë‹¤ë©´, edgeë§Œ ì¶”ê°€í•´ë„ ê·¸ ì•ˆì— í¬í•¨ëœ nodeë„ ìë™ìœ¼ë¡œ ì¶”ê°€ëœë‹¤ | . | . Basic Calculations . print(g.nodes()) # nodes print(g.edges()) # edges print(g.number_of_nodes()) # number of nodes print(g.number_of_edges()) # number of edges . [1, 2, 3, 4, 5, 6] [(1, 3), (2, 4), (2, 5), (2, 6), (3, 4), (4, 6), (5, 6)] 6 7 . ë„¤íŠ¸ì›Œí¬ ì‹œê°í™” . | nx.draw_networkx() í•¨ìˆ˜ë¥¼ ì´ìš©í•´ ì‹œê°í™” | draw_networkxëŠ” ë‹¤ë¥¸ ê²ƒê³¼ ì—°ê²°ì´ ë§ì€ ë…¸ë“œë¥¼ ì¤‘ì‹¬ì— ì˜¤ê²Œ ìë™ìœ¼ë¡œ ìœ„ì¹˜ë¥¼ ê²°ì •í•´ì„œ ê°€ì‹œí™”í•´ì£¼ë©°, nodeì˜ labelë„ ìë™ìœ¼ë¡œ í•¨ê»˜ ë³´ì—¬ì¤€ë‹¤ | nx.draw() í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´ë„ ë˜ì§€ë§Œ, draw_networkxê°€ ë¶€ê°€ ê¸°ëŠ¥ì´ ë” ë§ìŒ | . nx.draw_networkx(g) plt.axis('off') # turn off axis plt.show() . +) graphML íŒŒì¼ë¡œ ì €ì¥í•˜ê¸° . | graphML íŒŒì¼ë¡œ ë‚´ë³´ë‚¸ í›„, gephi ë“±ì˜ íˆ´ë¡œ ì‹œê°í™”í•  ìˆ˜ë„ ìˆë‹¤ | . nx.write_graphml(g, 'graph_test.graphml') . node, edgeì— attribute ë”í•˜ê¸° . | Add node attributes g.nodes[1]['gender']='male' # {'gender':'male'}ì´ë¼ëŠ” dictionaryì˜ ëŠë‚Œ. key-value pair. g.nodes[2]['gender']='female' g.nodes[3]['gender']='male' g.nodes[4]['gender']='female' g.nodes[5]['gender']='male' g.nodes[6]['gender']='male' print(nx.get_node_attributes(g, 'gender')) . {1: 'male', 2: 'female', 3: 'male', 4: 'female', 5: 'male', 6: 'male'} . | Add edge attributes g[1][3]['weight'] = 3 ## ì´ë ‡ê²Œ ì ‘ê·¼í•´ë„ ë¨: g.edges[1, 3]['weight'] = 3 g[2][4]['weight'] = 1 g[2][5]['weight'] = 4 g[2][6]['weight'] = 3 g[3][4]['weight'] = 2 g[4][6]['weight'] = 3 g[5][6]['weight'] = 4 print(nx.get_edge_attributes(g, 'weight')) . {(1, 3): 3, (2, 4): 1, (2, 5): 4, (2, 6): 3, (3, 4): 2, (4, 6): 3, (5, 6): 4} . | 1-3 ì‚¬ì´ì˜ edgeë¥¼ ì ‘ê·¼í•˜ëŠ” ë°©ë²•: g[1][3] = g.edges[1, 3] (ì–´ë–»ê²Œ ì ‘ê·¼í•˜ë“  ìƒê´€ì—†ìŒ) | . | . +) ì†ì„±ê¹Œì§€ í¬í•¨í•´ì„œ node, edge ë³´ê¸° . | data=True ì˜µì…˜ì„ ë„£ì–´ì£¼ë©´ ë¨ | . g.nodes(data=True) ## nodeë“¤ì˜ ì†ì„±ê¹Œì§€ í•¨ê»˜ ë³¼ ìˆ˜ ìˆìŒ . NodeDataView({1: {'gender': 'male'}, 2: {'gender': 'female'}, 3: {'gender': 'male'}, 4: {'gender': 'female'}, 5: {'gender': 'male'}, 6: {'gender': 'male'}}) . g.edges(data=True) ## edgeë“¤ì˜ ì†ì„±ê¹Œì§€ í•¨ê»˜ ë³¼ ìˆ˜ ìˆìŒ . EdgeDataView([(1, 3, {'weight': 3}), (2, 4, {'weight': 1}), (2, 5, {'weight': 4}), (2, 6, {'weight': 3}), (3, 4, {'weight': 2}), (4, 6, {'weight': 3}), (5, 6, {'weight': 4})]) . attribute í¬í•¨í•´ì„œ ì‹œê°í™” . | edgeì˜ â€˜weightâ€™ë¥¼ í¬í•¨í•´ì„œ ì‹œê°í™” pos=nx.spring_layout(g) # ê° nodeì˜ positionì„ ì •í•´ì„œ ê·¸ë ¤ì¤˜ì•¼ edge_labelë¥¼ ë§ì¶°ì„œ ë„£ì„ ìˆ˜ ìˆìŒ nx.draw_networkx(g, pos) labels = nx.get_edge_attributes(g,'weight') nx.draw_networkx_edge_labels(g, pos, edge_labels=labels) plt.axis('off') # turn off axis plt.show() . | nodeì˜ â€˜genderâ€™ attributeì— ë”°ë¼ ìƒ‰ ë‹¤ë¥´ê²Œ í‘œí˜„ color_map = [] for n, d in g.nodes(data=True): if d['gender'] == 'female': color_map.append('pink') # ì—¬ì„±: pink else: color_map.append('skyblue') # ë‚¨ì„±: skyblue pos=nx.spring_layout(g) nx.draw_networkx(g, pos, node_color=color_map) labels = nx.get_edge_attributes(g,'weight') plt.axis('off') # turn off axis plt.show() . | . íŠ¹ì • ì¡°ê±´ì˜ node, edge ì°¾ê¸° . | â€˜genderâ€™ê°€ â€˜femaleâ€™ì¸ nodeë§Œ ì°¾ê¸° female_nodes = [n for n, d in g.nodes(data=True) if d['gender'] == 'female'] print(female_nodes) . [2, 4] . | â€˜weightâ€™ê°€ 3ë³´ë‹¤ í° edgeë§Œ ì°¾ê¸° strong_edges = [(u, v) for u, v, d in g.edges(data=True) if d['weight'] &gt; 3] print(strong_edges) . [(2, 5), (5, 6)] . | . DiGraph ê·¸ë¦¬ê¸° &amp; self-loop . diG = nx.DiGraph() diG.add_edges_from([(1, 2), (2, 4), (4, 2), (3, 3), (1, 3), (5, 1)]) # edgeë§Œ ì¶”ê°€í•´ë„, ìë™ìœ¼ë¡œ ì´ì— í¬í•¨ëœ nodeë„ í•¨ê»˜ ì¶”ê°€ë¨ . â†’ ì‹œê°í™”í•´ë³´ê¸° . nx.draw_networkx(diG) plt.axis('off') # turn off axis plt.show() . +) self-loop í™•ì¸í•˜ê¸° . | self-loop: ìê¸° ìì‹ ìœ¼ë¡œ ëŒì•„ì˜¤ëŠ” ë£¨í”„ (edge that begin and end on the same node) | ex) ë²„ìŠ¤ ë…¸ì„  ì¤‘, A í”Œë«í¼ì—ì„œ ë– ë‚˜ì„œ A í”Œë«í¼ìœ¼ë¡œ ëŒì•„ì˜¤ëŠ” ìˆœí™˜ì„ ì´ self-loopë¡œ í‘œí˜„ë  ìˆ˜ ìˆìŒ. | ì´ë²ˆ diGì—ì„œ ì¶”ê°€í•œ (3, 3) edgeê°€ ë°”ë¡œ self-loop | self-loopëŠ” ê·¸ë˜í”„ë¥¼ ì‹œê°í™”í–ˆì„ ë•Œ ì˜ ëˆˆì— ë„ì§€ ì•Šê¸°ì—, nx.number_of_selfloops()í•¨ìˆ˜ë¡œ self-loopì˜ ê°œìˆ˜ë¥¼ íŒŒì•…í•  ìˆ˜ ìˆë‹¤. | . nx.number_of_selfloops(diG) # diGì—ëŠ” 1ê°œì˜ self-loopê°€ í¬í•¨ë˜ì–´ ìˆìŒ . 1 . ",
    "url": "https://chaelist.github.io/docs/network_analysis/network_basics/#%EA%B0%84%EB%8B%A8%ED%95%9C-%EB%84%A4%ED%8A%B8%EC%9B%8C%ED%81%AC-%EA%B7%B8%EB%A0%A4%EB%B3%B4%EA%B8%B0",
    "relUrl": "/docs/network_analysis/network_basics/#ê°„ë‹¨í•œ-ë„¤íŠ¸ì›Œí¬-ê·¸ë ¤ë³´ê¸°"
  },"126": {
    "doc": "Network Analysis ê¸°ì´ˆ",
    "title": "Network êµ¬ì¡° íŒŒì•…í•˜ê¸°",
    "content": "# ìœ„ì—ì„œ ìƒì„±í•œ g ë„¤íŠ¸ì›Œí¬ë¥¼ ì´ì–´ì„œ ì‚¬ìš© nx.draw_networkx(g) plt.axis('off') # turn off axis plt.show() . Neighbors &amp; Degree . | g.neighbors(node): ê·¸ë˜í”„ g ë‚´ì—ì„œ íŠ¹ì • nodeì™€ ì—°ê²°ëœ neighbor nodeë“¤ì„ íŒŒì•…í•  ìˆ˜ ìˆìŒ | g.degree(node): ê·¸ë˜í”„ g ë‚´ì—ì„œ íŠ¹ì • nodeì˜ neighbor ìˆ˜ë¥¼ íŒŒì•…í•  ìˆ˜ ìˆìŒ. (ì—°ê²°ëœ nodeì˜ ìˆ˜) print(list(g.neighbors(4))) # 4's neighbors print(g.degree(4)) # 4's degree, i.e., number of neighbors . [2, 3, 6] 3 . | . Shortest path . | nx.shortest_path(g, node1, node2)ë¥¼ í™œìš©í•˜ë©´ ë„¤íŠ¸ì›Œí¬ gì˜ node1ì—ì„œ node2ê¹Œì§€ì˜ shortest pathë¥¼ ì°¾ì„ ìˆ˜ ìˆë‹¤. | +) nx.shortest_path(g, node1, node2, weight='weight')ë¼ê³  í•˜ë©´ edgeì˜ weightì„ ê³ ë ¤í•´ì„œ ì°¾ì•„ì¤€ë‹¤ | . nx.shortest_path(g, 3, 5) . [3, 4, 2, 5] . +) ê·¸ëƒ¥ nx.shortest_path(g)ë¼ê³ ë§Œ í•˜ë©´ gì˜ ëª¨ë“  node ê°„ì˜ shortest pathë¥¼ ëª¨ë‘ ì°¾ì•„ì¤€ë‹¤ . Centrality . : nodeì˜ ì¤‘ìš”ì„±ì„ íŒë³„í•  ë•Œ í™œìš©. | Degree Centrality: tieê°€ ì–¼ë§ˆë‚˜ ë§ì€ì§€ì˜ ì •ë„ . | íŠ¹ì • nodeì˜ neighbor ìˆ˜ / ìµœëŒ€ë¡œ ê°€ì§ˆ ìˆ˜ ìˆëŠ” neighbor ìˆ˜ = degree / (n-1) | ìµœëŒ€ë¡œ ê°€ì§ˆ ìˆ˜ ìˆëŠ” neighbor ìˆ˜ëŠ” n-1. (self-loop ê³ ë ¤X) | . # ê° nodeë³„ degree centrality ê°’ì„ dictionary í˜•íƒœë¡œ ë³´ì—¬ì¤Œ nx.degree_centrality(g) # key: node, value: degree centrality score for that node . {1: 0.2, 2: 0.6000000000000001, 3: 0.4, 4: 0.6000000000000001, 5: 0.4, 6: 0.6000000000000001} . â†’ 2, 4, 6ì´ ê°€ì¥ ì¤‘ìš”í•œ nodeë¡œ íŒë³„ë¨ . | Betweenness Centrality: ì–¼ë§ˆë‚˜ bridge ì—­í• ì„ í•˜ëŠ”ì§€ì˜ ì •ë„ . | íŠ¹ì • nodeë¥¼ ì§€ë‚˜ê°€ëŠ” shortest pathì˜ ìˆ˜ / ëª¨ë“  ê°€ëŠ¥í•œ shortest pathì˜ ìˆ˜ | ì—¬ëŸ¬ ì§‘ë‹¨ì„ ì´ì–´ì£¼ëŠ” ì—­í• ì„ í•˜ëŠ” nodeë¥¼ íŒŒì•…í•  ë•Œ ìš©ì´ (ex. ì •ì¹˜ ê´€ì‹¬ ê·¸ë£¹ê³¼ ì˜ˆìˆ  ê´€ì‹¬ ê·¸ë£¹ì„ ì´ì–´ì£¼ëŠ” ì—­í• ì„ í•˜ëŠ” node íŒŒì•…) | . # ê° nodeë³„ betweenness centrality ê°’ì„ dictionary í˜•íƒœë¡œ ë³´ì—¬ì¤Œ nx.betweenness_centrality(g) . {1: 0.0, 2: 0.15000000000000002, 3: 0.4, 4: 0.6000000000000001, 5: 0.0, 6: 0.15000000000000002} . â†’ 4ê°€ ê°€ì¥ ì¤‘ìš”í•œ nodeë¡œ íŒë³„ë¨ . +) nx.betweenness_centrality(g, weight='weight')ë¼ê³  weight ì˜µì…˜ì„ ì ì–´ì£¼ë©´, ê° edgeì˜ weightë¥¼ ê³ ë ¤í•´ì„œ centralityë¥¼ ê³„ì‚° . | Closeness Centrality: ë‹¤ë¥¸ nodeë“¤ê³¼ ì–¼ë§ˆë‚˜ closeí•˜ê²Œ ì—°ê²°ë˜ì–´ ìˆë‚˜ . | íŠ¹ì • nodeì—ì„œì˜ ë‹¤ë¥¸ nodeê¹Œì§€ì˜ shortest pathë¥¼ ê³ ë ¤ | (n-1) / íŠ¹ì • nodeì—ì„œ ë‹¤ë¥¸ ëª¨ë“  nodeê¹Œì§€ì˜ shortest path distanceì˜ í•© | n-1ì€ sum of minimum possible distances. (ëª¨ë“  ë‹¤ë¥¸ nodeê¹Œì§€ 1ë¡œ ê°€ëŠ” ê²Œ minimumì´ë‹ˆê¹Œ) | ë¶„ëª¨(íŠ¹ì • nodeì—ì„œ ë‹¤ë¥¸ ëª¨ë“  nodeê¹Œì§€ì˜ shortest path distanceì˜ í•©)ê°€ ì‘ì„ìˆ˜ë¡ centralí•œ ê²ƒì´ë¯€ë¡œ, closeness centraliy ê°’ì€ ë†’ì„ìˆ˜ë¡ ë” ì¤‘ìš”ë„ê°€ ë†’ì€ ê²ƒ! | . # ê° nodeë³„ closeness centrality ê°’ì„ dictionary í˜•íƒœë¡œ ë³´ì—¬ì¤Œ nx.closeness_centrality(g) . {1: 0.38461538461538464, 2: 0.625, 3: 0.5555555555555556, 4: 0.7142857142857143, 5: 0.45454545454545453, 6: 0.625} . â†’ 4ê°€ ê°€ì¥ ì¤‘ìš”í•œ nodeë¡œ íŒë³„ë¨ . | Eigenvector centrality: ì–¼ë§ˆë‚˜ centralí•œ nodeë“¤ê³¼ ì—°ê²°ë˜ì–´ ìˆë‚˜ . | íŠ¹ì • nodeì˜ neighborë“¤ì˜ centralityë¥¼ ê³ ë ¤ | ex) ë‹¨ìˆœíˆ ë§ì€ followerê°€ ìˆëŠ” ì‚¬ëŒë³´ë‹¤ ë§ì€ followerê°€ ìˆëŠ” ì‚¬ëŒë“¤ì— ì˜í•´ ë§ì´ followë˜ëŠ” ì‚¬ëŒì„ ë” ì¤‘ìš”í•œ influencerë¼ê³  ê°„ì£¼ | Ax = &lambda;x â†’ eigenvector xì˜ në²ˆì§¸ ê°’ì´ në²ˆì§¸ nodeì˜ eigenvector centrality. | A: adjacency matrix of the graph, Î»: eigenvalue, x: eigenvector | . # ê° nodeë³„ eigenvector centrality ê°’ì„ dictionary í˜•íƒœë¡œ ë³´ì—¬ì¤Œ nx.eigenvector_centrality(g) . {1: 0.07902199743319213, 2: 0.5299719499101774, 3: 0.20983546432528058, 4: 0.47818048045123035, 5: 0.39915848301808887, 6: 0.5299719499101774} . â†’ 2, 6ì´ ê°€ì¥ ì¤‘ìš”í•œ nodeë¡œ íŒë³„ë¨ . +) nx.eigenvector_centrality(g, weight='weight')ë¼ê³  weight ì˜µì…˜ì„ ì ì–´ì£¼ë©´, ê° edgeì˜ weightë¥¼ ê³ ë ¤í•´ì„œ centralityë¥¼ ê³„ì‚° . | . Cliques . : completely connected network . # ì˜ˆì‹œë¡œ barbell graphë¥¼ ë§Œë“¤ì–´ ì‚¬ìš© barbell_g = nx.barbell_graph(m1=5, m2=1) nx.draw_networkx(barbell_g) plt.axis('off') # turn off axis plt.show() . +) networkXë¡œ ë§Œë“¤ ìˆ˜ ìˆëŠ” ê¸°ë³¸ ëª¨ì–‘ë“¤: https://networkx.org/documentation/stable//reference/generators.html . | triangles: 3ê°œì˜ nodeê°€ ëª¨ë‘ ì„œë¡œ ì—°ê²°ëœ ëª¨ì–‘ . | simplest complex clique: a triangle. | ì¹œêµ¬ ì¶”ì²œ ì‹œìŠ¤í…œì—ì„œ triangle ê°œë… í™œìš© ê°€ëŠ¥: ex) Aì™€ Bê°€ ì¹œêµ¬ì´ê³  Aì™€ Cê°€ ì¹œêµ¬ â†’ Aì™€ Cë„ ì„œë¡œ ì•Œ ê°€ëŠ¥ì„±ì´ ë†’ìŒ | . ## ê°ê°ì˜ nodeê°€ ëª‡ ê°œì˜ triangleì— ì†í•´ ìˆëŠ”ì§€ ì¶œë ¥ nx.triangles(barbell_g) . {0: 6, 1: 6, 2: 6, 3: 6, 4: 6, 5: 0, 6: 6, 7: 6, 8: 6, 9: 6, 10: 6} . ## barbell_g ê·¸ë˜í”„ì˜ 6 nodeê°€ ëª‡ ê°œì˜ triangleì— ì†í•´ ìˆëŠ”ì§€ë§Œ ì¶œë ¥ nx.triangles(barbell_g, 6) . 6 . | Maximal Cliques: ì°¾ì•„ì§€ëŠ” ìµœëŒ€ í¬ê¸°ì˜ clique. | a clique that, when extended by one more node is no longer a clique. | ë„¤íŠ¸ì›Œí¬ ì•ˆì˜ íŠ¹ì • communityë¥¼ ì°¾ëŠ” ë°ì— ì‘ìš©ë  ìˆ˜ ìˆë‹¤: Cliques form a good starting point for finding communities, as they are fully connected subgraphs within a larger graph. | . # ê°ê° maximal cliqueì„ ì´ë£¨ê³  ìˆëŠ” nodeë“¤ì˜ listë¥¼ ì¶œë ¥ list(nx.find_cliques(barbell_g)) . [[4, 0, 1, 2, 3], [4, 5], [6, 5], [6, 7, 8, 9, 10]] . ## íŠ¹ì • node 6ì´ ì†í•œ cliqueë“¤ì„ ëª¨ë‘ ì¶œë ¥ nx.cliques_containing_node(barbell_g, 6) . [[6, 5], [6, 7, 8, 9, 10]] . | . Subgraphs . : í° groupì—ì„œ ì¼ë¶€ë¥¼ ë–¼ì–´ì„œ Subgraphë¡œ ê·¸ë ¤ë³´ëŠ” ê²ƒì´ ìœ ìš©í•  ë•Œê°€ ìˆë‹¤ (íŠ¹ì • node ì‚¬ì´ì˜ path, communties / cliques, degree of seperation ë“±ì„ íŒŒì•…í•˜ê¸° ìš©ì´) . # ì˜ˆì‹œë¡œ ErdÅ‘s-RÃ©nyi graphë¥¼ ë§Œë“¤ì–´ ì‚¬ìš© G = nx.erdos_renyi_graph(n=20, p=0.2) plt.axis('off') # turn off axis plt.show() . | node 8ê³¼ ì´ì™€ ì—°ê²°ëœ neighbor ë…¸ë“œë“¤ì„ ì¶”ì¶œ nodes = list(G.neighbors(8)) nodes.append(8) nodes . [2, 4, 15, 16, 8] . | node 8ê³¼ ì´ì™€ ì—°ê²°ëœ nodeë“¤ë¡œ subgraphë¥¼ êµ¬ì„± G_eight = G.subgraph(nodes) # subgraphë¥¼ êµ¬ì„±í•  nodeì˜ listë¥¼ G.subgraph() í•¨ìˆ˜ì— ë„£ì–´ì¤€ë‹¤ G_eight.edges() # ë„£ì–´ì¤€ list ì† nodeë“¤ ì‚¬ì´ì˜ edgeê°€ ë°˜ì˜ë¨ . EdgeView([(2, 4), (2, 8), (4, 8), (8, 15), (8, 16)]) . â†’ subgraph ì‹œê°í™” . nx.draw_networkx(G_eight) plt.axis('off') # turn off axis plt.show() . | ì´ë ‡ê²Œ íŠ¹ì • nodeì™€ ì´ì— ì—°ê²°ëœ nodeë“¤ë§Œìœ¼ë¡œ êµ¬ì„±í•œ networkë¥¼ â€˜Ego Networkâ€™ë¼ê³  í•œë‹¤ | . *ì „ì²´ ë„¤íŠ¸ì›Œí¬ì™€ Ego Network . | ì „ì²´ ë„¤íŠ¸ì›Œí¬(Whole Network): ë„¤íŠ¸ì›Œí¬ë¥¼ êµ¬ì„±í•˜ê³  ìˆëŠ” ëª¨ë“  nodeì™€ ê·¸ ì‚¬ì´ì˜ ê´€ê³„ë¥¼ ëª¨ë‘ í¬í•¨ (ex. ì¤‘ì„¸ í”Œë¡œë‘ìŠ¤ ì§€ì—­ ì£¼ìš” ê°€ë¬¸ë“¤ ê°„ì˜ í˜¼ì¸ ê´€ê³„ ë„¤íŠ¸ì›Œí¬) | ego network: íŠ¹ì • nodeì˜ personal networkë¥¼ ì˜ë¯¸ (ex. Medici ê°€ë¬¸ì˜ ego network: Medici ê°€ë¬¸ê³¼ ì—°ê²°ëœ tieë§Œ í‘œí˜„) | . | íŠ¹ì • graphì™€ ì´ì˜ subgraphëŠ” ë™ì¼í•œ typeì„ ê°–ê²Œ ë¨. print(type(G)) print(type(G_eight)) . &lt;class 'networkx.classes.graph.Graph'&gt; &lt;class 'networkx.classes.graph.Graph'&gt; . | . ",
    "url": "https://chaelist.github.io/docs/network_analysis/network_basics/#network-%EA%B5%AC%EC%A1%B0-%ED%8C%8C%EC%95%85%ED%95%98%EA%B8%B0",
    "relUrl": "/docs/network_analysis/network_basics/#network-êµ¬ì¡°-íŒŒì•…í•˜ê¸°"
  },"127": {
    "doc": "Network Analysis ê¸°ì´ˆ",
    "title": "Visualization with nxviz",
    "content": "*nxvis: a graph visualization package for NetworkX . # ë¨¼ì € ì„¤ì¹˜í•´ì¤˜ì•¼ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤ $ conda install -c conda-forge nxviz ## conda installì´ ì¶”ì²œë˜ì§€ë§Œ, $ pip install nxvizë¡œ ì„¤ì¹˜í•´ë„ ê´œì°®ìŒ . nxviz example 1 . : ê°€ì¥ ìœ„ì—ì„œ ë§Œë“¤ì–´ë’€ë˜ â€˜gâ€™ ë„¤íŠ¸ì›Œí¬ë¥¼ ì‚¬ìš© . color_map = [] for n, d in g.nodes(data=True): if d['gender'] == 'female': color_map.append('pink') # ì—¬ì„±: pink else: color_map.append('skyblue') # ë‚¨ì„±: skyblue pos=nx.spring_layout(g) nx.draw_networkx(g, pos, node_color=color_map) labels = nx.get_edge_attributes(g,'weight') nx.draw_networkx_edge_labels(g, pos,edge_labels=labels) plt.axis('off') # turn off axis plt.show() . | Arc Plot: í•œ ì¤„ë¡œ ëŠ˜ì–´ì„  node ê°„ì˜ ì—°ê²° ê´€ê³„ë¥¼ ë°˜ì› ëª¨ì–‘ ë¼ì¸ìœ¼ë¡œ í‘œí˜„ . import nxviz as nv # importí•´ì„œ ì‚¬ìš© import matplotlib.pyplot as plt ap = nv.ArcPlot(g, node_color='gender', # ì„±ë³„ì— ë”°ë¼ ìƒ‰ì„ ë‹¤ë¥´ê²Œ í‘œí˜„ edge_width='weight') # edgeì˜ weightì— ë”°ë¼ ì„ ì˜ êµµê¸°ë¥¼ ë‹¤ë¥´ê²Œ í‘œí˜„ ap.draw() . | Circos Plot: Arc Plotì˜ ì–‘ ëì„ circleë¡œ ê²°í•©í•œ ëª¨ì–‘. ë™ê·¸ë¼ë¯¸ í˜•íƒœë¡œ nodeë“¤ì„ ê·¸ë ¤ì£¼ê³ , ì„œë¡œì˜ ì—°ê²°ê´€ê³„ê°€ ê³¡ì„ ìœ¼ë¡œ í‘œí˜„ë¨. c = nv.CircosPlot(g, node_labels=True, # node labelì„ í•¨ê»˜ ë³´ì—¬ì¤Œ node_color='gender', # ì„±ë³„ì— ë”°ë¼ ìƒ‰ì„ ë‹¤ë¥´ê²Œ í‘œí˜„ node_order='gender') # ì„±ë³„ì— ë”°ë¼ ë¬¶ì–´ì„œ ë³´ì—¬ì¤Œ (ìˆœì„œë¥¼ ì¡°ì •) c.draw() . | Matrix Plot: ê° nodeê°€ í–‰/ì—´ì— ë“¤ì–´ê°€ê³ , ì„œë¡œ ì—°ê²°ì´ ìˆëŠ” nodeë¼ë©´ í•´ë‹¹ cellì´ ì§„í•˜ê²Œ í‘œí˜„ë¨ . | weightì— ë”°ë¼ ìƒ‰ì˜ ê°•ë„ê°€ ë‹¤ë¥´ê²Œ í‘œí˜„ë¨ | Directed Networkë¼ë©´, Aâ†’B ì—°ê²°ì€ í–‰:A, ì—´:Bì— í•´ë‹¹ë˜ëŠ” ìœ„ì¹˜ì— ì¹ í•´ì„œ í‘œí˜„ | . m = nv.MatrixPlot(g) m.draw() . | . nxviz example 2: complex network . | networkì— nodeì™€ edgeê°€ ë§ìœ¼ë©´ ë§ì„ìˆ˜ë¡, ì‹œê°í™”í•˜ë©´ hairball ê°™ì€ í˜•íƒœê°€ ë˜ì–´ ì•Œì•„ë³´ê¸° ì–´ë µê¸° ë•Œë¬¸ì—, ë³µì¡í•œ networkì¼ìˆ˜ë¡ nxvizë¡œ ì‹œê°í™”í•˜ë©´ ë” ê¹”ë”í•˜ê²Œ ë³´ì—¬ì¤„ ìˆ˜ ìˆë‹¤. | . # ì˜ˆì‹œë¡œ nodeê°€ 30ê°œì¸ ErdÅ‘s-RÃ©nyi graphë¥¼ ë§Œë“¤ì–´ ì‚¬ìš© . from random import choice G = nx.erdos_renyi_graph(n=30, p=0.2) for n, d in G.nodes(data=True): G.nodes[n][\"class\"] = choice([\"one\", \"two\", \"three\"]) . â†’ í‰ë²”í•œ ì‹œê°í™” . color_map = [] for n, d in G.nodes(data=True): if d['class'] == 'one': color_map.append('pink') elif d['class'] == 'two': color_map.append('skyblue') else: color_map.append('lightgrey') nx.draw_networkx(G, node_color=color_map) plt.axis('off') # turn off axis plt.show() . | Arc Plot ap = nv.ArcPlot(G, node_color=\"class\", node_order='class') ap.draw() . | Circos Plot c = nv.CircosPlot(G, node_labels=True, node_color=\"class\", node_order=\"class\") c.draw() . | Matrix Plot . # ì˜ˆì‹œë¡œ Lollipop Graphë¥¼ ê·¸ë ¤ì„œ ì‚¬ìš© . import numpy.random as npr lp_G = nx.lollipop_graph(m=10, n=4) nx.draw_networkx(lp_G) plt.axis('off') # turn off axis plt.show() . â†’ Matrix Plotìœ¼ë¡œ í‘œí˜„ . m = nv.MatrixPlot(G) m.cmap = plt.cm.get_cmap(\"Greens\") # colormapì„ Greenìœ¼ë¡œ ì§€ì • m.draw() . | . ",
    "url": "https://chaelist.github.io/docs/network_analysis/network_basics/#visualization-with-nxviz",
    "relUrl": "/docs/network_analysis/network_basics/#visualization-with-nxviz"
  },"128": {
    "doc": "ë‰´ìŠ¤ ê¸°ì‚¬ Clustering",
    "title": "ë‰´ìŠ¤ ê¸°ì‚¬ Clustering",
    "content": ". | ë‰´ìŠ¤ ê¸°ì‚¬ Clustering . | ë‰´ìŠ¤ ê¸°ì‚¬ ì¤€ë¹„ | tokenize &amp; ëª…ì‚¬ë§Œ ì €ì¥ | Text Vectorí™” &amp; í•™ìŠµ | í•™ìŠµ ê²°ê³¼ í™•ì¸ | . | . ",
    "url": "https://chaelist.github.io/docs/ml_application/news_clustering/",
    "relUrl": "/docs/ml_application/news_clustering/"
  },"129": {
    "doc": "ë‰´ìŠ¤ ê¸°ì‚¬ Clustering",
    "title": "ë‰´ìŠ¤ ê¸°ì‚¬ Clustering",
    "content": ". | ë‰´ìŠ¤ ê¸°ì‚¬ì˜ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ìœ ì‚¬í•œ ê¸°ì‚¬ë¼ë¦¬ ë¬¶ì–´ë³´ë ¤ê³  í•¨ | . # ì‚¬ìš©í•  libraryë¥¼ ë¨¼ì € ëª¨ë‘ import import pandas as pd import konlpy from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.cluster import AgglomerativeClustering . ë‰´ìŠ¤ ê¸°ì‚¬ ì¤€ë¹„ . | ìš°ì„ , Clusteringí•  ê¸°ì‚¬ ë°ì´í„°ë¥¼ ìˆ˜ì§‘ | ë‹¤ìŒ ë‰´ìŠ¤ì—ì„œ, â€˜êµ­íšŒ/ì •ë‹¹â€™, â€˜ê¸ˆìœµâ€™, â€˜ê±´ê°•â€™, â€˜ì‚¬ê±´/ì‚¬ê³ â€™ 4ê°œ í† í”½ì˜ ê¸°ì‚¬ë¥¼ ì¡°ê¸ˆì”© ìˆ˜ì§‘í•´ ì˜´ | . ## 'ë‹¤ìŒ ë‰´ìŠ¤'ì—ì„œ ì§ì ‘ ìˆ˜ì§‘í•´ ì˜¨ ë°ì´í„°ë¥¼ dataframeìœ¼ë¡œ ì •ë¦¬í•´ ë‘  news_df.head() . | Â  | topic | article_title | news_text | . | 0 | êµ­íšŒ/ì •ë‹¹ | [í¬í† ]ì¬ë³´ì„  D-1, â€˜ì‹œë¯¼ë“¤ì˜ ì„ íƒì€?â€™ | [êµ­íšŒì‚¬ì§„ì·¨ì¬ë‹¨] 6ì¼ ì˜¤í›„ ì„œëŒ€ë¬¸êµ¬ í™ì œì—­ì—ì„œ ì—´ë¦° ë°•ì˜ì„  ë”ë¶ˆì–´ë¯¼ì£¼ë‹¹ ì„œìš¸ì‹œì¥â€¦ | . | 1 | êµ­íšŒ/ì •ë‹¹ | \"ì•½ì† ì§€í‚¨ë‹¤\"..ê¹€ì¢…ì¸, 8ì¼ êµ­ë¯¼ì˜í˜ ë– ë‚œë‹¤ | ê¹€ì¢…ì¸ êµ­ë¯¼ì˜í˜ ë¹„ìƒëŒ€ì±…ìœ„ì›ì¥ì´ 5ì¼ ì˜¤í›„ ì„œìš¸ ê´€ì•…êµ¬ ì„œìš¸ëŒ€ì…êµ¬ì—­ì—ì„œ ì˜¤ì„¸í›ˆ ì„œìš¸â€¦ | . | 2 | êµ­íšŒ/ì •ë‹¹ | \"ìœ„ì„ Â·ë¬´ëŠ¥ ì •ë¶€ ì‹¬íŒí•´ì•¼\"..ë¹¨ê°• ìš´ë™í™”ì— â€˜ê³¨ëª©â€™ ì°¾ì€ ì˜¤ì„¸í›ˆ | ì˜¤ì„¸í›ˆ êµ­ë¯¼ì˜í˜ ì„œìš¸ì‹œì¥ í›„ë³´ê°€ 6ì¼ ì„œìš¸ ì„±ë¶êµ¬ ì •ë¦‰ì‹œì¥ì—ì„œ ë§Œë‚œ í•œ ì‹œë¯¼ì—ê²Œ í—ˆâ€¦ | . | 3 | êµ­íšŒ/ì •ë‹¹ | ê¹€ì¢…ì¸, ê°•ë‚¨ ëŒ€ì¹˜ì—­ ì°¾ì•„ ì˜¤ì„¸í›ˆ í›„ë³´ ì§€ì§€í˜¸ì†Œ | (ì„œìš¸=ë‰´ìŠ¤1) êµ­íšŒì‚¬ì§„ì·¨ì¬ë‹¨ = ê¹€ì¢…ì¸ êµ­ë¯¼ì˜í˜ ë¹„ìƒëŒ€ì±…ìœ„ì›ì¥ì´ 6ì¼ ì˜¤í›„ ì„œìš¸â€¦ | . | 4 | êµ­íšŒ/ì •ë‹¹ | ì´ë‚™ì—° \"íŠ¹ê¶Œì¸µ ë“ì„¸í•˜ê³  ì°¨ë³„í•˜ëŠ” ì„œìš¸ë¡œ í‡´ë³´í•  í…ê°€\" | [ì´ë°ì¼ë¦¬ ì´ì •í˜„ ê¸°ì] ì´ë‚™ì—° ë”ë¶ˆì–´ë¯¼ì£¼ë‹¹ ìƒì„ì„ ëŒ€ìœ„ì›ì¥ì´ 6ì¼ ì˜¤ì„¸í›ˆ êµ­ë¯¼ì˜í˜.. | . Â  . news_df.groupby('topic')[['article_title']].count() . | topic | article_title | . | ê±´ê°• | 16 | . | êµ­íšŒ/ì •ë‹¹ | 13 | . | ê¸ˆìœµ | 15 | . | ì‚¬ê±´/ì‚¬ê³  | 15 | . tokenize &amp; ëª…ì‚¬ë§Œ ì €ì¥ . | ê¸°ì‚¬ì˜ ì£¼ì œë¥¼ íŒë‹¨í•˜ëŠ” ë°ì— ëª…ì‚¬ê°€ ê°€ì¥ ì¤‘ìš”í•˜ë‹¤ê³  ìƒê°í•´ ëª…ì‚¬ë§Œ ì‚¬ìš© | . documents_processed = [] for text in news_df['news_text']: okt = konlpy.tag.Okt() okt_pos = okt.pos(text) words = [] for word, pos in okt_pos: if 'Noun' in pos: words.append(word) documents_processed.append(' '.join(words)) print(len(documents_processed)) documents_processed[0] . 59 'êµ­íšŒ ì§„ì·¨ ì¬ë‹¨ ì˜¤í›„ ì„œëŒ€ë¬¸êµ¬ í™ì œì—­ ë°•ì˜ì„  ë¯¼ì£¼ë‹¹ ì„œìš¸ì‹œì¥ í›„ë³´ ì§‘ì¤‘ ìœ ì„¸ ì‹œë¯¼ íœ´ëŒ€í° ê´€ì‹¬ í‘œëª… ë…¸ì§„í™˜' . Text Vectorí™” &amp; í•™ìŠµ . tfidf_vectorizer = TfidfVectorizer(min_df=1, ngram_range=(1,1)) tfidf_vector = tfidf_vectorizer.fit_transform(documents_processed) tfidf_dense = tfidf_vector.todense() ## sparse arrayë¡œëŠ” Agg.Clustering í•™ìŠµì´ ì•ˆë˜ì–´ì„œ, dense arrayë¡œ ë°”ê¿”ì¤˜ì•¼ í•¨ . *TfidfVectorizer: ê° ë‹¨ì–´ì˜ Term Frequency - Inverse Document Frequencyë¥¼ ê¸°ë°˜ìœ¼ë¡œ textë¥¼ vectorí™”. | ngram_range: (1, 1) means only unigrams, (1, 2) means unigrams and bigrams, and (2, 2) means only bigrams. (default: (1,1)) | min_df: ignore terms that have a document frequency strictly lower than the given threshold. (default: 1) | . â†’ AgglomerativeClusteringìœ¼ë¡œ í•™ìŠµ . model = AgglomerativeClustering(linkage='complete', affinity='cosine', n_clusters=4) model.fit(tfidf_dense) model.labels_ . array([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 2, 0, 1, 2, 0, 2, 2, 2, 0, 2, 0, 2, 0, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 3], dtype=int64) . í•™ìŠµ ê²°ê³¼ í™•ì¸ . | ì›ë˜ daumì—ì„œ ë¶„ë¥˜ë˜ì–´ ìˆë˜ ì£¼ì œì™€ ë¹„êµí•´ì„œ, ìœ ì‚¬í•œ ê¸°ì‚¬ë¼ë¦¬ ì˜ ë¬¶ì˜€ë‚˜ í™•ì¸ | . # topic, article_titleì— clusterë¥¼ ë¶™ì—¬ì„œ í™•ì¸ cluster_df = pd.DataFrame(zip(news_df['topic'], news_df['article_title'], model.labels_), columns=['topic', 'title', 'cluster']) cluster_df.head() . | Â  | topic | title | cluster | . | 0 | êµ­íšŒ/ì •ë‹¹ | [í¬í† ]ì¬ë³´ì„  D-1, â€˜ì‹œë¯¼ë“¤ì˜ ì„ íƒì€?â€™ | 3 | . | 1 | êµ­íšŒ/ì •ë‹¹ | \"ì•½ì† ì§€í‚¨ë‹¤\"..ê¹€ì¢…ì¸, 8ì¼ êµ­ë¯¼ì˜í˜ ë– ë‚œë‹¤ | 3 | . | 2 | êµ­íšŒ/ì •ë‹¹ | \"ìœ„ì„ Â·ë¬´ëŠ¥ ì •ë¶€ ì‹¬íŒí•´ì•¼\"..ë¹¨ê°• ìš´ë™í™”ì— â€˜ê³¨ëª©â€™ ì°¾ì€ ì˜¤ì„¸í›ˆ | 3 | . | 3 | êµ­íšŒ/ì •ë‹¹ | ê¹€ì¢…ì¸, ê°•ë‚¨ ëŒ€ì¹˜ì—­ ì°¾ì•„ ì˜¤ì„¸í›ˆ í›„ë³´ ì§€ì§€í˜¸ì†Œ | 3 | . | 4 | êµ­íšŒ/ì •ë‹¹ | ì´ë‚™ì—° \"íŠ¹ê¶Œì¸µ ë“ì„¸í•˜ê³  ì°¨ë³„í•˜ëŠ” ì„œìš¸ë¡œ í‡´ë³´í•  í…ê°€\" | 3 | . â†’ ê° topicë³„ cluster ë°°ì • í˜„í™©ì„ í™•ì¸: . cluster_df.groupby(['topic', 'cluster'])[['cluster']].count() . | topic | cluster | count | . | ê±´ê°• | 0 | 5 | . | ^^ | 1 | 1 | . | ^^ | 2 | 10 | . | êµ­íšŒ/ì •ë‹¹ | 1 | 2 | . | ^^ | 3 | 11 | . | ê¸ˆìœµ | 0 | 2 | . | ^^ | 1 | 13 | . | ì‚¬ê±´/ì‚¬ê³  | 0 | 13 | . | ^^ | 3 | 2 | . | ì£¼ë¡œ â€˜ê±´ê°•â€™: 2, â€˜êµ­íšŒ/ì •ë‹¹â€™: 3, â€˜ê¸ˆìœµâ€™: 1, â€˜ì‚¬ê±´/ì‚¬ê³ â€™: 0ìœ¼ë¡œ ë§ì´ ë°°ì •ë˜ì—ˆìŒì„ í™•ì¸ | . â†’ daumì˜ ë¶„ë¥˜ì™€ ë‹¤ë¥¸ ë¶€ë¶„ì„ í™•ì¸: . cluster_df[cluster_df['topic'] == 'êµ­íšŒ/ì •ë‹¹'] . | Â  | topic | title | cluster | . | 0 | êµ­íšŒ/ì •ë‹¹ | [í¬í† ]ì¬ë³´ì„  D-1, â€˜ì‹œë¯¼ë“¤ì˜ ì„ íƒì€?â€™ | 3 | . | 1 | êµ­íšŒ/ì •ë‹¹ | \"ì•½ì† ì§€í‚¨ë‹¤\"..ê¹€ì¢…ì¸, 8ì¼ êµ­ë¯¼ì˜í˜ ë– ë‚œë‹¤ | 3 | . | 2 | êµ­íšŒ/ì •ë‹¹ | \"ìœ„ì„ Â·ë¬´ëŠ¥ ì •ë¶€ ì‹¬íŒí•´ì•¼\"..ë¹¨ê°• ìš´ë™í™”ì— â€˜ê³¨ëª©â€™ ì°¾ì€ ì˜¤ì„¸í›ˆ | 3 | . | 3 | êµ­íšŒ/ì •ë‹¹ | ê¹€ì¢…ì¸, ê°•ë‚¨ ëŒ€ì¹˜ì—­ ì°¾ì•„ ì˜¤ì„¸í›ˆ í›„ë³´ ì§€ì§€í˜¸ì†Œ | 3 | . | 4 | êµ­íšŒ/ì •ë‹¹ | ì´ë‚™ì—° \"íŠ¹ê¶Œì¸µ ë“ì„¸í•˜ê³  ì°¨ë³„í•˜ëŠ” ì„œìš¸ë¡œ í‡´ë³´í•  í…ê°€\" | 3 | . | 5 | êµ­íšŒ/ì •ë‹¹ | ì‹œë¯¼ë“¤ê³¼ ê¸°ë…ì‚¬ì§„ ì°ëŠ” ê¹€ì¢…ì¸ ë¹„ëŒ€ìœ„ì›ì¥ | 3 | . | 6 | êµ­íšŒ/ì •ë‹¹ | ê°•ë‚¨ ì°¾ì•„ ì˜¤ì„¸í›ˆ ì§€ì§€í˜¸ì†Œí•˜ëŠ” ê¹€ì¢…ì¸ | 3 | . | 7 | êµ­íšŒ/ì •ë‹¹ | ê¹€ì¢…ì¸, ì˜¤ì„¸í›ˆ í›„ë³´ ì§€ì›ìœ ì„¸ | 3 | . | 8 | êµ­íšŒ/ì •ë‹¹ | ì§€ì›ìœ ì„¸ ë§ˆì¹˜ê³  ì°¨ëŸ‰ì— ì˜¤ë¥¸ ê¹€ì¢…ì¸ | 3 | . | 9 | êµ­íšŒ/ì •ë‹¹ | ì˜¤ì„¸í›ˆ í›„ë³´ ì§€ì›ìœ ì„¸í•˜ëŠ” ê¹€ì¢…ì¸ | 3 | . | 10 | êµ­íšŒ/ì •ë‹¹ | ë‚´ ì„ ê±°â€™ì²˜ëŸ¼ ë›´ ì•ˆì² ìˆ˜ëŠ” êµ­ë¯¼ì˜í˜ì—ì„œ ë¬´ì—‡ì´ ë ê¹Œ | 3 | . | 11 | êµ­íšŒ/ì •ë‹¹ | ì›¹íˆ°ì—ì„œë„ ë¶€ë”ªíˆëŠ” ë„¤ì´ë²„ vs ì¹´ì¹´ì˜¤..â€™ì›ì¡° êµ­ë°¥ì§‘â€™ ë…¼ë€ë„? | 1 | . | 12 | êµ­íšŒ/ì •ë‹¹ | í† ìŠ¤, ë§¤ì¶œ 230% ê¸‰ì¦..\"ì¶œë²” í›„ ì²˜ìŒìœ¼ë¡œ ë§¤ì¶œ ì´ìµ ë™ì‹œ ê°œì„ \" | 1 | . | topic ê°„ ê²½ê³„ê°€ ëª¨í˜¸í•œ ê¸°ì‚¬ì˜ ê²½ìš° daumì—ì„œ ë°°ì •í•œ topicê³¼ ë‹¤ë¥¸ clusterë¡œ ë¬¶ì¸ ê²ƒâ€¦ | . ",
    "url": "https://chaelist.github.io/docs/ml_application/news_clustering/",
    "relUrl": "/docs/ml_application/news_clustering/"
  },"130": {
    "doc": "Numbers, List, String",
    "title": "Numbers, List, String",
    "content": ". | Numbers . | type() í•¨ìˆ˜ | ì‚°ìˆ  ì—°ì‚°ì (Arithmetic Operators) | . | List . | Indexing | Slicing | List ë³€ê²½í•˜ê¸° | Main List Fuctions | Other Common List Operators / Functions | . | String . | String ê¸°ì´ˆ | Main String Fuctions | String - Number Conversion | ë¬¸ìì—´ í¬ë§·íŒ… (string formatting) | . | . ",
    "url": "https://chaelist.github.io/docs/python_basics/numbers_list_string/",
    "relUrl": "/docs/python_basics/numbers_list_string/"
  },"131": {
    "doc": "Numbers, List, String",
    "title": "Numbers",
    "content": ". | Integer(ì •ìˆ˜) - ex. -2, 0, 1 | Float(ì†Œìˆ˜) - ex. 1.1, 3.14 | Bolean - True or False | . type() í•¨ìˆ˜ . : ê°ê° variableì˜ typeì„ í™•ì¸í•˜ëŠ” ë°©ë²• . # type() í•¨ìˆ˜ë¡œ ê°ê°ì˜ typeì„ ì²´í¬ a = 1.1 b = 5 c = True print('a:', type(a)) print('b:', type(b)) print('c:', type(c)) . a: &lt;class 'float'&gt; b: &lt;class 'int'&gt; c: &lt;class 'bool'&gt; . ì‚°ìˆ  ì—°ì‚°ì (Arithmetic Operators) . # ì—°ì‚° ì˜ˆì‹œ a = 5 b = 3 print(a + b) # addition print(a - b) # subtraction print(a * b) # multiplication print(a / b) # division (ë‚˜ëˆ„ì–´ì§„ ê²°ê³¼ë¥¼ ì†Œìˆ˜ë¡œ í‘œì‹œ) print(a // b) # quotient. ë‚˜ëˆˆ ëª«ì„ ë°˜í™˜ (ex. 5 // 3 = 1) print(a % b) # modulus. ë‚˜ë¨¸ì§€ ê°’ì„ ë°˜í™˜. (ex. 5 % 3 = 2) print(a ** b) # exponentiation (ì§€ìˆ˜. ì œê³±) (ex. 3**5ëŠ” 3ì˜ 5ì œê³±ì„ ì˜ë¯¸) . 8 2 15 1.6666666666666667 1 2 125 . ",
    "url": "https://chaelist.github.io/docs/python_basics/numbers_list_string/#numbers",
    "relUrl": "/docs/python_basics/numbers_list_string/#numbers"
  },"132": {
    "doc": "Numbers, List, String",
    "title": "List",
    "content": "a = [â€˜pythonâ€™, 1, 5] ì™€ ê°™ì´, [ ]ë¡œ í‘œí˜„ë¨ . # ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” ë°©ë²• a = [] b = list() print(a, b) # ë‘ ë°©ë²• ëª¨ë‘ ë™ì¼ . [] [] . Indexing . index numberë¥¼ ì´ìš©í•´ ê° elementì— ì ‘ê·¼í•  ìˆ˜ ìˆë‹¤. | index numberëŠ” 0ë¶€í„° ì‹œì‘ . | ì²«ë²ˆì§¸ ìš”ì†Œì˜ index number: 0 | ë‘ë²ˆì§¸ ìš”ì†Œì˜ index number: 1 | . | ì—­ìˆœ indexingë„ ê°€ëŠ¥ . | ë§ˆì§€ë§‰ ìš”ì†Œì˜ index number: -1 | ë§ˆì§€ë§‰ì—ì„œ ë‘ë²ˆì§¸ ìš”ì†Œì˜ index number: -2 | . | [ ]ë¥¼ ì‚¬ìš©í•´ indexing . | a[0]: aì˜ ì²«ë²ˆì§¸ ìš”ì†Œì˜ ê°’ì„ ë°˜í™˜ | . | . # Indexing a = ['python', 1, 5] print(a[0]) # ì²«ë²ˆì§¸ elementë¥¼ ì˜ë¯¸ print(a[2]) # ì„¸ë²ˆì§¸ elementë¥¼ ì˜ë¯¸ print(a[-1]) # ë§ˆì§€ë§‰ elementë¥¼ ì˜ë¯¸ (ì—­ìˆœ indexing) . python 5 5 . Slicing . list_name[index1:index2]ì™€ ê°™ì€ ë°©ì‹ìœ¼ë¡œ, íŠ¹ì • êµ¬ê°„ì˜ indexë“¤ì— ëª¨ë‘ ì ‘ê·¼ . | index1 &lt;= index &lt; index2 (index1 ì´ìƒ, index2 ë¯¸ë§Œ) | index numberê°€ index1ë³´ë‹¤ í¬ê±°ë‚˜ ê°™ê³  index2ë³´ë‹¤ ì‘ì€ ëª¨ë“  ìš”ì†Œë¥¼ ë°˜í™˜ | . # Slicing a = ['python', 1, 5] print(a[0:2]) # index0, index1 (index2ëŠ” ë¯¸í¬í•¨) print(a[1:]) # index1ì´ìƒ ~ ëê¹Œì§€ print(a[:2]) # ì²˜ìŒ ~ index1ê¹Œì§€ print(a[:]) # ê·¸ëƒ¥ a itself (ì²˜ìŒ ~ ë) . ['python', 1] [1, 5] ['python', 1] ['python', 1, 5] . +) list_name[index1:index2:step] . | index1 ì´ìƒ, index2 ë¯¸ë§Œì˜ ìš”ì†Œ ì¤‘, 1ë²ˆì§¸, 1+stepë²ˆì§¸, 1+2stepë²ˆì§¸,â€¦ì˜ ìš”ì†Œë¥¼ ë°˜í™˜ | . numbers = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] numbers[1:5:2] # index numberê°€ 1, 3ì¸ ìš”ì†Œ ë°˜í™˜ print(numbers[:6:3]) # index numberê°€ 0, 3ì¸ ìš”ì†Œ ë°˜í™˜ print(numbers[::2]) # index numberê°€ 0, 2, 4, 6, 8ì¸ ìš”ì†Œ ë°˜í™˜ . [2, 4] [1, 4] [1, 3, 5, 7, 9] . List ë³€ê²½í•˜ê¸° . List is mutable; List can be modified . # Modifying a list x = [1, 2, 3, 4] x[1] = 'python' print(x) # index 1ë²ˆ ìë¦¬ê°€ 'python'ìœ¼ë¡œ ë³€ê²½ë¨ . [1, 'python', 3, 4] . Main List Fuctions . | append(): í•œ ê°œì˜ element â€˜ì¶”ê°€â€™ # append x = [1, 2, 3, 4] x.append(5) x . [1, 2, 3, 4, 5] . | extend(): ìƒˆë¡œìš´ listë¥¼ ì¶”ê°€í•´ â€˜í™•ì¥â€™ # extend x = [1, 2, 3, 4] x.extend([6,7]) x . [1, 2, 3, 4, 6, 7] . cf) appendì— listë¥¼ ë„£ìœ¼ë©´? . x = [1, 2, 3, 4] x.append([6,7]) x # extendì™€ ë‹¤ë¥´ê²Œ, ì•„ì˜ˆ list ìì²´ê°€ í•˜ë‚˜ì˜ elementë¡œ ê°„ì£¼ë˜ì–´ ë“¤ì–´ê°. [1, 2, 3, 4, [6, 7]] . +) listë¼ë¦¬ +ë¡œ ë”í•´ì¤˜ë„ extendì˜ íš¨ê³¼: . x = [1, 2, 3, 4] x = x + [6,7] x . [1, 2, 3, 4, 6, 7] . | insert(): listì˜ â€˜íŠ¹ì • indexì—â€™ elementë¥¼ ì¶”ê°€ x = [1, 2, 3, 4] x.insert(2, 5) # index number = 2ì¸ ìœ„ì¹˜ì— 5ë¼ëŠ” elementë¥¼ ì¶”ê°€ x . [1, 2, 5, 3, 4] . | remove(): í•œ ê°œì˜ elementë¥¼ ì œê±° # remove x = [1, 2, 3, 4] x.remove(1) x . [2, 3, 4] . cf) ê°™ì€ ê°’ì˜ elementê°€ ë‘ ê°œì¸ ìƒí™©ì—ì„œ remove()ë¥¼ ì‚¬ìš©í•˜ë©´? . x = [1,2,3,4,1] x.remove(1) x ## ê°™ì€ ê°’ì´ ë‘ ê°œì¼ ê²½ìš°, ë” ì•ì— ìˆëŠ” elementë§Œ ì§€ì›Œì§„ë‹¤. [2, 3, 4, 1] . | index(): í•´ë‹¹ elementì˜ index numberë¥¼ ê°€ì ¸ì˜´ # index x = [1, 2, 3, 4] x.index(3) # 3ì€ 3ë²ˆì§¸ ê°’, ì¦‰ index numberê°€ 2ì¸ ê°’ì´ë¯€ë¡œ '2'ê°€ ë°˜í™˜ë¨ . 2 . cf) ê°™ì€ elementê°€ 2ê°œ ì´ìƒì¼ ë•Œ index() í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ë©´? . x = [1, 2, 3, 4, 3] x.index(3) # ë§¨ ì²˜ìŒ ë‚˜ì˜¤ëŠ” '3'ì˜ index numberì¸ 2ë§Œ return. 2 . | count(): í•´ë‹¹ elementì˜ ê°œìˆ˜ë¥¼ ì…ˆ # count x = [1, 2, 3, 4, 3] x.count(3) # 3ì´ ëª‡ ê°œì¸ì§€ ì„¸ê¸° . 2 . | . Other Common List Operators / Functions . | â€˜inâ€™ operator: íŠ¹ì • elementê°€ list ì•ˆì— ìˆëŠ”ì§€ ì—†ëŠ”ì§€, booleanê°’ì„ ë°˜í™˜ # 'in' x = [1, 2, 3, 4] print(1 in x) # True print(5 in x) # False . True False . | min(x), max(x): ìµœì†Œê°’, ìµœëŒ€ê°’ì„ ë°˜í™˜ # min, max x = [1, 2, 3, 4] print(min(x)) # ìµœì†Œê°’ print(max(x)) # ìµœëŒ€ê°’ . 1 4 . cf) string elementë“¤ë¡œ ì´ë£¨ì–´ì§„ listì˜ min(x), max(x) . y = ['c', 'b', 'a'] print(min(y)) print(max(y)) # stringì˜ ê²½ìš°ì—ë„, ê°ê° numberë¡œ ëœ unicodeë¥¼ ê°–ê¸°ì—, í•´ë‹¹ unicodeë¥¼ ë¹„êµí•´ì„œ max, minì„ returní•´ì¤Œ ## í•˜ì§€ë§Œ integarì™€ stringì²˜ëŸ¼ ì„œë¡œ ë‹¤ë¥¸ typeë“¤ì´ ì„ì¸ listëŠ” min, max ê³„ì‚° ë¶ˆê°€. a c . | sum(x): listì— ìˆëŠ” ëª¨ë“  ê°’ì„ ë‹¤ ë”í•´ì¤Œ (*ê°’ì´ ëª¨ë‘ integer í˜¹ì€ floatì—¬ì•¼ í•¨) # sum x = [1, 2, 3, 4] sum(x) . 10 . cf) integerì™€ floatì´ ê³µì¡´í•˜ëŠ” listë„ sum() ê°€ëŠ¥ . x = [1, 2, 3.5, 4] sum(x) . 10.5 . | len(x): listì˜ element ìˆ˜ë¥¼ ì•Œë ¤ì¤Œ # len(x) x = [1, 2, 3, 4] len(x) # elementê°€ 4ê°œë‹ˆê¹Œ 4 . 4 . | del x[index]: í•´ë‹¹ index numberë¥¼ ê°–ëŠ” elementë¥¼ ì‚­ì œ # del x[index] x = [1, 2, 3, 4] del x[0] # index numberê°€ 0ì¸ '1'ì„ ì‚­ì œ x . [2, 3, 4] . cf) slicingì„ í†µí•œ delë„ ê°€ëŠ¥ . x = [1, 2, 3, 4] del x[0:2] # index numberê°€ 0, 1ì¸ ê°’ì„ ëª¨ë‘ delete x . [3, 4] . *ì£¼ì˜: removeì™€ delì˜ ì°¨ì´ . x = [1, 2, 3, 4] x.remove(3) print('removeì˜ ê²°ê³¼:', x) # ì‹¤ì œ '3'ì´ë€ ê°’ì„ ê°–ëŠ” elementê°€ ì‚­ì œë˜ëŠ” ê²ƒ. x = [1, 2, 3, 4] del x[3] print('delì˜ ê²°ê³¼:', x) # index numberê°€ 3ì¸ element, ì¦‰ '4'ê°€ ì‚­ì œë˜ëŠ” ê²ƒ. removeì˜ ê²°ê³¼: [1, 2, 4] delì˜ ê²°ê³¼: [1, 2, 3] . | sort(): list ì•ˆì˜ ê°’ë“¤ì„ ìˆœì„œëŒ€ë¡œ ì •ë ¬í•´ì¤€ë‹¤ (ìˆ«ìëŠ” ì˜¤ë¦„ì°¨ìˆœ, stringì€ ì²« ê¸€ì ì•ŒíŒŒë²³ìˆœ) # x.sort() x = [1, 4, 3, 2, 5] x.sort() print(x) friends = ['Joseph', 'Glenn', 'Sally'] friends.sort() print(friends) . [1, 2, 3, 4, 5] ['Glenn', 'Joseph', 'Sally'] . cf) ëŒ€ë¬¸ìì™€ ì†Œë¬¸ìê°€ ê³µì¡´í•  ê²½ìš°: ëŒ€ë¬¸ìê°€ ë¨¼ì € ì•ŒíŒŒë²³ìˆœìœ¼ë¡œ ì •ë ¬ë˜ê³ , ê·¸ ë‹¤ìŒ ì†Œë¬¸ìê°€ ì •ë ¬ë¨ . fruits = ['apple', 'Banana', 'carrot', 'Dragonfruit'] fruits.sort() print(fruits) . ['Banana', 'Dragonfruit', 'apple', 'carrot'] . +) sorted(x): sort()ì™€ ë§ˆì°¬ê°€ì§€ë¡œ, list ë‚´ìš©ë¬¼ì„ ì•ŒíŒŒë²³ìˆœìœ¼ë¡œ ì •ë ¬. | x.sort()ëŠ” x ìì²´ë¥¼ ë³€ê²½ / sorted(x)ëŠ” xëŠ” ë³€í˜•í•˜ì§€ ì•Šì€ ì±„ ì•ŒíŒŒë²³ ìˆœìœ¼ë¡œ ì •ë ¬ëœ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜ | . x = [1, 4, 3, 2, 5] sorted(x) . [1, 2, 3, 4, 5] . | reverse(): list ì•ˆì˜ element ìˆœì„œë¥¼ ë°˜ëŒ€ë¡œ ë’¤ì§‘ì–´ì¤€ë‹¤ x = [1, 4, 3, 2, 5] x.reverse() print(x) . [5, 2, 3, 4, 1] . +) reversed(x): xëŠ” ë³€í˜•í•˜ì§€ ì•Šê³ , xì˜ elementë¥¼ ë°˜ëŒ€ ìˆœì„œë¡œ ì •ë ¬í•˜ëŠ” iteratorë¥¼ ë°˜í™˜í•´ì¤€ë‹¤ . | sorted(x)ì™€ ë‹¬ë¦¬, ë³€í˜•ëœ ë¦¬ìŠ¤íŠ¸ê°€ returnë˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ iteratorê°€ ë°˜í™˜ëœë‹¤ | . x = [1, 4, 3, 2, 5] [i for i in reversed(x)] # iteratorê°€ ë°˜í™˜ë˜ë¯€ë¡œ ì´ëŸ° ì‹ìœ¼ë¡œ ì‚¬ìš©í•´ì•¼ listë¡œ ë°˜í™˜ì‹œí‚¬ ìˆ˜ ìˆìŒ . [5, 2, 3, 4, 1] . +) indexingì„ í†µí•œ list ë’¤ì§‘ê¸°: . x = [1, 4, 3, 2, 5] x[::-1] . [5, 2, 3, 4, 1] . | . ",
    "url": "https://chaelist.github.io/docs/python_basics/numbers_list_string/#list",
    "relUrl": "/docs/python_basics/numbers_list_string/#list"
  },"133": {
    "doc": "Numbers, List, String",
    "title": "String",
    "content": "ë¬¸ìì—´. sequence of characters. â€˜ â€˜ì´ë‚˜ â€œ â€œë¥¼ í™œìš©í•´ í‘œí˜„ . String ê¸°ì´ˆ . | Indexing &amp; Slicing: listì²˜ëŸ¼, indexing &amp; slicing ê°€ëŠ¥ s = 'python' print(s[0]) print(s[-1]) print(s[0:3]) . p n pyt . | len(): stringì˜ ê¸€ììˆ˜ë¥¼ ì„¸ëŠ” ê°œë… # len() s = 'python' len(s) . 6 . | stringê°„ ë§ì…ˆ(+) h = 'Hello' w = 'World' s = h + w # ë‘ ê°œë¥¼ ë”í•˜ë©´ ë‹¨ìˆœíˆ ì•ë’¤ë¡œ ì´ì–´ë¶™ì—¬ì§„ë‹¤ s . 'HelloWorld' . | â€˜ë‚˜ â€œë¥¼ ì¤‘ê°„ì— ì‚½ì…í•˜ëŠ” ë²• print('Tom\\\\s Book') # \\ë¥¼ ì´ìš© print(\"Tom's Book\") # stringì„ ê°ì‹¸ëŠ” ë”°ì˜´í‘œë¥¼ ë‹¤ë¥¸ ì¢…ë¥˜ë¡œ ì‚¬ìš© . Tom's Book Tom's Book . | white space characters (ê³µë°±) . | \\t: tab | \\n: enter (newline) | . z1 = 'Tom is busy studying. \\nI am not busy. \\t\\tYou?' print(z1) . Tom is busy studying. I am not busy. You? . | . Main String Fuctions . | split(): whitespace(ê³µë°±)ì„ ê¸°ì¤€ìœ¼ë¡œ split . | split(â€˜aâ€™): â€˜aâ€™ë¥¼ ê¸°ì¤€ìœ¼ë¡œ split | split ê²°ê³¼ëŠ” listë¡œ ë‚˜íƒ€ë‚¨ | . # split() s = 'Today is a good day' print(s.split()) print(s.split('a')) print(s.split('good')) . ['Today', 'is', 'a', 'good', 'day'] ['Tod', 'y is ', ' good d', 'y'] ['Today is a ', ' day'] . +) join(): splitëœ ë¬¸ìì—´ì„ ë‹¤ì‹œ ëª¨ì•„ì£¼ê¸° . split_list = ['Today', 'is', 'a', 'good', 'day'] print(' '.join(split_list)) # ì¤‘ê°„ì— ê³µë°±ì„ ë‘ê³  listì˜ elementë“¤ì„ í•©ì³ì¤Œ print('/'.join(split_list)) # ì¤‘ê°„ì— /ë¥¼ ë‘ê³  í•©ì³ì¤Œ . Today is a good day Today/is/a/good/day . | strip(): ì–‘ìª½ ëì˜ whitespaceë¥¼ ì œê±° . | strip(â€˜nâ€™): ì–‘ìª½ ëì˜ â€˜nâ€™ ë¬¸ì ì œê±° | lstrip()ì€ ì™¼ìª½ ë elementë§Œ, rstrip()ì€ ì˜¤ëŠ˜ìª½ ë elementë§Œ ì œê±° | . # strip() t = '\\tpyth\\ton\\n' t.strip() ## ì–‘ ëì˜ whitespaceë§Œ ì œê±°í•´ì£¼ê³ , ì¤‘ê°„ì˜ \\tëŠ” ì œê±°ë˜ì§€ ì•ŠëŠ”ë‹¤ . 'pyth\\ton' . | replace(â€˜aâ€™, â€˜bâ€™): ëª¨ë“  â€˜aâ€™ë¥¼ â€˜bâ€™ë¡œ ëŒ€ì²´ # replace() s = 'python is important' print(s.replace('o', 'a')) print(s) #ìœ ì˜ì‚¬í•­: stringì€ immutable! replaceë¥¼ í•´ë„ ì›ë³¸ sê°€ ë°”ë€ŒëŠ” ê²ƒì€ ì•„ë‹ˆë‹¤. ## ë°”ë€ ê²°ê³¼ë¥¼ ì €ì¥í•˜ê³  ì‹¶ìœ¼ë©´ ìƒˆë¡œìš´ variableë¡œ ë”°ë¡œ ì €ì¥í•´ë‘¬ì•¼ í•¨ . pythan is impartant python is important . *ë¬´ì–¸ê°€ë¥¼ ì—†ì• ê³  ì‹¶ì„ ë•Œì—ë„ replace()ë¥¼ ì‚¬ìš© . ex) replace(â€˜aâ€™, â€˜â€˜)ë¼ê³  í•˜ë©´ â€˜aâ€™ë¥¼ ë‹¤ ì—†ì• ì£¼ëŠ” ê¸°ëŠ¥. (ë‘ë²ˆì§¸ â€˜â€™ ì•ˆì„ ë¹„ì›Œë‘ë©´ ë¨) . s = 'python, is, important,' print(s.replace(',', '')) . python is important . | find(): í•´ë‹¹ ë‹¨ì–´ê°€ ì¡´ì¬í•˜ë©´ ì²«ë²ˆì§¸ characterì˜ index numberë¥¼ ì¶œë ¥ / ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ -1ì„ ì¶œë ¥ s = 'Data science is important' print(s.find('science')) # ì²«ê¸€ì 's'ì˜ index number ì¶œë ¥ print(s.find('python')) # ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ -1 ì¶œë ¥ . 5 -1 . cf) ì¡´ì¬ ìœ ë¬´ë§Œì„ í™•ì¸í•˜ê³  ì‹¶ë‹¤ë©´ find ëŒ€ì‹  inì„ ì‚¬ìš©í•´ë„ ëœë‹¤ . s = 'Data science is important' print('science' in s) print('python' in s) . True False . +) findë¥¼ ì‚¬ìš©í•˜ëŠ” ìƒí™© ì˜ˆì‹œ . data = 'From stephen.marquard@uct.ac.za Sat Jan 5 09:14:16 2008' # ì´ ë°ì´í„°ì—ì„œ ë³´ë‚¸ ì‚¬ëŒì˜ ë©”ì¼ ë„ë©”ì¸ë§Œì„ ì¶”ì¶œí•˜ê³  ì‹¶ìŒ atpos = data.find('@') # '@'ì˜ ìœ„ì¹˜ë¥¼ ì•Œì•„ëƒ„ print(atpos) sppos = data.find(' ', atpos) # ' '(ë¹ˆì¹¸)ì´ ëª‡ ë²ˆì§¸ì— ìˆë‚˜ atpos, ì¦‰ 21ë²ˆì§¸ ë¬¸ì ë’¤ì—ì„œë¶€í„° íƒìƒ‰ (atpost ì•ì˜ ë¹ˆì¹¸ì€ ë¬´ì‹œ) print(sppos) host = data[atpos+1 : sppos] # '@' ë’¤ë¶€í„° ê·¸ ë‹¤ìŒ ë‚˜ì˜¤ëŠ” ' '(ë¹ˆì¹¸)ê¹Œì§€ë¥¼ slicingí•´ì„œ ë©”ì¼ ë„ë©”ì¸ë§Œ ì¶”ì¶œ print(host) . 21 31 uct.ac.za . | upper(), lower(), capitalize() . | upper(): ëª¨ë“  ë¬¸ìë¥¼ uppercaseë¡œ ë°”ê¿”ì¤Œ | lower(): ëª¨ë“  ë¬¸ìë¥¼ lowercaseë¡œ ë°”ê¿”ì¤Œ | capitalize(): ë¬¸ìì—´ì˜ ê°€ì¥ ì²«ë²ˆì§¸ ë¬¸ìëŠ” uppercaseë¡œ, ë‚˜ë¨¸ì§€ëŠ” lowercaseë¡œ ë§ì¶°ì¤Œ | . # ìˆ«ì/íŠ¹ìˆ˜ê¸°í˜¸ì—ëŠ” ë°˜ì˜ë˜ì§€ ì•Šê³ , ì•ŒíŒŒë²³ì—ë§Œ ì ìš©ë¨ a = 'hAve a Nice daY 12#' print(a.upper()) print(a.lower()) print(a.capitalize()) . HAVE A NICE DAY 12# have a nice day 12# Have a nice day 12# . | startswith(), endswith() . | startswith(): ë¬¸ìì—´ì´ ()ì•ˆì˜ íŠ¹ì • ë¬¸ìë¡œ ì‹œì‘ë˜ëŠ”ì§€ í™•ì¸, ê²°ê³¼ëŠ” boolean ê°’ìœ¼ë¡œ ë°˜í™˜ â€“ if X.startswith('Y'): ì´ëŸ° ì‹ìœ¼ë¡œ ifë¬¸ì—ì„œ ì£¼ë¡œ ì‚¬ìš© | endswith(): ë¬¸ìì—´ì´ íŠ¹ì • ë¬¸ìë¡œ ëë‚˜ëŠ”ì§€ ì—¬ë¶€ë¥¼ íŒë³„ | . line = 'Have a nice day' print(line.startswith('Have')) # True print(line.startswith('H')) # True print(line.startswith('h')) # ëŒ€ë¬¸ì Hì™€ ì†Œë¬¸ì hëŠ” ë‹¤ë¥´ê¸° ë•Œë¬¸ì—, Falseê°€ ë°˜í™˜ë¨ print(line.endswith('day')) . True True False True . | isalpha(), isalnum(), isdigit() . | isalpha(): ë¬¸ìì—´ì´ ëª¨ë‘ ë¬¸ìë¡œë§Œ ì´ë£¨ì–´ì ¸ ìˆëŠ”ì§€ íŒë³„ (ì˜ì–´/í•œê¸€ ë“± ì–¸ì–´ëŠ” ìƒê´€X) | isalnum(): ë¬¸ìì—´ì´ ëª¨ë‘ ë¬¸ì/ìˆ«ìë¡œë§Œ ì´ë£¨ì–´ì ¸ ìˆëŠ”ì§€ íŒë³„. íŠ¹ìˆ˜ë¬¸ì/ê³µë°± ë“±ì´ í•¨ê»˜ ë“¤ì–´ ìˆìœ¼ë©´ Falseë¥¼ return | isdigit(): ë¬¸ìì—´ì´ ëª¨ë‘ ìˆ«ìë¡œë§Œ ì´ë£¨ì–´ì ¸ ìˆëŠ”ì§€ íŒë³„. | . line1 = 'Pythonì½”ë”©ì€ì¦ê±°ì›Œ' line2 = 'start123' line3 = '12345' line4 = 'Python ì½”ë”©' print(line1.isalpha()) print(line2.isalpha()) print(line4.isalpha()) # ê³µë°±ë§Œ í•˜ë‚˜ ìˆì–´ë„ Falseê°€ ë¨ print(line1.isalnum()) print(line2.isalnum()) print(line3.isalnum()) print(line2.isdigit()) print(line3.isdigit()) . True False False True True True False True . | count(): listì˜ elementë¥¼ ì„¸ëŠ” ê²ƒê³¼ ë™ì¼í•˜ê²Œ ì ìš©ë¨ . x = 'python python' x.count('python') . 2 . | . String - Number Conversion . | int(x): from string/float to integer # string -&gt; integer x = '123' # ì´ë ‡ê²Œ '' ì•ˆì´ integerì—¬ì•¼ë§Œ int(x)ë¡œ ë³€í™˜ ê°€ëŠ¥ print(x, type(x)) y = int(x) print(y, type(y)) . 123 &lt;class 'str'&gt; 123 &lt;class 'int'&gt; . | float(x): from string to float # string -&gt; float -&gt; integer x = '123.123' # ì´ë ‡ê²Œ '' ì•ˆì´ floatì´ë©´, int(x)ë¥¼ ë°”ë¡œ í•  ìˆ˜ ì—†ìŒ. ë¨¼ì € float(x)ë¥¼ í•´ì¤˜ì•¼ í•¨. print(x, type(x)) y1 = float(x) print(y1, type(y1)) y2 = int(y1) # ì´ì œ y1ì€ floatì´ê¸°ì—, ì—¬ê¸°ì— int(x)ë¥¼ í•´ì£¼ë©´ integerê°€ ë¨. print(y2, type(y2)) . 123.123 &lt;class 'str'&gt; 123.123 &lt;class 'float'&gt; 123 &lt;class 'int'&gt; . | str(number): from number to string # integer -&gt; string z = 123 print(z, type(z)) s = str(z) print(s, type(s)) . 123 &lt;class 'int'&gt; 123 &lt;class 'str'&gt; . | . ë¬¸ìì—´ í¬ë§·íŒ… (string formatting) . | â€˜formatâ€™ method print(\"ì˜¤ëŠ˜ì€ {}ì›” {}ì¼ì…ë‹ˆë‹¤\".format(11, 12)) . ì˜¤ëŠ˜ì€ 11ì›” 12ì¼ì…ë‹ˆë‹¤ . *{ }ì˜ ìˆœì„œ ì§€ì •í•˜ê¸° . # 'format' method - {}ì˜ ìˆœì„œ ì •í•˜ê¸° print(\"ì €ëŠ” {1}, {0}, {2}ë¥¼ ì¢‹ì•„í•©ë‹ˆë‹¤\".format(\"íŠ¸ì™€ì´ìŠ¤\", \"ìœ ì¬ì„\", \"ë¹„í‹€ì¦ˆ\")) . ì €ëŠ” ìœ ì¬ì„, íŠ¸ì™€ì´ìŠ¤, ë¹„í‹€ì¦ˆë¥¼ ì¢‹ì•„í•©ë‹ˆë‹¤ . *ì†Œìˆ˜ì  ì œí•œ ì§€ì •í•˜ê¸° . # 'format' method - ì†Œìˆ˜ì  ì œí•œ ì§€ì • print(\"{0} ë‚˜ëˆ„ê¸° {1}ì€ {2:.2f}ì…ë‹ˆë‹¤\".format(1, 3, 1/3)) # :.2fë¼ê³  í•˜ë©´ floating point(ì†Œìˆ˜) ë‘˜ì§¸ìë¦¬ê¹Œì§€ ì¶œë ¥í•˜ë¼ëŠ” ëœ» . 1 ë‚˜ëˆ„ê¸° 3ì€ 0.33ì…ë‹ˆë‹¤ . | :.4fëŠ” ì†Œìˆ˜ì  ë„·ì§¸ì§œë¦¬ê¹Œì§€ ì¶œë ¥í•˜ë¼ëŠ” ëœ» | :.0fëŠ” ì •ìˆ˜ë¡œ ì¶œë ¥í•˜ë¼ëŠ” ëœ» (cf. ì •ìˆ˜ë¡œ í•˜ë ¤ë©´ :dë¼ê³  í•´ë„ ë¨) | :fë¼ê³  í•˜ë©´ ê·¸ëƒ¥ ì†Œìˆ˜ì  ì œí•œ ì—†ì´ floating pointë¡œ ì¶œë ¥í•˜ë¼ëŠ” ëœ» | . | % ê¸°í˜¸ . | ì˜¤ë˜ëœ ë°©ì‹. C/ìë°” ë“± ì–¸ì–´ì˜ ë¬¸ìì—´ í¬ë§·íŒ… ë°©ì‹ê³¼ ìœ ì‚¬ | pythonì—ì„œëŠ” ê¶Œì¥ë˜ì§€ëŠ” ì•ŠëŠ”ë‹¤ (f-string ë“± ë” íš¨ìœ¨ì ì¸ ë°©ë²•ì´ ìˆê¸° ë•Œë¬¸) | %s, %dì™€ ê°™ì€ â€˜í¬ë§· ìŠ¤íŠ¸ë§â€™ì„ ì‚¬ìš© | . name = 'ìµœì§€ì•„' age = 25 print(\"ì œ ì´ë¦„ì€ %sì´ê³  %dì‚´ì…ë‹ˆë‹¤.\" % (name, age)) . ì œ ì´ë¦„ì€ ìµœì§€ì•„ì´ê³  25ì‚´ì…ë‹ˆë‹¤. | %s: strí˜• | %d: intí˜• | %f: floatí˜• | %.[ìˆ«ì]f: floatí˜• (ìˆ«ìë¥¼ í†µí•´ ì†Œìˆ˜ì ì„ ì§€ì •) | . | f-string (íŒŒì´ì¬ 3.6ë¶€í„° ë‚˜ì˜¨ ë°©ì‹) name = 'ìµœì§€ì•„' age = 25 print(f\"ì œ ì´ë¦„ì€ {name}ì´ê³  {age}ì‚´ì…ë‹ˆë‹¤.\") . ì œ ì´ë¦„ì€ ìµœì§€ì•„ì´ê³  25ì‚´ì…ë‹ˆë‹¤. | . ",
    "url": "https://chaelist.github.io/docs/python_basics/numbers_list_string/#string",
    "relUrl": "/docs/python_basics/numbers_list_string/#string"
  },"134": {
    "doc": "Numpy",
    "title": "Numpy",
    "content": " ",
    "url": "https://chaelist.github.io/docs/numpy",
    "relUrl": "/docs/numpy"
  },"135": {
    "doc": "Numpy ì—°ì‚°ê³¼ í†µê³„",
    "title": "Numpy ì—°ì‚°ê³¼ í†µê³„",
    "content": ". | Numpy ê¸°ë³¸ ì—°ì‚°ê³¼ í†µê³„ . | Numpy ê¸°ë³¸ ì—°ì‚° | Numpy Boolean ì—°ì‚° | Numpy ê¸°ë³¸ í†µê³„ | nanì„ í¬í•¨í•œ array í†µê³„ëŸ‰ ê³„ì‚° | . | Numpyë¥¼ í™œìš©í•œ í–‰ë ¬ ì—°ì‚° . | í–‰ë ¬ â€˜ìš”ì†Œë³„ ê³±í•˜ê¸°â€™ | í–‰ë ¬ ê°„ ë§ì…ˆ &amp; ê³±ì…ˆ | ì „ì¹˜ í–‰ë ¬, ë‹¨ìœ„ í–‰ë ¬, ì—­í–‰ë ¬ | . | . ",
    "url": "https://chaelist.github.io/docs/numpy/numpy_arithmetics/",
    "relUrl": "/docs/numpy/numpy_arithmetics/"
  },"136": {
    "doc": "Numpy ì—°ì‚°ê³¼ í†µê³„",
    "title": "Numpy ê¸°ë³¸ ì—°ì‚°ê³¼ í†µê³„",
    "content": "Numpy ê¸°ë³¸ ì—°ì‚° . : NumpyëŠ” ìˆ˜í•™ì  ì—°ì‚°ì´ ë§¤ìš° ê°„ë‹¨í•˜ë‹¤ëŠ” ì ì—ì„œ listì™€ ì°¨ë³„í™”ëœë‹¤ . | ê° ì›ì†Œì— ë™ì¼í•œ ì‚¬ì¹™ì—°ì‚° ì ìš©: import numpy as np # importí•´ì•¼ ì‚¬ìš© ê°€ëŠ¥ array1 = np.arange(10) print(array1) . [0 1 2 3 4 5 6 7 8 9] . # ê° ì›ì†Œì— 2ì”© ë”í•˜ê¸° . array1 + 2 . array([ 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]) . # ê° ì›ì†Œë¥¼ 2ì”© ë‚˜ëˆ„ê¸° . array1 / 2 . array([0. , 0.5, 1. , 1.5, 2. , 2.5, 3. , 3.5, 4. , 4.5]) . # ê° ì›ì†Œì— 2ì”© ê³±í•˜ê¸° . array1 * 2 . array([ 0, 2, 4, 6, 8, 10, 12, 14, 16, 18]) . # ê° ì›ì†Œ ì œê³±í•˜ê¸° . array1 ** 2 . array([ 0, 1, 4, 9, 16, 25, 36, 49, 64, 81]) . cf) listë¡œ ë™ì¼í•œ ì‚¬ì¹™ì—°ì‚°ì„ í•´ì£¼ë ¤ë©´ forë¬¸ì„ ì‚¬ìš©í•´ì„œ ìš”ì†Œë³„ë¡œ ì—°ì‚°ì„ í•´ì¤˜ì•¼ í•œë‹¤ . # ì•„ë˜ì™€ ê°™ì´ ê³„ì‚°í•´ì•¼ array1 + 2ì™€ ê°™ì€ ê²°ê³¼ê°€ ë‚˜ì˜¨ë‹¤ list1 = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] new_list = [] for a in list1: new_list.append(a + 2) new_list . [2, 3, 4, 5, 6, 7, 8, 9, 10, 11] . | Numpy Array ê°„ ì‚¬ì¹™ì—°ì‚°: array1 = np.arange(10) array2 = np.arange(10, 20) print(array1) print(array2) . [0 1 2 3 4 5 6 7 8 9] [10 11 12 13 14 15 16 17 18 19] . # numpy array ê°„ ë§ì…ˆ . array1 + array2 # ê° ìš”ì†Œë³„ë¡œ ì°¨ë¡€ëŒ€ë¡œ ë”í•´ì§„ë‹¤: 0+10, 1+11, 2+12, ... , 9+19 . array([10, 12, 14, 16, 18, 20, 22, 24, 26, 28]) . # numpy array ê°„ ë‚˜ëˆ—ì…ˆ . array1 / array2 # ê° ìš”ì†Œë³„ë¡œ ì°¨ë¡€ëŒ€ë¡œ ë‚˜ëˆ ì§„ë‹¤: 0/10, 1/11, 2/12, ... , 9/19 . array([0. , 0.09090909, 0.16666667, 0.23076923, 0.28571429, 0.33333333, 0.375 , 0.41176471, 0.44444444, 0.47368421]) . # numpy array ê°„ ê³±ì…ˆ . array1 * array2 # ê° ìš”ì†Œë³„ë¡œ ì°¨ë¡€ëŒ€ë¡œ ê³±í•´ì§„ë‹¤: 0*10, 1*11, 2*12, ... , 9*19 . array([ 0, 11, 24, 39, 56, 75, 96, 119, 144, 171]) . cf) list ë‘ ê°œë¥¼ numpy array ê°„ ë§ì…ˆê³¼ ë™ì¼í•˜ê²Œ ìš”ì†Œë³„ë¡œ ë”í•´ì£¼ë ¤ë©´ forë¬¸ì„ ì‚¬ìš©í•´ì„œ ê³„ì‚°í•´ì•¼ í•œë‹¤ . ## ì•„ë˜ì™€ ê°™ì´ ê³„ì‚°í•´ì•¼ array1 + array2ì™€ ê°™ì€ ê²°ê³¼ê°€ ë‚˜ì˜¨ë‹¤ list1 = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] ist2 = [10, 11, 12, 13, 14, 15, 16, 17, 18, 19] added_list = [] for a in list1: added_list.append(a + list2[list1.index(a)]) added_list . [10, 12, 14, 16, 18, 20, 22, 24, 26, 28] . +) ë‹¨ìˆœíˆ list1 + list2 í•´ë²„ë¦¬ë©´? . list1 + list2 # ì•„ë˜ì™€ ê°™ì´ list1.extend(list2)ì˜ ê²°ê³¼ê°€ ë‚˜ì˜¨ë‹¤ . [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19] . | . Numpy Boolean ì—°ì‚° . : ì´ ì›ë¦¬ê°€ boolean indexingì— ì‚¬ìš©ë˜ëŠ” ê²ƒ. array1 = np.array([2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31]) array1 &gt; 4 # ê° ì›ì†Œë³„ë¡œ 4ë³´ë‹¤ í°ì§€ ì—¬ë¶€ë¥¼ íŒë‹¨í•´ Boolean ê°’ì„ ë°˜í™˜ # ê²°ê³¼ê°’ë„ array í˜•íƒœ. Boolean ê°’ë“¤ì„ elementë¡œ í•˜ëŠ” numpy array . array([False, False, True, True, True, True, True, True, True, True, True]) . &gt;, &lt;, &gt;=, &lt;=, == ë“± ì‚¬ìš© ê°€ëŠ¥ . array1 % 2 == 0 . array([ True, False, False, False, False, False, False, False, False, False, False]) . *np.where() í•¨ìˆ˜: í•´ë‹¹ Boolean ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ê°’ë“¤ì˜ indexê°’ì„ ë°˜í™˜ . | np.where( ) ì•ˆì—ëŠ” booleanê°’ë“¤ë¡œ ì´ë£¨ì–´ì§„ numpy arrayê°€ ë“¤ì–´ê°„ë‹¤ | ê²°ê³¼ë¡œ Trueì¸ elementì˜ indexê°’ë§Œ ë°˜í™˜ (arrayí˜•íƒœë¡œ ë°˜í™˜) | . np.where(array1 &gt; 4) # '4ë³´ë‹¤ í¬ë‹¤'ëŠ” ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ê°’ë“¤ì˜ indexê°’ ë°˜í™˜ . (array([ 2, 3, 4, 5, 6, 7, 8, 9, 10]),) . &gt;&gt; Boolean ì—°ì‚°ì„ í™œìš©í•œ Boolean Indexing: 2ê°€ì§€ ë°©ë²• . # 1ë²ˆì§¸ ë°©ë²•: ì§ì ‘ indexing print(array1[array1 &gt; 4]) # 2ë²ˆì§¸ ë°©ë²•: np.where() ì´ìš© filter = np.where(array1 &gt; 4) print(array1[filter]) . [ 5 7 11 13 17 19 23 29 31] [ 5 7 11 13 17 19 23 29 31] . Numpy ê¸°ë³¸ í†µê³„ . | ìµœëŒ“ê°’, ìµœì†Ÿê°’, í‰ê· ê°’ array1 = np.array([14, 6, 13, 21, 23, 31, 9, 5]) print(array1.max()) # ìµœëŒ“ê°’ print(array1.min()) # ìµœì†Ÿê°’ print(array1.mean()) # í‰ê· ê°’ . 31 5 15.25 . cf) listë„ max(x), min(x) ê¸°ëŠ¥ì€ ìˆì§€ë§Œ, í‰ê·  êµ¬í•˜ê¸° ë“±ì˜ ê¸°ëŠ¥ì€ ì—†ìŒ . | ì¤‘ì•™ê°’ array1 = np.array([8, 12, 9, 15, 16]) array2 = np.array([14, 6, 13, 21, 23, 31, 9, 5]) print(np.median(array1)) # ì¤‘ì•™ê°’ print(np.median(array2)) # ì¤‘ì•™ê°’ - ì§ìˆ˜ ê°œì˜ ê°’ì´ ìˆëŠ” arrayì¼ ê²½ìš°, ì¤‘ì•™ê°’ ë‘ ê°œë¥¼ í‰ê·  ë‚¸ ê°’ì„ return ## array2ì˜ ê²½ìš°, ì¤‘ì•™ê°’ì´ 13ê³¼ 14 ë‘ ê°œ. &gt;&gt; ë‘ ê°œë¥¼ í‰ê· ë‚´ë©´ 13.5 . 12.0 13.5 . | í‘œì¤€í¸ì°¨, ë¶„ì‚° array1 = np.array([14, 6, 13, 21, 23, 31, 9, 5]) print(array1.std()) # í‘œì¤€ í¸ì°¨ print(array1.var()) # ë¶„ì‚° . 8.496322733983215 72.1875 . | . nanì„ í¬í•¨í•œ array í†µê³„ëŸ‰ ê³„ì‚° . | í•˜ë‚˜ë¼ë„ np.nan(=Nullê°’)ì´ í¬í•¨ëœ arrayëŠ” sum, mean ë“±ì„ ê³„ì‚°í•˜ë©´ nanìœ¼ë¡œë°–ì— ì•ˆë‚˜ì˜¨ë‹¤: array1 = np.array([14, 6, 13, 21, 23, np.nan, 9, 5]) print(array1.sum()) # np.sum(array1)ê³¼ ë™ì¼ print(array1.mean()) # np.mean(array1)ê³¼ ë™ì¼ . nan nan . | np.nansum(), np.nanmean()ì„ í™œìš©í•˜ë©´ nanì„ ì œì™¸í•œ í†µê³„ëŸ‰ì„ í™•ì¸ ê°€ëŠ¥: print(np.nansum(array1)) print(np.nanmean(array1)) . 91.0 13.0 . | +) np.nanstd(), np.nanmin(), np.nanmedian() ë“±â€¦ | . ",
    "url": "https://chaelist.github.io/docs/numpy/numpy_arithmetics/#numpy-%EA%B8%B0%EB%B3%B8-%EC%97%B0%EC%82%B0%EA%B3%BC-%ED%86%B5%EA%B3%84",
    "relUrl": "/docs/numpy/numpy_arithmetics/#numpy-ê¸°ë³¸-ì—°ì‚°ê³¼-í†µê³„"
  },"137": {
    "doc": "Numpy ì—°ì‚°ê³¼ í†µê³„",
    "title": "Numpyë¥¼ í™œìš©í•œ í–‰ë ¬ ì—°ì‚°",
    "content": "í–‰ë ¬ â€˜ìš”ì†Œë³„ ê³±í•˜ê¸°â€™ . | ìš”ì†Œë³„ ê³±í•˜ê¸°(Element-wise Multiplication): ê°™ì€ í–‰/ì—´ì— ìˆëŠ” ìš”ì†Œë¼ë¦¬ ê³±í•´ì„œ ìƒˆë¡œìš´ í–‰ë ¬ì„ ë§Œë“œëŠ” ì—°ì‚° | í–‰ë ¬ ë§ì…ˆê³¼ ë§ˆì°¬ê°€ì§€ë¡œ ê°™ì€ ì°¨ì›ì„ ê°–ëŠ” í–‰ë ¬ ì‚¬ì´ì—ë§Œ ì—°ì‚°ì´ ê°€ëŠ¥í•˜ë‹¤ | A âˆ˜ Bë¼ê³  í‘œê¸° | . # numpyë¥¼ ì´ìš©í•œ 'ìš”ì†Œë³„ ê³±í•˜ê¸°' -- A * Bì™€ ê°™ì´ ë³„í‘œ(*)ë¥¼ ì‚¬ìš©í•˜ë©´ ëœë‹¤ A = np.array([ [1, 2, 3], [4, 5, 6], [7, 8, 9] ]) B = np.array([ [0, 1, 2], [2, 0, 1], [1, 2, 0] ]) A * B . array([[ 0, 2, 6], [ 8, 0, 6], [ 7, 16, 0]]) . í–‰ë ¬ ê°„ ë§ì…ˆ &amp; ê³±ì…ˆ . | í–‰ë ¬ ê°„ ë§ì…ˆ A = np.array([ [1, 2, 3], [4, 5, 6], [7, 8, 9] ]) B = np.array([ [0, 1, 2], [2, 0, 1], [1, 2, 0] ]) A + B . array([[ 1, 3, 5], [ 6, 5, 7], [ 8, 10, 9]]) . | ìŠ¤ì¹¼ë¼ê³± 5 * A . array([[ 5, 10, 15], [20, 25, 30], [35, 40, 45]]) . | í–‰ë ¬ê°„ ê³±ì…‰ (ë‚´ì ê³±) . | ì „ì œ: Aì˜ ì—´ ìˆ˜ = Bì˜ í–‰ ìˆ˜ | A * BëŠ” â€˜ìš”ì†Œë³„ ê³±í•˜ê¸°â€™ê°€ ë˜ë‹ˆê¹Œ ì£¼ì˜ | . *ë‘ ê°€ì§€ ë°©ë²•: . | np.dot(A, B) np.dot(A, B) . array([[ 7, 7, 4], [16, 16, 13], [25, 25, 22]]) . | A @ B A @ B ## np.dotì´ë‘ ë™ì¼í•œ ê²°ê³¼ë¥¼ ë‚¸ë‹¤. ë” ê°„ê²°í•œ ë°©ë²•. array([[ 7, 7, 4], [16, 16, 13], [25, 25, 22]]) . | . | ì—°ì‚° ì„ì–´ì„œ ê³„ì‚°í•˜ê¸° . | ì¼ë°˜ ì—°ì‚°ê³¼ ë§ˆì°¬ê°€ì§€ë¡œ, ()ê°€ ë¨¼ì € ê³„ì‚°ë˜ê³ , ê·¸ ì•ˆì—ì„œë„ ë§ì…ˆë³´ë‹¤ ê³±ì…ˆì´ ë¨¼ì € ê³„ì‚°ë¨. | . A @ B + (A + 2*B) . array([[ 8, 11, 11], [24, 21, 21], [34, 37, 31]]) . | . ì „ì¹˜ í–‰ë ¬, ë‹¨ìœ„ í–‰ë ¬, ì—­í–‰ë ¬ . A = np.array([ [1, -1, 2], [3, 2, 2], [4, 1, 2] ]) A . array([[ 1, -1, 2], [ 3, 2, 2], [ 4, 1, 2]]) . | ì „ì¹˜ í–‰ë ¬ . | ê¸°ì¡´ í–‰ë ¬ Aì—ì„œ í–‰ê³¼ ì—´ì„ ë°”ê¾¼ í–‰ë ¬ | ATë¼ê³  í‘œê¸° (Aì˜ ì „ì¹˜ í–‰ë ¬) | . *ë‘ ê°€ì§€ ë°©ë²•: . | np.transpose(A) A_transpose = np.transpose(A) A_transpose . array([[ 1, 3, 4], [-1, 2, 1], [ 2, 2, 2]]) . | A.T A_transpose = A.T # .Të§Œ ë¶™ì—¬ì£¼ë©´ ë¨. ë” ê°„ê²°í•œ ë°©ë²•. A_transpose . array([[ 1, 3, 4], [-1, 2, 1], [ 2, 2, 2]]) . | . | ë‹¨ìœ„ í–‰ë ¬(identity matrix) . | ì •ì‚¬ê°í˜• ëª¨ì–‘ì´ë©°, ëŒ€ê°ì„ ìœ¼ë¡œëŠ” ì›ì†Œê°€ ì­‰ 1ì´ê³ , ê·¸ ì™¸ì—ëŠ” ëª¨ë‘ 0ì¸ í–‰ë ¬. | ì–´ë–¤ í–‰ë ¬ì´ë“  ê°„ì— ë‹¨ìœ„ í–‰ë ¬ì„ ê³±í•˜ë©´ ê¸°ì¡´ í–‰ë ¬ì´ ê·¸ëŒ€ë¡œ ìœ ì§€ë¨. | ë³´í†µ Ië¼ê³  í‘œê¸° | . *np.identity(ìˆ«ì)ë¡œ ìƒì„± . I = np.identity(3) # 3x3 ë‹¨ìœ„í–‰ë ¬ I . array([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]]) . *ë‹¨ìœ„í–‰ë ¬ Ië¥¼ Aì— ê³±í•˜ë©´ ê·¸ëŒ€ë¡œ Aê°€ ê²°ê³¼ë¡œ ë‚˜ì˜´ . A @ I . array([[ 1., -1., 2.], [ 3., 2., 2.], [ 4., 1., 2.]]) . | ì—­í–‰ë ¬(inverse matrix) . | íŠ¹ì • í–‰ë ¬ Aì— ê³±í–ˆì„ ë•Œ ë‹¨ìœ„ í–‰ë ¬ Iê°€ ë‚˜ì˜¤ë„ë¡ í•˜ëŠ” í–‰ë ¬ | A-1ë¼ê³  í‘œê¸° (Aì˜ ì—­í–‰ë ¬) | í•˜ì§€ë§Œ ëª¨ë“  í–‰ë ¬ì— ì—­í–‰ë ¬ì´ ìˆëŠ” ê²ƒì€ ì•„ë‹ˆë‹¤! ë¬´ì—‡ì„ ê³±í•´ë“  Iê°€ ì•ˆë‚˜ì˜¤ëŠ” í–‰ë ¬ë„ ìˆìŒ | . *numpyì˜ linalg ëª¨ë“ˆì˜ pinv í•¨ìˆ˜ë¥¼ ì‚¬ìš© . | â€»ì´ í•¨ìˆ˜ëŠ” ì—­í–‰ë ¬ì´ ì—†ëŠ” ê²½ìš°ì—ë„ ê°€ì¥ ë¹„ìŠ·í•œ íš¨ê³¼ë¥¼ ë‚´ëŠ” í–‰ë ¬ì„ returní•´ì¤€ë‹¤) | cf) np.linalg.inv(A)ë„ ì—­í–‰ë ¬ì„ returní•´ì£¼ëŠ” í•¨ìˆ˜ì´ì§€ë§Œ, ì—­í–‰ë ¬ì´ ì—†ëŠ” ê²½ìš°ì—ëŠ” ì‘ë™X | . A_inverse = np.linalg.pinv(A) A_inverse . array([[-0.2, -0.4, 0.6], [-0.2, 0.6, -0.4], [ 0.5, 0.5, -0.5]]) . *Aì™€ Aì˜ ì—­í–‰ë ¬ì„ ê³±í•˜ë©´ Iê°€ ë‚˜ì˜¨ë‹¤ . A @ A_inverse . array([[ 1.00000000e+00, 7.77156117e-16, -8.88178420e-16], [ 0.00000000e+00, 1.00000000e+00, -8.88178420e-16], [ 0.00000000e+00, 4.44089210e-16, 1.00000000e+00]]) . | ëŒ€ê°ì„ ì€ ë‹¤ 1ì´ ë§ëŠ”ë°, ë‹¤ë¥¸ ë¶€ë¶„ë“¤ì— ë‚˜ì˜¤ëŠ” ìˆ«ìë“¤ì€ 0ì— ê°€ê¹ê¸´ í•˜ì§€ë§Œ(e-16ì€ 0.00000~ê°€ 16ê°œ ìˆëŠ” ê²ƒ) ì™„ì „íˆ 0ì´ë¼ê³  ë‚˜ì˜¤ì§€ ì•ŠëŠ”ë‹¤.Â Â» ì´ë ‡ê²Œ ì¡°ê¸ˆì”© ì´ìƒí•œ ì´ìœ ëŠ” ë¬´ìˆ˜íˆ ë§ì€ ì†Œìˆ˜0ì€ ì»´í“¨í„°ê°€ ë‹¤ í‘œí˜„í•˜ê¸° ì–´ë µê¸° ë•Œë¬¸.Â Â» ì†Œìˆ˜0ì„ ì‚¬ìš©í•  ë•ŒëŠ” ì—¬ê¸° ë³´ì´ëŠ” ê²ƒì²˜ëŸ¼ ì•½ê°„ì˜ ì˜¤ì°¨ê°€ ë°œìƒí•  ìˆ˜ ë°–ì— ì—†ë‹¤. | . | . +) í–‰ë ¬ì‹ (determinant) . | ì—­í–‰ë ¬ ì¡´ì¬ ì—¬ë¶€ë¥¼ í…ŒìŠ¤íŠ¸í•˜ëŠ” ì‹. í–‰ë ¬ì‹ì´ 0ì´ë©´ ì—­í–‰ë ¬ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ. np.linalg.det(A) . -9.999999999999998 . | . ",
    "url": "https://chaelist.github.io/docs/numpy/numpy_arithmetics/#numpy%EB%A5%BC-%ED%99%9C%EC%9A%A9%ED%95%9C-%ED%96%89%EB%A0%AC-%EC%97%B0%EC%82%B0",
    "relUrl": "/docs/numpy/numpy_arithmetics/#numpyë¥¼-í™œìš©í•œ-í–‰ë ¬-ì—°ì‚°"
  },"138": {
    "doc": "Numpy ê¸°ì´ˆ",
    "title": "Numpy ê¸°ì´ˆ",
    "content": ". | Numpy Array ìƒì„± . | listë¥¼ ë°›ì•„ì„œ numpy array ìƒì„± | np.full(), np.zeros(), np.ones() | random ìˆ«ìë¡œ ì±„ì›Œì§„ array ìƒì„± | np.arange() | . | Numpy Array ë³€ê²½ . | Data Type í™•ì¸ ë° ë³€ê²½ | tolist() | reshape() | Numpy Array ì •ë ¬ | np.argsort() | . | Indexing &amp; Slicing . | Indexing | Slicing | Boolean Indexing | . | . *Numpy: ëŒ€ê·œëª¨ ë‹¤ì°¨ì› ë°°ì—´ì„ ì‰½ê²Œ ìˆ˜í•™ì ìœ¼ë¡œ ì—°ì‚°í•  ìˆ˜ ìˆê²Œ ì§€ì›í•˜ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ . ",
    "url": "https://chaelist.github.io/docs/numpy/numpy_basics/",
    "relUrl": "/docs/numpy/numpy_basics/"
  },"139": {
    "doc": "Numpy ê¸°ì´ˆ",
    "title": "Numpy Array ìƒì„±",
    "content": "listë¥¼ ë°›ì•„ì„œ numpy array ìƒì„± . import numpy as np # importí•´ì•¼ ì‚¬ìš© ê°€ëŠ¥; ë³´í†µ npë¡œ ì¤„ì—¬ì„œ import list1 = [1,2,3] print(type(list1)) array1 = np.array(list1) ## ì´ë ‡ê²Œ í•˜ë©´ listê°€ numpy array í˜•íƒœë¡œ ë°”ë€œ # ë¬¼ë¡ , array1 = np.array([1, 2, 3]) ì´ë ‡ê²Œ ì§ì ‘ assigní•˜ëŠ” ê²ƒë„ ê°€ëŠ¥ print(type(array1)) print(array1) . &lt;class 'list'&gt; &lt;class 'numpy.ndarray'&gt; [1 2 3] . +) ë‹¤ì°¨ì› listë¡œ í–‰ë ¬ ìƒì„±í•˜ê¸° . A = np.array([[1,2,3], # ë‹¤ì°¨ì› listë¥¼ assigní•´ì¤„ ìˆ˜ ìˆë‹¤ [2,3,4]]) print('type:', type(A)) print('array í˜•íƒœ:', A.shape) # (2, 3)ì˜ í–‰ë ¬ í˜•íƒœ print(A) . type: &lt;class 'numpy.ndarray'&gt; array í˜•íƒœ: (2, 3) [[1 2 3] [2 3 4]] . np.full(), np.zeros(), np.ones() . | np.full(): ëª¨ë“  ê°’ì´ ê°™ì€ numpy array ìƒì„± array1 = np.full(6, 7) # 7ì´ë¼ëŠ” ê°’ìœ¼ë¡œ ì±„ì›Œì§„ 6ê°œì§œë¦¬ arrayë¥¼ ë§Œë“¤ë¼ëŠ” ëœ» print(array1) . [7 7 7 7 7 7] . | np.zeros(): ëª¨ë“  ê°’ì´ 0ì¸ numpy array ìƒì„± array2 = np.zeros(6, dtype=int) # dtype=intë¼ê³  ì“°ì§€ ì•Šìœ¼ë©´ 'float' íƒ€ì…ìœ¼ë¡œ, ëª¨ë“  ê°’ì´ '0.' ## np.full(6, 0)ì´ë¼ê³  í•´ë„ ë™ì¼í•œ ê²°ê³¼ë¥¼ ë‚¼ ìˆ˜ ìˆìŒ print(array2) . [0 0 0 0 0 0] . +) 0ìœ¼ë¡œ ì±„ì›Œì§„ (2, 4) í–‰ë ¬ ë§Œë“¤ê¸° . C = np.zeros((2, 4)) # ()ì•ˆì— ()ë¥¼ ë˜ ë„£ì–´ì„œ ì°¨ì›ì„ ì¨ì¤˜ì•¼ í•¨ C . array([[0., 0., 0., 0.], [0., 0., 0., 0.]]) . | np.ones(): ëª¨ë“  ê°’ì´ 1ì¸ numpy array ìƒì„± array3 = np.ones(6, dtype=int) print(array3) . [1 1 1 1 1 1] . | . random ìˆ«ìë¡œ ì±„ì›Œì§„ array ìƒì„± . : np.random.random(), np.random.randint(), np.random.rand(), np.random.randn() . | np.random.random(): 0~1 ì‚¬ì´ì˜ ëœë¤í•œ floating numberë¡œ êµ¬ì„±ëœ array ìƒì„± . | ì›ì†Œ ê°œìˆ˜ë§Œì„ inputìœ¼ë¡œ ë°›ëŠ”ë‹¤. | . array1 = np.random.random(6) # 6ê°œì§œë¦¬ arrayë¥¼ ë§Œë“¤ë˜, 0~1 ì‚¬ì´ì˜ ëœë¤í•œ floating numberë¡œ ì±„ìš°ë¼ëŠ” ëœ» array2 = np.random.random(6) ## ë˜‘ê°™ì€ ì½”ë“œë¡œ ë‘ ë²ˆ ìƒì„±í•´ë„ ì „í˜€ ë‹¤ë¥¸ ìˆ«ìë“¤ë¡œ êµ¬ì„±ëœ arrayê°€ ëœë‹¤ print(array1) print(array2) . [0.88406249 0.39299664 0.92697636 0.85366744 0.07601589 0.33080729] [0.61871553 0.55877544 0.21509023 0.93186876 0.95658264 0.25799333] . | np.random.randint(): ì •ìˆ˜ í˜•íƒœì˜ ëœë¤ ìˆ«ìë¡œ êµ¬ì„±ëœ array ìƒì„± . | np.random.random()ê³¼ ë‹¬ë¦¬ 2ê°œì˜ ë³€ìˆ˜ë¥¼ inputìœ¼ë¡œ ë°›ëŠ”ë‹¤ . np.random.randint(5, size=8) # 0ì—ì„œ 4ê¹Œì§€ì˜ ì •ìˆ˜ë¡œ ëœë¤í•˜ê²Œ ì›ì†Œê°€ 8ê°œì¸ array ìƒì„± . array([3, 4, 0, 3, 4, 4, 4, 3]) . | +) ë³€ìˆ˜ 3ê°œë¥¼ inputìœ¼ë¡œ ë„£ëŠ” ê²ƒë„ ê°€ëŠ¥ . np.random.randint(2, 10, size=(3, 5)) # 2ì—ì„œ 9ê¹Œì§€ì˜ ì •ìˆ˜ë¡œ ëœë¤í•˜ê²Œ (3, 5) ì‚¬ì´ì¦ˆì˜ array ìƒì„± . array([[4, 3, 5, 8, 8], [5, 6, 9, 2, 5], [3, 4, 6, 8, 5]]) . | . | np.random.rand(): randomí•œ ê°’ë“¤ì„ ë„£ì€ â€˜í–‰ë ¬â€™ ë§Œë“¤ê¸° (0~1 ì‚¬ì´ì˜ ê· ì¼ë¶„í¬ì—ì„œ randomí•œ floating numberë¡œ êµ¬ì„±) . | () ì•ˆì— ì°¨ì›ì„ ì ìœ¼ë©´ ë¨ | . np.random.rand(3, 5) # 3x5 í–‰ë ¬ì„ ë§Œë“¤ê² ë‹¤ëŠ” ëœ» . array([[0.20518818, 0.83743088, 0.58408188, 0.24389283, 0.28567188], [0.50262559, 0.50765122, 0.5385608 , 0.861137 , 0.69626678], [0.80102188, 0.44550328, 0.91234899, 0.91133805, 0.90298844]]) . | np.random.randn(): rand()ì™€ ë§ˆì°¬ê°€ì§€ë¡œ randomí•œ ê°’ë“¤ì„ ë„£ì€ â€˜í–‰ë ¬â€™ ë§Œë“¤ë˜, í‰ê·  0, í‘œì¤€í¸ì°¨ 1ì˜ ê°€ìš°ì‹œì•ˆ í‘œì¤€ì •ê·œë¶„í¬ì—ì„œ randomí•œ floating numberë¥¼ ì¶”ì¶œí•´ êµ¬ì„± . | () ì•ˆì— ì°¨ì›ì„ ì ìœ¼ë©´ ë¨ | . np.random.randn(3, 5) # 3x5 í–‰ë ¬ . array([[-0.13089314, 1.12283706, 0.50684862, -1.41899365, 0.07714081], [-0.20499738, 0.24543592, 0.19121621, -1.12409184, -0.8251468 ], [ 1.03761123, -0.99567926, 0.78983874, 1.2103671 , -0.84700707]]) . | . np.arange() . : íŠ¹ì • ìˆ«ì ë²”ìœ„ë§Œí¼ì„ rangeë¡œ í•˜ëŠ” numpy array ìƒì„± . | range() í•¨ìˆ˜ì™€ ì‘ë™ ë°©ì‹ì´ ê±°ì˜ ìœ ì‚¬ (ì°¸ê³ ) | . | np.arange(a) array1 = np.arange(6) # 0ë¶€í„° 5ê¹Œì§€ë¥¼ ì›ì†Œë¡œ í•˜ëŠ” array ìƒì„± print(array1) . [0 1 2 3 4 5] . | np.arange(a, b) array1 = np.arange(2, 7) # 2ë¶€í„° 6ê¹Œì§€ë¥¼ ì›ì†Œë¡œ í•˜ëŠ” array ìƒì„± print(array1) . [2 3 4 5 6] . | np.arange(a, b, k) array1 = np.arange(3, 17, 3) # 3ë¶€í„° 16ê¹Œì§€, 3ì˜ ê°„ê²©ìœ¼ë¡œ ì¦ê°€í•˜ëŠ” ì›ì†Œë“¤ì„ ê°–ëŠ” array ìƒì„± print(array1) . [ 3 6 9 12 15] . | . ",
    "url": "https://chaelist.github.io/docs/numpy/numpy_basics/#numpy-array-%EC%83%9D%EC%84%B1",
    "relUrl": "/docs/numpy/numpy_basics/#numpy-array-ìƒì„±"
  },"140": {
    "doc": "Numpy ê¸°ì´ˆ",
    "title": "Numpy Array ë³€ê²½",
    "content": "Data Type í™•ì¸ ë° ë³€ê²½ . | data type í™•ì¸í•˜ê¸° array1 = np.array([1,2,3]) print(array1.dtype) # 3ê°œì˜ elementê°€ ëª¨ë‘ intì´ë¯€ë¡œ, dtypeì„ í•˜ë©´ intë¼ê³  ë‚˜ì˜´ . int64 . | ë°ì´í„° ìœ í˜• í˜¼ì¬ëœ ë¦¬ìŠ¤íŠ¸ë¥¼ Arrayë¡œ ë³€ê²½ . | ì„œë¡œ ë‹¤ë¥¸ ë°ì´í„° ìœ í˜•ì´ ì„ì—¬ ìˆëŠ” ë¦¬ìŠ¤íŠ¸ë¥¼ arrayë¡œ ë³€ê²½í•  ê²½ìš°, ë°ì´í„° í¬ê¸°ê°€ ë” í° ë°ì´í„° íƒ€ì…ìœ¼ë¡œ í˜• ë³€í™˜ì´ ì¼ê´„ ì ìš©ëœë‹¤. | . list2 = [1, 2, 'test'] array2 = np.array(list2) print(array2) print(array2.dtype) ## Unicode ë¬¸ìì—´ë¡œ ì¼ê´„ ë³€í™˜ë¨ list3 = [1, 2, 3.0] array3 = np.array(list3) print(array3) print(array3.dtype) ## float íƒ€ì…ìœ¼ë¡œ ì¼ê´„ ë³€í™˜ë¨ . ['1' '2' 'test'] &lt;U21 [1. 2. 3.] float64 . | astype( ) : array ë‚´ elementë“¤ì„ ì›í•˜ëŠ” íƒ€ì…ìœ¼ë¡œ ë³€ê²½ array_int = np.array([1,2,3]) array_float = array_int.astype('float64') # int -&gt; float print(array_float, array_float.dtype) array_float1 = np.array([1.1, 2.1, 3.1]) array_int1 = array_float1.astype('int32') # float -&gt; int print(array_int1, array_int1.dtype) . [1. 2. 3.] float64 [1 2 3] int32 . | . tolist() . : numpy arrayë¥¼ listë¡œ ë°”ê¾¸ê¸° . array1 = np.array([2, 3, 5, 7]) print(type(array1), array1) list1 = array1.tolist() # tolist() ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ë©´ listë¡œ íƒ€ì…ì´ ë°”ë€œ print(type(list1), list1) . &lt;class 'numpy.ndarray'&gt; [2 3 5 7] &lt;class 'list'&gt; [2, 3, 5, 7] . reshape() . : numpy arrayì˜ shape ë³€ê²½. | reshape(m, n): mí–‰ nì—´ì§œë¦¬ arrayë¡œ ë³€í™˜í•˜ë¼ëŠ” ëœ». array1 = np.arange(10) print('array1:\\n', array1) array2 = array1.reshape(2, 5) # 2í–‰ 5ì—´ì§œë¦¬ arrayë¡œ ë³€í™˜ print('array2:\\n', array2) array3 = array1.reshape(5, 2) # 5í–‰ 2ì—´ì§œë¦¬ arrayë¡œ ë³€í™˜ print('array3:\\n', array3) . array1: [0 1 2 3 4 5 6 7 8 9] array2: [[0 1 2 3 4] [5 6 7 8 9]] array3: [[0 1] [2 3] [4 5] [6 7] [8 9]] . | reshape(m, -1): mí–‰, ê·¸ë¦¬ê³  ì—´ ìˆ˜ì€ ìë™ìœ¼ë¡œ ë§ì¶°ì„œ ë³€í™˜í•˜ë¼ëŠ” ëœ». | â€» ì¸ìë¡œ -1ë¥¼ ì ìš©í•˜ë©´, ê³ ì •ëœ í–‰/ì—´(ìˆ«ìê°€ assignëœ í–‰/ì—´)ì— ë§ê²Œ ìë™ìœ¼ë¡œ shape ë³€í™˜. | ë°˜ëŒ€ë¡œ, reshape(-1, n)ì´ë¼ê³  í•˜ë©´ nì—´, ê·¸ë¦¬ê³  ìë™ìœ¼ë¡œ ë§ì¶˜ í–‰ ìˆ˜ë¡œ ë³€í™˜ | reshape(1, -1): ì›ë³¸ ndarrayê°€ ì–´ë–¤ í˜•íƒœë¼ë„ (ex. 3ì°¨ì›ì´ë¼ë„), ë°˜ë“œì‹œ 1ê°œì˜ í–‰ì„ ê°–ëŠ” 2ì°¨ì› arrayë¡œ ë³€í™˜ë¨ | . array1 = np.arange(10) print(array1) array2 = array1.reshape(-1, 5) # ê³ ì •ëœ 5ê°œ ì—´ì— ë§ê²Œ, í–‰ì€ 2ê°œë¡œ ë³€í™˜ë¨ print('array2 shape:', array2.shape) array3 = array1.reshape(5, -1) # ê³ ì •ëœ 5ê°œ í–‰ì— ë§ê²Œ, ì—´ì€ 2ê°œë¡œ ë³€í™˜ë¨ print('array3 shape:', array3.shape) . [0 1 2 3 4 5 6 7 8 9] array2 shape: (2, 5) array3 shape: (5, 2) . | 3ì°¨ì› ì´ìƒìœ¼ë¡œë„ ë³€ê²½ ê°€ëŠ¥ . | ex) reshape((m, n, o))ë¼ê³  í•˜ë©´ 3ì°¨ì› arrayë¡œ ë³€ê²½ ê°€ëŠ¥ | . array1 = np.arange(8) array3d = array1.reshape((2,2,2)) # 3ì°¨ì› print('array3d:\\n', array3d) print('array3d: %dì°¨ì›' %array3d.ndim) . array3d: [[[0 1] [2 3]] [[4 5] [6 7]]] array3d: 3ì°¨ì› . | . Numpy Array ì •ë ¬ . : np.sort()ì™€ ndarray.sort() . | np.sort(): ì›ë³¸ í–‰ë ¬ì„ ë°”ê¾¸ì§€ ì•Šì€ ì±„, ìˆœì„œëŒ€ë¡œ ì •ë ¬ëœ ìƒˆë¡œìš´ í–‰ë ¬ì„ return . org_array = np.array([3,1,9,5]) # np.sort( )ë¡œ ì •ë ¬ print(np.sort(org_array)) # sortëœ ìƒˆë¡œìš´ arrayê°€ ë°˜í™˜ë¨. print(org_array) # ì›ë˜ì˜ í–‰ë ¬ì€ ë³€í•˜ì§€ ì•ŠìŒ. ## í–‰ë ¬ ìì²´ê°€ ë³€í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¯€ë¡œ, sort_array1 = np.sort(org_array) ì´ë ‡ê²Œ ì €ì¥í•´ë‘ê³  ì“°ëŠ” ê²ƒì´ ìœ ìš©í•˜ë‹¤. [1 3 5 9] [3 1 9 5] . | .sort(): ì›ë³¸ í–‰ë ¬ ìì²´ë¥¼ ì •ë ¬í•´ì¤Œ. # .sort( )ë¡œ ì •ë ¬ print(org_array.sort()) # ê²°ê³¼ë¡œ ì•„ë¬´ê²ƒë„ ë°˜í™˜ë˜ì§€ ì•ŠìŒ. ê·¸ì € ì›ë˜ í–‰ë ¬ì´ ë°”ë€ ê²ƒ ë¿ print(org_array) # .sort()ë¥¼ í•˜ê³  ë‚˜ë‹ˆ, ì›ë˜ì˜ í–‰ë ¬ì´ ë³€í•¨ . None [1 3 5 9] . | ë‚´ë¦¼ì°¨ìˆœìœ¼ë¡œ ì •ë ¬í•˜ê¸° ìœ„í•´ì„œëŠ” [::-1]ì„ ì ìš© sort_array1_desc = np.sort(org_array)[::-1] print(sort_array1_desc) org_array[::-1].sort() print(org_array) . [9 5 3 1] [9 5 3 1] . | 2ì°¨ì› ì´ìƒì˜ í–‰ë ¬: axis ì¶• ê°’ ì„¤ì •ì„ í†µí•´ row ë°©í–¥ / column ë°©í–¥ìœ¼ë¡œ ê°ê° ì •ë ¬ ê°€ëŠ¥ . array2d = np.array([[8,12], [7,1]]) sort_array2d_axis0 = np.sort(array2d, axis=0) ## axis=0ì´ë©´ row ë°©í–¥ print('row ë°©í–¥ ì •ë ¬:\\n', sort_array2d_axis0) sort_array2d_axis1 = np.sort(array2d, axis=1) ## axis=1ì´ë©´ column ë°©í–¥ print('column ë°©í–¥ ì •ë ¬:\\n', sort_array2d_axis1) . row ë°©í–¥ ì •ë ¬: [[ 7 1] [ 8 12]] column ë°©í–¥ ì •ë ¬: [[ 8 12] [ 1 7]] . | . np.argsort() . : í–‰ë ¬ì„ ì§ì ‘ ì •ë ¬í•˜ëŠ” ëŒ€ì‹ , ì—°ê²°ëœ ê°’ì˜ ìˆœì„œëŒ€ë¡œ indexë¥¼ ì •ë ¬í•´ì¤€ë‹¤. org_array = np.array([3,1,9,5]) sort_indices = np.argsort(org_array) # ì˜¤ë¦„ì°¨ìˆœ ì •ë ¬í•  ë•Œì˜ ì¸ë±ìŠ¤ ìˆœì„œë¥¼ sort_indicesì— ì €ì¥ print(type(sort_indices)) # argsortì˜ ê²°ê³¼ë¬¼ë„ numpy array íƒ€ì…. print('ì •ë ¬ëœ ì¸ë±ìŠ¤:', sort_indices) print('ì›ë³¸ í–‰ë ¬:', org_array) # ì›ë³¸ í–‰ë ¬ì—ëŠ” ì•„ë¬´ëŸ° ì˜í–¥ì´ ì—†ìŒ . &lt;class 'numpy.ndarray'&gt; ì •ë ¬ëœ ì¸ë±ìŠ¤: [1 0 3 2] ì›ë³¸ í–‰ë ¬: [3 1 9 5] . cf) ë‚´ë¦¼ì°¨ìˆœ . org_array = np.array([3,1,9,5]) sort_indices = np.argsort(org_array)[::-1] print(sort_indices) . [2 3 0 1] . *í™œìš© ì˜ˆì‹œ: argsort( )ë¥¼ ì´ìš©í•œ indexingì„ í™œìš©í•´ ì„±ì ìˆœìœ¼ë¡œ ì´ë¦„ ì¶œë ¥í•˜ê¸° . name_array = np.array(['John', 'Mike', 'Sarah', 'Kate', 'Samuel']) score_array = np.array([78, 95, 94, 98, 88]) sort_score_indices = np.argsort(score_array) # ì„±ì ì´ ë‚®ì€ ìˆœì„œëŒ€ë¡œ index ì •ë ¬ name_array[sort_score_indices] # indexingì„ í†µí•´ ì„±ì  ë‚®ì€ ìˆœëŒ€ë¡œ ì´ë¦„ ì¶œë ¥ . array(['John', 'Samuel', 'Sarah', 'Mike', 'Kate'], dtype='&lt;U6') . ",
    "url": "https://chaelist.github.io/docs/numpy/numpy_basics/#numpy-array-%EB%B3%80%EA%B2%BD",
    "relUrl": "/docs/numpy/numpy_basics/#numpy-array-ë³€ê²½"
  },"141": {
    "doc": "Numpy ê¸°ì´ˆ",
    "title": "Indexing &amp; Slicing",
    "content": "Indexing . | í‰ë²”í•œ indexing array1 = np.array([2, 3, 5, 7, 11]) array1[2] ## index 2, ì¦‰ 3ë²ˆì§¸ elementê°€ ë°˜í™˜ë¨ . 5 . | indexingì„ í†µí•œ array ë‚´ ê°’ ìˆ˜ì • array1[0] = 9 array1[2] = 0 print(array1) . [ 9 3 0 7 11] . | listë¡œ ì¸ë±ì‹± array1 = np.array([2, 3, 5, 7, 11]) array1[[1, 3, 4]] ## 1ë²ˆ, 3ë²ˆ, 4ë²ˆ indexì˜ ê°’ë“¤ì´ ì¶”ì¶œë¨ . array([ 3, 7, 11]) . | numpy arrayë¡œ numpy array ì¸ë±ì‹± array1 = np.array([2, 3, 5, 7, 11]) array2 = np.array([2, 1, 3]) array1[array2] . array([5, 3, 7]) . | 2ì°¨ì› array (í–‰ë ¬) ì¸ë±ì‹± A = np.arange(1, 10).reshape(3, 3) ## 1~10ê¹Œì§€ ì›ì†Œë¡œ ê°–ëŠ” arrayë¥¼ ë§Œë“¤ê³ , 3x3 í–‰ë ¬ë¡œ reshape print(A) print(A[0, 2]) # 0í–‰(ì²«ë²ˆì§¸), 2ì—´(ì„¸ë²ˆì§¸)ì— ìœ„ì¹˜í•œ ê°’ì¸ 3ë¥¼ ë°›ì•„ì˜¤ëŠ” ê²ƒ print(A[0][2]) # A[0, 2]ì™€ ë™ì¼í•œ ê²°ê³¼ . [[1 2 3] [4 5 6] [7 8 9]] 3 3 . +) ì¶”ê°€: 2ì°¨ì› arrayì—ì„œ 1ì°¨ì›ìœ¼ë¡œë§Œ indexing í•˜ë©´ 1ì°¨ì› ndarrayë¡œ ë³€í™˜ë¨ . print(A[0]) # ì²«ë²ˆì§¸ rowë¥¼ 1ì°¨ì› ndarrayë¡œ ë°˜í™˜ print(A[0].shape) # ì›ì†Œ 3ê°œì§œë¦¬ 1ì°¨ì› arrayê°€ ë¨ . [1 2 3] (3,) . | . Slicing . | ê¸°ë³¸ Slicing array1 = np.array([2, 3, 5, 7, 9, 11, 13]) print(array1[:3] print(array1[3:]) print(array1[:]) . [2, 3, 5] [ 7, 9, 11, 13] [ 2, 3, 5, 7, 9, 11, 13] . +) ì¶”ê°€: . array1 = np.array([2, 3, 5, 7, 9, 11, 13]) array1[2:6:2] # 3ë²ˆì§¸ element (index2) ë¶€í„° 7ë²ˆì§¸ element (index6) ê¹Œì§€, indexë¥¼ 2ì”© ê±´ë„ˆë›°ì–´ì„œ. # ê²°ê³¼ì ìœ¼ë¡œ index 2, 4, 6ì— í•´ë‹¹í•˜ëŠ” elementë“¤ì´ ì¶œë ¥ë¨. [ 5, 9, 13] . | 2ì°¨ì› array (í–‰ë ¬) slicing A = np.arange(1, 10).reshape(3, 3) print(A) print('\\nA[0:2, 0:2] \\n', A[0:2, 0:2]) print('\\nA[1:3, :] \\n', A[1:3, :]) print('\\nA[:2, 1:] \\n', A[:2, 1:]) . [[1 2 3] [4 5 6] [7 8 9]] A[0:2, 0:2] [[1 2] [4 5]] A[1:3, :] [[4 5 6] [7 8 9]] A[:2, 1:] [[2 3] [5 6]] . +) ì¶”ê°€: rowë‚˜ column ì¤‘ í•œ ìª½ì€ slicing, ë‹¤ë¥¸ ìª½ì€ ë‹¨ì¼ ê°’ indexingì„ ì ìš©í•´ë„ ë¨ . print(A[:2, 0]) # 1ë²ˆì¨°, 2ë²ˆì§¸ í–‰ì˜ 1ë²ˆì§¸ elementë¥¼ ê°ê° ì¶”ì¶œ print(A[:2, 0].shape) # ì›ì†Œ 2ê°œì§œë¦¬ 1ì°¨ì› arrayê°€ ë¨ . [1 4] (2,) . | . Boolean Indexing . : ì¡°ê±´ í•„í„°ë§ê³¼ ê²€ìƒ‰ì„ ë™ì‹œì— í•  ìˆ˜ ìˆëŠ” ì¸ë±ì‹± ë°©ì‹. ìì£¼ ì´ìš©ëœë‹¤. array1 = np.arange(1, 10) print(array1) print(array1[array1 &gt; 5]) # 5ë³´ë‹¤ í° ìˆ«ìë“¤ë§Œ indexingë¨ . [1 2 3 4 5 6 7 8 9] [6 7 8 9] . ",
    "url": "https://chaelist.github.io/docs/numpy/numpy_basics/#indexing--slicing",
    "relUrl": "/docs/numpy/numpy_basics/#indexing--slicing"
  },"142": {
    "doc": "Pandas",
    "title": "Pandas",
    "content": " ",
    "url": "https://chaelist.github.io/docs/pandas",
    "relUrl": "/docs/pandas"
  },"143": {
    "doc": "Pandas ê¸°ì´ˆ",
    "title": "Pandas ê¸°ì´ˆ",
    "content": ". | Pandas DataFrame ë§Œë“¤ê¸° . | From lists of lists, array of arrays, list of series | From dict of lists, dict of arrays, dict of series | From list of dicts | . | DataFrame ë³µì‚¬ë³¸ ë§Œë“¤ê¸° | CSV, Excel ë°ì´í„° ì·¨ê¸‰ . | CSV íŒŒì¼ ì½ê³  ì“°ê¸° | Excel íŒŒì¼ ì½ê³  ì“°ê¸° | . | Indexing &amp; Slicing . | Indexing | Slicing | ì¡°ê±´ìœ¼ë¡œ Indexing | ìˆ«ì ìœ„ì¹˜ ê¸°ë°˜ Indexing &amp; Slicing | . | ì¡°ê±´ indexing: Query í•¨ìˆ˜ | pd.set_option() | . *Pandas: ë°ì´í„° ë¶„ì„ ëª©ì ìœ¼ë¡œ ë„ë¦¬ ì‚¬ìš©ë˜ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ . ",
    "url": "https://chaelist.github.io/docs/pandas/pandas_basics/",
    "relUrl": "/docs/pandas/pandas_basics/"
  },"144": {
    "doc": "Pandas ê¸°ì´ˆ",
    "title": "Pandas DataFrame ë§Œë“¤ê¸°",
    "content": "From lists of lists, array of arrays, list of series . : 2ì°¨ì› ë¦¬ìŠ¤íŠ¸, 2ì°¨ì› numpy array, pandas Seriesë¥¼ ë‹´ê³  ìˆëŠ” ë¦¬ìŠ¤íŠ¸ë¡œ DataFrameì„ ë§Œë“¤ê¸° . import pandas as pd # importí•´ì•¼ ì‚¬ìš© ê°€ëŠ¥; ë³´í†µ pdë¡œ ì¤„ì—¬ì„œ import # 2ì°¨ì› ë¦¬ìŠ¤íŠ¸ two_dimensional_list = [['Emily', 50, 86], ['Abby', 89, 31], ['Cornelia', 68, 91], ['Kai', 88, 75]] # 2ì°¨ì› numpy array two_dimensional_array = np.array(two_dimensional_list) # pandas seriesë¥¼ ë‹´ê³  ìˆëŠ” ë¦¬ìŠ¤íŠ¸ list_of_series = [ pd.Series(['Emily', 50, 86]), pd.Series(['Abby', 89, 31]), pd.Series(['Cornelia', 68, 91]), pd.Series(['Kai', 88, 75]) ] # ì•„ë˜ ì…‹ì€ ëª¨ë‘ ë™ì¼í•œ ê²°ê³¼ ë°˜í™˜ df1 = pd.DataFrame(two_dimensional_list) df2 = pd.DataFrame(two_dimensional_array) df3 = pd.DataFrame(list_of_series) df1 ## ë”°ë¡œ columnê³¼ row(index)ì— ëŒ€í•œ ì„¤ì •ì´ ì—†ìœ¼ë©´ ê·¸ëƒ¥ 0, 1, 2, ... ìˆœì„œë¡œ ê°’ì´ ë§¤ê²¨ì§ . | Â  | 0 | 1 | 2 | . | 0 | Emily | 50 | 86 | . | 1 | Abby | 89 | 31 | . | 2 | Cornelia | 68 | 91 | . | 3 | Kai | 88 | 75 | . +) ì¶”ê°€: ì¹¼ëŸ¼ëª…ê³¼ indexëª…ì„ ì§€ì–´ì£¼ê¸° . two_dimensional_list = [['Emily', 50, 86], ['Abby', 89, 31], ['Cornelia', 68, 91], ['Kai', 88, 75]] my_df = pd.DataFrame(two_dimensional_list, columns=['name', 'french_score', 'math_score'], index=['a', 'b', 'c', 'd']) my_df . | Â  | name | french_score | math_score | . | a | Emily | 50 | 86 | . | b | Abby | 89 | 31 | . | c | Cornelia | 68 | 91 | . | d | Kai | 88 | 75 | . From dict of lists, dict of arrays, dict of series . : íŒŒì´ì¬ ì‚¬ì „(dictionary)ìœ¼ë¡œ DataFrame ë§Œë“¤ê¸° . | ì‚¬ì „ì˜ keyë¡œëŠ” column ì´ë¦„ì„ ì“°ê³ , ê·¸ columnì— í•´ë‹¹í•˜ëŠ” ë¦¬ìŠ¤íŠ¸, numpy array, í˜¹ì€ pandas Seriesë¥¼ ì‚¬ì „ì˜ valueë¡œ ë„£ì–´ì£¼ë©´ ëœë‹¤ | . names = ['Emily', 'Abby', 'Cornelia', 'Kai'] french_scores = [50, 89, 68, 88] math_scores = [86, 31, 91, 75] dict1 = { 'name': names, 'french_score': french_scores, 'math_score': math_scores } dict2 = { 'name': np.array(names), 'french_score': np.array(french_scores), 'math_score': np.array(math_scores) } dict3 = { 'name': pd.Series(names), 'french_score': pd.Series(french_scores), 'math_score': pd.Series(math_scores) } # ì•„ë˜ ì…‹ì€ ëª¨ë‘ ë™ì¼í•œ ê²°ê³¼ ë°˜í™˜ df1 = pd.DataFrame(dict1) df2 = pd.DataFrame(dict2) df3 = pd.DataFrame(dict3) df1 . | Â  | name | french_score | math_score | . | 0 | Emily | 50 | 86 | . | 1 | Abby | 89 | 31 | . | 2 | Cornelia | 68 | 91 | . | 3 | Kai | 88 | 75 | . Â  . +) DataFrame.from_dict() : from_dict()ë¥¼ ì´ìš©í•˜ë©´ dictionary í˜•íƒœì˜ ë°ì´í„°ë¥¼ row ë°©í–¥ì´ë‚˜ column ë°©í–¥ ì¤‘ ì–´ë–¤ì‹ìœ¼ë¡œë“  DataFrameìœ¼ë¡œ ë§Œë“¤ ìˆ˜ ìˆë‹¤. | (default) dictionaryì˜ keyë¥¼ columnìœ¼ë¡œ: data = { 'name': ['Emily', 'Abby', 'Cornelia', 'Kai'], 'french_score': [50, 89, 68, 88], 'math_score': [86, 31, 91, 75] } pd.DataFrame.from_dict(data) # pd.DataFrame(data)ê³¼ ë™ì¼í•œ ê²°ê³¼ . | Â  | name | french_score | math_score | . | 0 | Emily | 50 | 86 | . | 1 | Abby | 89 | 31 | . | 2 | Cornelia | 68 | 91 | . | 3 | Kai | 88 | 75 | . | dictionaryì˜ keyë¥¼ rowë¡œ: data = { 'row1': ['Emily', 50, 86], 'row2': ['Abby', 89, 31], 'row3': ['Cornelia', 68, 91], 'row4': ['Kai', 88, 75] } pd.DataFrame.from_dict(data, orient='index', columns=['name', 'french_score', 'math_score']) . | Â  | name | french_score | math_score | . | row1 | Emily | 50 | 86 | . | row2 | Abby | 89 | 31 | . | row3 | Cornelia | 68 | 91 | . | row4 | Kai | 88 | 75 | . | . From list of dicts . : ì‚¬ì „ì´ ë‹´ê¸´ ë¦¬ìŠ¤íŠ¸ë¡œ DataFrame ë§Œë“¤ê¸° . my_list = [ {'name': 'Emily', 'french_score': 50, 'math_score': 86}, {'name': 'Abby', 'french_score': 89, 'math_score': 31}, {'name': 'Cornelia', 'french_score': 68, 'math_score': 91}, {'name': 'Kai', 'french_score': 88, 'math_score': 75} ] df = pd.DataFrame(my_list) df . | Â  | name | french_score | math_score | . | 0 | Emily | 50 | 86 | . | 1 | Abby | 89 | 31 | . | 2 | Cornelia | 68 | 91 | . | 3 | Kai | 88 | 75 | . â€» ì•Œì•„ë‘˜ ì‚¬ì‹¤: . | DictionaryëŠ” dataì˜ ìˆœì„œê°€ ì—†ê¸° ë•Œë¬¸ì— ì…ë ¥ ìˆœì„œëŒ€ë¡œ columnì´ ë§Œë“¤ì–´ì§€ëŠ” ê²ƒì€ ì•„ë‹ˆë‹¤. | ë°˜ë©´, lists, arrays, seriesëŠ” ìˆœì„œê°€ ìˆëŠ” ìë£Œêµ¬ì¡°ì´ê¸° ë•Œë¬¸ì—, ì…ë ¥í•œ ìˆœì„œëŒ€ë¡œ columnì„ ë§Œë“¤ì–´ ì¤€ë‹¤. | . ",
    "url": "https://chaelist.github.io/docs/pandas/pandas_basics/#pandas-dataframe-%EB%A7%8C%EB%93%A4%EA%B8%B0",
    "relUrl": "/docs/pandas/pandas_basics/#pandas-dataframe-ë§Œë“¤ê¸°"
  },"145": {
    "doc": "Pandas ê¸°ì´ˆ",
    "title": "DataFrame ë³µì‚¬ë³¸ ë§Œë“¤ê¸°",
    "content": ". | a = b ë°©ì‹: ì›ë³¸ ë°ì´í„°ê°€ ë³€í•˜ë©´ ë˜‘ê°™ì´ ë³€í•˜ëŠ” â€˜ê¹Šì€ ë³µì‚¬â€™ | df.copy() ë°©ì‹: ë³µì‚¬ ë‹¹ì‹œì˜ ìƒíƒœë§Œ ë³µì‚¬ë˜ëŠ” â€˜ì–•ì€ ë³µì‚¬â€™ | . a = pd.DataFrame([['Kim', 23], ['Lee', 12], ['Chung', 28]], columns = ['Name', 'Age']) a . | Â  | Name | Age | . | 0 | Kim | 23 | . | 1 | Lee | 12 | . | 2 | Chung | 28 | . # ë‘ ê°€ì§€ ë°©ì‹ìœ¼ë¡œ ê°ê° ë³µì‚¬í•˜ê¸° . just_copy = a ## a = b ë°©ì‹ì˜ 'ê¹Šì€ ë³µì‚¬' pandas_copy = a.copy() ## ë³µì‚¬ ë‹¹ì‹œì˜ ìƒíƒœë§Œ ë³µì‚¬í•˜ëŠ” 'ì–•ì€ ë³µì‚¬' . # ì›ë³¸ ë°ì´í„° ë³€ê²½í•´ë³´ê¸° . print(a, '\\n') print(just_copy, '\\n') # ë‹¨ìˆœíˆ a = b ë°©ì‹ìœ¼ë¡œ í•œ 'ê¹Šì€ ë³µì‚¬'ì˜ ê²½ìš°, ì›ë°ì´í„°ê°€ ë³€í•˜ë©´ í•¨ê»˜ ë”°ë¼ ë³€í•œë‹¤ print(pandas_copy) # aë¥¼ ë°”ê¾¸ê¸° ì „ ìƒíƒœë¥¼ ë³´ì¡´í•˜ë ¤ë©´, ì´ë ‡ê²Œ df.copy() ë°©ì‹ì„ ì¨ì•¼ í•œë‹¤ . Name Age 0 ë³€ê²½ë¨! 23 1 ë³€ê²½ë¨! 12 2 ë³€ê²½ë¨! 28 Name Age 0 ë³€ê²½ë¨! 23 1 ë³€ê²½ë¨! 12 2 ë³€ê²½ë¨! 28 Name Age 0 Kim 23 1 Lee 12 2 Chung 28 . ",
    "url": "https://chaelist.github.io/docs/pandas/pandas_basics/#dataframe-%EB%B3%B5%EC%82%AC%EB%B3%B8-%EB%A7%8C%EB%93%A4%EA%B8%B0",
    "relUrl": "/docs/pandas/pandas_basics/#dataframe-ë³µì‚¬ë³¸-ë§Œë“¤ê¸°"
  },"146": {
    "doc": "Pandas ê¸°ì´ˆ",
    "title": "CSV, Excel ë°ì´í„° ì·¨ê¸‰",
    "content": "CSV íŒŒì¼ ì½ê³  ì“°ê¸° . | CSV íŒŒì¼ì„ DataFrameìœ¼ë¡œ ì½ì–´ì˜¤ê¸° df = pd.read_csv('ê²½ë¡œ/íŒŒì¼ëª….csv', header=1, index_col=0, encoding='utf-8') # ê°™ì€ í´ë” ì•ˆì— ìˆëŠ” íŒŒì¼ì€ 'íŒŒì¼ëª….csv'ë¼ê³ ë§Œ ì¨ì¤˜ë„ ë¨ . | header=1ì´ë¼ê³  í•˜ë©´ íŒŒì¼ì˜ ë‘ë²ˆì§¸ í–‰(index_num=1)ì´ headerë¡œ ì„¤ì •ë¨ (ì²«ë²ˆì§¸ í–‰ì€ ì½ì–´ì˜¤ì§€ ì•ŠìŒ) . | header=Noneì´ë¼ê³  í•˜ë©´ ì²« í–‰ì´ headerê°€ ë˜ëŠ” ëŒ€ì‹  0ë¶€í„° ì‹œì‘í•˜ëŠ” ìˆ«ìë¡œ headerê°€ ì±„ì›Œì§ (default: csv íŒŒì¼ì˜ ì²« í–‰ì´ headerë¡œ ë“¤ì–´ê°) | . | index_col=0ì´ë¼ê³  í•˜ë©´ ì²« ë²ˆì§¸ ì—´ì´ indexë¡œ ì„¤ì •ë¨ (default: None) . | index_col='ì—´ì´ë¦„' ì´ëŸ° ì‹ìœ¼ë¡œ ì—´ì˜ ì´ë¦„ì„ ì§ì ‘ strìœ¼ë¡œ ë„£ì–´ì¤˜ë„ ëœë‹¤ | . | í•œê¸€ ë“± ê¸€ìê°€ ê¹¨ì§ˆ ë•ŒëŠ” encoding='utf-8'ê³¼ ê°™ì´ ì¸ì½”ë”© ë°©ì‹ì„ ì„¤ì •í•´ì¤„ ìˆ˜ë„ ìˆë‹¤ | . | DataFrameì„ CSV íŒŒì¼ë¡œ ë‚´ë³´ë‚´ê¸° df.to_csv('ê²½ë¡œ/íŒŒì¼ëª….cvs', index=False) # ê°™ì€ í´ë” ì•ˆìœ¼ë¡œ ì €ì¥í•˜ë ¤ë©´ 'íŒŒì¼ëª….csv'ë¼ê³ ë§Œ ì¨ì¤˜ë„ ë¨ . | ì½ì–´ì˜¬ ë•Œì™€ ë§ˆì°¬ê°€ì§€ë¡œ, encoding='utf-8'ê³¼ ê°™ì€ ì˜µì…˜ë„ ì§€ì • ê°€ëŠ¥ | index=Falseë¼ê³  í•˜ë©´ indexëŠ” ì €ì¥ë˜ì§€ ì•ŠìŒ (ë³´í†µ indexëŠ” 0ë¶€í„° ì‹œì‘í•˜ëŠ” ìˆ«ìë¡œ ì±„ì›Œì§€ê¸°ì—, êµ³ì´ ì €ì¥í•˜ì§€ ì•Šì•„ë„ ë˜ëŠ” ê²½ìš°ê°€ ë§ìŒ) | . | . Excel íŒŒì¼ ì½ê³  ì“°ê¸° . | Excel íŒŒì¼ì„ DataFrameìœ¼ë¡œ ì½ì–´ì˜¤ê¸° df = pd.read_excel('íŒŒì¼ëª….xlsx', sheet_name='Sheet2', usecols=\"C:X\", header=2, encoding='utf-8') # index_col=1ì´ë‚˜ header=2 ê°™ì€ ë°©ì‹ìœ¼ë¡œ ì–´ëŠ ì—´ì„ indexë¡œ ê°€ì ¸ê°ˆê±´ì§€, ì–´ëŠ í–‰ì„ headerë¡œ ì‚¼ì„ê±´ì§€ ì§€ì • ê°€ëŠ¥. # usecols='C:X' í˜¹ì€ usecols='A,C,D' ì´ëŸ° ì‹ìœ¼ë¡œí•´ì„œ ì‚¬ìš©í•  ì—´ ì§€ì • ê°€ëŠ¥ . | sheet_name='Sheet2'ëŠ” íŒŒì¼ì—ì„œ Sheet2ë§Œ ê°€ì ¸ì˜¤ê² ë‹¤ëŠ” ì˜ë¯¸. | sheet_nameì—ëŠ” int/str/listë¥¼ ë„£ì–´ì¤„ ìˆ˜ ìˆë‹¤ (ex. sheet_name=0, sheet_name=[0, 1, \"Sheet5\"]) | . | usecols='C:XëŠ” C~Xê¹Œì§€ì˜ ì—´ì„ ê°€ì ¸ì˜¤ê² ë‹¤ëŠ” ì˜ë¯¸. (C, Xë„ í¬í•¨) . | usecols='A,C,D'ì´ë ‡ê²Œ ì§€ì •í•˜ëŠ” ê²ƒë„ ê°€ëŠ¥. (A, C, D ì—´ë§Œ ê°€ì ¸ì˜¤ê² ë‹¤ëŠ” ì˜ë¯¸) | . | index_colì´ë‚˜ header, encodingì€ read_csvì—ì„œì™€ ê°™ì€ ê¸°ëŠ¥ | . | DataFrameì„ Excel íŒŒì¼ë¡œ ë‚´ë³´ë‚´ê¸° df.to_excel('íŒŒì¼ëª….xlsx') . | encoding='utf-8'ì´ë‚˜ index=Falseì™€ ê°™ì€ ì˜µì…˜ ì¶”ê°€ ê°€ëŠ¥ | . | ì—¬ëŸ¬ ê°œì˜ DataFrameì„ ì„œë¡œ ë‹¤ë¥¸ ì‹œíŠ¸ë¡œ í•´ì„œ Excel íŒŒì¼ë¡œ ì €ì¥ writer = pd.ExcelWriter('íŒŒì¼ëª….xlsx', engine='xlsxwriter') # Write each dataframe to a different worksheet. df1.to_excel(writer, sheet_name='df1', encoding='utf-8') # encoding='utf-8'ì€ ì˜µì…˜. df2.to_excel(writer, sheet_name='df2', encoding='utf-8') df3.to_excel(writer, sheet_name='df3', encoding='utf-8') # Close the Pandas Excel writer and output the Excel file. writer.save() . | . ",
    "url": "https://chaelist.github.io/docs/pandas/pandas_basics/#csv-excel-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%B7%A8%EA%B8%89",
    "relUrl": "/docs/pandas/pandas_basics/#csv-excel-ë°ì´í„°-ì·¨ê¸‰"
  },"147": {
    "doc": "Pandas ê¸°ì´ˆ",
    "title": "Indexing &amp; Slicing",
    "content": "iphone_df = pd.read_csv('data/iphone.csv', index_col=0) ## ë°ì´í„° ì¶œì²˜: codeit iphone_df . | Â  | ì¶œì‹œì¼ | ë””ìŠ¤í”Œë ˆì´ | ë©”ëª¨ë¦¬ | ì¶œì‹œ ë²„ì „ | Face ID | . | iPhone 7 | 2016-09-16 | 4.7 | 2GB | iOS 10.0 | No | . | iPhone 7 Plus | 2016-09-16 | 5.5 | 3GB | iOS 10.0 | No | . | iPhone 8 | 2017-09-22 | 4.7 | 2GB | iOS 11.0 | No | . | iPhone 8 Plus | 2017-09-22 | 5.5 | 3GB | iOS 11.0 | No | . | iPhone X | 2017-11-03 | 5.8 | 3GB | iOS 11.1 | Yes | . | iPhone XS | 2018-09-21 | 5.8 | 4GB | iOS 12.0 | Yes | . | iPhone XS Max | 2018-09-21 | 6.5 | 4GB | iOS 12.0 | Yes | . Indexing . : .loc[í–‰ ë°©í–¥, ì—´ ë°©í–¥] . | í–‰ ë°©í–¥ &amp; ì—´ ë°©í–¥ ## iPhone Xì˜ ë©”ëª¨ë¦¬ ê°€ì ¸ì˜¤ê¸° iphone_df.loc['iPhone X', 'ë©”ëª¨ë¦¬'] . '3GB' . | í–‰ ë°©í–¥ ## iPhone Xì— ëŒ€í•œ ëª¨ë“  ì •ë³´ ê°€ì ¸ì˜¤ê¸° iphone_df.loc['iPhone X', :] ## iphone_df.loc['iPhone X'] ì´ë ‡ê²Œ ì¨ë„ ë™ì¼í•œ ê²°ê³¼. ì¶œì‹œì¼ 2017-11-03 ë””ìŠ¤í”Œë ˆì´ 5.8 ë©”ëª¨ë¦¬ 3GB ì¶œì‹œ ë²„ì „ iOS 11.1 Face ID Yes Name: iPhone X, dtype: object . â€» í•˜ë‚˜ì˜ í–‰/ì—´ë§Œ ì¶”ì¶œí•˜ë©´ â€˜Pandas Seriesâ€™ í˜•íƒœë¡œ ê°€ì ¸ì™€ì§ . type(iphone_df.loc['iPhone X']) . pandas.core.series.Series . | í–‰ ë°©í–¥ - 2ê°œ ì´ìƒ # iPhone Xì™€ iPhone 8ì— ëŒ€í•œ ì •ë³´ ê°€ì ¸ì˜¤ê¸° iphone_df.loc[['iPhone X', 'iPhone 8']] . | Â  | ì¶œì‹œì¼ | ë””ìŠ¤í”Œë ˆì´ | ë©”ëª¨ë¦¬ | ì¶œì‹œ ë²„ì „ | Face ID | . | iPhone X | 2017-11-03 | 5.8 | 3GB | iOS 11.1 | Yes | . | iPhone 8 | 2017-09-22 | 4.7 | 2GB | iOS 11.0 | No | . â€» ë‘ ê°œ ì´ìƒì˜ í–‰/ì—´ì„ ì¶”ì¶œí•˜ë©´ â€˜Pandas DataFrameâ€™ í˜•íƒœë¡œ ê°€ì ¸ì™€ì§ . type(iphone_df.loc[['iPhone X', 'iPhone 8']]) . pandas.core.frame.DataFrame . | ì—´ ë°©í–¥ # ê° iPhone ëª¨ë¸ì— ëŒ€í•œ ì¶œì‹œì¼ ì •ë³´ë§Œ ë‹¤ ê°€ì ¸ì˜¤ê¸° iphone_df.loc[:,'ì¶œì‹œì¼'] ## iphone_df['ì¶œì‹œì¼'] ì´ë ‡ê²Œ ì¨ë„ ë™ì¼í•œ ê²°ê³¼. (loc ì‚¬ìš©X) . iPhone 7 2016-09-16 iPhone 7 Plus 2016-09-16 iPhone 8 2017-09-22 iPhone 8 Plus 2017-09-22 iPhone X 2017-11-03 iPhone XS 2018-09-21 iPhone XS Max 2018-09-21 Name: ì¶œì‹œì¼, dtype: object . *ì‚¬ì‹¤ ì—´ ë°©í–¥ indexingì€ iphone_df['ì¶œì‹œì¼']ê³¼ ê°™ì´ locì„ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ê²Œ ë” ê°„ë‹¨í•˜ë‹¤ . | ì—´ ë°©í–¥ - 2ê°œ ì´ìƒ # ì¶œì‹œì¼ê³¼ ë””ìŠ¤í”Œë ˆì´ ì •ë³´ ê°€ì ¸ì˜¤ê¸° iphone_df[['ì¶œì‹œì¼', 'ë””ìŠ¤í”Œë ˆì´']] ## iphone_df.loc[:,['ì¶œì‹œì¼', 'ë””ìŠ¤í”Œë ˆì´']] ì´ë ‡ê²Œ ì¨ë„ ë™ì¼í•œ ê²°ê³¼. | Â  | ì¶œì‹œì¼ | ë””ìŠ¤í”Œë ˆì´ | . | iPhone 7 | 2016-09-16 | 4.7 | . | iPhone 7 Plus | 2016-09-16 | 5.5 | . | iPhone 8 | 2017-09-22 | 4.7 | . | iPhone 8 Plus | 2017-09-22 | 5.5 | . | iPhone X | 2017-11-03 | 5.8 | . | iPhone XS | 2018-09-21 | 5.8 | . | iPhone XS Max | 2018-09-21 | 6.5 | . | . Slicing . iphone_df . | Â  | ì¶œì‹œì¼ | ë””ìŠ¤í”Œë ˆì´ | ë©”ëª¨ë¦¬ | ì¶œì‹œ ë²„ì „ | Face ID | . | iPhone 7 | 2016-09-16 | 4.7 | 2GB | iOS 10.0 | No | . | iPhone 7 Plus | 2016-09-16 | 5.5 | 3GB | iOS 10.0 | No | . | iPhone 8 | 2017-09-22 | 4.7 | 2GB | iOS 11.0 | No | . | iPhone 8 Plus | 2017-09-22 | 5.5 | 3GB | iOS 11.0 | No | . | iPhone X | 2017-11-03 | 5.8 | 3GB | iOS 11.1 | Yes | . | iPhone XS | 2018-09-21 | 5.8 | 4GB | iOS 12.0 | Yes | . | iPhone XS Max | 2018-09-21 | 6.5 | 4GB | iOS 12.0 | Yes | . | í–‰ ë°©í–¥ # iPhone 8ë¶€í„° iPhone XSê¹Œì§€ì˜ ëª¨ë“  row ë°˜í™˜ iphone_df.loc['iPhone 8':'iPhone XS'] . | Â  | ì¶œì‹œì¼ | ë””ìŠ¤í”Œë ˆì´ | ë©”ëª¨ë¦¬ | ì¶œì‹œ ë²„ì „ | Face ID | . | iPhone 8 | 2017-09-22 | 4.7 | 2GB | iOS 11.0 | No | . | iPhone 8 Plus | 2017-09-22 | 5.5 | 3GB | iOS 11.0 | No | . | iPhone X | 2017-11-03 | 5.8 | 3GB | iOS 11.1 | Yes | . | iPhone XS | 2018-09-21 | 5.8 | 4GB | iOS 12.0 | Yes | . | ì—´ ë°©í–¥ # ë©”ëª¨ë¦¬ë¶€í„° Face IDê¹Œì§€ì˜ column ë°˜í™˜ iphone_df.loc[:, 'ë©”ëª¨ë¦¬':'Face ID'] . | Â  | ë©”ëª¨ë¦¬ | ì¶œì‹œ ë²„ì „ | Face ID | . | iPhone 7 | 2GB | iOS 10.0 | No | . | iPhone 7 Plus | 3GB | iOS 10.0 | No | . | iPhone 8 | 2GB | iOS 11.0 | No | . | iPhone 8 Plus | 3GB | iOS 11.0 | No | . | iPhone X | 3GB | iOS 11.1 | Yes | . | iPhone XS | 4GB | iOS 12.0 | Yes | . | iPhone XS Max | 4GB | iOS 12.0 | Yes | . â€» ì£¼ì˜! indexingê³¼ ë‹¤ë¥´ê²Œ, slicingì€ locì„ ì•ˆì“°ê³  ì§ì ‘í•  ìˆ˜ëŠ” ì—†ìŒ! . # loc ì•ˆì“°ê³  columnì„ slicingí•˜ë ¤ê³  í•˜ë©´ ì•„ë˜ì™€ ê°™ì´ ì´ìƒí•˜ê²Œ ëœ¬ë‹¤ iphone_df['ë©”ëª¨ë¦¬':'Face ID'] . | ì¶œì‹œì¼ | ë””ìŠ¤í”Œë ˆì´ | ë©”ëª¨ë¦¬ | ì¶œì‹œ ë²„ì „ | Face ID | . | í–‰ ë°©í–¥ &amp; ì—´ ë°©í–¥ ## row, column ëª¨ë‘ slicingí•˜ê¸° iphone_df.loc['iPhone 7':'iPhone X', 'ë©”ëª¨ë¦¬':'Face ID'] #ìˆœì„œëŠ” ë¬´ì¡°ê±´ row ë‹¤ìŒ column . | Â  | ë©”ëª¨ë¦¬ | ì¶œì‹œ ë²„ì „ | Face ID | . | iPhone 7 | 2GB | iOS 10.0 | No | . | iPhone 7 Plus | 3GB | iOS 10.0 | No | . | iPhone 8 | 2GB | iOS 11.0 | No | . | iPhone 8 Plus | 3GB | iOS 11.0 | No | . | iPhone X | 3GB | iOS 11.1 | Yes | . | . ì¡°ê±´ìœ¼ë¡œ Indexing . iphone_df . | Â  | ì¶œì‹œì¼ | ë””ìŠ¤í”Œë ˆì´ | ë©”ëª¨ë¦¬ | ì¶œì‹œ ë²„ì „ | Face ID | . | iPhone 7 | 2016-09-16 | 4.7 | 2GB | iOS 10.0 | No | . | iPhone 7 Plus | 2016-09-16 | 5.5 | 3GB | iOS 10.0 | No | . | iPhone 8 | 2017-09-22 | 4.7 | 2GB | iOS 11.0 | No | . | iPhone 8 Plus | 2017-09-22 | 5.5 | 3GB | iOS 11.0 | No | . | iPhone X | 2017-11-03 | 5.8 | 3GB | iOS 11.1 | Yes | . | iPhone XS | 2018-09-21 | 5.8 | 4GB | iOS 12.0 | Yes | . | iPhone XS Max | 2018-09-21 | 6.5 | 4GB | iOS 12.0 | Yes | . | ë””ìŠ¤í”Œë ˆì´ê°€ 5ì¸ì¹˜ ì´ìƒì¸ ìŠ¤ë§ˆíŠ¸í° ì •ë³´ë§Œ ì¶”ì¶œí•˜ê¸° iphone_df.loc[iphone_df['ë””ìŠ¤í”Œë ˆì´'] &gt; 5] . | Â  | ì¶œì‹œì¼ | ë””ìŠ¤í”Œë ˆì´ | ë©”ëª¨ë¦¬ | ì¶œì‹œ ë²„ì „ | Face ID | . | iPhone 7 Plus | 2016-09-16 | 5.5 | 3GB | iOS 10.0 | No | . | iPhone 8 Plus | 2017-09-22 | 5.5 | 3GB | iOS 11.0 | No | . | iPhone X | 2017-11-03 | 5.8 | 3GB | iOS 11.1 | Yes | . | iPhone XS | 2018-09-21 | 5.8 | 4GB | iOS 12.0 | Yes | . | iPhone XS Max | 2018-09-21 | 6.5 | 4GB | iOS 12.0 | Yes | . | FaceIDê°€ ê°€ëŠ¥í•œ ìŠ¤ë§ˆíŠ¸í° ì •ë³´ë§Œ ì¶”ì¶œ iphone_df.loc[iphone_df['Face ID'] == 'Yes'] . | Â  | ì¶œì‹œì¼ | ë””ìŠ¤í”Œë ˆì´ | ë©”ëª¨ë¦¬ | ì¶œì‹œ ë²„ì „ | Face ID | . | iPhone X | 2017-11-03 | 5.8 | 3GB | iOS 11.1 | Yes | . | iPhone XS | 2018-09-21 | 5.8 | 4GB | iOS 12.0 | Yes | . | iPhone XS Max | 2018-09-21 | 6.5 | 4GB | iOS 12.0 | Yes | . | ë””ìŠ¤í”Œë ˆì´ê°€ 5ì¸ì¹˜ ì´ìƒ â€œANDâ€ FaceIDê°€ ê°€ëŠ¥í•œ ìŠ¤ë§ˆíŠ¸í° ì •ë³´ ì¶”ì¶œ condition = (iphone_df['ë””ìŠ¤í”Œë ˆì´'] &gt; 5) &amp; (iphone_df['Face ID'] == 'Yes') iphone_df[condition] # iphone_df.loc[condition]ê³¼ ë™ì¼í•œ ê²°ê³¼ . | Â  | ì¶œì‹œì¼ | ë””ìŠ¤í”Œë ˆì´ | ë©”ëª¨ë¦¬ | ì¶œì‹œ ë²„ì „ | Face ID | . | iPhone X | 2017-11-03 | 5.8 | 3GB | iOS 11.1 | Yes | . | iPhone XS | 2018-09-21 | 5.8 | 4GB | iOS 12.0 | Yes | . | iPhone XS Max | 2018-09-21 | 6.5 | 4GB | iOS 12.0 | Yes | . | ë””ìŠ¤í”Œë ˆì´ê°€ 5ì¸ì¹˜ ì´ìƒ â€œORâ€ FaceIDê°€ ê°€ëŠ¥í•œ ìŠ¤ë§ˆíŠ¸í° ì •ë³´ ì¶”ì¶œ condition = (iphone_df['ë””ìŠ¤í”Œë ˆì´'] &gt; 5) | (iphone_df['Face ID'] == 'Yes') iphone_df[condition] . | Â  | ì¶œì‹œì¼ | ë””ìŠ¤í”Œë ˆì´ | ë©”ëª¨ë¦¬ | ì¶œì‹œ ë²„ì „ | Face ID | . | iPhone 7 Plus | 2016-09-16 | 5.5 | 3GB | iOS 10.0 | No | . | iPhone 8 Plus | 2017-09-22 | 5.5 | 3GB | iOS 11.0 | No | . | iPhone X | 2017-11-03 | 5.8 | 3GB | iOS 11.1 | Yes | . | iPhone XS | 2018-09-21 | 5.8 | 4GB | iOS 12.0 | Yes | . | iPhone XS Max | 2018-09-21 | 6.5 | 4GB | iOS 12.0 | Yes | . | . ìˆ«ì ìœ„ì¹˜ ê¸°ë°˜ Indexing &amp; Slicing . : .iloc[í–‰ ë°©í–¥, ì—´ ë°©í–¥] . â€» iloc: integer + location. ìˆ«ì ê¸°ë°˜ìœ¼ë¡œ ìœ„ì¹˜ì— ì ‘ê·¼í•´ indexingí•  ë•ŒëŠ” iloc ì‚¬ìš© . iphone_df . | Â  | ì¶œì‹œì¼ | ë””ìŠ¤í”Œë ˆì´ | ë©”ëª¨ë¦¬ | ì¶œì‹œ ë²„ì „ | Face ID | . | iPhone 7 | 2016-09-16 | 4.7 | 2GB | iOS 10.0 | No | . | iPhone 7 Plus | 2016-09-16 | 5.5 | 3GB | iOS 10.0 | No | . | iPhone 8 | 2017-09-22 | 4.7 | 2GB | iOS 11.0 | No | . | iPhone 8 Plus | 2017-09-22 | 5.5 | 3GB | iOS 11.0 | No | . | iPhone X | 2017-11-03 | 5.8 | 3GB | iOS 11.1 | Yes | . | iPhone XS | 2018-09-21 | 5.8 | 4GB | iOS 12.0 | Yes | . | iPhone XS Max | 2018-09-21 | 6.5 | 4GB | iOS 12.0 | Yes | . | 3ë²ˆì§¸ í–‰ &amp; 4ë²ˆì§¸ ì—´ì˜ ê°’ ì¶”ì¶œ iphone_df.iloc[2, 4] . 'No' . | 2ë²ˆì§¸,4ë²ˆì§¸ í–‰ &amp; 2ë²ˆì§¸,5ë²ˆì§¸ ì—´ ì¶”ì¶œ iphone_df.iloc[[1,3], [1,4]] . | Â  | ë””ìŠ¤í”Œë ˆì´ | Face ID | . | iPhone 7 Plus | 5.5 | No | . | iPhone 8 Plus | 5.5 | No | . | ilocìœ¼ë¡œ slicing iphone_df.iloc[3:, 1:4] . | Â  | ë””ìŠ¤í”Œë ˆì´ | ë©”ëª¨ë¦¬ | ì¶œì‹œ ë²„ì „ | . | iPhone 8 Plus | 5.5 | 3GB | iOS 11.0 | . | iPhone X | 5.8 | 3GB | iOS 11.1 | . | iPhone XS | 5.8 | 4GB | iOS 12.0 | . | iPhone XS Max | 6.5 | 4GB | iOS 12.0 | . | í–‰: 4ë²ˆì§¸ í–‰(index_num=3)ë¶€í„° ëê¹Œì§€ | ì—´: 2ë²ˆì§¸ ì—´(index_num=1)ë¶€í„° 4ë²ˆì§¸ ì—´(index_num=3)ê¹Œì§€ . | 1:4ë¡œ slicingí–ˆìœ¼ë‹ˆ 5ë²ˆì§¸ ì—´(index_num=4)ëŠ” í¬í•¨ë˜ì§€ ì•ŠëŠ”ë‹¤ (listì—ì„œì˜ slicingê³¼ ë™ì¼) | cf) iphone_df.loc['iPhone 8':'iPhone XS']ì—ì„œëŠ” iPhone8ì—ì„œ iPhone XSê¹Œì§€ ì–‘ ë ëª¨ë‘ í¬í•¨ | . | . | . ",
    "url": "https://chaelist.github.io/docs/pandas/pandas_basics/#indexing--slicing",
    "relUrl": "/docs/pandas/pandas_basics/#indexing--slicing"
  },"148": {
    "doc": "Pandas ê¸°ì´ˆ",
    "title": "ì¡°ê±´ indexing: Query í•¨ìˆ˜",
    "content": ". | df.loc[]ì„ í™œìš©í•´ ì¡°ê±´ìœ¼ë¡œ indexingí•˜ëŠ” ê²ƒê³¼ ë™ì¼í•˜ì§€ë§Œ, ë” ê°„ê²°í•œ ë¬¸ë²•ìœ¼ë¡œ ì‘ì„±í•  ìˆ˜ ìˆë‹¤ (df.query() í•¨ìˆ˜ë„ ê²°êµ­ì€ df.loc[]ì˜ í˜•íƒœë¡œ êµ¬í˜„ë¨) | . data = { 'name': ['Emily', 'Abby', 'Cornelia', 'Kai'], 'group': ['A', 'B', 'A', 'C'], 'english_score': [50, 89, 68, 88], 'math_score': [86, 31, 91, 75] } df = pd.DataFrame(data) df . | Â  | name | group | english_score | math_score | . | 0 | Emily | A | 50 | 86 | . | 1 | Abby | B | 89 | 31 | . | 2 | Cornelia | A | 68 | 91 | . | 3 | Kai | C | 88 | 75 | . | ë¹„êµ ì—°ì‚°ì(==, &gt;, &lt;, != ë“±) . df.query('english_score &gt; 80') . | Â  | name | group | english_score | math_score | . | 1 | Abby | B | 89 | 31 | . | 3 | Kai | C | 88 | 75 | . | in, not in . df.query('group in [\"B\", \"C\"]') # df.query('group == [\"B\", \"C\"]') ì´ë ‡ê²Œ ì‘ì„±í•´ë„ ë™ì¼ . | Â  | name | group | english_score | math_score | . | 1 | Abby | B | 89 | 31 | . | 3 | Kai | C | 88 | 75 | . | and(&amp;), or(|) . | ê° ì¡°ê±´ì„ ê´„í˜¸ë¡œ ëª…í™•íˆ êµ¬ë¶„í•´ì£¼ëŠ” ê²ƒì´ ì¢‹ë‹¤ | . df.query('(group == \"A\") &amp; (math_score &gt; 90)') . | Â  | name | group | english_score | math_score | . | 2 | Cornelia | A | 68 | 91 | . | indexë¥¼ ì§€ì¹­ . | indexì— ì´ë¦„ì´ ìˆë‹¤ë©´ ê·¸ ì´ë¦„(df.index.name)ì„ ê¸°ë¡í•´ì¤˜ì•¼ í•¨ | ë§Œì•½ ì¹¼ëŸ¼ ì¤‘ì—ë„ â€˜indexâ€™ë¼ëŠ” ì¹¼ëŸ¼ì´ ìˆìœ¼ë©´ ê·¸ ì¹¼ëŸ¼ìœ¼ë¡œ ì—°ì‚°ë¨ | . df.query('index &gt;= 2') . | Â  | name | group | english_score | math_score | . | 2 | Cornelia | A | 68 | 91 | . | 3 | Kai | C | 88 | 75 | . | f-string ì‚¬ìš© . | f-stringì„ ì‚¬ìš©í•˜ë©´ ì™¸ë¶€ ë³€ìˆ˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œë„ indexingí•  ìˆ˜ ìˆë‹¤ | . english_score_mean = df['english_score'].mean() # 73.75 df.query(f'english_score &gt;= {english_score_mean}') . | Â  | name | group | english_score | math_score | . | 1 | Abby | B | 89 | 31 | . | 3 | Kai | C | 88 | 75 | . | . ",
    "url": "https://chaelist.github.io/docs/pandas/pandas_basics/#%EC%A1%B0%EA%B1%B4-indexing-query-%ED%95%A8%EC%88%98",
    "relUrl": "/docs/pandas/pandas_basics/#ì¡°ê±´-indexing-query-í•¨ìˆ˜"
  },"149": {
    "doc": "Pandas ê¸°ì´ˆ",
    "title": "pd.set_option()",
    "content": "num_df = pd.DataFrame({'Num': np.random.randn(3)*1000000000}) num_df . | Â  | Num | . | 0 | -4.69191e+08 | . | 1 | -2.64508e+08 | . | 2 | 9.69347e+08 | . â†’ ìœ„ì™€ ê°™ì´ ìˆ«ìê°€ ë„ˆë¬´ ì»¤ì„œ scientific notationìœ¼ë¡œ ë‚˜ì˜¤ëŠ” ê²½ìš° / í˜¹ì€ ì†Œìˆ˜ì  ë°‘ ìë¦¬ê°€ ë„ˆë¬´ ê¸¸ê²Œ í‘œì‹œë˜ì–´ì„œ ë³µì¡í•œ ê²½ìš°, set_optionìœ¼ë¡œ float display formatì„ ë³€ê²½í•´ë‘ë©´ ìœ ìš©í•˜ë‹¤: . pd.set_option('display.float_format', lambda x: '%.2f' % x) # float type ìˆ«ìëŠ” ì†Œìˆ˜ì  ë‘ë²ˆì§¸ìë¦¬ê¹Œì§€ë§Œ í‘œì‹œí•˜ê² ë‹¤ëŠ” ì˜ë¯¸ . num_df . | Â  | Num | . | 0 | 1139992755.31 | . | 1 | 1196549705.47 | . | 2 | 777867250.54 | . +) reset_optionì„ í™œìš©í•˜ë©´ ì„¤ì •í•´ë‘” optionì„ ì´ˆê¸°í™”í•  ìˆ˜ ìˆë‹¤: . pd.reset_option('display.float_format') . ",
    "url": "https://chaelist.github.io/docs/pandas/pandas_basics/#pdset_option",
    "relUrl": "/docs/pandas/pandas_basics/#pdset_option"
  },"150": {
    "doc": "Pandas ë°ì´í„° ë¶„ì„",
    "title": "Pandas ë°ì´í„° ë¶„ì„",
    "content": ". | í° DataFrame ì‚´í´ë³´ê¸° . | ë°ì´í„° ë¶„í¬ íŒŒì•… | Type Conversion | ë°ì´í„° ì •ë ¬ | Aggregation í•¨ìˆ˜ë¡œ ë°ì´í„° ì†ì„± íŒŒì•… | Seriesë³„ë¡œ ì¶”ì¶œí•´ì„œ ì‚´í´ë³´ê¸° | . | ê²°ì† ë°ì´í„° (NaN) ì²˜ë¦¬ . | isna() | notna() | fillna() | dropna() | . | . ",
    "url": "https://chaelist.github.io/docs/pandas/pandas_data_analysis/",
    "relUrl": "/docs/pandas/pandas_data_analysis/"
  },"151": {
    "doc": "Pandas ë°ì´í„° ë¶„ì„",
    "title": "í° DataFrame ì‚´í´ë³´ê¸°",
    "content": "ë°ì´í„° ë¶„í¬ íŒŒì•… . | df.head(): ë§¨ ìœ„ì˜ í–‰ 5ê°œë§Œ ë³´ì—¬ì¤Œ. | ë°ì´í„°ê°€ í° ê²½ìš°, ì´ë ‡ê²Œ ìœ„ì˜ í–‰ë§Œ ì¼ë¶€ ê°€ì ¸ì™€ì„œ ë°ì´í„° êµ¬ì¡°ë¥¼ ì‚´í´ë³´ë©´ ë„ì›€ì´ ëœë‹¤. | . laptops_df = pd.read_csv('data/laptops.csv') ## ë°ì´í„° ì¶œì²˜: codeit laptops_df.head() . | Â  | brand | model | ram | hd_type | hd_size | screen_size | price | processor_brand | processor_model | clock_speed | graphic_card_brand | graphic_card_size | os | weight | comments | . | 0 | Dell | Inspiron 15-3567 | 4 | hdd | 1024 | 15.6 | 40000 | intel | i5 | 2.5 | intel | nan | linux | 2.5 | nan | . | 1 | Apple | MacBook Air | 8 | ssd | 128 | 13.3 | 55499 | intel | i5 | 1.8 | intel | 2 | mac | 1.35 | nan | . | 2 | Apple | MacBook Air | 8 | ssd | 256 | 13.3 | 71500 | intel | i5 | 1.8 | intel | 2 | mac | 1.35 | nan | . | 3 | Apple | MacBook Pro | 8 | ssd | 128 | 13.3 | 96890 | intel | i5 | 2.3 | intel | 2 | mac | 3.02 | nan | . | 4 | Apple | MacBook Pro | 8 | ssd | 256 | 13.3 | 112666 | intel | i5 | 2.3 | intel | 2 | mac | 3.02 | nan | . +) ì›í•˜ëŠ” ìˆ˜ë§Œí¼ì˜ í–‰ì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ë„ ìˆìŒ . laptops_df.head(7) ## ìœ„ì˜ 7í–‰ë§Œ ë¶ˆëŸ¬ì˜¨ë‹¤ëŠ” ëœ» . | Â  | brand | model | ram | hd_type | hd_size | screen_size | price | processor_brand | processor_model | clock_speed | graphic_card_brand | graphic_card_size | os | weight | comments | . | 0 | Dell | Inspiron 15-3567 | 4 | hdd | 1024 | 15.6 | 40000 | intel | i5 | 2.5 | intel | nan | linux | 2.5 | nan | . | 1 | Apple | MacBook Air | 8 | ssd | 128 | 13.3 | 55499 | intel | i5 | 1.8 | intel | 2 | mac | 1.35 | nan | . | 2 | Apple | MacBook Air | 8 | ssd | 256 | 13.3 | 71500 | intel | i5 | 1.8 | intel | 2 | mac | 1.35 | nan | . | 3 | Apple | MacBook Pro | 8 | ssd | 128 | 13.3 | 96890 | intel | i5 | 2.3 | intel | 2 | mac | 3.02 | nan | . | 4 | Apple | MacBook Pro | 8 | ssd | 256 | 13.3 | 112666 | intel | i5 | 2.3 | intel | 2 | mac | 3.02 | nan | . | 5 | Apple | MacBook Pro (TouchBar) | 16 | ssd | 512 | 15 | 226000 | intel | i7 | 2.7 | intel | 2 | mac | 2.5 | nan | . | 6 | Apple | MacBook Pro (TouchBar) | 16 | ssd | 512 | 13.3 | 158000 | intel | i5 | 2.9 | intel | 2 | mac | 1.37 | nan | . | df.tail(): ë§¨ ë’¤ì˜ 5ê°œ í–‰ë§Œ ë³´ì—¬ì¤Œ. | head()ì™€ ë§ˆì°¬ê°€ì§€ë¡œ, tail(10)ê³¼ ê°™ì´ ìˆ˜ë¥¼ ì§€ì •í•˜ëŠ” ê²ƒë„ ê°€ëŠ¥. | . laptops_df.tail() . | Â  | brand | model | ram | hd_type | hd_size | screen_size | price | processor_brand | processor_model | clock_speed | graphic_card_brand | graphic_card_size | os | weight | comments | . | 162 | Asus | A555LF | 8 | hdd | 1024 | 15.6 | 39961 | intel | i3 4th gen | 1.7 | nvidia | 2 | windows | 2.3 | nan | . | 163 | Asus | X555LA-XX172D | 4 | hdd | 500 | 15.6 | 28489 | intel | i3 4th gen | 1.9 | intel | nan | linux | 2.3 | nan | . | 164 | Asus | X554LD | 2 | hdd | 500 | 15.6 | 29199 | intel | i3 4th gen | 1.9 | intel | 1 | linux | 2.3 | nan | . | 165 | Asus | X550LAV-XX771D | 2 | hdd | 500 | 15.6 | 29990 | intel | i3 4th gen | 1.7 | intel | nan | linux | 2.5 | nan | . | 166 | Asus | X540LA-XX538T | 4 | hdd | 1024 | 15.6 | 30899 | intel | i3 5th gen | 2 | intel | nan | windows | 2.3 | nan | . | df.shape: í–‰ê³¼ ì—´ì˜ ê°œìˆ˜ë¥¼ ì•Œë ¤ì¤Œ. - ë°ì´í„°ì˜ ë¶„í¬ë¥¼ í•œ ëˆˆì— í™•ì¸í•  ìˆ˜ ìˆë‹¤. laptops_df.shape # 167ê°œì˜ í–‰ê³¼ 15ê°œì˜ ì—´ë¡œ êµ¬ì„±ëœ ë°ì´í„°ë¼ëŠ” ëœ» . (167, 15) . | df.columns: columnëª…ì„ ëª¨ë‘ ì¶”ì¶œ. - ì–´ë–¤ columnë“¤ì´ ìˆëŠ”ì§€ í•œ ëˆˆì— í™•ì¸í•  ìˆ˜ ìˆë‹¤. laptops_df.columns . Index(['brand', 'model', 'ram', 'hd_type', 'hd_size', 'screen_size', 'price', 'processor_brand', 'processor_model', 'clock_speed', 'graphic_card_brand', 'graphic_card_size', 'os', 'weight', 'comments'], dtype='object') . | df.info(): ì´ ë°ì´í„° ê±´ìˆ˜ì™€ ë°ì´í„° íƒ€ì…, Null ê°œìˆ˜ë¥¼ ì•Œ ìˆ˜ ìˆë‹¤ laptops_df.info() # ì˜ˆë¥¼ ë“¤ì–´, weight ì¹¼ëŸ¼ì—ì„œ non_null ê°’ì´ 160ê°œë¼ëŠ” ê²ƒì€, 167ê°œ ë°ì´í„° ì¤‘ 7ê°œëŠ” Nullì´ë¼ëŠ” ì˜ë¯¸ . &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 167 entries, 0 to 166 Data columns (total 15 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 brand 167 non-null object 1 model 167 non-null object 2 ram 167 non-null int64 3 hd_type 167 non-null object 4 hd_size 167 non-null int64 5 screen_size 167 non-null float64 6 price 167 non-null int64 7 processor_brand 167 non-null object 8 processor_model 167 non-null object 9 clock_speed 166 non-null float64 10 graphic_card_brand 163 non-null object 11 graphic_card_size 81 non-null float64 12 os 167 non-null object 13 weight 160 non-null float64 14 comments 55 non-null object dtypes: float64(4), int64(3), object(8) memory usage: 19.7+ KB . | df.describe(): ë°ì´í„°ì˜ ë¶„í¬ë„ íŒŒì•… . | ìˆ«ìí˜•(int, float ë“±) ì¹¼ëŸ¼ì˜ ë¶„í¬ë„ë§Œ ì¡°ì‚¬í•˜ë©°, object íƒ€ì…ì˜ ì¹¼ëŸ¼ì€ ì¶œë ¥ì—ì„œ ì œì™¸ë¨ | countëŠ” Not Nullì¸ ë°ì´í„° ê±´ìˆ˜ | meanì€ ì „ì²´ ë°ì´í„°ì˜ í‰ê· ê°’, stdëŠ” í‘œì¤€í¸ì°¨, minì€ ìµœì†Ÿê°’, maxëŠ” ìµœëŒ“ê°’ | . laptops_df.describe() . | Â  | ram | hd_size | screen_size | price | clock_speed | graphic_card_size | weight | . | count | 167 | 167 | 167 | 167 | 166 | 81 | 160 | . | mean | 6.8982 | 768.91 | 14.7752 | 64132.9 | 2.32108 | 52.1605 | 2.25081 | . | std | 3.78748 | 392.991 | 1.37653 | 42797.7 | 0.554187 | 444.134 | 0.648446 | . | min | 2 | 32 | 10.1 | 13872 | 1.1 | 1 | 0.78 | . | 25% | 4 | 500 | 14 | 35457.5 | 1.9 | 2 | 1.9 | . | 50% | 8 | 1024 | 15.6 | 47990 | 2.3 | 2 | 2.2 | . | 75% | 8 | 1024 | 15.6 | 77494.5 | 2.6 | 4 | 2.6 | . | max | 16 | 2048 | 17.6 | 226000 | 3.8 | 4000 | 4.2 | . â€» ë°ì´í„°ì˜ ë¶„í¬ë„ë¥¼ ì•„ëŠ” ê²ƒì€ ë¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ì¤‘ìš”í•œ ìš”ì†Œì´ë‹¤. (ex. íšŒê·€ì—ì„œ ê²°ì • ê°’ì´ ì •ê·œë¶„í¬ë¥¼ ë”°ë¥´ì§€ ì•Šê³  íŠ¹ì • ê°’ìœ¼ë¡œ ì™œê³¡ë˜ì–´ ìˆë‹¤ë©´ ì˜ˆì¸¡ ì„±ëŠ¥ì´ ì €í•˜ë¨) . | . Type Conversion . | df.astype()ì„ í™œìš©í•˜ë©´ íŠ¹ì • columnì˜ ë°ì´í„° íƒ€ì…ì„ ì›í•˜ëŠ” ëŒ€ë¡œ ë³€ê²½í•  ìˆ˜ ìˆë‹¤. | ex1) ìˆ«ìí˜• ë³€ìˆ˜ì¸ë° stringìœ¼ë¡œ ì €ì¥ë˜ì–´ ìˆì–´ì„œ df.describe()ë¡œ ë¶„í¬ íŒŒì•…ì´ ì•ˆë˜ëŠ” ê²½ìš°, ìˆ«ìí˜• ë³€ìˆ˜ë¡œ ë°”ê¿”ì£¼ë©´ ì¢‹ë‹¤ | ex2) ë°ì´í„°ë¥¼ mergeí•  ë•Œ, í†µí•© ê¸°ì¤€ì´ ë˜ëŠ” columnì˜ ë°ì´í„° íƒ€ì…ì´ ë‘ dfì—ì„œ ëª¨ë‘ ê°™ì•„ì•¼ í•œë‹¤ â†’ ë¯¸ë¦¬ typeì„ ì²´í¬í•˜ê³  í†µì¼í•´ì¤˜ì•¼ ë°ì´í„° ëˆ„ë½ ì—†ì´ mergeëœë‹¤ | . # 'ram' columnì˜ type ë³€ê²½í•´ë³´ê¸° print(laptops_df['ram'].dtypes) # ì´ì „ type ì²´í¬ laptops_df['ram'] = laptops_df['ram'].astype('str') # ë°”ê¾¸ê³ ì í•˜ëŠ” typeìœ¼ë¡œ ë³€ê²½ print(laptops_df['ram'].dtypes) # ë°”ë€ type ì²´í¬ laptops_df['ram'] = laptops_df['ram'].astype('int') # int typeìœ¼ë¡œ ì›ìƒ ë³µêµ¬ print(laptops_df['ram'].dtypes) # ë‹¤ì‹œ ë°”ë€ type ì²´í¬ . int64 object int64 . +) ì—¬ëŸ¬ ì»¬ëŸ¼ì„ í•œ ë²ˆì— ê°ê° íƒ€ì… ë³€ê²½í•˜ê¸°: . data = {'col1': [1, 2], 'col2': [3, 4]} df = pd.DataFrame(data) print(df, '\\n') print(df.dtypes, '\\n') # col1ì€ int32ë¡œ, col2ëŠ” str íƒ€ì…ìœ¼ë¡œ ë³€ê²½ df = df.astype({'col1': 'int32', 'col2':'str'}) print(df.dtypes) . col1 col2 0 1 3 1 2 4 col1 int64 col2 int64 dtype: object col1 int32 col2 object dtype: object . ë°ì´í„° ì •ë ¬ . : df.sort_values()ë¡œ íŠ¹ì • ì—´ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬í•˜ê¸° . | inplace=Trueë¥¼ ì¨ì£¼ë©´ dataframe ìì²´ê°€ ë°”ë€Œê³ , ì¨ì£¼ì§€ ì•Šìœ¼ë©´ ê·¸ëƒ¥ ì •ë ¬ëœ ê²°ê³¼ê°€ returnë˜ê³  ì›ë³¸ dataframeì€ ë°”ë€Œì§€ ì•ŠëŠ”ë‹¤. | . *ì˜¤ë¦„ì°¨ìˆœ ì •ë ¬: . ## ê°€ê²© ê¸°ì¤€ìœ¼ë¡œ ì˜¤ë¦„ì°¨ìˆœ ì •ë ¬ (ascending=Trueê°€ defaultë¼, ì•ˆì¨ì¤˜ë„ ë¨.) laptops_df.sort_values(by='price').head() ##ë‹¤ í™•ì¸í•˜ë©´ ë„ˆë¬´ ë§ìœ¼ë‹ˆ, ê°€ê²©ì´ ë‚®ì€ ìˆœìœ¼ë¡œ top5ë§Œ í™•ì¸ . | Â  | brand | model | ram | hd_type | hd_size | screen_size | price | processor_brand | processor_model | clock_speed | graphic_card_brand | graphic_card_size | os | weight | comments | . | 148 | Acer | Aspire SW3-016 | 2 | ssd | 32 | 10.1 | 13872 | intel | Atom Z8300 | 1.44 | intel | nan | windows | 1.2 | nan | . | 83 | Acer | A315-31CDC UN.GNTSI.001 | 2 | ssd | 500 | 15.6 | 17990 | intel | Celeron | 1.1 | intel | nan | windows | 2.1 | nan | . | 108 | Acer | Aspire ES-15 NX.GKYSI.010 | 4 | hdd | 500 | 15.6 | 17990 | amd | A4-7210 | 1.8 | amd | nan | windows | 2.4 | nan | . | 100 | Acer | A315-31-P4CRUN.GNTSI.002 | 4 | hdd | 500 | 15.6 | 18990 | intel | pentium | 1.1 | intel | nan | windows | nan | nan | . | 73 | Acer | Aspire ES1-523 | 4 | hdd | 1024 | 15.6 | 19465 | amd | A4-7210 | 1.8 | amd | nan | linux | 2.4 | nan | . *ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬: . # ê°€ê²© ê¸°ì¤€ìœ¼ë¡œ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬ (ê°€ê²© ë†’ì€ ê²ƒë¶€í„° ìˆœì„œëŒ€ë¡œ.) laptops_df.sort_values(by='price', ascending=False).head() ##ê°€ê²© ë†’ì€ top5ë§Œ í™•ì¸ . | Â  | brand | model | ram | hd_type | hd_size | screen_size | price | processor_brand | processor_model | clock_speed | graphic_card_brand | graphic_card_size | os | weight | comments | . | 5 | Apple | MacBook Pro (TouchBar) | 16 | ssd | 512 | 15 | 226000 | intel | i7 | 2.7 | intel | 2 | mac | 2.5 | nan | . | 90 | Alienware | 15 Notebook | 16 | hdd | 1024 | 15.6 | 199000 | intel | i7 | 2.6 | nvidia | 8 | windows | 3.5 | Maximum Display Resolution : 1920 x 1080 pixel | . | 96 | Alienware | AW13R3-7000SLV-PUS | 8 | ssd | 256 | 13.3 | 190256 | intel | i7 | 3 | nvidia | 6 | windows | 2.6 | 13.3 inch FHD (1920 x 1080) IPS Anti-Glare 300-nits Display 1 Lithium ion batteries required. (included) | . | 31 | Acer | Predator 17 | 16 | ssd | 256 | 17.3 | 178912 | intel | i7 | 2.6 | nvidia | nan | windows | 4.2 | Integrated Graphics | . | 154 | Microsoft | Surface Book CR9-00013 | 8 | ssd | 128 | 13.5 | 178799 | intel | i5 | 1.8 | intel | nan | windows | 1.5 | nan | . Aggregation í•¨ìˆ˜ë¡œ ë°ì´í„° ì†ì„± íŒŒì•… . : min(), max(), sum(), count() ë“± ì‚¬ìš© ê°€ëŠ¥. | ì „ì²´ dataframeì˜ ì†ì„± ì¼ê´„ íŒŒì•… laptops_df.count() # ëª¨ë“  ì¹¼ëŸ¼ì˜ countë¥¼ ë°˜í™˜í•œë‹¤ (NaNì´ ì•„ë‹Œ ê°’ë§Œ ì…ˆ) . brand 167 model 167 ram 167 hd_type 167 hd_size 167 screen_size 167 price 167 processor_brand 167 processor_model 167 clock_speed 166 graphic_card_brand 163 graphic_card_size 81 os 167 weight 160 comments 55 model_len 167 Expensive_Affordable 167 price_cat 167 dtype: int64 . | íŠ¹ì • ì¹¼ëŸ¼ë“¤ì˜ ì†ì„±ë§Œ í™•ì¸ laptops_df[['screen_size', 'price']].mean() # íŠ¹ì • ì¹¼ëŸ¼ë“¤ë§Œ ì¶”ì¶œí•´ì„œ í•¨ìˆ˜ ì ìš© . screen_size 14.775210 price 64132.898204 dtype: float64 . | . Seriesë³„ë¡œ ì¶”ì¶œí•´ì„œ ì‚´í´ë³´ê¸° . | srs.unique(): ì¤‘ë³µì„ ì œì™¸í•˜ê³  ì–´ë–¤ ê°’ì´ ìˆë‚˜ íŒŒì•… . | dataframeì—ëŠ” ì ìš©ë˜ì§€ ì•Šê³  seriesì—ë§Œ ì ìš©ë˜ëŠ” í•¨ìˆ˜ | . laptops_df['brand'].unique() # ê²¹ì¹˜ëŠ” ê±¸ ì œì™¸í•˜ê³  ì´ ëª‡ ê°œì˜ ë¸Œëœë“œê°€ ìˆë‚˜ ì‚´í´ë´„ . array(['Dell', 'Apple', 'Acer', 'HP', 'Lenovo', 'Alienware', 'Microsoft', 'Asus'], dtype=object) . +) uniqueí•œ ê°’ì˜ ìˆ˜ êµ¬í•˜ê¸° . laptops_df['brand'].nunique() ## len(laptops_df['brand'].unique())ì™€ ë™ì¼í•œ ê²°ê³¼ë¥¼ ëƒ„ . 8 . | srs.value_counts(): ê° ê°’ì´ ëª‡ ë²ˆì”© ë‚˜ì˜¤ëŠ”ì§€ íŒŒì•… laptops_df['brand'].value_counts() # ê° ë¸Œëœë“œê°€ ëª‡ ë²ˆ ë‚˜ì˜¤ëŠ”ì§€ ì‚´í´ë´„ . HP 55 Acer 35 Dell 31 Lenovo 18 Asus 9 Apple 7 Alienware 6 Microsoft 6 Name: brand, dtype: int64 . | srs.describe(): df.describe()ë¥¼ í•  ë•Œì™€ ê°™ì€ íš¨ê³¼. (ë°ì´í„° ë¶„í¬ ìš”ì•½) laptops_df['brand'].describe() ## 'freq'ëŠ” 'top' ë¹ˆë„ë¡œ ë“±ì¥í•˜ëŠ” 'HP'ê°€ 55ë²ˆ ë‚˜ì˜¨ë‹¤ëŠ” ëœ» . count 167 unique 8 top HP freq 55 Name: brand, dtype: object . | . ",
    "url": "https://chaelist.github.io/docs/pandas/pandas_data_analysis/#%ED%81%B0-dataframe-%EC%82%B4%ED%8E%B4%EB%B3%B4%EA%B8%B0",
    "relUrl": "/docs/pandas/pandas_data_analysis/#í°-dataframe-ì‚´í´ë³´ê¸°"
  },"152": {
    "doc": "Pandas ë°ì´í„° ë¶„ì„",
    "title": "ê²°ì† ë°ì´í„° (NaN) ì²˜ë¦¬",
    "content": ". | ê²°ì¸¡ì¹˜ê°€ ìˆìœ¼ë©´ ë¨¸ì‹ ëŸ¬ë‹ì„ í•  ìˆ˜ ì—†ê¸°ì—, ê²°ì¸¡ì¹˜ë¥¼ ì‚­ì œí•˜ê±°ë‚˜ ë‹¤ë¥¸ ë°©ë²•ìœ¼ë¡œ ì±„ì›Œì¤˜ì•¼ í•œë‹¤ | . isna() . : ê° ê°’ì´ NaNì¸ì§€ ì•„ë‹Œì§€ë¥¼ Trueë‚˜ Falseë¡œ ì•Œë ¤ì¤€ë‹¤ (NaNì´ë©´ True, ê°’ì´ ì¡´ì¬í•˜ë©´ False) . laptops_df.isna().head(3) . | Â  | brand | model | ram | hd_type | hd_size | screen_size | price | processor_brand | processor_model | clock_speed | graphic_card_brand | graphic_card_size | os | weight | comments | . | 0 | False | False | False | False | False | False | False | False | False | False | False | True | False | False | True | . | 1 | False | False | False | False | False | False | False | False | False | False | False | False | False | False | True | . | 2 | False | False | False | False | False | False | False | False | False | False | False | False | False | False | True | . +) ì¹¼ëŸ¼ë³„ ê²°ì† ë°ì´í„° ìˆ˜ êµ¬í•˜ê¸° . | df.isna().sum()í•˜ë©´ TrueëŠ” 1, FalseëŠ” 0ìœ¼ë¡œ ê³„ì‚°ë˜ë¯€ë¡œ ê° ì¹¼ëŸ¼ë³„ ê²°ì† ë°ì´í„° ìˆ˜ë¥¼ êµ¬í•  ìˆ˜ ìˆë‹¤ | . laptops_df.isna().sum() . brand 0 model 0 ram 0 hd_type 0 hd_size 0 screen_size 0 price 0 processor_brand 0 processor_model 0 clock_speed 1 graphic_card_brand 4 graphic_card_size 86 os 0 weight 7 comments 112 dtype: int64 . notna() . : isna()ì™€ ë°˜ëŒ€ë¡œ, ê°’ì´ ì¡´ì¬í•˜ë©´ True, NaN(ê²°ì¸¡ì¹˜)ë©´ Falseë¥¼ ë°˜í™˜ . laptops_df.notna().head(3) . | Â  | brand | model | ram | hd_type | hd_size | screen_size | price | processor_brand | processor_model | clock_speed | graphic_card_brand | graphic_card_size | os | weight | comments | . | 0 | True | True | True | True | True | True | True | True | True | True | True | False | True | True | False | . | 1 | True | True | True | True | True | True | True | True | True | True | True | True | True | True | False | . | 2 | True | True | True | True | True | True | True | True | True | True | True | True | True | True | False | . fillna() . : ê²°ì† ë°ì´í„° ëŒ€ì²´ . | fillna(ëŒ€ì²´í•  ê°’)ìœ¼ë¡œ ì ì–´ì£¼ë©´ NaN(ê²°ì¸¡ì¹˜)ê°€ ëª¨ë‘ â€˜ëŒ€ì²´í•  ê°’â€™ìœ¼ë¡œ ë°”ë€ë‹¤ | inplace=Trueë¥¼ í•´ì£¼ë©´ dataframe ìì²´ê°€ ë³€ê²½ë¨ | . # graphic_card_brand ì¹¼ëŸ¼ì˜ NaN ê°’ì„ ëª¨ë‘ 'N/A'ë¡œ ëŒ€ì²´ laptops_df['graphic_card_brand'].fillna('N/A', inplace=True) laptops_df.isna().sum() ## graphic_card_brand ì¹¼ëŸ¼ì˜ NaN ê°’ì´ ëª¨ë‘ ì—†ì–´ì§ . brand 0 model 0 ram 0 hd_type 0 hd_size 0 screen_size 0 price 0 processor_brand 0 processor_model 0 clock_speed 1 graphic_card_brand 0 graphic_card_size 86 os 0 weight 7 comments 112 dtype: int64 . +) NaN ê°’ì„ í•´ë‹¹ ì¹¼ëŸ¼ì˜ í‰ê· ê°’ìœ¼ë¡œ ëŒ€ì²´í•˜ê¸° . # graphic_card_size ì¹¼ëŸ¼ì˜ NaN ê°’ì„ í‰ê· ê°’ìœ¼ë¡œ ëŒ€ì²´í•´ë³´ê¸° laptops_df['graphic_card_size'].fillna(laptops_df['graphic_card_size'].mean(), inplace=True) laptops_df.head(3) ## index=0 í–‰ì˜ graphic_card_size ê°’ì´ ì›ë˜ëŠ” NaNì´ì—ˆëŠ”ë°, í‰ê· ê°’ì¸ '52.1605'ë¡œ ë°”ë€ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤ . | Â  | brand | model | ram | hd_type | hd_size | screen_size | price | processor_brand | processor_model | clock_speed | graphic_card_brand | graphic_card_size | os | weight | comments | . | 0 | Dell | Inspiron 15-3567 | 4 | hdd | 1024 | 15.6 | 40000 | intel | i5 | 2.5 | intel | 52.1605 | linux | 2.5 | nan | . | 1 | Apple | MacBook Air | 8 | ssd | 128 | 13.3 | 55499 | intel | i5 | 1.8 | intel | 2 | mac | 1.35 | nan | . | 2 | Apple | MacBook Air | 8 | ssd | 256 | 13.3 | 71500 | intel | i5 | 1.8 | intel | 2 | mac | 1.35 | nan | . dropna() . : ê²°ì¸¡ì¹˜ê°€ ìˆëŠ” í–‰ ì‚­ì œ . | NaN ê°’ì„ ì ì ˆí•œ ê°’ìœ¼ë¡œ ëŒ€ì²´í•´ì¤˜ë„ ì¢‹ì§€ë§Œ, ì ì ˆí•˜ê²Œ ëŒ€ì²´ë˜ì§€ ì•ŠëŠ” ê²½ìš° ë¨¸ì‹ ëŸ¬ë‹ì˜ ì„±ëŠ¥ì— ì•…ì˜í–¥ì„ ì¤„ ìˆ˜ ìˆë‹¤. ê·¸ë ‡ê¸°ì— ì•„ì˜ˆ ê²°ì¸¡ì¹˜ê°€ ìˆëŠ” í–‰ì„ ì‚­ì œí•´ë²„ë¦¬ëŠ” ê²ƒë„ ì¢‹ì€ ë°©ë²•ì´ë‹¤. | df.dropna()ë¼ê³ ë§Œ í•˜ë©´ í•˜ë‚˜ì˜ ì¹¼ëŸ¼ì—ë¼ë„ ê²°ì¸¡ì¹˜ê°€ ìˆëŠ” ëª¨ë“  í–‰ì´ ì‚­ì œë¨ | df.dropna(subset=['ì¹¼ëŸ¼ëª…'])ì´ë¼ê³  ì œí•œì„ ê±¸ë©´, í•´ë‹¹ ì¹¼ëŸ¼ì— ê²°ì¸¡ì¹˜ê°€ ìˆëŠ” ë°ì´í„°ë§Œ ì‚­ì œëœë‹¤ | . laptops_df.dropna(subset=['weight'], inplace=True) ## 'weight' ì¹¼ëŸ¼ì— ê²°ì¸¡ì¹˜ê°€ ìˆëŠ” í–‰ë§Œ ì‚­ì œ # ì´ ê²½ìš°, 'comment' ì¹¼ëŸ¼ì€ ê²°ì¸¡ì¹˜ê°€ ë§ì€ ê²ƒì´ ë‹¹ì—°í•˜ê¸°ì—, dropna()ë¼ê³ ë§Œ í•˜ë©´ ëŒ€ë¶€ë¶„ì˜ ë°ì´í„°ê°€ ì‚­ì œë¨ laptops_df.isna().sum() . brand 0 model 0 ram 0 hd_type 0 hd_size 0 screen_size 0 price 0 processor_brand 0 processor_model 0 clock_speed 1 graphic_card_brand 3 graphic_card_size 0 os 0 weight 0 comments 105 dtype: int64 . ",
    "url": "https://chaelist.github.io/docs/pandas/pandas_data_analysis/#%EA%B2%B0%EC%86%90-%EB%8D%B0%EC%9D%B4%ED%84%B0-nan-%EC%B2%98%EB%A6%AC",
    "relUrl": "/docs/pandas/pandas_data_analysis/#ê²°ì†-ë°ì´í„°-nan-ì²˜ë¦¬"
  },"153": {
    "doc": "Pandas ë°ì´í„° ê°€ê³µ",
    "title": "Pandas ë°ì´í„° ê°€ê³µ",
    "content": ". | Pandas DataFrame ë³€ê²½í•˜ê¸° . | ë°ì´í„° ìˆ˜ì •í•˜ê¸° | df.replace()ë¡œ ì¼ê´„ ìˆ˜ì • | ìƒˆë¡œìš´ í–‰/ì—´ ì¶”ê°€ | df.drop() | df.drop_duplicates() | df.insert() | df.transpose() | . | Index, ì¹¼ëŸ¼ ë³€ê²½í•˜ê¸° . | df.rename() | Index ì´ë¦„ ë¶™ì´ê¸° | df.reset_index() | df.set_index() | ì¹¼ëŸ¼ ìˆœì„œ ë³€ê²½í•˜ê¸° | . | . ",
    "url": "https://chaelist.github.io/docs/pandas/pandas_data_modifying/",
    "relUrl": "/docs/pandas/pandas_data_modifying/"
  },"154": {
    "doc": "Pandas ë°ì´í„° ê°€ê³µ",
    "title": "Pandas DataFrame ë³€ê²½í•˜ê¸°",
    "content": "iphone_df = pd.read_csv('data/iphone.csv', index_col=0) ## ë°ì´í„° ì¶œì²˜: codeit iphone_df . | Â  | ì¶œì‹œì¼ | ë””ìŠ¤í”Œë ˆì´ | ë©”ëª¨ë¦¬ | ì¶œì‹œ ë²„ì „ | Face ID | . | iPhone 7 | 2016-09-16 | 4.7 | 2GB | iOS 10.0 | No | . | iPhone 7 Plus | 2016-09-16 | 5.5 | 3GB | iOS 10.0 | No | . | iPhone 8 | 2017-09-22 | 4.7 | 2GB | iOS 11.0 | No | . | iPhone 8 Plus | 2017-09-22 | 5.5 | 3GB | iOS 11.0 | No | . | iPhone X | 2017-11-03 | 5.8 | 3GB | iOS 11.1 | Yes | . | iPhone XS | 2018-09-21 | 5.8 | 4GB | iOS 12.0 | Yes | . | iPhone XS Max | 2018-09-21 | 6.5 | 4GB | iOS 12.0 | Yes | . ë°ì´í„° ìˆ˜ì •í•˜ê¸° . iphone_df.loc['iPhone 8', 'ë©”ëª¨ë¦¬'] = '2.5GB' iphone_df . | Â  | ì¶œì‹œì¼ | ë””ìŠ¤í”Œë ˆì´ | ë©”ëª¨ë¦¬ | ì¶œì‹œ ë²„ì „ | Face ID | . | iPhone 7 | 2016-09-16 | 4.7 | 2GB | iOS 10.0 | No | . | iPhone 7 Plus | 2016-09-16 | 5.5 | 3GB | iOS 10.0 | No | . | iPhone 8 | 2017-09-22 | 4.7 | 2.5GB | iOS 11.0 | No | . | iPhone 8 Plus | 2017-09-22 | 5.5 | 3GB | iOS 11.0 | No | . | iPhone X | 2017-11-03 | 5.8 | 3GB | iOS 11.1 | Yes | . | iPhone XS | 2018-09-21 | 5.8 | 4GB | iOS 12.0 | Yes | . | iPhone XS Max | 2018-09-21 | 6.5 | 4GB | iOS 12.0 | Yes | . df.replace()ë¡œ ì¼ê´„ ìˆ˜ì • . # 'ë©”ëª¨ë¦¬' ì¹¼ëŸ¼ì˜ '4GB' ê°’ë“¤ì„ ë‹¤ '5GB'ë¡œ replace. iphone_df.replace({'ë©”ëª¨ë¦¬': '4GB'}, '5GB') ## iphone_dfì— ë‹¤ì‹œ ì €ì¥í•´ì£¼ê±°ë‚˜ inplace=Trueí•˜ë©´ df ìì²´ë¥¼ ë°”ê¿€ ìˆ˜ ìˆìŒ . | Â  | ì¶œì‹œì¼ | ë””ìŠ¤í”Œë ˆì´ | ë©”ëª¨ë¦¬ | ì¶œì‹œ ë²„ì „ | Face ID | . | iPhone 7 | 2016-09-16 | 4.7 | 2GB | iOS 10.0 | No | . | iPhone 7 Plus | 2016-09-16 | 5.5 | 3GB | iOS 10.0 | No | . | iPhone 8 | 2017-09-22 | 4.7 | 2.5GB | iOS 11.0 | No | . | iPhone 8 Plus | 2017-09-22 | 5.5 | 3GB | iOS 11.0 | No | . | iPhone X | 2017-11-03 | 5.8 | 3GB | iOS 11.1 | Yes | . | iPhone XS | 2018-09-21 | 5.8 | 5GB | iOS 12.0 | Yes | . | iPhone XS Max | 2018-09-21 | 6.5 | 5GB | iOS 12.0 | Yes | . Â  . +) ë‹¤ì–‘í•œ df.replace() í™œìš©ë²•: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.replace.html . | df ì „ì²´ì—ì„œ 0ì€ 10ìœ¼ë¡œ, 1ì€ 100ìœ¼ë¡œ ë°”ê¾¸ê¸°: df.replace({0: 10, 1: 100}) . | A ì¹¼ëŸ¼ì˜ 0ê³¼ B ì¹¼ëŸ¼ì˜ 5ë¥¼ 100ìœ¼ë¡œ ë°”ê¾¸ê¸°: df.replace({'A': 0, 'B': 5}, 100) . | A ì¹¼ëŸ¼ì—ì„œë§Œ, 0ì€ 100ìœ¼ë¡œ, 4ëŠ” 400ìœ¼ë¡œ ë°”ê¾¸ê¸°: df.replace({'A': {0: 100, 4: 400}}) . | . Â  . ìƒˆë¡œìš´ í–‰/ì—´ ì¶”ê°€ . | í–‰ ì¶”ê°€ iphone_df.loc['iPhone XR'] = ['2018-10-26', 6.1, '3GB', 'iOS 12.0.1', 'Yes'] iphone_df . | Â  | ì¶œì‹œì¼ | ë””ìŠ¤í”Œë ˆì´ | ë©”ëª¨ë¦¬ | ì¶œì‹œ ë²„ì „ | Face ID | . | iPhone 7 | 2016-09-16 | 4.7 | 2GB | iOS 10.0 | No | . | iPhone 7 Plus | 2016-09-16 | 5.5 | 3GB | iOS 10.0 | No | . | iPhone 8 | 2017-09-22 | 4.7 | 2.5GB | iOS 11.0 | No | . | iPhone 8 Plus | 2017-09-22 | 5.5 | 3GB | iOS 11.0 | No | . | iPhone X | 2017-11-03 | 5.8 | 3GB | iOS 11.1 | Yes | . | iPhone XS | 2018-09-21 | 5.8 | 4GB | iOS 12.0 | Yes | . | iPhone XS Max | 2018-09-21 | 6.5 | 4GB | iOS 12.0 | Yes | . | iPhone XR | 2018-10-26 | 6.1 | 3GB | iOS 12.0.1 | Yes | . | ì—´ ì¶”ê°€ iphone_df['ì œì¡°ì‚¬'] = 'Apple' iphone_df . | Â  | ì¶œì‹œì¼ | ë””ìŠ¤í”Œë ˆì´ | ë©”ëª¨ë¦¬ | ì¶œì‹œ ë²„ì „ | Face ID | ì œì¡°ì‚¬ | . | iPhone 7 | 2016-09-16 | 4.7 | 2GB | iOS 10.0 | No | Apple | . | iPhone 7 Plus | 2016-09-16 | 5.5 | 3GB | iOS 10.0 | No | Apple | . | iPhone 8 | 2017-09-22 | 4.7 | 2.5GB | iOS 11.0 | No | Apple | . | iPhone 8 Plus | 2017-09-22 | 5.5 | 3GB | iOS 11.0 | No | Apple | . | iPhone X | 2017-11-03 | 5.8 | 3GB | iOS 11.1 | Yes | Apple | . | iPhone XS | 2018-09-21 | 5.8 | 4GB | iOS 12.0 | Yes | Apple | . | iPhone XS Max | 2018-09-21 | 6.5 | 4GB | iOS 12.0 | Yes | Apple | . | iPhone XR | 2018-10-26 | 6.1 | 3GB | iOS 12.0.1 | Yes | Apple | . | . df.drop() . : í–‰/ì—´ ì‚­ì œ . | í–‰ ì‚­ì œ: axis=0 í˜¹ì€ axis='index' iphone_df.drop('iPhone XR', axis='index', inplace=True) # inplace=Trueë¼ê³  í•´ì•¼ ì›ë³¸ df ìì²´ê°€ ë³€ê²½ë¨. # inplace=Falseë¼ê³  í•˜ë©´ ì›ë³¸ dfëŠ” ê·¸ëŒ€ë¡œ ìˆê³ , í–‰/ì—´ì´ ì‚­ì œëœ ìƒˆë¡œìš´ dfê°€ ê²°ê³¼ë¡œ ë°˜í™˜ë¨ iphone_df . | Â  | ì¶œì‹œì¼ | ë””ìŠ¤í”Œë ˆì´ | ë©”ëª¨ë¦¬ | ì¶œì‹œ ë²„ì „ | Face ID | ì œì¡°ì‚¬ | . | iPhone 7 | 2016-09-16 | 4.7 | 2GB | iOS 10.0 | No | Apple | . | iPhone 7 Plus | 2016-09-16 | 5.5 | 3GB | iOS 10.0 | No | Apple | . | iPhone 8 | 2017-09-22 | 4.7 | 2.5GB | iOS 11.0 | No | Apple | . | iPhone 8 Plus | 2017-09-22 | 5.5 | 3GB | iOS 11.0 | No | Apple | . | iPhone X | 2017-11-03 | 5.8 | 3GB | iOS 11.1 | Yes | Apple | . | iPhone XS | 2018-09-21 | 5.8 | 4GB | iOS 12.0 | Yes | Apple | . | iPhone XS Max | 2018-09-21 | 6.5 | 4GB | iOS 12.0 | Yes | Apple | . | ì—´ ì‚­ì œ: axis=1 í˜¹ì€ axis='columns' iphone_df.drop('ì œì¡°ì‚¬', axis='columns', inplace=True) iphone_df . | Â  | ì¶œì‹œì¼ | ë””ìŠ¤í”Œë ˆì´ | ë©”ëª¨ë¦¬ | ì¶œì‹œ ë²„ì „ | Face ID | . | iPhone 7 | 2016-09-16 | 4.7 | 2GB | iOS 10.0 | No | . | iPhone 7 Plus | 2016-09-16 | 5.5 | 3GB | iOS 10.0 | No | . | iPhone 8 | 2017-09-22 | 4.7 | 2.5GB | iOS 11.0 | No | . | iPhone 8 Plus | 2017-09-22 | 5.5 | 3GB | iOS 11.0 | No | . | iPhone X | 2017-11-03 | 5.8 | 3GB | iOS 11.1 | Yes | . | iPhone XS | 2018-09-21 | 5.8 | 4GB | iOS 12.0 | Yes | . | iPhone XS Max | 2018-09-21 | 6.5 | 4GB | iOS 12.0 | Yes | . | ì—¬ëŸ¬ ì¤„ í•œ ë²ˆì— ì‚­ì œ . iphone_df.drop(['iPhone 7', 'iPhone 8','iPhone X'], axis='index', inplace=True) iphone_df . | Â  | ì¶œì‹œì¼ | ë””ìŠ¤í”Œë ˆì´ | ë©”ëª¨ë¦¬ | ì¶œì‹œ ë²„ì „ | Face ID | . | iPhone 7 Plus | 2016-09-16 | 5.5 | 3GB | iOS 10.0 | No | . | iPhone 8 Plus | 2017-09-22 | 5.5 | 3GB | iOS 11.0 | No | . | iPhone XS | 2018-09-21 | 5.8 | 4GB | iOS 12.0 | Yes | . | iPhone XS Max | 2018-09-21 | 6.5 | 4GB | iOS 12.0 | Yes | . | íŠ¹ì • ì¡°ê±´ì˜ ì¤„ë§Œ ì‚­ì œí•˜ê¸° . liverpool_df = pd.read_csv('data/liverpool.csv', index_col=0) ## ë°ì´í„° ì¶œì²˜: codeit liverpool_df . | Â  | position | born | number | nationality | . | Roberto Firmino | FW | 1991 | no. 9 | Brazil | . | Sadio Mane | FW | 1992 | no. 10 | Senegal | . | Mohamed Salah | FW | 1992 | no. 11 | Egypt | . | Joe Gomez | DF | 1997 | no. 12 | England | . | Alisson Becker | GK | 1992 | no. 13 | Brazil | . â†’ â€˜nationalityâ€™ê°€ â€˜Brazilâ€™ ì¶œì‹ ì¸ ì„ ìˆ˜ë§Œ ì‚­ì œí•˜ê¸° . drop_index = liverpool_df.loc[liverpool_df['nationality'] == 'Brazil'].index liverpool_df.drop(drop_index, axis='index', inplace=True) liverpool_df . | Â  | position | born | number | nationality | . | Sadio Mane | FW | 1992 | no. 10 | Senegal | . | Mohamed Salah | FW | 1992 | no. 11 | Egypt | . | Joe Gomez | DF | 1997 | no. 12 | England | . | . df.drop_duplicates() . : ì¤‘ë³µëœ í–‰ì„ ì‚­ì œ. ì¤‘ë³µëœ í–‰ë“¤ ì¤‘ ê°€ì¥ ìœ„ì— ìˆëŠ” í–‰ë§Œ ë‚¨ê¸°ê³  ë‹¤ ì‚­ì œí•´ì¤€ë‹¤. cake_df = pd.DataFrame({ 'brand': ['Yummmy', 'Yummmy', 'Sweet', 'Sweet', 'Sweet'], 'taste': ['Chocolate', 'Chocolate', 'Strawberry', 'Strawberry', 'Cheese'], 'rating': [4, 4, 3.5, 15, 5] }) cake_df . | Â  | brand | taste | rating | . | 0 | Yummmy | Chocolate | 4 | . | 1 | Yummmy | Chocolate | 4 | . | 2 | Sweet | Strawberry | 3.5 | . | 3 | Sweet | Strawberry | 15 | . | 4 | Sweet | Cheese | 5 | . | ê¸°ë³¸ drop_duplicates(): ëª¨ë“  ì—´ì˜ ê°’ì´ ë‹¤ ì¼ì¹˜í•˜ëŠ” í–‰ë§Œ ì¤‘ë³µë°ì´í„°ë¡œ ê°„ì£¼í•´ ì‚­ì œ cake_df.drop_duplicates() # inplace=Trueë¥¼ í•˜ì§€ ì•Šìœ¼ë©´ ì‹¤ì œ dfê°€ ë³€í˜•ë˜ì§€ëŠ” ì•ŠìŒ. | Â  | brand | taste | rating | . | 0 | Yummmy | Chocolate | 4 | . | 2 | Sweet | Strawberry | 3.5 | . | 3 | Sweet | Strawberry | 15 | . | 4 | Sweet | Cheese | 5 | . | subset=[] ì¡°ê±´: ì§€ì •í•œ ì—´(ë“¤)ì—ì„œë§Œ ê°’ì´ ì¼ì¹˜í•˜ë©´ ì¤‘ë³µë°ì´í„°ë¡œ ê°„ì£¼í•´ ì‚­ì œ cake_df.drop_duplicates(subset=['brand', 'taste']) # inplace=Trueë¥¼ í•˜ì§€ ì•Šìœ¼ë©´ ì‹¤ì œ dfê°€ ë³€í˜•ë˜ì§€ëŠ” ì•ŠìŒ. | Â  | brand | taste | rating | . | 0 | Yummmy | Chocolate | 4 | . | 2 | Sweet | Strawberry | 3.5 | . | 4 | Sweet | Cheese | 5 | . | . +) df.duplicated()ë¥¼ ì‚¬ìš©í•˜ë©´ drop_duplicatesë¥¼ í•˜ê¸° ì „ì— ë¯¸ë¦¬ ì¤‘ë³µê°’ì´ ì–´ëŠ ì •ë„ ìˆëŠ”ì§€ í™•ì¸í•  ìˆ˜ ìˆìŒ (ë§ˆì°¬ê°€ì§€ë¡œ subset='ì¹¼ëŸ¼ëª…'ë„ ì‚¬ìš© ê°€ëŠ¥!) . df.insert() . : íŠ¹ì • ìœ„ì¹˜ì— ì—´ì„ ì‚½ì…í•´ì£¼ëŠ” í•¨ìˆ˜ . iphone_df.insert(loc=3, column='ìˆœìœ„', value=np.arange(1, len(iphone_df)+1)) # loc=3ì´ë©´ 4ë²ˆì§¸ ì—´ì— ì‚½ì…í•˜ê² ë‹¤ëŠ” ëœ» # columnì—ëŠ” ìƒˆë¡œ ì‚½ì…í•  ì—´ì˜ ì´ë¦„ì„ ì…ë ¥ # valueì—ëŠ” ìƒˆë¡œ ì‚½ì…í•  ì—´ì— ë“¤ì–´ê°€ëŠ” ê°’ë“¤ì„ ì…ë ¥ (listë‚˜ np array í˜•íƒœ) ## ì´ ê²½ìš°ì—ëŠ”, 1ë¶€í„° í•´ë‹¹ dfì˜ ì—´ ê°œìˆ˜ë§Œí¼ ì¦ê°€í•˜ê²Œ 1,2,3,... ì´ëŸ° ì‹ì˜ ì—´ì„ ì¶”ê°€ iphone_df . | Â  | ì¶œì‹œì¼ | ë””ìŠ¤í”Œë ˆì´ | ë©”ëª¨ë¦¬ | ìˆœìœ„ | ì¶œì‹œ ë²„ì „ | Face ID | . | iPhone 7 Plus | 2016-09-16 | 5.5 | 3GB | 1 | iOS 10.0 | No | . | iPhone 8 Plus | 2017-09-22 | 5.5 | 3GB | 2 | iOS 11.0 | No | . | iPhone XS | 2018-09-21 | 5.8 | 4GB | 3 | iOS 12.0 | Yes | . | iPhone XS Max | 2018-09-21 | 6.5 | 4GB | 4 | iOS 12.0 | Yes | . df.transpose() . (í˜¹ì€ df.T): í–‰ê³¼ ì—´ ë°”ê¾¸ê¸° (numpyì— transposeí•˜ëŠ” ê²ƒê³¼ ë™ì¼) . iphone_df.transpose() # iphone_df.Të¼ê³  í•´ë„ ë™ì¼ . | Â  | iPhone 7 Plus | iPhone 8 Plus | iPhone XS | iPhone XS Max | . | ì¶œì‹œì¼ | 2016-09-16 | 2017-09-22 | 2018-09-21 | 2018-09-21 | . | ë””ìŠ¤í”Œë ˆì´ | 5.5 | 5.5 | 5.8 | 6.5 | . | ë©”ëª¨ë¦¬ | 3GB | 3GB | 4GB | 4GB | . | ìˆœìœ„ | 1 | 2 | 3 | 4 | . | ì¶œì‹œ ë²„ì „ | iOS 10.0 | iOS 11.0 | iOS 12.0 | iOS 12.0 | . | Face ID | No | No | Yes | Yes | . ",
    "url": "https://chaelist.github.io/docs/pandas/pandas_data_modifying/#pandas-dataframe-%EB%B3%80%EA%B2%BD%ED%95%98%EA%B8%B0",
    "relUrl": "/docs/pandas/pandas_data_modifying/#pandas-dataframe-ë³€ê²½í•˜ê¸°"
  },"155": {
    "doc": "Pandas ë°ì´í„° ê°€ê³µ",
    "title": "Index, ì¹¼ëŸ¼ ë³€ê²½í•˜ê¸°",
    "content": "liverpool_df . | Â  | position | born | number | nationality | . | Roberto Firmino | FW | 1991 | no. 9 | Brazil | . | Sadio Mane | FW | 1992 | no. 10 | Senegal | . | Mohamed Salah | FW | 1992 | no. 11 | Egypt | . | Joe Gomez | DF | 1997 | no. 12 | England | . | Alisson Becker | GK | 1992 | no. 13 | Brazil | . df.rename() . : ì¹¼ëŸ¼ëª… ë°”ê¾¸ê¸° . liverpool_df.rename(columns={'position':'Position'}, inplace=True) liverpool_df . | Â  | Position | born | number | nationality | . | Roberto Firmino | FW | 1991 | no. 9 | Brazil | . | Sadio Mane | FW | 1992 | no. 10 | Senegal | . | Mohamed Salah | FW | 1992 | no. 11 | Egypt | . | Joe Gomez | DF | 1997 | no. 12 | England | . | Alisson Becker | GK | 1992 | no. 13 | Brazil | . +) ì¹¼ëŸ¼ëª… ì—¬ëŸ¬ ê°œ í•œë²ˆì— ë°”ê¾¸ê¸° . liverpool_df.rename(columns={'born':'Born', 'number':'Number', 'nationality':'Nationality'}, inplace=True) liverpool_df . | Â  | Position | Born | Number | Nationality | . | Roberto Firmino | FW | 1991 | no. 9 | Brazil | . | Sadio Mane | FW | 1992 | no. 10 | Senegal | . | Mohamed Salah | FW | 1992 | no. 11 | Egypt | . | Joe Gomez | DF | 1997 | no. 12 | England | . | Alisson Becker | GK | 1992 | no. 13 | Brazil | . Index ì´ë¦„ ë¶™ì´ê¸° . : df.index.name í™œìš© . liverpool_df.index.name = 'Player Name' liverpool_df . | Player Name | Position | Born | Number | Nationality | . | Roberto Firmino | FW | 1991 | no. 9 | Brazil | . | Sadio Mane | FW | 1992 | no. 10 | Senegal | . | Mohamed Salah | FW | 1992 | no. 11 | Egypt | . | Joe Gomez | DF | 1997 | no. 12 | England | . | Alisson Becker | GK | 1992 | no. 13 | Brazil | . **df.index.nameì€ Index ì´ë¦„ì„ ì¶œë ¥í•´ì£¼ëŠ” ì½”ë“œ . liverpool_df.index.name . 'Player Name' . df.reset_index() . : ìƒˆë¡œìš´ ìˆ«ìí˜• indexë¥¼ ìƒì„±í•¨ê³¼ ë™ì‹œì— ê¸°ì¡´ indexë¥¼ ì¹¼ëŸ¼ìœ¼ë¡œ ì¶”ê°€ . | ì¸ë±ìŠ¤ê°€ ì—°ì†ëœ int ìˆ«ìí˜• ë°ì´í„°ê°€ ì•„ë‹ ê²½ìš°ì— ë‹¤ì‹œ ì´ë¥¼ ì—°ì† int ìˆ«ìí˜• ë°ì´í„°ë¡œ ë°”ê¿”ì¤„ ë•Œ ì£¼ë¡œ ì‚¬ìš© | +) ì°¸ê³ : ë§Œì•½ ê¸°ì¡´ indexê°€ ì´ë¦„ì´ ì—†ì—ˆë‹¤ë©´, â€˜indexâ€™ë¼ëŠ” ì´ë¦„ì˜ ì¹¼ëŸ¼ëª…ìœ¼ë¡œ ì¶”ê°€ë¨ | . liverpool_df.reset_index() # inplace=Trueë¥¼ í•´ì£¼ì§€ ì•Šì•˜ê¸°ì— df ìì²´ëŠ” ë°”ë€Œì§€ ì•ŠìŒ ## ê¸°ì¡´ indexì˜€ë˜ 'Player Name'ê°€ ì¹¼ëŸ¼ìœ¼ë¡œ ë“¤ì–´ê°€ê³ , 0ë¶€í„° ì—°ì† ìˆ«ìí˜•ìœ¼ë¡œ ìƒˆë¡­ê²Œ indexê°€ í• ë‹¹ë¨ . | Â  | Player Name | Position | Born | Number | Nationality | . | 0 | Roberto Firmino | FW | 1991 | no. 9 | Brazil | . | 1 | Sadio Mane | FW | 1992 | no. 10 | Senegal | . | 2 | Mohamed Salah | FW | 1992 | no. 11 | Egypt | . | 3 | Joe Gomez | DF | 1997 | no. 12 | England | . | 4 | Alisson Becker | GK | 1992 | no. 13 | Brazil | . +) drop=Trueë¡œ í•˜ë©´ ì´ì „ indexëŠ” ì—†ì• ê³  ìƒˆë¡œìš´ indexë§Œ ë‚¨ëŠ”ë‹¤ . liverpool_df.reset_index(drop=True) # inplace=Trueë¥¼ í•´ì£¼ì§€ ì•Šì•˜ê¸°ì— df ìì²´ëŠ” ë°”ë€Œì§€ ì•ŠìŒ ## ê¸°ì¡´ indexì˜€ë˜ 'Player Name'ëŠ” ì—†ì–´ì§€ê³ , 0ë¶€í„° ì—°ì† ìˆ«ìí˜•ìœ¼ë¡œ ìƒˆë¡­ê²Œ indexê°€ í• ë‹¹ë¨ . | Â  | Position | Born | Number | Nationality | . | 0 | FW | 1991 | no. 9 | Brazil | . | 1 | FW | 1992 | no. 10 | Senegal | . | 2 | FW | 1992 | no. 11 | Egypt | . | 3 | DF | 1997 | no. 12 | England | . | 4 | GK | 1992 | no. 13 | Brazil | . df.set_index() . : ë‹¤ë¥¸ ì¹¼ëŸ¼ì„ indexë¡œ ì„¤ì •, ê¸°ì¡´ì˜ indexëŠ” ì—†ì–´ì§„ë‹¤ . liverpool_df.set_index('Number') # inplace=Trueë¥¼ í•´ì£¼ì§€ ì•Šì•˜ê¸°ì— df ìì²´ëŠ” ë°”ë€Œì§€ ì•ŠìŒ ## ê¸°ì¡´ indexì˜€ë˜ 'Player Name'ëŠ” ì—†ì–´ì§€ê³ , 'Number' ì¹¼ëŸ¼ì´ indexê°€ ë¨ . | Number | Position | Born | Nationality | . | no. 9 | FW | 1991 | Brazil | . | no. 10 | FW | 1992 | Senegal | . | no. 11 | FW | 1992 | Egypt | . | no. 12 | DF | 1997 | England | . | no. 13 | GK | 1992 | Brazil | . +) ê¸°ì¡´ indexë¥¼ ìƒì§€ ì•Šê³  set_index í•˜ë ¤ë©´ ë¯¸ë¦¬ indexë¥¼ ë³„ë„ì˜ ì—´ì— ì €ì¥í•´ì£¼ë©´ ëœë‹¤ . liverpool_df['Player Name'] = liverpool_df.index # ê¸°ì¡´ indexë¥¼ ë³„ë„ì˜ ì—´ì— ì €ì¥ liverpool_df.set_index('Number', inplace=True) # ë‹¤ìŒë„ ê°™ì€ íš¨ê³¼: # liverpool_df = liverpool_df.reset_index().set_index('Number') liverpool_df . | Number | Position | Born | Nationality | Player Name | . | no. 9 | FW | 1991 | Brazil | Roberto Firmino | . | no. 10 | FW | 1992 | Senegal | Sadio Mane | . | no. 11 | FW | 1992 | Egypt | Mohamed Salah | . | no. 12 | DF | 1997 | England | Joe Gomez | . | no. 13 | GK | 1992 | Brazil | Alisson Becker | . ì¹¼ëŸ¼ ìˆœì„œ ë³€ê²½í•˜ê¸° . liverpool_df = pd.read_csv('data/liverpool.csv', index_col=0) ## ë‹¤ì‹œ ë¶ˆëŸ¬ì˜´ liverpool_df . | Â  | position | born | number | nationality | . | Roberto Firmino | FW | 1991 | no. 9 | Brazil | . | Sadio Mane | FW | 1992 | no. 10 | Senegal | . | Mohamed Salah | FW | 1992 | no. 11 | Egypt | . | Joe Gomez | DF | 1997 | no. 12 | England | . | Alisson Becker | GK | 1992 | no. 13 | Brazil | . | sorted()ë¥¼ í™œìš©í•˜ì—¬ alphabetical orderë¡œ columnëª… ì •ë ¬ . columns = list(liverpool_df.columns) # columnì„ listë¡œ ë°›ì•„ì˜´ srt_col = sorted(columns) # columnëª… ì•ŒíŒŒë²³ìˆœ ì •ë ¬ liverpool_df_srt = liverpool_df[srt_col] # ì›í•˜ëŠ” ìˆœì„œì˜ ì—´ listë¥¼ ì´ë ‡ê²Œ ë„£ì–´ì£¼ë©´ ë¨ liverpool_df_srt ## ì—´ ìˆœì„œ: born, nationality, number, position . | Â  | born | nationality | number | position | . | Roberto Firmino | 1991 | Brazil | no. 9 | FW | . | Sadio Mane | 1992 | Senegal | no. 10 | FW | . | Mohamed Salah | 1992 | Egypt | no. 11 | FW | . | Joe Gomez | 1997 | England | no. 12 | DF | . | Alisson Becker | 1992 | Brazil | no. 13 | GK | . +) list(df.columns): ì¹¼ëŸ¼ëª…ë“¤ì„ listë¡œ ì¶œë ¥í•´ì¤€ë‹¤ . list(liverpool_df.columns) . ['position', 'born', 'number', 'nationality'] . | reversed()ë¥¼ í™œìš©í•˜ì—¬ ì›ë˜ ìˆœì„œì˜ ë°˜ëŒ€ë¡œ columnëª… ì •ë ¬ . rvs_col = list(reversed(columns)) # ì›ë˜ column ìˆœì„œì˜ ë°˜ëŒ€ë¡œ ì •ë ¬ liverpool_df_rvs = liverpool_df[rvs_col] liverpool_df_rsv ## ì—´ ìˆœì„œ: nationality, number, born, position . | Â  | nationality | number | born | position | . | Roberto Firmino | Brazil | no. 9 | 1991 | FW | . | Sadio Mane | Senegal | no. 10 | 1992 | FW | . | Mohamed Salah | Egypt | no. 11 | 1992 | FW | . | Joe Gomez | England | no. 12 | 1997 | DF | . | Alisson Becker | Brazil | no. 13 | 1992 | GK | . | . ",
    "url": "https://chaelist.github.io/docs/pandas/pandas_data_modifying/#index-%EC%B9%BC%EB%9F%BC-%EB%B3%80%EA%B2%BD%ED%95%98%EA%B8%B0",
    "relUrl": "/docs/pandas/pandas_data_modifying/#index-ì¹¼ëŸ¼-ë³€ê²½í•˜ê¸°"
  },"156": {
    "doc": "Pandas ë°ì´í„° ê²°í•© & ìš”ì•½",
    "title": "Pandas ë°ì´í„° ê²°í•© &amp; ìš”ì•½",
    "content": ". | Merge, Join, Concatenate . | pd.merge(df1, df2) | df1.join(df2) | pd.concat([df1, df2]) | . | ë°ì´í„° ìš”ì•½ ì§‘ê³„ . | df.groupby() | pd.pivot_table() | df.pivot() | . | Multi-Index, Multi-Header ë‹¤ë£¨ê¸° . | Multi-Index ë‹¤ë£¨ê¸° | Multi-Header ë‹¤ë£¨ê¸° | . | unstack, stackìœ¼ë¡œ pivot . | df.unstack() | df.stack() | . | . ",
    "url": "https://chaelist.github.io/docs/pandas/pandas_merge_group/#pandas-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EA%B2%B0%ED%95%A9--%EC%9A%94%EC%95%BD",
    "relUrl": "/docs/pandas/pandas_merge_group/#pandas-ë°ì´í„°-ê²°í•©--ìš”ì•½"
  },"157": {
    "doc": "Pandas ë°ì´í„° ê²°í•© & ìš”ì•½",
    "title": "Merge, Join, Concatenate",
    "content": "pd.merge(df1, df2) . df1 = pd.DataFrame({'customer_id': [1001, 1002, 1003, 1004, 1005], 'name': ['Annie', 'Chloe', 'Jacob', 'David', 'Ellie'], 'sex': ['F', 'F', 'M', 'M', 'F']}) df2 = pd.DataFrame({'customer_id': [1001, 1001, 1005, 1004, 1002, 1008], 'amount_spent': [10000, 20000, 12000, 5000, 30000, 35000]}) print(df1, '\\n') print(df2) . customer_id name sex 0 1001 Annie F 1 1002 Chloe F 2 1003 Jacob M 3 1004 David M 4 1005 Ellie F customer_id amount_spent 0 1001 10000 1 1001 20000 2 1005 12000 3 1004 5000 4 1002 30000 5 1008 35000 . | inner join (default): ë‘ DataFrameì— ëª¨ë‘ ë°ì´í„°ê°€ ì¡´ì¬í•˜ëŠ” í–‰ë§Œ ë‚¨ëŠ”ë‹¤ pd.merge(df1, df2) # ë³´í†µ defaultë¡œ 'inner join' ë°©ì‹ì´ ì‚¬ìš©ë˜ë©°, ìë™ìœ¼ë¡œ ì´ë¦„ì´ ê°™ì€ columnì„ ê¸°ì¤€ìœ¼ë¡œ mergeëœë‹¤ . | Â  | customer_id | name | sex | amount_spent | . | 0 | 1001 | Annie | F | 10000 | . | 1 | 1001 | Annie | F | 20000 | . | 2 | 1002 | Chloe | F | 30000 | . | 3 | 1004 | David | M | 5000 | . | 4 | 1005 | Ellie | F | 12000 | . | â€˜onâ€™ìœ¼ë¡œ í†µí•©í•  ì—´ ì§€ì • pd.merge(df1, df2, on='customer_id') # onì— ì–´ë–¤ ì—´ì„ ê¸°ì¤€ìœ¼ë¡œ í†µí•©í•  ê²ƒì¸ì§€ ì§€ì •í•´ì¤„ ìˆ˜ ìˆë‹¤. (ë‘ dfì— ëª¨ë‘ ì¡´ì¬í•˜ëŠ” ì—´ì´ì—¬ì•¼ í•¨) ## ì´ ì˜ˆì‹œì—ì„œëŠ” ìƒê´€ ì—†ì§€ë§Œ, ê¸°ì¤€ ì—´ì´ ì•„ë‹ˆë©´ì„œ ì´ë¦„ì´ ê°™ì€ ì—´ì´ 2ê°œ ì´ìƒì¼ ê²½ìš°ì—ëŠ” ê¼­ onì„ ì§€ì •í•´ì¤˜ì•¼ í•¨ . | Â  | customer_id | name | sex | amount_spent | . | 0 | 1001 | Annie | F | 10000 | . | 1 | 1001 | Annie | F | 20000 | . | 2 | 1002 | Chloe | F | 30000 | . | 3 | 1004 | David | M | 5000 | . | 4 | 1005 | Ellie | F | 12000 | . | outer join: ì–´ëŠ í•œ ìª½ì˜ DataFrameì—ë§Œ ìˆëŠ” ê°’ì´ë¼ë„ ë‹¤ ë³´ì—¬ì£¼ëŠ” ë°©ì‹ pd.merge(df1, df2, how='outer') # í•œ ìª½ì—ë§Œ ë°ì´í„°ê°€ ìˆë‹¤ë©´, ë°˜ëŒ€ìª½ DataFrameì˜ í•´ë‹¹ ë¶€ë¶„ì€ NaNìœ¼ë¡œ ì±„ì›Œì§ . | Â  | customer_id | name | sex | amount_spent | . | 0 | 1001 | Annie | F | 10000 | . | 1 | 1001 | Annie | F | 20000 | . | 2 | 1002 | Chloe | F | 30000 | . | 3 | 1003 | Jacob | M | NaN | . | 4 | 1004 | David | M | 5000 | . | 5 | 1005 | Ellie | F | 12000 | . | 6 | 1008 | NaN | NaN | 35000 | . | left outer join: ì™¼ìª½ DataFrame (df1)ì— ì¡´ì¬í•˜ëŠ” ê°’ì„ ê¸°ì¤€ìœ¼ë¡œ merge pd.merge(df1, df2, how='left') . | Â  | customer_id | name | sex | amount_spent | . | 0 | 1001 | Annie | F | 10000 | . | 1 | 1001 | Annie | F | 20000 | . | 2 | 1002 | Chloe | F | 30000 | . | 3 | 1003 | Jacob | M | NaN | . | 4 | 1004 | David | M | 5000 | . | 5 | 1005 | Ellie | F | 12000 | . | right outer join: ì˜¤ë¥¸ìª½ DataFrame (df2)ì— ì¡´ì¬í•˜ëŠ” ê°’ì„ ê¸°ì¤€ìœ¼ë¡œ merge pd.merge(df1, df2, how='right') . | Â  | customer_id | name | sex | amount_spent | . | 0 | 1001 | Annie | F | 10000 | . | 1 | 1001 | Annie | F | 20000 | . | 2 | 1005 | Ellie | F | 12000 | . | 3 | 1004 | David | M | 5000 | . | 4 | 1002 | Chloe | F | 30000 | . | 5 | 1008 | NaN | NaN | 35000 | . | í‚¤ê°€ ë˜ëŠ” ê¸°ì¤€ì—´ì˜ ì´ë¦„ì´ ë‘ ë°ì´í„°í”„ë ˆì„ì—ì„œ ë‹¤ë¥¸ ê²½ìš° df1 = pd.DataFrame({'customer_id': [1001, 1002, 1003, 1004, 1005], 'name': ['Annie', 'Chloe', 'Jacob', 'David', 'Ellie'], 'sex': ['F', 'F', 'M', 'M', 'F']}) df3 = pd.DataFrame({'customer': ['Chloe', 'Jacob', 'Ellie', 'Haley', 'Serene'], 'VIP_type': ['VIP', 'Gold', 'VIP', 'Silver', 'Gold']}) print(df1, '\\n') print(df3) . customer_id name sex 0 1001 Annie F 1 1002 Chloe F 2 1003 Jacob M 3 1004 David M 4 1005 Ellie F customer VIP_type 0 Chloe VIP 1 Jacob Gold 2 Ellie VIP 3 Haley Silver 4 Serene Gold . # í‚¤ê°€ ë˜ëŠ” ê¸°ì¤€ì—´ì˜ ì´ë¦„ì´ ë‘ ë°ì´í„°í”„ë ˆì„ì—ì„œ ë‹¤ë¥¸ ê²½ìš°: left_on, right_on ì¸ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ê¸°ì¤€ì—´ì„ ëª…ì‹œ . pd.merge(df1, df3, left_on='name', right_on=\"customer\") # default: inner join . | Â  | customer_id | name | sex | customer | VIP_type | . | 0 | 1002 | Chloe | F | Chloe | VIP | . | 1 | 1003 | Jacob | M | Jacob | Gold | . | 2 | 1005 | Ellie | F | Ellie | VIP | . | . df1.join(df2) . | join ë©”ì†Œë“œë¥¼ ì‚¬ìš©í•´ë„ mergeì™€ ê°™ì€ ê²°ê³¼ë¥¼ ë‚¼ ìˆ˜ ìˆë‹¤ | í•˜ì§€ë§Œ join()ì€ indexë¥¼ ê¸°ì¤€ìœ¼ë¡œ ê²°í•©í•˜ëŠ” ê²ƒì´ ê¸°ë³¸ì´ê³ , how=â€™leftâ€™ê°€ defaultë‹¤ | . # joinì„ ì“°ê¸° ìœ„í•´, df1ê³¼ df2 ëª¨ë‘ ê²°í•©í•  ê¸°ì¤€ì´ ë˜ëŠ” ì—´ì¸ 'customer_id'ë¥¼ indexë¡œ ì§€ì •í•´ì¤€ë‹¤ df1.set_index('customer_id', inplace=True) df2.set_index('customer_id', inplace=True) print(df1, '\\n') print(df2) . name sex customer_id 1001 Annie F 1002 Chloe F 1003 Jacob M 1004 David M 1005 Ellie F amount_spent customer_id 1001 10000 1001 20000 1005 12000 1004 5000 1002 30000 1008 35000 . | left outer join (default) df1.join(df2) # how='left'ê°€ defaultê°’ . | customer_id | name | sex | amount_spent | . | 1001 | Annie | F | 10000 | . | 1001 | Annie | F | 20000 | . | 1002 | Chloe | F | 30000 | . | 1003 | Jacob | M | NaN | . | 1004 | David | M | 5000 | . | 1005 | Ellie | F | 12000 | . | inner join df1.join(df2, how='inner') # howì— 'right', 'outer', 'innner' ëª¨ë‘ ì‚¬ìš© ê°€ëŠ¥ . | customer_id | name | sex | amount_spent | . | 1001 | Annie | F | 10000 | . | 1001 | Annie | F | 20000 | . | 1002 | Chloe | F | 30000 | . | 1004 | David | M | 5000 | . | 1005 | Ellie | F | 12000 | . | . pd.concat([df1, df2]) . | ê¸°ì¤€ ì—´(key column)ì„ ì‚¬ìš©í•˜ì§€ ì•Šê³  ë‹¨ìˆœíˆ ë°ì´í„°ë¥¼ ì—°ê²°(concatenate)í•œë‹¤ | ê¸°ë³¸ì ìœ¼ë¡œëŠ” ìœ„/ì•„ë˜ë¡œ ë°ì´í„° í–‰ì„ ì—°ê²°í•˜ë©°, axis=1ë¡œ ì¸ìˆ˜ë¥¼ ì„¤ì •í•˜ë©´ ì˜†ìœ¼ë¡œ ì—°ê²°í•´ì¤€ë‹¤ | . import numpy as np df1 = pd.DataFrame( np.arange(6).reshape(3, 2), columns=['a', 'b']) df2 = pd.DataFrame( 5 + np.arange(6).reshape(2,3), columns=['a', 'b', 'c']) print(df1, '\\n') print(df2) . a b 0 0 1 1 2 3 2 4 5 a b c 0 5 6 7 1 8 9 10 . | ìœ„ ì•„ë˜ë¡œ ì—°ê²° (default) pd.concat([df1, df2]) ## ìœ„ ì•„ë˜ë¡œ ì—°ê²°ë¨. (columnëª… ê¸°ì¤€) . | Â  | a | b | c | . | 0 | 0 | 1 | NaN | . | 1 | 2 | 3 | NaN | . | 2 | 4 | 5 | NaN | . | 0 | 5 | 6 | 7 | . | 1 | 8 | 9 | 10 | . +) ê·¸ëƒ¥ ì´ì–´ë¶™ì´ë©´ í–‰ ì¸ë±ìŠ¤ë²ˆí˜¸ë„ ê·¸ëŒ€ë¡œ ê°€ì ¸ì˜¤ê¸°ì—, ignore_index=Trueë¡œ ì¸ë±ìŠ¤ë¥¼ ì¬ë°°ì—´í•  ìˆ˜ ìˆë‹¤ . pd.concat([df1, df2], ignore_index=True) . | Â  | a | b | c | . | 0 | 0 | 1 | NaN | . | 1 | 2 | 3 | NaN | . | 2 | 4 | 5 | NaN | . | 3 | 5 | 6 | 7 | . | 4 | 8 | 9 | 10 | . | ì˜†ìœ¼ë¡œ ì—°ê²°: axis=1 ì„¤ì • pd.concat([df1, df2], axis=1) # axis=1ë¡œ ì„¤ì •í•´ì£¼ë©´, ì˜†ìœ¼ë¡œ ì—°ê²°ëœë‹¤ (index ê¸°ì¤€) . | Â  | a | b | a | b | c | . | 0 | 0 | 1 | 5 | 6 | 7 | . | 1 | 2 | 3 | 8 | 9 | 10 | . | 2 | 4 | 5 | NaN | NaN | NaN | . | Series ê°„ ê²°í•© sr1 = pd.Series(['e0','e1','e2','e3'], name = 'e') sr2 = pd.Series(['g0','g1','g2','g3'], name = 'g') result1 = pd.concat([sr1, sr2], axis = 1) #ì—´ë°©í–¥ ì—°ê²°, type = ë°ì´í„°í”„ë ˆì„ì´ ë¨ print(result1, '\\n') result2 = pd.concat([sr1, sr2], axis = 0) #í–‰ë°©í–¥ ì—°ê²°, type = ì‹œë¦¬ì¦ˆ print(result2, '\\n') ## ì‚¬ì‹¤, pd.concat([sr1, sr2], axis = 0)ëŠ” sr1.append(sr2)ì™€ ë™ì¼í•œ ê²°ê³¼ result3 = sr1.append(sr2) #ì´ë ‡ê²Œ í•˜ë©´ í–‰ë°©í–¥ concatê³¼ ë™ì¼ print(result3) . e g 0 e0 g0 1 e1 g1 2 e2 g2 3 e3 g3 0 e0 1 e1 2 e2 3 e3 0 g0 1 g1 2 g2 3 g3 dtype: object 0 e0 1 e1 2 e2 3 e3 0 g0 1 g1 2 g2 3 g3 dtype: object . +) ì°¸ê³ ) pd.merge()ëŠ” Seriesê°„ ê²°í•©ì—ëŠ” ì‚¬ìš©í•  ìˆ˜ ì—†ë‹¤. (ê³µí†µì˜ columnì´ ìˆì–´ì•¼ í•˜ëŠ” ê²Œ ì „ì œì¡°ê±´) . pd.merge(sr1, sr2) ## seriesë¼ë¦¬ mergeí•˜ë ¤ê³  í•˜ë©´ error. MergeError Traceback (most recent call last) &lt;ipython-input-43-176c5dc5c9a2&gt; in &lt;module&gt;() ----&gt; 1 pd.merge(sr1, sr2) . | . ",
    "url": "https://chaelist.github.io/docs/pandas/pandas_merge_group/#merge-join-concatenate",
    "relUrl": "/docs/pandas/pandas_merge_group/#merge-join-concatenate"
  },"158": {
    "doc": "Pandas ë°ì´í„° ê²°í•© & ìš”ì•½",
    "title": "ë°ì´í„° ìš”ì•½ ì§‘ê³„",
    "content": "df.groupby() . : ê·¸ë£¹ë³„ë¡œ ìš”ì•½í•´ì„œ ì§‘ê³„í•˜ê¸° . titanic_df = pd.read_csv('data/titanic.csv') ## ë°ì´í„° ì¶œì²˜: kaggle titanic_df.head() . | Â  | PassengerId | Survived | Pclass | Name | Sex | Age | SibSp | Parch | Ticket | Fare | Cabin | Embarked | . | 0 | 1 | 0 | 3 | Braund, Mr. Owen Harris | male | 22 | 1 | 0 | A/5 21171 | 7.25 | nan | S | . | 1 | 2 | 1 | 1 | Cumings, Mrs. John Bradley (Florence Briggs Thayer) | female | 38 | 1 | 0 | PC 17599 | 71.2833 | C85 | C | . | 2 | 3 | 1 | 3 | Heikkinen, Miss. Laina | female | 26 | 0 | 0 | STON/O2. 3101282 | 7.925 | nan | S | . | 3 | 4 | 1 | 1 | Futrelle, Mrs. Jacques Heath (Lily May Peel) | female | 35 | 1 | 0 | 113803 | 53.1 | C123 | S | . | 4 | 5 | 0 | 3 | Allen, Mr. William Henry | male | 35 | 0 | 0 | 373450 | 8.05 | nan | S | . | df.groupby(â€˜ì¹¼ëŸ¼ëª…â€™).í•¨ìˆ˜() . | DataFrameì— groupby()ë¥¼ í˜¸ì¶œí•´ ë°˜í™˜ëœ ê²°ê³¼ì— aggregation í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ë©´, groupby() ëŒ€ìƒ ì¹¼ëŸ¼ì„ ì œì™¸í•œ ëª¨ë“  ì¹¼ëŸ¼ì— aggregation í•¨ìˆ˜ë¥¼ ì ìš©í•œ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤€ë‹¤ | aggregation í•¨ìˆ˜ ì¢…ë¥˜: count, sum, max, mean, nunique ë“±. (*nunique: unique count) | ëŒ€ìƒ ì¹¼ëŸ¼ì´ indexë¡œ ë“¤ì–´ê° | . # 'Pclass' ê¸°ì¤€ìœ¼ë¡œ ë°ì´í„° ìˆ˜ë¥¼ ì§‘ê³„ titanic_df.groupby('Pclass').count() ## 3ë“±ì„ì´ ê°€ì¥ ì‚¬ëŒì´ ë§ìŒì„ ìœ ì¶” ê°€ëŠ¥. | Pclass | PassengerId | Survived | Name | Sex | Age | SibSp | Parch | Ticket | Fare | Cabin | Embarked | . | 1 | 216 | 216 | 216 | 216 | 186 | 216 | 216 | 216 | 216 | 176 | 214 | . | 2 | 184 | 184 | 184 | 184 | 173 | 184 | 184 | 184 | 184 | 16 | 184 | . | 3 | 491 | 491 | 491 | 491 | 355 | 491 | 491 | 491 | 491 | 12 | 491 | . | as_index=False: ëŒ€ìƒ ì¹¼ëŸ¼ì´ indexê°€ ì•„ë‹Œ ì¹¼ëŸ¼ìœ¼ë¡œ ë“¤ì–´ê° # ì´ë ‡ê²Œ as_index=Falseë¥¼ ë„£ì–´ì£¼ë©´ indexê°€ ì•„ë‹Œ ì¹¼ëŸ¼ìœ¼ë¡œ ë“¤ì–´ê° titanic_df.groupby('Pclass', as_index=False).count() . | Â  | Pclass | PassengerId | Survived | Name | Sex | Age | SibSp | Parch | Ticket | Fare | Cabin | Embarked | . | 0 | 1 | 216 | 216 | 216 | 216 | 186 | 216 | 216 | 216 | 216 | 176 | 214 | . | 1 | 2 | 184 | 184 | 184 | 184 | 173 | 184 | 184 | 184 | 184 | 16 | 184 | . | 2 | 3 | 491 | 491 | 491 | 491 | 355 | 491 | 491 | 491 | 491 | 12 | 491 | . | íŠ¹ì • ì¹¼ëŸ¼ë§Œ í•„í„°ë§í•´ì„œ í•¨ìˆ˜ ì ìš© . titanic_df.groupby('Pclass')[['PassengerId', 'Survived']].count() . | Pclass | PassengerId | Survived | . | 1 | 216 | 216 | . | 2 | 184 | 184 | . | 3 | 491 | 491 | . +) ì´ë ‡ê²Œ í•˜ëŠ” ê²ƒë„ ê°€ëŠ¥: . titanic_df.groupby('Pclass', as_index=False)[['PassengerId', 'Survived']].count() . | Â  | Pclass | PassengerId | Survived | . | 0 | 1 | 216 | 216 | . | 1 | 2 | 184 | 184 | . | 2 | 3 | 491 | 491 | . | ì—¬ëŸ¬ ê°œì˜ aggregation í•¨ìˆ˜ë¥¼ ì ìš© . | ì ìš©í•˜ë ¤ëŠ” í•¨ìˆ˜ëª…ì„ agg( ) ë‚´ì— ì¸ìë¡œ ì…ë ¥í•´ì„œ ì‚¬ìš©í•´ì•¼ í•¨ | . titanic_df.groupby('Pclass')['Age'].agg([max, min]) . | Pclass | max | min | . | 1 | 80 | 0.92 | . | 2 | 70 | 0.67 | . | 3 | 74 | 0.42 | . | ì—¬ëŸ¬ ì¹¼ëŸ¼ì— ì—¬ëŸ¬ aggregation í•¨ìˆ˜ ì ìš© . titanic_df.groupby('Pclass')[['Age', 'Fare', 'SibSp']].agg([max, min]) . | Â  | : Age | | : Fare | | : SibSp | | Â  | Â  | Â  | . | Pclass | max | min | max | min | max | min | . | 1 | 80 | 0.92 | 512.329 | 0 | 3 | 0 | . | 2 | 70 | 0.67 | 73.5 | 0 | 3 | 0 | . | 3 | 74 | 0.42 | 69.55 | 0 | 8 | 0 | . | ì—¬ëŸ¬ ì¹¼ëŸ¼ì— ì„œë¡œ ë‹¤ë¥¸ Aggregation í•¨ìˆ˜ë¥¼ ì ìš© . | dictionary í˜•íƒœë¡œ í•¨ìˆ˜ë¥¼ ì ìš©í•  ì¹¼ëŸ¼ê³¼ í•¨ìˆ˜ëª…ì„ ì…ë ¥ | â€» aggregation ë°©ì‹ ì¤‘ nuniqueì˜ ê²½ìš°, 'Age': pd.Series.nuniqueì™€ ê°™ì€ ë°©ì‹ìœ¼ë¡œ í‘œê¸°í•´ì¤˜ì•¼ í•œë‹¤ | . agg_format = {'Age':'max', 'SibSp':'sum', 'Fare':'mean'} titanic_df.groupby('Pclass').agg(agg_format) . | Pclass | Age | SibSp | Fare | . | 1 | 80 | 90 | 84.1547 | . | 2 | 70 | 74 | 20.6622 | . | 3 | 74 | 302 | 13.6756 | . | . +) string value groupby . # sample dataframe ìƒì„±: ì´ìš©ìì™€ ì´ìš©í•œ ì•±ì´ ë§¤ì¹­ë˜ì–´ ìˆëŠ” ë°ì´í„° users = ['A', 'B', 'A', 'B', 'C', 'D', 'E', 'D', 'E'] apps_used = ['Google', 'Naver', 'YouTube', 'Google', 'YouTube', 'Facebook', 'Instagram', 'Naver', 'Google'] sample_df = pd.DataFrame({'users':users, 'apps_used':apps_used}) sample_df . | Â  | users | apps_used | . | 0 | A | Google | . | 1 | B | Naver | . | 2 | A | YouTube | . | 3 | B | Google | . | 4 | C | YouTube | . | 5 | D | Facebook | . | 6 | E | Instagram | . | 7 | D | Naver | . | 8 | E | Google | . | listë¡œ ë¬¶ê¸°: ê° ì´ìš©ìë³„ ì´ìš©í•œ ì•± ì¡°í•© sample_df.groupby('users')['apps_used'].apply(list) . users A [Google, YouTube] B [Naver, Google] C [YouTube] D [Facebook, Naver] E [Instagram, Google] Name: apps_used, dtype: object . | setìœ¼ë¡œ ë¬¶ê¸°: ìˆœì„œ ê³ ë ¤X sample_df.groupby('users')['apps_used'].apply(set) . users A {Google, YouTube} B {Google, Naver} C {YouTube} D {Facebook, Naver} E {Google, Instagram} Name: apps_used, dtype: object . | apply lambdaë¥¼ í™œìš©í•´ ì›í•˜ëŠ” ëª¨ì–‘ìœ¼ë¡œ ê²°í•© sample_df.groupby('users')['apps_used'].apply(lambda x: ', '.join(x)) . users A Google, YouTube B Naver, Google C YouTube D Facebook, Naver E Instagram, Google Name: apps_used, dtype: object . | . Â  . pd.pivot_table() . : ì—‘ì…€ì—ì„œì²˜ëŸ¼ pivot ëŒë¦¬ê¸° (ì—‘ì…€ì—ì„œ pivot ëŒë¦¬ëŠ” ê²ƒì˜ ë™ì‘ ë°©ì‹ì„ ìƒê°í•˜ë©´ ì‰¬ì›€) . | ex) pd.pivot_table(df, index='ì¹¼ëŸ¼1', columns=['ì¹¼ëŸ¼2','ì¹¼ëŸ¼3'], values='ì¹¼ëŸ¼4', fill_value=0, aggfunc='sum') . | index(ì—´)ë‚˜ columns(í–‰)ì— str í•˜ë‚˜ë§Œ ë„£ì–´ë„ ë˜ê³ , [ ] ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ì—¬ëŸ¬ ê°œ ë„£ìœ¼ë©´ multiindex / multiheaderë˜ëŠ” ê²ƒ | valuesê°€ ì‹¤ì œ pivotì„ ì±„ìš¸ ê°’ | aggfuncì—ëŠ” ì§‘ê³„ í•¨ìˆ˜ë¥¼ ì ì–´ ì¤Œ. - â€˜sumâ€™ì´ë©´ í•©ì‚° / â€˜meanâ€™ì´ë©´ í‰ê·  | fill_value=0ì€ NaN ê°’ë“¤ì„ 0ìœ¼ë¡œ ì±„ì›Œì„œ returní•˜ê² ë‹¤ëŠ” ëœ» | . | . titanic_df.head() . | Â  | PassengerId | Survived | Pclass | Name | Sex | Age | SibSp | Parch | Ticket | Fare | Cabin | Embarked | . | 0 | 1 | 0 | 3 | Braund, Mr. Owen Harris | male | 22 | 1 | 0 | A/5 21171 | 7.25 | nan | S | . | 1 | 2 | 1 | 1 | Cumings, Mrs. John Bradley (Florence Briggs Thayer) | female | 38 | 1 | 0 | PC 17599 | 71.2833 | C85 | C | . | 2 | 3 | 1 | 3 | Heikkinen, Miss. Laina | female | 26 | 0 | 0 | STON/O2. 3101282 | 7.925 | nan | S | . | 3 | 4 | 1 | 1 | Futrelle, Mrs. Jacques Heath (Lily May Peel) | female | 35 | 1 | 0 | 113803 | 53.1 | C123 | S | . | 4 | 5 | 0 | 3 | Allen, Mr. William Henry | male | 35 | 0 | 0 | 373450 | 8.05 | nan | S | . | ê¸°ë³¸ ì˜ˆì‹œ . pd.pivot_table(titanic_df, # í”¼ë²—í•  ë°ì´í„°í”„ë ˆì„ index='Pclass', # í–‰ ìœ„ì¹˜ì— ë“¤ì–´ê°ˆ ì—´ columns='Sex', # ì—´ ìœ„ì¹˜ì— ë“¤ì–´ê°ˆ ì—´ values='Age', # ë°ì´í„°ë¡œ ì‚¬ìš©í•  ì—´ aggfunc='mean') # ë°ì´í„° ì§‘ê³„í•¨ìˆ˜ . | Sex | female | male | . | Pclass | Â  | Â  | . | 1 | 34.6118 | 41.2814 | . | 2 | 28.723 | 30.7407 | . | 3 | 21.75 | 26.5076 | . | ì—¬ëŸ¬ ê°œì˜ ë°ì´í„° ì§‘ê³„í•¨ìˆ˜ ë„£ê¸° pd.pivot_table(titanic_df, index='Pclass', columns='Sex', values='Survived', aggfunc=['mean', 'sum']) . | Â  | : mean | | : sum| | Â  | Â  | . | Sex | female | male | female | male | . | Pclass | Â  | Â  | Â  | Â  | . | 1 | 0.968085 | 0.368852 | 91 | 45 | . | 2 | 0.921053 | 0.157407 | 70 | 17 | . | 3 | 0.5 | 0.135447 | 72 | 47 | . | ë§ì€ ì¹¼ëŸ¼ì„ ë™ì‹œì— ì¸ìë¡œ ì…ë ¥ . | index=, columns=, values=ì— ëª¨ë‘ list í˜•íƒœë¡œ ì—¬ëŸ¬ ê°œì˜ ì¹¼ëŸ¼ì„ ì¸ìë¡œ ë„£ì„ ìˆ˜ ìˆë‹¤ | . pd.pivot_table(titanic_df, index=['Pclass', 'Sex'], columns='Survived', values=['Age', 'Fare'], aggfunc=['mean', 'max']) . | Â  | Â  | : mean ||| | : max ||| | Â  | Â  | Â  | Â  | Â  | Â  | . | Â  | Â  | : Age | | : Fare | | : Age | | : Fare | | Â  | Â  | Â  | Â  | . | Â  | Survived | : 0 | : 1 | : 0 | : 1 | : 0 | : 1 | : 0 | : 1 | . | Pclass | Sex | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | . | 1 | female | 25.6667 | 34.939 | 110.604 | 105.978 | 50 | 63 | 151.55 | 512.329 | . | ^^ | male | 44.582 | 36.248 | 62.8949 | 74.6373 | 71 | 80 | 263 | 512.329 | . | 2 | female | 36 | 28.0809 | 18.25 | 22.289 | 57 | 55 | 26 | 65 | . | ^^ | male | 33.369 | 16.022 | 19.489 | 21.0951 | 70 | 62 | 73.5 | 39 | . | 3 | female | 23.8182 | 19.3298 | 19.7731 | 12.4645 | 48 | 63 | 69.55 | 31.3875 | . | ^^ | male | 27.2558 | 22.2742 | 12.2045 | 15.5797 | 74 | 45 | 69.55 | 56.4958 | . | . df.pivot() . : ë°ì´í„° í˜•íƒœë¥¼ ë³€ê²½í•´ì¤€ë‹¤. ì›í•˜ëŠ” í˜•íƒœë¡œ ë°ì´í„°ë¥¼ ë³€í˜•í•´ì¤€ë‹¤ëŠ” ì ì—ì„œ pd.pivot_tableê³¼ ìœ ì‚¬í•˜ì§€ë§Œ, pivot_tableê³¼ ë‹¬ë¦¬ ë°ì´í„°ë¥¼ ìš”ì•½ ì§‘ê³„í•´ì£¼ëŠ” ê¸°ëŠ¥ì€ ì—†ìŒ . pivot_sample = titanic_df.groupby(['Pclass', 'Sex'])[['Fare']].mean().reset_index() pivot_sample . | Â  | Pclass | Sex | Fare | . | 0 | 1 | female | 106.126 | . | 1 | 1 | male | 67.2261 | . | 2 | 2 | female | 21.9701 | . | 3 | 2 | male | 19.7418 | . | 4 | 3 | female | 16.1188 | . | 5 | 3 | male | 12.6616 | . â†’ â€˜Pclassâ€™ ì •ë³´ê°€ indexë¡œ, â€˜Sexâ€™ ì •ë³´ê°€ columnìœ¼ë¡œ, ê·¸ë¦¬ê³  â€˜Fareâ€™ê°€ valueë¡œ ë“¤ì–´ê°€ë„ë¡ reshape: . pivot_sample.pivot(index='Pclass', columns='Sex', values='Fare') . | Pclass | female | male | . | 1 | 106.126 | 67.2261 | . | 2 | 21.9701 | 19.7418 | . | 3 | 16.1188 | 12.6616 | . ",
    "url": "https://chaelist.github.io/docs/pandas/pandas_merge_group/#%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%9A%94%EC%95%BD-%EC%A7%91%EA%B3%84",
    "relUrl": "/docs/pandas/pandas_merge_group/#ë°ì´í„°-ìš”ì•½-ì§‘ê³„"
  },"159": {
    "doc": "Pandas ë°ì´í„° ê²°í•© & ìš”ì•½",
    "title": "Multi-Index, Multi-Header ë‹¤ë£¨ê¸°",
    "content": ": groupbyë‚˜ pivot_tableë¡œ ë°ì´í„°ë¥¼ ìš”ì•½ ì§‘ê³„í•˜ë‹¤ ë³´ë©´, multi-indexë‚˜ multi-headerë¥¼ ë‹¤ë£¨ê²Œ ëœë‹¤ . Multi-Index ë‹¤ë£¨ê¸° . gdf = titanic_df.groupby(['Pclass', 'Sex'])[['Age', 'Fare', 'Survived']].mean() gdf . | Â  | Â  | : Age | : Fare | : Survived | . | Pclass | Sex | Â  | Â  | Â  | . | 1 | female | 34.6118 | 106.126 | 0.968085 | . | ^^ | male | 41.2814 | 67.2261 | 0.368852 | . | 2 | female | 28.723 | 21.9701 | 0.921053 | . | ^^ | male | 30.7407 | 19.7418 | 0.157407 | . | 3 | female | 21.75 | 16.1188 | 0.5 | . | ^^ | male | 26.5076 | 12.6616 | 0.135447 | . | ì¸ë±ì‹±: 1st level # [Pclass]ì—´ì˜ [1]í–‰ë§Œ ì¸ë±ì‹± (index_number=1ì´ ì•„ë‹ˆë¼, ì¸ë±ìŠ¤ ì´ë¦„ì´ 1) gdf.loc[1] ## 1ì€ int íƒ€ì…ì´ë¼ì„œ '1'ì´ë¼ê³  ì“°ì§€ ì•ŠìŒ . | Sex | Age | Fare | Survived | . | female | 34.6118 | 106.126 | 0.968085 | . | male | 41.2814 | 67.2261 | 0.368852 | . | ì¸ë±ì‹±: 1st level - 2nd level (ìˆœì„œëŒ€ë¡œ, ì°¨ê·¼ì°¨ê·¼) # [Pclass]ì—´ì˜ [1]í–‰ ì¤‘, [Sex] ì—´ì˜ [female]í–‰ë§Œ ì¸ë±ì‹± (ìˆœì„œ: 1, female) ## ì´ë ‡ê²Œ tuple í˜•íƒœë¡œ ë„£ì–´ì£¼ë ¤ë©´, ë” ë°–ì— ìˆëŠ” indexë¶€í„° ì°¨ê·¼ì°¨ê·¼ ì ‘ê·¼í•´ì•¼ í•œë‹¤ gdf.loc[(1, 'female')] ## pivot_df.loc[1].loc['female'] ì´ë ‡ê²Œ í•´ë„ ë™ì¼í•œ ê²°ê³¼. Age 34.611765 Fare 106.125798 Survived 0.968085 Name: (1, female), dtype: float64 . | ì¸ë±ì‹±: 2nd levelì— ë°”ë¡œ ì ‘ê·¼ . | ë©€í‹°ì¸ë±ì„œ xsë¥¼ ì´ìš©í•˜ë©´ ê·¸ë£¹ ë²”ì£¼ì™€ ìƒê´€ ì—†ì´ ìˆ˜ì¤€(level)ë§Œ ëª…ì‹œí•´ì£¼ë©´ ì¸ë±ì‹±ì´ ê°€ëŠ¥ (cf.locì™€ ilocë¥¼ ì´ìš©í•˜ë ¤ë©´ í° ê·¸ë£¹ë¶€í„° ìˆœì°¨ì ìœ¼ë¡œ ì¸ë±ì‹±ì„ í•´ì•¼ í•¨) | . # Sexê·¸ë£¹ì˜ maleê°’ì„ ê°–ëŠ” í–‰ì„ ëª¨ë‘ ì¶”ì¶œ, ì¦‰ ë“±ê¸‰(class)ë³„ maleì— ëŒ€í•œ ìë£Œë¥¼ ì¸ë±ì‹± gdf.xs('male', level='Sex') . | Pclass | Age | Fare | Survived | . | 1 | 41.2814 | 67.2261 | 0.368852 | . | 2 | 30.7407 | 19.7418 | 0.157407 | . | 3 | 26.5076 | 12.6616 | 0.135447 | . +) level=ì—ëŠ” ì¸ë±ìŠ¤ëª…ì„ ì ì–´ì¤˜ë„ ë˜ê³ , í•´ë‹¹ ì¸ë±ìŠ¤ì˜ ë ˆë²¨ì„ intë¡œ ë„£ì–´ì¤˜ë„ ëœë‹¤. (0ë¶€í„° ì‹œì‘) . ## 'Sex' levelì€ ë‘ë²ˆì§¸ levelì´ë¯€ë¡œ, level=1ì´ë¼ê³  ë„£ì–´ì¤˜ë„ ìœ„ì™€ ê°™ì€ ê²°ê³¼. gdf.xs('male', level=1) . | ë©€í‹° ì¸ë±ìŠ¤ í•´ì œ: reset_index() í™œìš© . gdf.reset_index() . | Â  | Pclass | Sex | Age | Fare | Survived | . | 0 | 1 | female | 34.6118 | 106.126 | 0.968085 | . | 1 | 1 | male | 41.2814 | 67.2261 | 0.368852 | . | 2 | 2 | female | 28.723 | 21.9701 | 0.921053 | . | 3 | 2 | male | 30.7407 | 19.7418 | 0.157407 | . | 4 | 3 | female | 21.75 | 16.1188 | 0.5 | . | 5 | 3 | male | 26.5076 | 12.6616 | 0.135447 | . | . Multi-Header ë‹¤ë£¨ê¸° . gdf2 = titanic_df.groupby('Pclass')[['Age', 'Fare']].agg(['mean', 'max']) gdf2 . | Â  | : Age | | : Fare | | Â  | Â  | . | Â  | : mean | : max | : mean | : max | . | Pclass | Â  | Â  | Â  | Â  | . | 1 | 38.2334 | 80 | 84.1547 | 512.329 | . | 2 | 29.8776 | 70 | 20.6622 | 73.5 | . | 3 | 25.1406 | 74 | 13.6756 | 69.55 | . | ì¸ë±ì‹±: 1st level . # 'Age' ì—´ì„ indexing gdf2['Age'] . | Pclass | mean | max | . | 1 | 38.2334 | 80 | . | 2 | 29.8776 | 70 | . | 3 | 25.1406 | 74 | . | ì¸ë±ì‹±: 1st level - 2nd level (ìˆœì„œëŒ€ë¡œ, ì°¨ê·¼ì°¨ê·¼) . # 'Age' ì—´ì˜ 'mean' ì—´ì„ indexing gdf2['Age']['mean'] ## gdf2[('Age', 'mean')] ì´ë ‡ê²Œ ë„£ì–´ì¤˜ë„ ë™ì¼í•œ ê²°ê³¼. Pclass 1 38.233441 2 29.877630 3 25.140620 Name: mean, dtype: float64 . | ì¸ë±ì‹±: 2nd levelì— ë°”ë¡œ ì ‘ê·¼ . | ì—´ ë°©í–¥ ë©€í‹°ì¸ë±ì‹±ì—ë„ xsë¥¼ ì‚¬ìš© â€“ axis=1ì´ë¼ê³  í•˜ë©´ ì—´ ë°©í–¥ì„ ì˜ë¯¸ | . # 'mean' í–‰ì„ ëª¨ë‘ ì¶”ì¶œ -- 'Age'ì™€ 'Fare'ì˜ 'mean'ì„ ëª¨ë‘ ì¶”ì¶œ gdf2.xs('mean', level=1, axis=1) . | Pclass | Age | Fare | . | 1 | 38.2334 | 84.1547 | . | 2 | 29.8776 | 20.6622 | . | 3 | 25.1406 | 13.6756 | . | ë©€í‹° í—¤ë” í•´ì œ: columnëª…ì„ ì•„ë˜ì™€ ê°™ì´ ìƒˆë¡œ ë„£ì–´ì£¼ë©´ ëœë‹¤ . # ì´ë ‡ê²Œ columnëª…ì„ ë„£ì–´ì£¼ë©´ ëœë‹¤ (ë‘ ë ˆë²¨ì˜ ì¹¼ëŸ¼ëª…ì„ ëª¨ë‘ ë‹´ì€ ì´ë¦„ìœ¼ë¡œ ì ì–´ì•¼ ì¢‹ìŒ) gdf2.columns = ['Age_mean', 'Age_max', 'Fare_mean','Fare_max'] gdf2 . | Pclass | Age_mean | Age_max | Fare_mean | Fare_max | . | 1 | 38.2334 | 80 | 84.1547 | 512.329 | . | 2 | 29.8776 | 70 | 20.6622 | 73.5 | . | 3 | 25.1406 | 74 | 13.6756 | 69.55 | . +) map ì‚¬ìš©í•´ì„œ ìƒˆë¡œìš´ ì¹¼ëŸ¼ëª… ìƒì„±í•˜ê¸° . gdf2.columns = gdf2.columns.map('{0[0]}_{0[1]}'.format) # column1_column2ì˜ í˜•íƒœë¡œ ìƒˆ columnëª…ì„ ìƒì„±í•´ì¤Œ gdf2 . | Pclass | Age_mean | Age_max | Fare_mean | Fare_max | . | 1 | 38.2334 | 80 | 84.1547 | 512.329 | . | 2 | 29.8776 | 70 | 20.6622 | 73.5 | . | 3 | 25.1406 | 74 | 13.6756 | 69.55 | . | . ",
    "url": "https://chaelist.github.io/docs/pandas/pandas_merge_group/#multi-index-multi-header-%EB%8B%A4%EB%A3%A8%EA%B8%B0",
    "relUrl": "/docs/pandas/pandas_merge_group/#multi-index-multi-header-ë‹¤ë£¨ê¸°"
  },"160": {
    "doc": "Pandas ë°ì´í„° ê²°í•© & ìš”ì•½",
    "title": "unstack, stackìœ¼ë¡œ pivot",
    "content": "df.unstack() . : indexì˜ íŠ¹ì • levelì„ column ë°©í–¥ìœ¼ë¡œ pivotí•´ì£¼ëŠ” í•¨ìˆ˜ (multi-indexë¥¼ í’€ì–´ì¤€ë‹¤) . gdf3 = titanic_df.groupby(['Pclass', 'Sex'])[['Age']].mean() gdf3 . | | | : Age | Â  | . | Pclass | Sex | Â  | . | 1 | female | 34.6118 | . | ^^ | male | 41.2814 | . | 2 | female | 28.723 | . | ^^ | male | 30.7407 | . | 3 | female | 21.75 | . | ^^ | male | 26.5076 | . | level=0: 1st level ì¸ë±ìŠ¤ë¥¼ column ë°©í–¥ìœ¼ë¡œ pivot gdf3.unstack(level=0) . | Â  | : Age || | Â  | Â  | . | Pclass | : 1 | : 2 | : 3 | . | Sex | Â  | Â  | Â  | . | female | 34.6118 | 28.723 | 21.75 | . | male | 41.2814 | 30.7407 | 26.5076 | . | level=1: 2nd level ì¸ë±ìŠ¤ë¥¼ column ë°©í–¥ìœ¼ë¡œ pivot . gdf3.unstack(level=1) . | Â  | :Age | | Â  | . | Sex | :female | :male | . | Pclass | Â  | Â  | . | 1 | 34.6118 | 41.2814 | . | 2 | 28.723 | 30.7407 | . | 3 | 21.75 | 26.5076 | . +) ì´ ê²½ìš°, 2nd levelì´ ë§ˆì§€ë§‰ levelì´ë¯€ë¡œ, level=-1ë¼ê³  ì ì–´ë„ ë™ì¼í•œ ê²°ê³¼ . | . df.stack() . : columnì˜ íŠ¹ì • levelì„ index ë°©í–¥ìœ¼ë¡œ pivotí•´ì£¼ëŠ” í•¨ìˆ˜ (unstackê³¼ ë°˜ëŒ€: multi-indexë¥¼ ë§Œë“¤ì–´ì¤€ë‹¤) . gdf4 = titanic_df.groupby('Pclass')[['Age']].agg(['mean', 'max']) gdf4 . | Â  | : Age | | Â  | . | Â  | : mean | : max | . | Pclass | Â  | Â  | . | 1 | 38.2334 | 80 | . | 2 | 29.8776 | 70 | . | 3 | 25.1406 | 74 | . â†’ 2ë²ˆì§¸ level (ë§ˆì§€ë§‰ level)ì„ index ë°©í–¥ìœ¼ë¡œ pivot . gdf4.stack(level=-1) . | | | : Age | Â  | . | Pclass | Â  | Â  | . | 1 | mean | 38.2334 | . | ^^ | max | 80 | . | 2 | mean | 29.8776 | . | ^^ | max | 70 | . | 3 | mean | 25.1406 | . | ^^ | max | 74 | . ",
    "url": "https://chaelist.github.io/docs/pandas/pandas_merge_group/#unstack-stack%EC%9C%BC%EB%A1%9C-pivot",
    "relUrl": "/docs/pandas/pandas_merge_group/#unstack-stackìœ¼ë¡œ-pivot"
  },"161": {
    "doc": "Pandas ë°ì´í„° ê²°í•© & ìš”ì•½",
    "title": "Pandas ë°ì´í„° ê²°í•© & ìš”ì•½",
    "content": " ",
    "url": "https://chaelist.github.io/docs/pandas/pandas_merge_group/",
    "relUrl": "/docs/pandas/pandas_merge_group/"
  },"162": {
    "doc": "Pandas plot() í•¨ìˆ˜",
    "title": "Pandas plot() í•¨ìˆ˜",
    "content": ". | ì„  ê·¸ë˜í”„ | ë§‰ëŒ€ ê·¸ë˜í”„ | íŒŒì´ ê·¸ë˜í”„ | ë°•ìŠ¤ í”Œë¡¯ | ì‚°ì ë„ (Scatter Plot) | . *Pandas ë‚´ì¥ ê¸°ëŠ¥ì¸ .plot() í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ë©´ ì‰½ê²Œ ê·¸ë˜í”„ë¥¼ ê·¸ë¦´ ìˆ˜ ìˆë‹¤. (ë‹¹ì—°íˆ elementê°€ â€˜ìˆ«ìí˜•â€™ì¼ë•Œë§Œ ê·¸ë˜í”„ ê·¸ë ¤ì§) . ",
    "url": "https://chaelist.github.io/docs/visualization/pandas_plot/",
    "relUrl": "/docs/visualization/pandas_plot/"
  },"163": {
    "doc": "Pandas plot() í•¨ìˆ˜",
    "title": "ì„  ê·¸ë˜í”„",
    "content": "import pandas as pd . %matplotlib inline # ê·¸ë˜í”„ì˜ ê²°ê³¼ë¥¼ ì¶œë ¥ ì„¸ì…˜ì— ë‚˜íƒ€ë‚˜ê²Œ í•˜ëŠ” ì„¤ì • . df = pd.read_csv('data/broadcast.csv', index_col=0) ## ë°ì´í„° ì¶œì²˜: codeit df . | Â  | KBS | MBC | SBS | TV CHOSUN | JTBC | Channel A | MBN | . | 2011 | 35.951 | 18.374 | 11.173 | 9.102 | 7.38 | 3.771 | 2.809 | . | 2012 | 36.163 | 16.022 | 11.408 | 8.785 | 7.878 | 5.874 | 3.31 | . | 2013 | 31.989 | 16.778 | 9.673 | 9.026 | 7.81 | 5.35 | 3.825 | . | 2014 | 31.21 | 15.663 | 9.108 | 9.44 | 7.49 | 5.776 | 4.572 | . | 2015 | 27.777 | 16.573 | 9.099 | 9.94 | 7.267 | 6.678 | 5.52 | . | 2016 | 27.583 | 14.982 | 8.669 | 9.829 | 7.727 | 6.624 | 5.477 | . | 2017 | 26.89 | 12.465 | 8.661 | 8.886 | 9.453 | 6.056 | 5.215 | . | ê¸°ë³¸ ê·¸ë˜í”„ ê·¸ë ¤ë³´ê¸° df.plot() # df.plot(kind='line') ì´ë ‡ê²Œ ì¨ë„ ë™ì¼. ì„ (line) ê·¸ë˜í”„ê°€ defaultì´ê¸° ë•Œë¬¸. | 1ê°œ ê°’ì— ëŒ€í•´ì„œë§Œ ê·¸ë˜í”„ ê·¸ë¦¬ê¸° df.plot(y='KBS') # KBSì— ëŒ€í•´ì„œë§Œ ê·¸ë˜í”„ë¥¼ ê·¸ë¦¼ # df['KBS'].plot() ì´ë ‡ê²Œ ì¨ë„ ê±°ì˜ ë™ì¼. | 2ê°œ ê°’ì— ëŒ€í•´ ê·¸ë˜í”„ ê·¸ë¦¬ê¸° df.plot(y=['KBS', 'MBC']); # df[['KBS', 'MBC']].plot() ì´ë ‡ê²Œ ì¨ë„ ë™ì¼ . | . ",
    "url": "https://chaelist.github.io/docs/visualization/pandas_plot/#%EC%84%A0-%EA%B7%B8%EB%9E%98%ED%94%84",
    "relUrl": "/docs/visualization/pandas_plot/#ì„ -ê·¸ë˜í”„"
  },"164": {
    "doc": "Pandas plot() í•¨ìˆ˜",
    "title": "ë§‰ëŒ€ ê·¸ë˜í”„",
    "content": "df = pd.read_csv('data/sports.csv',index_col=0) ## ë°ì´í„° ì¶œì²˜: codeit df . | Â  | Male | Female | . | Swimming | 103 | 178 | . | Baseball | 363 | 289 | . | Basketball | 151 | 97 | . | Golf | 154 | 232 | . | Soccer | 413 | 109 | . | Bowling | 88 | 129 | . | ê¸°ë³¸ ë§‰ëŒ€ ê·¸ë˜í”„ df.plot(kind='bar') . | ê°€ë¡œ ë°©í–¥ ë§‰ëŒ€ ê·¸ë˜í”„ df.plot(kind='barh'); # 'h' is for 'horizontal' . | ëˆ„ì  ë§‰ëŒ€ ê·¸ë˜í”„ df.plot(kind='bar', stacked=True); # ë‚¨+ì—¬ í†µí‹€ì–´ ì–´ë–¤ ìš´ë™ì´ ê°€ì¥ ì¸ê¸°ê°€ ë§ì€ì§€ ë³¼ ìˆ˜ ìˆë‹¤ . | 1ê°œ ê°’ì— ëŒ€í•´ì„œë§Œ ê·¸ë˜í”„ ê·¸ë¦¬ê¸° df['Female'].plot(kind='bar'); # ì—¬ì„±ì„ ëŒ€ìƒìœ¼ë¡œí•œ ì¡°ì‚¬ê²°ê³¼ë§Œ ë³´ê³  ì‹¶ì„ ë•Œ . | . ",
    "url": "https://chaelist.github.io/docs/visualization/pandas_plot/#%EB%A7%89%EB%8C%80-%EA%B7%B8%EB%9E%98%ED%94%84",
    "relUrl": "/docs/visualization/pandas_plot/#ë§‰ëŒ€-ê·¸ë˜í”„"
  },"165": {
    "doc": "Pandas plot() í•¨ìˆ˜",
    "title": "íŒŒì´ ê·¸ë˜í”„",
    "content": "df = pd.read_csv('data/broadcast.csv', index_col=0) ## ë°ì´í„° ì¶œì²˜: codeit df . | Â  | KBS | MBC | SBS | TV CHOSUN | JTBC | Channel A | MBN | . | 2011 | 35.951 | 18.374 | 11.173 | 9.102 | 7.38 | 3.771 | 2.809 | . | 2012 | 36.163 | 16.022 | 11.408 | 8.785 | 7.878 | 5.874 | 3.31 | . | 2013 | 31.989 | 16.778 | 9.673 | 9.026 | 7.81 | 5.35 | 3.825 | . | 2014 | 31.21 | 15.663 | 9.108 | 9.44 | 7.49 | 5.776 | 4.572 | . | 2015 | 27.777 | 16.573 | 9.099 | 9.94 | 7.267 | 6.678 | 5.52 | . | 2016 | 27.583 | 14.982 | 8.669 | 9.829 | 7.727 | 6.624 | 5.477 | . | 2017 | 26.89 | 12.465 | 8.661 | 8.886 | 9.453 | 6.056 | 5.215 | . â†’ 2017ë…„ ë°ì´í„°ë§Œ ì¶”ì¶œ . df.loc[2017] . KBS 26.890 MBC 12.465 SBS 8.661 TV CHOSUN 8.886 JTBC 9.453 Channel A 6.056 MBN 5.215 Name: 2017, dtype: float64 . â†’ 2017ë…„ ë°ì´í„°ë¡œ íŒŒì´ ê·¸ë˜í”„ ê·¸ë¦¬ê¸° . df.loc[2017].plot(kind='pie') . +) ì¶”ê°€ tip: pieê°€ ë™ê·¸ë—ê²Œ ê·¸ë ¤ì§€ì§€ ì•ŠëŠ” ê²½ìš°, axis('equal')ì„ ë¶™ì—¬ì£¼ë©´ ëœë‹¤ . df.loc[2017].plot(kind='pie').axis('equal') . ###íˆìŠ¤í† ê·¸ë¨ . df = pd.read_csv('data/body.csv', index_col=0) ## ë°ì´í„° ì¶œì²˜: codeit df.head() . | Number | Height | Weight | . | 1 | 176 | 85.2 | . | 2 | 175.3 | 67.7 | . | 3 | 168.6 | 75.2 | . | 4 | 168.1 | 67.1 | . | 5 | 175.3 | 63 | . | ê¸°ë³¸ íˆìŠ¤í† ê·¸ë¨: 10ê°œ êµ¬ê°„ìœ¼ë¡œ ë‚˜ëˆ ì„œ ê·¸ë ¤ì§ df.plot(kind='hist', y='Height'); # 10ê°œ êµ¬ê°„ìœ¼ë¡œ ë‚˜ë‰˜ëŠ” ê²Œ default setting . | yì¶•ì˜ â€˜Frequencyâ€™ëŠ” ë¹ˆë„ìˆ˜. ì´ íˆìŠ¤í† ê·¸ë¨ì˜ ê²½ìš°, í‚¤ê°€ 175-177.5ì¸ í•™ìƒì´ 250ëª… ìˆë‹¤ëŠ” ëœ» | . | êµ¬ê°„ ìˆ˜ ì¡°ì • df.plot(kind='hist', y='Height', bins=15); # 15ê°œ êµ¬ê°„ìœ¼ë¡œ ë‚˜ëˆ„ê¸° . | binsëŠ” ìˆ«ìê°€ í¬ë‹¤ê³  ë¬´ì¡°ê±´ ì¢‹ì€ ê²Œ ì•„ë‹ˆë¼, ì¸ì‚¬ì´íŠ¸ë¥¼ ê°€ì ¸ì˜¤ê¸° ì¢‹ì€ ì ë‹¹í•œ ìˆ«ìë¥¼ ì˜ ê³¨ë¼ì•¼í•œë‹¤ | . | . ",
    "url": "https://chaelist.github.io/docs/visualization/pandas_plot/#%ED%8C%8C%EC%9D%B4-%EA%B7%B8%EB%9E%98%ED%94%84",
    "relUrl": "/docs/visualization/pandas_plot/#íŒŒì´-ê·¸ë˜í”„"
  },"166": {
    "doc": "Pandas plot() í•¨ìˆ˜",
    "title": "ë°•ìŠ¤ í”Œë¡¯",
    "content": "df = pd.read_csv('data/exam.csv') ## ë°ì´í„° ì¶œì²˜: codeit df.head() . | Â  | gender | race/ethnicity | parental level of education | lunch | test preparation course | math score | reading score | writing score | . | 0 | female | group B | bachelorâ€™s degree | standard | none | 72 | 72 | 74 | . | 1 | female | group C | some college | standard | completed | 69 | 90 | 88 | . | 2 | female | group B | masterâ€™s degree | standard | none | 90 | 95 | 93 | . | 3 | male | group A | associateâ€™s degree | free/reduced | none | 47 | 57 | 44 | . | 4 | male | group C | some college | standard | none | 76 | 78 | 75 | . *â€˜math scoreâ€™ ë°ì´í„° ë¶„í¬ í™•ì¸í•´ë³´ê¸°(ìµœì†Ÿê°’, 1ì‚¬ë¶„ìœ„ê°’, 2ì‚¬ë¶„ìœ„ê°’, â€¦) . df['math score'].describe() . count 1000.00000 mean 66.08900 std 15.16308 min 0.00000 25% 57.00000 50% 66.00000 75% 77.00000 max 100.00000 Name: math score, dtype: float64 . *â€˜math scoreâ€™ ë°ì´í„° ë¶„í¬ ë°•ìŠ¤í”Œë¡¯ìœ¼ë¡œ í™•ì¸í•˜ê¸° . df.plot(kind='box', y='math score') . â€» box plot ì„¤ëª… . | ë°•ìŠ¤ ìœ„~ì•„ë˜ê°€ IQR(Interquartile Range) | ë°•ìŠ¤ ì¤‘ì•™ì˜ ì„ ì´ ì¤‘ì•™ê°’ (= Q2, 2ì‚¬ë¶„ìœ„ê°’, 50% ì§€ì ) | ë°•ìŠ¤ ë§¨ ìœ—ë¶€ë¶„: Q3 (3ì‚¬ë¶„ìœ„ê°’, 75% ì§€ì ) | ë°•ìŠ¤ ë§¨ ì•„ë«ë¶€ë¶„: Q1 (1ì‚¬ë¶„ìœ„ê°’, 25% ì§€ì ) | ë°•ìŠ¤ ìœ„ìª½ ì„ (ìˆ˜ì—¼)ì˜ ë ë¶€ë¶„: Upperfence ë‚´ ìµœëŒ€ê°’ (= Q3 + 1.5*IQRë³´ë‹¤ ì‘ì€ ê°’ ì¤‘ ê°€ì¥ í° ê°’) . | Upperfence(ìƒìœ„ ê²½ê³„): Q3 + 1.5*IQR | . | ë°•ìŠ¤ ì•„ë˜ìª½ ì„ (ìˆ˜ì—¼)ì˜ ë ë¶€ë¶„: Lowerfence ë‚´ ìµœì†Œê°’ (= Q1 - 1.5*IQRë³´ë‹¤ í° ê°’ ì¤‘ ê°€ì¥ ì‘ì€ ê°’) . | Lowerfence(í•˜ìœ„ ê²½ê³„): Q1 - 1.5*IQR | . | ë°•ìŠ¤ &amp; ìˆ˜ì—¼ ë¶€ë¶„ì„ ë²—ì–´ë‚œ ë™ê·¸ë€ ì ë“¤ì€ outlier(ì´ìƒì ) | . ",
    "url": "https://chaelist.github.io/docs/visualization/pandas_plot/#%EB%B0%95%EC%8A%A4-%ED%94%8C%EB%A1%AF",
    "relUrl": "/docs/visualization/pandas_plot/#ë°•ìŠ¤-í”Œë¡¯"
  },"167": {
    "doc": "Pandas plot() í•¨ìˆ˜",
    "title": "ì‚°ì ë„ (Scatter Plot)",
    "content": "df = pd.read_csv('data/exam.csv') ## ë°ì´í„° ì¶œì²˜: codeit df.head() . | Â  | gender | race/ethnicity | parental level of education | lunch | test preparation course | math score | reading score | writing score | . | 0 | female | group B | bachelorâ€™s degree | standard | none | 72 | 72 | 74 | . | 1 | female | group C | some college | standard | completed | 69 | 90 | 88 | . | 2 | female | group B | masterâ€™s degree | standard | none | 90 | 95 | 93 | . | 3 | male | group A | associateâ€™s degree | free/reduced | none | 47 | 57 | 44 | . | 4 | male | group C | some college | standard | none | 76 | 78 | 75 | . | ìˆ˜í•™ ì ìˆ˜ì™€ ì½ê¸° ì ìˆ˜ ê°„ì˜ ì—°ê´€ì„± í™•ì¸ df.plot(kind='scatter', x='math score', y='reading score') . | ì½ê¸° ì ìˆ˜ì™€ ì“°ê¸° ì ìˆ˜ ê°„ì˜ ì—°ê´€ì„± í™•ì¸ df.plot(kind='scatter', x='reading score', y='writing score') . | ì½ê¸° ì ìˆ˜ì™€ ì“°ê¸° ì ìˆ˜ ê°„ì˜ ì—°ê´€ì„±ì´ ìˆ˜í•™ ì ìˆ˜ì™€ ì½ê¸° ì ìˆ˜ ê°„ì˜ ì—°ê´€ì„±ë³´ë‹¤ í¬ë‹¤ê³  íŒë‹¨ | . | . ",
    "url": "https://chaelist.github.io/docs/visualization/pandas_plot/#%EC%82%B0%EC%A0%90%EB%8F%84-scatter-plot",
    "relUrl": "/docs/visualization/pandas_plot/#ì‚°ì ë„-scatter-plot"
  },"168": {
    "doc": "Pandas str, dt, ì¡°ê±´ë¬¸",
    "title": "Pandas str, dt, ì¡°ê±´ë¬¸",
    "content": ". | ë¬¸ìì—´ ì²˜ë¦¬ í•¨ìˆ˜ â€˜strâ€™ . | ë¬¸ìì—´ ì¸ë±ì‹±: str[ ] | ë¬¸ìì—´ ë¶„í• : str.split( ) | íŠ¹ì • ê¸€ì ê¸°ì¤€ í•„í„°ë§ | ì •ê·œì‹ &amp; findall í™œìš© | ë¬¸ì ëŒ€ì²´: str.replace( ) | ë¬¸ìì—´ íŒ¨ë”© | ì–‘ ë ê³µë°± ì œê±°: str.strip( ) | ëŒ€ì†Œë¬¸ì ë³€ê²½ | . | ë‚ ì§œí˜• ë°ì´í„° ë³€í™˜/ê°€ê³µ . | â€˜Datetimeâ€™ íƒ€ì…ìœ¼ë¡œ ë³€í™˜/ê°€ê³µ | Datetime ë°ì´í„° ê°€ê³µí•˜ê¸° | ìˆ«ìë¡œ ì½í˜€ì˜¨ ë‚ ì§œí˜• ë°ì´í„° ìˆ˜ì • | relativedeltaë¡œ ê¸°ê°„ ê³„ì‚° | +) ë” ê°„ë‹¨í•œ ê¸°ê°„ ê³„ì‚° | . | ì¡°ê±´ë¬¸ìœ¼ë¡œ ë°ì´í„° ê°€ê³µ . | lambda ì‹ + apply() | lambda ì‹ + applymap() | pd.where() | np.where() í™œìš© | . | . ",
    "url": "https://chaelist.github.io/docs/pandas/pandas_str_dt_con/",
    "relUrl": "/docs/pandas/pandas_str_dt_con/"
  },"169": {
    "doc": "Pandas str, dt, ì¡°ê±´ë¬¸",
    "title": "ë¬¸ìì—´ ì²˜ë¦¬ í•¨ìˆ˜ â€˜strâ€™",
    "content": ". | DataFrameì— ì§ì ‘ ì ìš©ì€ ì•ˆë˜ê³ , Seriesì— ì ìš©í•´ì•¼ í•œë‹¤ | . df = pd.read_csv('data/á„Œá…¥á†«á„€á…®á†¨_á„‡á…¥á†¸á„Œá…¥á†¼á„ƒá…©á†¼á„á…©á„ƒá…³.txt', sep='\\t', encoding='cp949') ## ë°ì´í„° ì¶œì²˜: í–‰ì •í‘œì¤€ì½”ë“œê´€ë¦¬ì‹œìŠ¤í…œ df.head() . | Â  | ë²•ì •ë™ì½”ë“œ | ë²•ì •ë™ëª… | íì§€ì—¬ë¶€ | . | 0 | 1100000000 | ì„œìš¸íŠ¹ë³„ì‹œ | ì¡´ì¬ | . | 1 | 1111000000 | ì„œìš¸íŠ¹ë³„ì‹œ ì¢…ë¡œêµ¬ | ì¡´ì¬ | . | 2 | 1111010100 | ì„œìš¸íŠ¹ë³„ì‹œ ì¢…ë¡œêµ¬ ì²­ìš´ë™ | ì¡´ì¬ | . | 3 | 1111010200 | ì„œìš¸íŠ¹ë³„ì‹œ ì¢…ë¡œêµ¬ ì‹ êµë™ | ì¡´ì¬ | . | 4 | 1111010300 | ì„œìš¸íŠ¹ë³„ì‹œ ì¢…ë¡œêµ¬ ê¶ì •ë™ | ì¡´ì¬ | . ë¬¸ìì—´ ì¸ë±ì‹±: str[ ] . | ê¸°ì¡´ì˜ string indexing ë°©ë²•ê³¼ ë™ì¼ (ì°¸ê³ ) | . df['ë²•ì •ë™ëª…'].str[:5].head() # ì• 5ìë¦¬ê¹Œì§€ ì¶”ì¶œ . 0 ì„œìš¸íŠ¹ë³„ì‹œ 1 ì„œìš¸íŠ¹ë³„ì‹œ 2 ì„œìš¸íŠ¹ë³„ì‹œ 3 ì„œìš¸íŠ¹ë³„ì‹œ 4 ì„œìš¸íŠ¹ë³„ì‹œ Name: ë²•ì •ë™ëª…, dtype: object . ë¬¸ìì—´ ë¶„í• : str.split( ) . | stringì— split()ì„ ì ìš©í•˜ëŠ” ë°©ë²•ê³¼ ë™ì¼ (ì°¸ê³ ) | . df['ë²•ì •ë™ëª…'].str.split().head() # ê³µë°± ê¸°ì¤€ ë¶„ë¦¬ . 0 [ì„œìš¸íŠ¹ë³„ì‹œ] 1 [ì„œìš¸íŠ¹ë³„ì‹œ, ì¢…ë¡œêµ¬] 2 [ì„œìš¸íŠ¹ë³„ì‹œ, ì¢…ë¡œêµ¬, ì²­ìš´ë™] 3 [ì„œìš¸íŠ¹ë³„ì‹œ, ì¢…ë¡œêµ¬, ì‹ êµë™] 4 [ì„œìš¸íŠ¹ë³„ì‹œ, ì¢…ë¡œêµ¬, ê¶ì •ë™] Name: ë²•ì •ë™ëª…, dtype: object . +) ë¶„í• ëœ ê°œë³„ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°”ë¡œ ë°ì´í„° í”„ë ˆì„ìœ¼ë¡œ ë§Œë“œë ¤ë©´, expand=Trueì˜µì…˜ì„ ì¶”ê°€ . df['ë²•ì •ë™ëª…'].str.split(\" \", expand=True).head() ## df['ë²•ì •ë™ëª…'].str.split(expand=True).head()ë¼ê³ ë§Œ í•´ë„ ë¨ (ê³µë°± ê¸°ì¤€ìœ¼ë¡œ ë¶„ë¦¬í•˜ëŠ” ê²½ìš°) . | Â  | 0 | 1 | 2 | 3 | 4 | . | 0 | ì„œìš¸íŠ¹ë³„ì‹œ | None | None | None | None | . | 1 | ì„œìš¸íŠ¹ë³„ì‹œ | ì¢…ë¡œêµ¬ | None | None | None | . | 2 | ì„œìš¸íŠ¹ë³„ì‹œ | ì¢…ë¡œêµ¬ | ì²­ìš´ë™ | None | None | . | 3 | ì„œìš¸íŠ¹ë³„ì‹œ | ì¢…ë¡œêµ¬ | ì‹ êµë™ | None | None | . | 4 | ì„œìš¸íŠ¹ë³„ì‹œ | ì¢…ë¡œêµ¬ | ê¶ì •ë™ | None | None | . íŠ¹ì • ê¸€ì ê¸°ì¤€ í•„í„°ë§ . | str.startswith(): íŠ¹ì • ê¸€ìë¡œ ì‹œì‘í•˜ëŠ” ë°ì´í„°ë§Œ í•„í„°ë§ # 'ëŒ€ì „'ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” ë°ì´í„°ë§Œ í•„í„°ë§ df[df['ë²•ì •ë™ëª…'].str.startswith(\"ëŒ€ì „\")].head() . | Â  | ë²•ì •ë™ì½”ë“œ | ë²•ì •ë™ëª… | íì§€ì—¬ë¶€ | . | 3870 | 3000000000 | ëŒ€ì „ê´‘ì—­ì‹œ | ì¡´ì¬ | . | 3871 | 3011000000 | ëŒ€ì „ê´‘ì—­ì‹œ ë™êµ¬ | ì¡´ì¬ | . | 3872 | 3011010100 | ëŒ€ì „ê´‘ì—­ì‹œ ë™êµ¬ ì›ë™ | ì¡´ì¬ | . | 3873 | 3011010200 | ëŒ€ì „ê´‘ì—­ì‹œ ë™êµ¬ ì¸ë™ | ì¡´ì¬ | . | 3874 | 3011010300 | ëŒ€ì „ê´‘ì—­ì‹œ ë™êµ¬ íš¨ë™ | ì¡´ì¬ | . | str.endswith(): íŠ¹ì • ê¸€ìë¡œ ëë‚˜ëŠ” ë°ì´í„°ë§Œ í•„í„°ë§ # 'êµ¬'ìœ¼ë¡œ ëë‚˜ëŠ” ë°ì´í„°ë§Œ í•„í„°ë§ df[df['ë²•ì •ë™ëª…'].str.endswith(\"êµ¬\")].head() . | Â  | ë²•ì •ë™ì½”ë“œ | ë²•ì •ë™ëª… | íì§€ì—¬ë¶€ | . | 1 | 1111000000 | ì„œìš¸íŠ¹ë³„ì‹œ ì¢…ë¡œêµ¬ | ì¡´ì¬ | . | 94 | 1114000000 | ì„œìš¸íŠ¹ë³„ì‹œ ì¤‘êµ¬ | ì¡´ì¬ | . | 179 | 1117000000 | ì„œìš¸íŠ¹ë³„ì‹œ ìš©ì‚°êµ¬ | ì¡´ì¬ | . | 229 | 1120000000 | ì„œìš¸íŠ¹ë³„ì‹œ ì„±ë™êµ¬ | ì¡´ì¬ | . | 332 | 1121500000 | ì„œìš¸íŠ¹ë³„ì‹œ ê´‘ì§„êµ¬ | ì¡´ì¬ | . | str.contains(): íŠ¹ì • ê¸€ìë¥¼ í¬í•¨í•˜ëŠ” ë°ì´í„°ë§Œ í•„í„°ë§ . # 'ì„œëŒ€ë¬¸êµ¬'ê°€ ë“¤ì–´ê°„ ë°ì´í„°ë§Œ í•„í„°ë§ df[df['ë²•ì •ë™ëª…'].str.contains(\"ì„œëŒ€ë¬¸êµ¬\")].head() . | Â  | ë²•ì •ë™ì½”ë“œ | ë²•ì •ë™ëª… | íì§€ì—¬ë¶€ | . | 597 | 1141000000 | ì„œìš¸íŠ¹ë³„ì‹œ ì„œëŒ€ë¬¸êµ¬ | ì¡´ì¬ | . | 635 | 1141010100 | ì„œìš¸íŠ¹ë³„ì‹œ ì„œëŒ€ë¬¸êµ¬ ì¶©ì •ë¡œ2ê°€ | ì¡´ì¬ | . | 636 | 1141010200 | ì„œìš¸íŠ¹ë³„ì‹œ ì„œëŒ€ë¬¸êµ¬ ì¶©ì •ë¡œ3ê°€ | ì¡´ì¬ | . | 637 | 1141010300 | ì„œìš¸íŠ¹ë³„ì‹œ ì„œëŒ€ë¬¸êµ¬ í•©ë™ | ì¡´ì¬ | . | 638 | 1141010400 | ì„œìš¸íŠ¹ë³„ì‹œ ì„œëŒ€ë¬¸êµ¬ ë¯¸ê·¼ë™ | ì¡´ì¬ | . | . ì •ê·œì‹ &amp; findall í™œìš© . | íŠ¹ì • ì •ê·œì‹ì— ë§¤ì¹­ë˜ëŠ” ê°’ ì¶”ì¶œí•˜ê¸° (ì°¸ê³ : ì •ê·œì‹) | . df['ë²•ì •ë™ëª…'].str.findall('\\w+ë™').head() . 0 [] 1 [] 2 [ì²­ìš´ë™] 3 [ì‹ êµë™] 4 [ê¶ì •ë™] Name: ë²•ì •ë™ëª…, dtype: object . ë¬¸ì ëŒ€ì²´: str.replace( ) . | stringì— replace()ì„ ì ìš©í•˜ëŠ” ë°©ë²•ê³¼ ë™ì¼ (ì°¸ê³ ) | . df['ë²•ì •ë™ëª…'].str.replace(\" \", \"_\").head() # ê³µë°±ì„ \"_\"ë¡œ ëŒ€ì²´ . 0 ì„œìš¸íŠ¹ë³„ì‹œ 1 ì„œìš¸íŠ¹ë³„ì‹œ_ì¢…ë¡œêµ¬ 2 ì„œìš¸íŠ¹ë³„ì‹œ_ì¢…ë¡œêµ¬_ì²­ìš´ë™ 3 ì„œìš¸íŠ¹ë³„ì‹œ_ì¢…ë¡œêµ¬_ì‹ êµë™ 4 ì„œìš¸íŠ¹ë³„ì‹œ_ì¢…ë¡œêµ¬_ê¶ì •ë™ Name: ë²•ì •ë™ëª…, dtype: object . ë¬¸ìì—´ íŒ¨ë”© . | ê³ ì •ëœ ê¸¸ì´ë¡œ, ë‚¨ëŠ” ë¶€ë¶„ ì±„ìš°ê¸° | . # ë¬¸ìì—´ ê¸¸ì´ë¥¼ 20ìë¡œ ë§ì¶”ë ¤ê³  í•¨ &amp; ë‚¨ëŠ” ë§Œí¼ ì™¼ìª½ì„ \"_\"ë¡œ ì±„ìš°ê¸° df['ë²•ì •ë™ëª…'].str.pad(width=20, side='left', fillchar='_').head() ## side='right'ì´ë¼ê³  í•˜ë©´ ì˜¤ë¥¸ìª½ì´ \"-\"ë¡œ ì±„ì›Œì§ . 0 _______________ì„œìš¸íŠ¹ë³„ì‹œ 1 ___________ì„œìš¸íŠ¹ë³„ì‹œ ì¢…ë¡œêµ¬ 2 _______ì„œìš¸íŠ¹ë³„ì‹œ ì¢…ë¡œêµ¬ ì²­ìš´ë™ 3 _______ì„œìš¸íŠ¹ë³„ì‹œ ì¢…ë¡œêµ¬ ì‹ êµë™ 4 _______ì„œìš¸íŠ¹ë³„ì‹œ ì¢…ë¡œêµ¬ ê¶ì •ë™ Name: ë²•ì •ë™ëª…, dtype: object . +) side=ë¥¼ ì§€ì •í•´ì£¼ì§€ ì•Šìœ¼ë©´ (=default) ì¢Œìš°ê°€ ê· ì¼í•˜ê²Œ ì±„ì›Œì§„ë‹¤. (ê¸€ìê°€ ê°€ìš´ë°ë¡œ.) . # ê¸€ìë¥¼ ê°€ìš´ë°ì— ë‘ê³  ì¢Œìš°ë¡œ \"_\"ë¥¼ ì±„ì›Œì„œ ë¬¸ìì—´ ê¸¸ì´ 20ìë¥¼ ë§ì¶°ì¤Œ df['ë²•ì •ë™ëª…'].str.center(width=20, fillchar='_').head() . 0 _______ì„œìš¸íŠ¹ë³„ì‹œ________ 1 _____ì„œìš¸íŠ¹ë³„ì‹œ ì¢…ë¡œêµ¬______ 2 ___ì„œìš¸íŠ¹ë³„ì‹œ ì¢…ë¡œêµ¬ ì²­ìš´ë™____ 3 ___ì„œìš¸íŠ¹ë³„ì‹œ ì¢…ë¡œêµ¬ ì‹ êµë™____ 4 ___ì„œìš¸íŠ¹ë³„ì‹œ ì¢…ë¡œêµ¬ ê¶ì •ë™____ Name: ë²•ì •ë™ëª…, dtype: object . ì–‘ ë ê³µë°± ì œê±°: str.strip( ) . | stringì— strip()ì„ ì ìš©í•˜ëŠ” ë°©ë²•ê³¼ ë™ì¼ (ì°¸ê³ ) | rstip()ê³¼ lstrip()ë„ ì ìš© ê°€ëŠ¥. | . df2 = pd.DataFrame({'col1':['abcde ',' FFFFghij ','abCCe '], 'col2':[' fgHAAij ',' fghij ','lmnop ']}) df2 . | Â  | col1 | col2 | . | 0 | abcde | fgHAAij | . | 1 | FFFFghij | fghij | . | 2 | abCCe | lmnop | . Â  . df2['col1'].str.strip() # ì• ë’¤ ê³µë°± ì œê±° ## ë§ˆì°¬ê°€ì§€ë¡œ, rstip()ê³¼ lstrip()ë„ ì ìš© ê°€ëŠ¥! . 0 abcde 1 FFFFghij 2 abCCe Name: col1, dtype: object . â†’ listë¡œ ë³€í™˜í•´ì„œ ì‚´í´ë³´ë©´ í™•ì‹¤íˆ ì• ë’¤ ê³µë°±ì´ ì œê±°ë˜ì—ˆìŒì„ í™•ì¸ ê°€ëŠ¥ . df2['col1'].str.strip().to_list() . ['abcde', 'FFFFghij', 'abCCe'] . ëŒ€ì†Œë¬¸ì ë³€ê²½ . | str.lower(): ì†Œë¬¸ìë¡œ ë³€ê²½ df2['col1'].str.lower() . 0 abcde 1 ffffghij 2 abcce Name: col1, dtype: object . | str.upper(): ëŒ€ë¬¸ìë¡œ ë³€ê²½ df2['col1'].str.upper() . 0 ABCDE 1 FFFFGHIJ 2 ABCCE Name: col1, dtype: object . | str.swapcase(): ì†Œë¬¸ì â†” ëŒ€ë¬¸ì ì„œë¡œ ë°”ê¿”ì¤Œ df2['col1'].str.swapcase() # ì†Œë¬¸ìëŠ” ëŒ€ë¬¸ìë¡œ, ëŒ€ë¬¸ìëŠ” ì†Œë¬¸ìë¡œ . 0 ABCDE 1 ffffGHIJ 2 ABccE Name: col1, dtype: object . | . ",
    "url": "https://chaelist.github.io/docs/pandas/pandas_str_dt_con/#%EB%AC%B8%EC%9E%90%EC%97%B4-%EC%B2%98%EB%A6%AC-%ED%95%A8%EC%88%98-str",
    "relUrl": "/docs/pandas/pandas_str_dt_con/#ë¬¸ìì—´-ì²˜ë¦¬-í•¨ìˆ˜-str"
  },"170": {
    "doc": "Pandas str, dt, ì¡°ê±´ë¬¸",
    "title": "ë‚ ì§œí˜• ë°ì´í„° ë³€í™˜/ê°€ê³µ",
    "content": "â€˜Datetimeâ€™ íƒ€ì…ìœ¼ë¡œ ë³€í™˜/ê°€ê³µ . transaction = pd.read_csv('data/transaction_1.csv') ## ë°ì´í„° ì¶œì²˜: https://github.com/wikibook/pyda100 transaction.head() . | Â  | transaction_id | price | payment_date | customer_id | . | 0 | T0000000113 | 210000 | 2019-02-01 01:36:57 | PL563502 | . | 1 | T0000000114 | 50000 | 2019-02-01 01:37:23 | HD678019 | . | 2 | T0000000115 | 120000 | 2019-02-01 02:34:19 | HD298120 | . | 3 | T0000000116 | 210000 | 2019-02-01 02:47:23 | IK452215 | . | 4 | T0000000117 | 170000 | 2019-02-01 04:33:46 | PL542865 | . *pd.to_datetime(): ë°ì´í„°ë¥¼ datetime typeìœ¼ë¡œ ë³€í™˜í•´ì£¼ëŠ” í•¨ìˆ˜ . | datetime í˜•íƒœë¡œ ìƒê¸´ ë°ì´í„°: ê·¸ëƒ¥ pd.to_datetime() ì•ˆì— ë„£ì–´ì£¼ë©´ ì•Œì•„ì„œ ì—°/ì›”/ì¼ ë“±ì´ ì‹ë³„ëœë‹¤ print(transaction['payment_date'].dtypes) # pd.to_datetime í•¨ìˆ˜ë¥¼ í™œìš©í•´ datetime typeìœ¼ë¡œ ë³€í™˜ transaction['payment_date'] = pd.to_datetime(transaction['payment_date']) print(transaction['payment_date'].dtypes) . object datetime64[ns] . | str, int ë“± íƒ€ì…ì˜ ë°ì´í„° ë³€í™˜í•˜ê¸° (ì—°/ì›”/ì¼ì˜ êµ¬ì¡°ê°€ ëª¨í˜¸í•œ ê²½ìš°) . | format=ì„ í†µí•´ ì–´ë–¤ í˜•íƒœë¡œ ë°ì´í„°ê°€ ë‹´ê²¨ ìˆëŠ”ì§€ ëª…ì‹œí•´ì£¼ë©´ ì•Œë§ê²Œ datetimeìœ¼ë¡œ ë³€í™˜ ê°€ëŠ¥ | ex) ë°ì´í„°ê°€ â€˜12/08/2021â€™ì˜ í˜•íƒœë¼ë©´, format='%d/%m/%Yì´ë¼ê³  ëª…ì‹œ | . df = pd.DataFrame({'date_example': [201901, 201902, 201903, 201904, 201905]}) df['datetime'] = pd.to_datetime(df['date_example'], format='%Y%m') df . | Â  | date_example | datetime | . | 0 | 201901 | 2019-01-01 00:00:00 | . | 1 | 201902 | 2019-02-01 00:00:00 | . | 2 | 201903 | 2019-03-01 00:00:00 | . | 3 | 201904 | 2019-04-01 00:00:00 | . | 4 | 201905 | 2019-05-01 00:00:00 | . +) ë°ì´í„° íƒ€ì… í™•ì¸: . df.dtypes . date_example int64 datetime datetime64[ns] dtype: object . | . Datetime ë°ì´í„° ê°€ê³µí•˜ê¸° . | .dt.strftime(): ë‚ ì§œì™€ ì‹œê°„ ì •ë³´ë¥¼ ì§€ì •í•œ íŠ¹ì • ë¬¸ìì—´ í˜•íƒœë¡œ ë°”ê¿”ì¤€ë‹¤ . | %Y: ì•ì˜ ë¹ˆìë¦¬ë¥¼ 0ìœ¼ë¡œ ì±„ìš°ëŠ” 4ìë¦¬ ì—°ë„ ìˆ«ì | %m: ì•ì˜ ë¹ˆìë¦¬ë¥¼ 0ìœ¼ë¡œ ì±„ìš°ëŠ” 2ìë¦¬ ì›” ìˆ«ì | %d: ì•ì˜ ë¹ˆìë¦¬ë¥¼ 0ìœ¼ë¡œ ì±„ìš°ëŠ” 2ìë¦¬ ì¼ ìˆ«ì | .dt.strftime(\"%Yë…„ %mì›” %dì¼\") ì´ëŸ° ì‹ìœ¼ë¡œ (= â€˜2020ë…„ 12ì›” 25ì¼â€™ì˜ í˜•ì‹) ì •ë¦¬í•˜ëŠ” ê²ƒë„ ê°€ëŠ¥. | . # '201902'ì™€ ê°™ì€ í˜•íƒœë¡œ ë‚ ì§œë¥¼ ì—°ì›” ë‹¨ìœ„ë¡œ ì •ë¦¬ transaction['payment_month'] = transaction['payment_date'].dt.strftime('%Y%m') transaction[['payment_date', 'payment_month']].head() . | Â  | payment_date | payment_month | . | 0 | 2019-02-01 01:36:57 | 201902 | . | 1 | 2019-02-01 01:37:23 | 201902 | . | 2 | 2019-02-01 02:34:19 | 201902 | . | 3 | 2019-02-01 02:47:23 | 201902 | . | 4 | 2019-02-01 04:33:46 | 201902 | . â†’ ì´ë ‡ê²Œ ì›” ë‹¨ìœ„ë¡œ ì •ë¦¬í•´ë†“ìœ¼ë©´, ì›”ë³„ ê¸ˆì•¡ ë¹„êµ ë“±ì´ ìš©ì´í•˜ë‹¤. # month ê¸°ì¤€ìœ¼ë¡œ priceì˜ í•© ì§‘ê³„ transaction.groupby('payment_month').sum()['price'] ## transaction.groupby('payment_month')['price'].sum()ë¼ê³  í•´ë„ ë¨. payment_month 201902 160185000 201903 160370000 201904 160510000 201905 155420000 201906 74755000 Name: price, dtype: int64 . cf) strftime() ë©”ì†Œë“œë¡œ ì •ë¦¬í•œ ë°ì´í„°ëŠ” str íƒ€ì… (= object íƒ€ì…) . transaction[['payment_date', 'payment_month']].dtypes . payment_date datetime64[ns] payment_month object dtype: object . | .dt.weekday: í•´ë‹¹ ë‚ ì§œì˜ ìš”ì¼ì„ ê³„ì‚°í•´ì¤€ë‹¤ . | ìš”ì¼ì€ ìˆ«ìë¡œ í‘œì‹œ (ì›”:0, í™”:1, â€¦., í† :5, ì¼:6) | . transaction['weekday'] = transaction['payment_date'].dt.weekday transaction.head() . | Â  | transaction_id | price | payment_date | customer_id | payment_month | weekday | . | 0 | T0000000113 | 210000 | 2019-02-01 01:36:57 | PL563502 | 201902 | 4 | . | 1 | T0000000114 | 50000 | 2019-02-01 01:37:23 | HD678019 | 201902 | 4 | . | 2 | T0000000115 | 120000 | 2019-02-01 02:34:19 | HD298120 | 201902 | 4 | . | 3 | T0000000116 | 210000 | 2019-02-01 02:47:23 | IK452215 | 201902 | 4 | . | 4 | T0000000117 | 170000 | 2019-02-01 04:33:46 | PL542865 | 201902 | 4 | . +) dt.year, dt.month, dt.day ë“±ì˜ ì†ì„±ë„ ìˆìŒ . transaction['payment_date'].dt.year.head() # ì—°ë„ë³„ë¡œ ì •ë¦¬ . 0 2019 1 2019 2 2019 3 2019 4 2019 Name: payment_date, dtype: int64 . | . ìˆ«ìë¡œ ì½í˜€ì˜¨ ë‚ ì§œí˜• ë°ì´í„° ìˆ˜ì • . : excel ë‚ ì§œí˜• ë°ì´í„°ê°€ ìˆ«ìë¡œ ì˜ëª» ì½í˜€ì˜¤ëŠ” ê²½ìš°, pd.to_timedelta()ë¥¼ í™œìš©í•´ ìˆ˜ì •í•´ì¤„ ìˆ˜ ìˆë‹¤ . kokyaku_data = pd.read_excel('data/kokyaku_daicho.xlsx') ## ë°ì´í„° ì¶œì²˜: https://github.com/wikibook/pyda10 kokyaku_data.head() . | Â  | ê³ ê°ì´ë¦„ | ì§€ì—­ | ë“±ë¡ì¼ | . | 0 | ê¹€ í˜„ì„± | Hì‹œ | 2018-01-04 00:00:00 | . | 1 | ê¹€ ë„ìœ¤ | Eì‹œ | 42782 | . | 2 | ê¹€ ì§€í•œ | Aì‹œ | 2018-01-07 00:00:00 | . | 3 | ê¹€ í•˜ìœ¤ | Fì‹œ | 42872 | . | 4 | ê¹€ ì‹œì˜¨ | Eì‹œ | 43127 | . | â€˜ë“±ë¡ì¼â€™ ì¹¼ëŸ¼ì˜ ëª‡ëª‡ ë°ì´í„°ê°€ ë‚ ì§œê°€ ì•„ë‹Œ ìˆ«ìí˜•ìœ¼ë¡œ ì½í˜€ì˜¨ ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤ | . # ìˆ«ìë¡œ ì½í˜€ì˜¨ ë°ì´í„°ë¥¼ íŒë³„í•´, flg_is_serialì— ì €ì¥ (True/Falseë¡œ) flg_is_serial = kokyaku_data['ë“±ë¡ì¼'].astype('str').str.isdigit() ## isdigit()ì„ í†µí•´ ìˆ«ìì¸ì§€ë¥¼ íŒë³„. flg_is_serial.sum() ## ìˆ«ìë¡œ ëœ ë‚ ì§œ ì •ë³´ê°€ 22ê°œì„ì„ ì•Œ ìˆ˜ ìˆìŒ. 22 . â†’ to_timedelta()ë¥¼ í™œìš©í•´ ìˆ«ì ë°ì´í„°ë¥¼ â€˜~ì¼â€™ ë°ì´í„°ë¡œ ë°”ê¿”ì£¼ê³ , 1900/01/01ì— ë”í•˜ê¸° . | ì—‘ì…€ì—ì„œ, 42782ë¥¼ ë‚ ì§œ í˜•íƒœë¡œ ë‚˜íƒ€ë‚´ë©´ â€˜1900/01/01â€™ì„ ê¸°ì¤€ìœ¼ë¡œ 42782ì¼ì„ ë”í•œ, â€˜2017/02/16â€™ì´ ëœë‹¤. (ì—‘ì…€ì˜ ë‚ ì§œ ê¸°ì–µë²•) | pd.to_timedelta(ì´ë¦„.astype(â€˜floatâ€™), unit=â€™Dâ€™)ëŠ” â€˜42782 daysâ€™ì™€ ê°™ì€ í˜•íƒœë¡œ ìˆ«ì ë°ì´í„°ë¥¼ â€˜~ì¼â€™ ë°ì´í„°ë¡œ ë°”ê¿”ì¤€ë‹¤ . | unit=â€™Dâ€™ëŠ” â€˜daysâ€™ë¥¼ ì˜ë¯¸. â€˜~ì¼â€™ë¡œ ë°”ê¾¸ë¼ëŠ” ê²ƒ. | . | . # to_timedelta í™œìš© from_serial = pd.to_timedelta(kokyaku_data.loc[flg_is_serial, 'ë“±ë¡ì¼'].astype('float'), unit='D') + pd.to_datetime('1900/01/01') from_serial.head() ## ë‚ ì§œí˜•ìœ¼ë¡œ ì˜ ë³€í™˜ëœ ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤ . 1 2017-02-18 3 2017-05-19 4 2018-01-29 21 2017-07-06 27 2017-06-17 Name: ë“±ë¡ì¼, dtype: datetime64[ns] . +) ì˜ˆì‹œë¡œ, to_timedelta, unit=â€™Dâ€™ê°€ ì–´ë–¤ ë°©ì‹ìœ¼ë¡œ ë°ì´í„°ë¥¼ ë³€í™˜í•˜ëŠ”ì§€ ë³´ì—¬ì£¼ê¸° ìœ„í•´ ì´ë ‡ê²Œ ì¶œë ¥í•´ë´„: . pd.to_timedelta(kokyaku_data.loc[flg_is_serial, 'ë“±ë¡ì¼'].astype('float'), unit='D').iloc[0] . Timedelta('42782 days 00:00:00') . +) ì¶”ê°€: ì‚¬ì‹¤, â€˜42782â€™ê°€ ì—‘ì…€ì—ì„œëŠ” â€˜2017-02-16â€™ë¼ê³  ë‚˜ì˜¨ë‹¤. (ìœ„ì—ì„œëŠ” 2017-02-18ë¡œ ë³€í™˜.) . | 1) ì—‘ì…€ì€ ìˆ«ìê°€ 0ì´ ì•„ë‹Œ 1ë¶€í„° ì‹œì‘í•˜ê³ , 2) 1900ë…„ì€ í‰ë…„ì¸ë° 1900/02/29ì¼ì„ ìœ íš¨í•œ ë‚ ì§œë¡œ ê³„ì‚°í•˜ê¸° ë•Œë¬¸. (ì—‘ì…€ì˜ ë²„ê·¸) | ê·¸ë˜ì„œ ì—‘ì…€ì˜ ë‚ ì§œ í˜•ì‹ ìˆ«ìë¥¼ ë‹¨ìˆœíˆ íŒŒì´ì¬ìœ¼ë¡œ ê³„ì‚°í•˜ë©´ ì´í‹€ì´ ì–´ê¸‹ë‚˜ëŠ” ê²ƒ.. | ê·¸ë˜ì„œ ì›ë˜ëŠ” ë‹¤ìŒê³¼ ê°™ì´ -2ë¥¼ í•´ì¤˜ì„œ ê³„ì‚°í•´ì•¼ ì—‘ì…€ ë‚ ì§œì— ë§ì¶œ ìˆ˜ ìˆë‹¤. | . from_serial = pd.to_timedelta(kokyaku_data.loc[flg_is_serial, 'ë“±ë¡ì¼'].astype('float') - 2, unit='D') + pd.to_datetime('1900/01/01') from_serial.head() . 1 2017-02-16 3 2017-05-17 4 2018-01-27 21 2017-07-04 27 2017-06-15 Name: ë“±ë¡ì¼, dtype: datetime64[ns] . relativedeltaë¡œ ê¸°ê°„ ê³„ì‚° . (relative delta: ë‚ ì§œ ë¹„êµ í•¨ìˆ˜) . customer = pd.read_csv('data/customer_master.csv') ## ë°ì´í„° ì¶œì²˜: https://github.com/wikibook/pyda10 customer.head() # ìŠ¤í¬ì¸ ì„¼í„° íšŒì› ë°ì´í„°. start_dateëŠ” ê°€ì…ì¼, end_dateëŠ” íƒˆí‡´ì¼. ## end_dateê°€ NaNì¸ ê²ƒì€ ì•„ì§ íƒˆí‡´í•˜ì§€ ì•Šì€ íšŒì› . | Â  | customer_id | name | class | gender | start_date | end_date | campaign_id | is_deleted | . | 0 | OA832399 | XXXX | C01 | F | 2015-05-01 00:00:00 | NaN | CA1 | 0 | . | 1 | PL270116 | XXXXX | C01 | M | 2015-05-01 00:00:00 | NaN | CA1 | 0 | . | 2 | OA974876 | XXXXX | C01 | M | 2015-05-01 00:00:00 | NaN | CA1 | 0 | . | 3 | HD024127 | XXXXX | C01 | F | 2015-05-01 00:00:00 | NaN | CA1 | 0 | . | 4 | HD661448 | XXXXX | C03 | F | 2015-05-01 00:00:00 | NaN | CA1 | 0 | . â†’ start_dateì™€ end_date ì¹¼ëŸ¼ì„ datetime íƒ€ì…ìœ¼ë¡œ ë³€ê²½ . customer['start_date'] = pd.to_datetime(customer['start_date']) customer['end_date'] = pd.to_datetime(customer['end_date']) customer.head() ## NaTëŠ” datetime typeì˜ NaN . | Â  | customer_id | name | class | gender | start_date | end_date | campaign_id | is_deleted | . | 0 | OA832399 | XXXX | C01 | F | 2015-05-01 | NaT | CA1 | 0 | . | 1 | PL270116 | XXXXX | C01 | M | 2015-05-01 | NaT | CA1 | 0 | . | 2 | OA974876 | XXXXX | C01 | M | 2015-05-01 | NaT | CA1 | 0 | . | 3 | HD024127 | XXXXX | C01 | F | 2015-05-01 | NaT | CA1 | 0 | . | 4 | HD661448 | XXXXX | C03 | F | 2015-05-01 | NaT | CA1 | 0 | . â†’ ê°€ì…~íƒˆí‡´ê¹Œì§€ì˜ íšŒì› ê¸°ê°„ ê³„ì‚°í•˜ê¸° . from dateutil.relativedelta import relativedelta # ë‚ ì§œ ë¹„êµ í•¨ìˆ˜ relativedeltaë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´ ë¼ì´ë¸ŒëŸ¬ë¦¬ import customer['calc_date'] = customer['end_date'] # ë‚ ì§œ ê³„ì‚°ìš© ì¹¼ëŸ¼ì„ ë”°ë¡œ ë³µì œ customer['calc_date'] = customer['calc_date'].fillna(pd.to_datetime('20190430')) # ê²°ì¸¡ì¹˜ì—ëŠ” ì¼ê´„ì ìœ¼ë¡œ 2019.04.30ì„ ëŒ€ì… customer['membership_period'] = 0 for i in range(len(customer)): delta = relativedelta(customer['calc_date'].iloc[i], customer['start_date'].iloc[i]) customer['membership_period'].iloc[i] = delta.years*12 + delta.months # íšŒì› ê¸°ê°„ì„ ì›” ë‹¨ìœ„ë¡œ ê³„ì‚° (ëª‡ ë‹¬ í›„ì— íƒˆí‡´í–ˆë‚˜) customer.head() . | Â  | customer_id | name | class | gender | start_date | end_date | campaign_id | is_deleted | calc_date | membership_period | . | 0 | OA832399 | XXXX | C01 | F | 2015-05-01 | NaT | CA1 | 0 | 2019-04-30 | 47 | . | 1 | PL270116 | XXXXX | C01 | M | 2015-05-01 | NaT | CA1 | 0 | 2019-04-30 | 47 | . | 2 | OA974876 | XXXXX | C01 | M | 2015-05-01 | NaT | CA1 | 0 | 2019-04-30 | 47 | . | 3 | HD024127 | XXXXX | C01 | F | 2015-05-01 | NaT | CA1 | 0 | 2019-04-30 | 47 | . | 4 | HD661448 | XXXXX | C03 | F | 2015-05-01 | NaT | CA1 | 0 | 2019-04-30 | 47 | . +) relativedelta ì‚¬ìš©ë²• ì‚´í´ë³´ê¸° . delta = relativedelta(pd.to_datetime('20190430'), pd.to_datetime('20170301')) print(delta) ## ëª‡ë…„ ëª‡ê°œì›” ë©°ì¹  ì°¨ì´ ë‚˜ëŠ”ì§€ ì €ì¥ë˜ì–´ ìˆìŒ print(delta.years) print(delta.months) print(delta.days) . relativedelta(years=+2, months=+1, days=+29) 2 1 29 . +) ë” ê°„ë‹¨í•œ ê¸°ê°„ ê³„ì‚° . | ë‘ columnì´ ë™ì¼í•œ datetime íƒ€ì…ì„ ê°€ì§€ê³  ìˆë‹¤ë©´, ê°„ë‹¨íˆ ë¹¼ê¸°(-)ë¡œ ê¸°ê°„ì„ ê³„ì‚°í•  ìˆ˜ ìˆë‹¤. | ê·¸ ê²°ê³¼ëŠ” 000 daysì™€ ê°™ì´ ì¼ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ë‚˜ì˜¤ë©°, .dt.daysë¥¼ ë¶™ì—¬ì£¼ë©´ int í˜•íƒœë¡œ ì¼ìˆ˜ë§Œ ì¶”ì¶œëœë‹¤ | . # ìœ„ì—ì„œ relativedeltaë¡œ ê³„ì‚°í•œ ê²ƒê³¼ ë‹¬ë¦¬, 'days' ê¸°ì¤€ì˜ íšŒì› ê¸°ê°„ ê³„ì‚° customer['membership_period'] = (customer['calc_date'] - customer['start_date']).dt.days customer.head() . | Â  | customer_id | name | class | gender | start_date | end_date | campaign_id | is_deleted | calc_date | membership_period | . | 0 | OA832399 | XXXX | C01 | F | 2015-05-01 | NaT | CA1 | 0 | 2019-04-30 | 1460 | . | 1 | PL270116 | XXXXX | C01 | M | 2015-05-01 | NaT | CA1 | 0 | 2019-04-30 | 1460 | . | 2 | OA974876 | XXXXX | C01 | M | 2015-05-01 | NaT | CA1 | 0 | 2019-04-30 | 1460 | . | 3 | HD024127 | XXXXX | C01 | F | 2015-05-01 | NaT | CA1 | 0 | 2019-04-30 | 1460 | . | 4 | HD661448 | XXXXX | C03 | F | 2015-05-01 | NaT | CA1 | 0 | 2019-04-30 | 1460 | . ",
    "url": "https://chaelist.github.io/docs/pandas/pandas_str_dt_con/#%EB%82%A0%EC%A7%9C%ED%98%95-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EB%B3%80%ED%99%98%EA%B0%80%EA%B3%B5",
    "relUrl": "/docs/pandas/pandas_str_dt_con/#ë‚ ì§œí˜•-ë°ì´í„°-ë³€í™˜ê°€ê³µ"
  },"171": {
    "doc": "Pandas str, dt, ì¡°ê±´ë¬¸",
    "title": "ì¡°ê±´ë¬¸ìœ¼ë¡œ ë°ì´í„° ê°€ê³µ",
    "content": "lambda ì‹ + apply() . : series.apply(lambda x: ì‹)ê³¼ ê°™ì€ í˜•ì‹ìœ¼ë¡œ ì‚¬ìš©. laptops_df = pd.read_csv('data/laptops.csv') ## ë°ì´í„° ì¶œì²˜: codeit laptops_df.head() . | Â  | brand | model | ram | hd_type | hd_size | screen_size | price | processor_brand | processor_model | clock_speed | graphic_card_brand | graphic_card_size | os | weight | comments | . | 0 | Dell | Inspiron 15-3567 | 4 | hdd | 1024 | 15.6 | 40000 | intel | i5 | 2.5 | intel | nan | linux | 2.5 | nan | . | 1 | Apple | MacBook Air | 8 | ssd | 128 | 13.3 | 55499 | intel | i5 | 1.8 | intel | 2 | mac | 1.35 | nan | . | 2 | Apple | MacBook Air | 8 | ssd | 256 | 13.3 | 71500 | intel | i5 | 1.8 | intel | 2 | mac | 1.35 | nan | . | 3 | Apple | MacBook Pro | 8 | ssd | 128 | 13.3 | 96890 | intel | i5 | 2.3 | intel | 2 | mac | 3.02 | nan | . | 4 | Apple | MacBook Pro | 8 | ssd | 256 | 13.3 | 112666 | intel | i5 | 2.3 | intel | 2 | mac | 3.02 | nan | . | ê° ì¤„ì˜ â€˜modelâ€™ëª…ì˜ ê¸¸ì´ë¥¼ ì„¸ì„œ model_lenì´ë¼ëŠ” ì¹¼ëŸ¼ìœ¼ë¡œ ì €ì¥í•˜ê¸° . laptops_df['model_len'] = laptops_df['model'].apply(lambda x: len(x)) laptops_df[['model', 'model_len']].head() . | Â  | model | model_len | . | 0 | Inspiron 15-3567 | 16 | . | 1 | MacBook Air | 11 | . | 2 | MacBook Air | 11 | . | 3 | MacBook Pro | 11 | . | 4 | MacBook Pro | 11 | . | lambda ì‹ì—ì„œ if-else ì ˆ ì‚¬ìš©: ê° ì¤„ì˜ ê°€ê²©ì´ ë¹„ì‹¼ì§€ ì•„ë‹Œì§€ ì •ì˜í•˜ëŠ” ìƒˆë¡œìš´ ì¹¼ëŸ¼ ìƒì„±í•˜ê¸° . # price ì¹¼ëŸ¼ì˜ í‰ê·  -- ì•„ë˜ì—ì„œ Expensive ì—¬ë¶€ íŒë‹¨ ê¸°ì¤€ìœ¼ë¡œ ì‚¬ìš© laptops_df['price'].mean() . 64132.89820359281 . â†’ ê° í–‰ì˜ â€˜priceâ€™ë¥¼ ë³´ê³  64133 ì´ìƒì´ë©´ Expensive, ì•„ë‹ˆë©´ Affordableì´ë¼ê³  ì •ì˜í•˜ëŠ” ìƒˆë¡œìš´ ì—´ ìƒì„± . laptops_df['Expensive_Affordable'] = laptops_df['price'].apply(lambda x: 'Expensive' if x &gt; 64133 else 'Affordable') laptops_df[['price','Expensive_Affordable']].head() . | Â  | price | Expensive_Affordable | . | 0 | 40000 | Affordable | . | 1 | 55499 | Affordable | . | 2 | 71500 | Expensive | . | 3 | 96890 | Expensive | . | 4 | 112666 | Expensive | . | lambda ì‹ì— í•¨ìˆ˜ë¥¼ ë°›ì•„ì„œ ì‚¬ìš©: ê° ì¤„ì˜ ê°€ê²©ì„ ì„¸ë¶„í™”í•´ ë¶„ë¥˜í•˜ëŠ” ìƒˆë¡œìš´ ì¹¼ëŸ¼ ìƒì„±í•˜ê¸° . # price ì¹¼ëŸ¼ì˜ ë¶„í¬ -- ì•„ë˜ì—ì„œ ì¹´í…Œê³ ë¦¬ íŒë‹¨ ê¸°ì¤€ìœ¼ë¡œ ì‚¬ìš© laptops_df['price'].describe() . count 167.000000 mean 64132.898204 std 42797.674010 min 13872.000000 25% 35457.500000 50% 47990.000000 75% 77494.500000 max 226000.000000 Name: price, dtype: float64 . â†’ ê° í–‰ì˜ â€˜priceâ€™ë¥¼ ë³´ê³  ì•„ë˜ get_category() í•¨ìˆ˜ì— ë”°ë¥¸ ë¶„ë¥˜ë¥¼ ì ì–´ì£¼ëŠ” ìƒˆë¡œìš´ ì—´ ìƒì„± . def get_category(price): if price &lt;= 35457: cat = 'cheap' elif price &lt;= 47990: cat = 'affordable' elif price &lt;= 77494: cat = 'expensive' else: cat = 'above budget' return cat laptops_df['price_cat'] = laptops_df['price'].apply(lambda x: get_category(x)) laptops_df[['price', 'price_cat']].head(10) . | Â  | price | price_cat | . | 0 | 40000 | affordable | . | 1 | 55499 | expensive | . | 2 | 71500 | expensive | . | 3 | 96890 | above budget | . | 4 | 112666 | above budget | . | 5 | 226000 | above budget | . | 6 | 158000 | above budget | . | 7 | 96990 | above budget | . | 8 | 33225 | cheap | . | 9 | 21990 | cheap | . | . lambda ì‹ + applymap() . : series.applymap(lambda x: ì‹)ê³¼ ê°™ì€ í˜•ì‹ìœ¼ë¡œ ì‚¬ìš©. | df.apply()ì˜ ê²½ìš° row / column basisë¡œ ì‘ìš©í•˜ê³ , df.applymap()ì˜ ê²½ìš° element-wiseë¡œ ì‘ìš©í•œë‹¤ëŠ” ì°¨ì´ê°€ ìˆê¸° ë•Œë¬¸ì—, ê° elementë¥¼ í•˜ë‚˜ í•˜ë‚˜ ì¡°ê±´ì‹ìœ¼ë¡œ ê²€ì‚¬í•´ì„œ ë³€ê²½í•˜ê³ ì í•˜ëŠ” ê²½ìš°ì—ëŠ” applymap()ì„ ì‚¬ìš©í•´ì•¼ í•œë‹¤ | . df = pd.DataFrame({ 'A': [0, 1, 2, 3], 'B': [1, 0, 0, 0], 'C': [3, 0, 2, 0], 'D': [0, 0, 1, 4]}) df . | Â  | A | B | C | D | . | 0 | 0 | 1 | 3 | 0 | . | 1 | 1 | 0 | 0 | 0 | . | 2 | 2 | 0 | 2 | 1 | . | 3 | 3 | 0 | 0 | 4 | . â†’ ìœ ë¬´ë§Œì„ ë‚˜íƒ€ë‚´ê¸° ìœ„í•´, 0ë³´ë‹¤ í° ìˆ«ìëŠ” ëª¨ë‘ â€˜Oâ€™ë¡œ, ê·¸ ì™¸ëŠ” ê³µë°±ìœ¼ë¡œ ë³€ê²½: . df.applymap(lambda x: 'O' if x &gt; 0 else '') . | Â  | A | B | C | D | . | 0 | Â  | O | O | Â  | . | 1 | O | Â  | Â  | Â  | . | 2 | O | Â  | O | O | . | 3 | O | Â  | Â  | O | . pd.where() . : series.where(seriesê°ì²´ì— ëŒ€í•œ ì¡°ê±´ë¬¸, ê±°ì§“ ê°’ì— ëŒ€í•œ ëŒ€ì²´ ê°’)ì˜ í˜•íƒœë¡œ ì‚¬ìš© . df = pd.DataFrame({'a': [1, 2, 3, 4, 5], 'b': [10, 20, 30, 40, 50]}) df . | Â  | a | b | . | 0 | 1 | 10 | . | 1 | 2 | 20 | . | 2 | 3 | 30 | . | 3 | 4 | 40 | . | 4 | 5 | 50 | . | seriesA.where(sereiesAì— ëŒ€í•œ ì¡°ê±´, ê°’) . df['c'] = df['a'].where(df['a'] &lt; 3, 10) df . | Â  | a | b | c | . | 0 | 1 | 10 | 1 | . | 1 | 2 | 20 | 2 | . | 2 | 3 | 30 | 10 | . | 3 | 4 | 40 | 10 | . | 4 | 5 | 50 | 10 | . | seriesB.where(sereiesAì— ëŒ€í•œ ì¡°ê±´, ê°’) . # aì—´ ì¤‘ 3ë³´ë‹¤ ì‘ì€ ê°’(1,2)ì—ëŠ” bì—´ì˜ ê°’, 3ì´ìƒì¸ ê°’ì— ëŒ€í•´ì„œëŠ” 100ì„ ë„£ëŠ”ë‹¤ df['d'] = df['b'].where(df['a'] &lt; 3, 100) df . | Â  | a | b | c | d | . | 0 | 1 | 10 | 1 | 10 | . | 1 | 2 | 20 | 2 | 20 | . | 2 | 3 | 30 | 10 | 100 | . | 3 | 4 | 40 | 10 | 100 | . | 4 | 5 | 50 | 10 | 100 | . | . np.where() í™œìš© . : np.where(ì¡°ê±´, ì¡°ê±´ì´ ë¶€í•©í•  ë•Œì˜ ê°’, ì•„ë‹ ë•Œì˜ ê°’)ì˜ í˜•íƒœë¡œ ì‚¬ìš© . import numpy as np df = pd.DataFrame({'a':[1, 2, 3, 4, 5], 'b':[5, 4, 3, 2, 1]}) df['flag'] = np.where(df['a'] &lt; df['b'], 'b is bigger', 'a is bigger') df . | Â  | a | b | flag | . | 0 | 1 | 5 | b is bigger | . | 1 | 2 | 4 | b is bigger | . | 2 | 3 | 3 | a is bigger | . | 3 | 4 | 2 | a is bigger | . | 4 | 5 | 1 | a is bigger | . ",
    "url": "https://chaelist.github.io/docs/pandas/pandas_str_dt_con/#%EC%A1%B0%EA%B1%B4%EB%AC%B8%EC%9C%BC%EB%A1%9C-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EA%B0%80%EA%B3%B5",
    "relUrl": "/docs/pandas/pandas_str_dt_con/#ì¡°ê±´ë¬¸ìœ¼ë¡œ-ë°ì´í„°-ê°€ê³µ"
  },"172": {
    "doc": "Plotly",
    "title": "Plotly",
    "content": ". | Bar graph | Scatter Plot . | Dot Plot | Bubble Chart | . | Line Chart | Pie Graph . | Treemap | Sunburst Chart | . | Statistical Charts . | Box Plot | Strip Plot | Violin Plot | Histogram | . | . *Plotly : ì„¸ë ¨ëœ opensource interactive graphing library. *Plotly Express : easy-to-use, high-level visualization library (Plotlyì™€ Plotly Expressì˜ ê´€ê³„ëŠ” Matplotlibê³¼ Seabornì˜ ê´€ê³„ì™€ ìœ ì‚¬) . | pip install plotlyë¡œ ì„¤ì¹˜í•´ì„œ ì‚¬ìš© | . *Chart Studio : plotlyë¡œ ì‹œê°í™”í•œ ì°¨íŠ¸ë¥¼ chart studioì— uploadí•˜ë©´ ì‰½ê²Œ web ìƒì— embedí•  ìˆ˜ ìˆë‹¤ . | pip install chart_studioë¡œ ì„¤ì¹˜ | usernameê³¼ api_keyë¥¼ í™œìš©í•´ ì—°ê²°: import chart_studio username = '' # ìì‹ ì˜ username (plotly accountë¥¼ ë§Œë“¤ì–´ì•¼ í•¨) api_key = '' # ìì‹ ì˜ api key (settings &gt; regenerate key) chart_studio.tools.set_credentials_file(username=username, api_key=api_key) . | ì‘ì„±í•œ ì°¨íŠ¸ë¥¼ uploadí•˜ëŠ” ì½”ë“œ: chart_studio.plotly.plot(fig, filename = 'íŒŒì¼ì´ë¦„', auto_open=True) # fig: ì‘ì„±í•œ ì°¨íŠ¸ë¥¼ ì €ì¥í•œ ë³€ìˆ˜ ## ìœ„ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ë©´ ìƒˆë¡œìš´ windowë¡œ í•´ë‹¹ ì°¨íŠ¸ì˜ ë§í¬ê°€ ì—´ë¦¬ê³ , notebookì—ë„ linkë¥¼ ì•„ë˜ì— returní•´ì¤Œ . | . ",
    "url": "https://chaelist.github.io/docs/visualization/plotly/",
    "relUrl": "/docs/visualization/plotly/"
  },"173": {
    "doc": "Plotly",
    "title": "Bar graph",
    "content": ". | https://plotly.com/python/bar-charts/ | . # í•„ìš”í•œ libraryë¥¼ import import plotly.express as px import plotly.io as pio import pandas as pd import numpy as np pio.templates.default = \"plotly_white\" # default templateì„ ì§€ì • (ì§€ì •í•˜ì§€ ì•Šìœ¼ë©´ ê¸°ë³¸ì€ 'plotly') . bills_df = px.data.tips() # plotly expressì—ì„œ ì œê³µë˜ëŠ” ê¸°ë³¸ data ì‚¬ìš© bills_df.head() . | Â  | total_bill | tip | sex | smoker | day | time | size | . | 0 | 16.99 | 1.01 | Female | No | Sun | Dinner | 2 | . | 1 | 10.34 | 1.66 | Male | No | Sun | Dinner | 3 | . | 2 | 21.01 | 3.5 | Male | No | Sun | Dinner | 3 | . | 3 | 23.68 | 3.31 | Male | No | Sun | Dinner | 2 | . | 4 | 24.59 | 3.61 | Female | No | Sun | Dinner | 4 | . | daily average bills per sex . fig = px.bar(bills_df.groupby(['day', 'sex'])[['total_bill']].mean().reset_index(), x='day', y='total_bill', color='sex', title='Average Bills per Day', category_orders={'day':['Thur', 'Fri', 'Sat', 'Sun']}, color_discrete_sequence=px.colors.qualitative.Pastel) fig.show() . | ì—°ì†ì ì´ì§€ ì•Šì€ color paletteëŠ” color_discrete_sequence ì˜µì…˜ìœ¼ë¡œ ì§€ì • | ë³´í†µ discrete_sequenceì—ëŠ” px.colors.qualitativeì— ìˆëŠ” ìƒ‰ì„ ë„£ì§€ë§Œ, px.colors.sequential / px.colors.diverging / px.colors.cyclicalì— ìˆëŠ” ìƒ‰ì„ ë„£ì–´ë„ ê´œì°®ë‹¤ | colorë¡œ êµ¬ë¶„ì„ ë„£ìœ¼ë©´ defaultë¡œ ëˆ„ì  ê·¸ë˜í”„ í˜•íƒœë¡œ ì‹œê°í™”ë¨ | . | daily average bills per sex (2) . fig = px.bar(bills_df.groupby(['day', 'sex'])[['total_bill']].mean().reset_index(), x='day', y='total_bill', color='sex', height=400, title='Average Bills per Day', category_orders={'day':['Thur', 'Fri', 'Sat', 'Sun']}, color_discrete_sequence=px.colors.qualitative.Pastel, barmode='group') # ì–‘ ì˜†ìœ¼ë¡œ ë†“ì´ëŠ” êµ¬ì¡° # barmode='stack'ë¼ê³  í•˜ë©´ ëˆ„ì  (dafault setting) fig.show() . | barmode='group' ì˜µì…˜ì„ ì¶”ê°€í•´ì£¼ë©´ ê·¸ë£¹ë³„ ê°’ì´ ìœ„ì•„ë˜ ëŒ€ì‹  ì–‘ì˜†ìœ¼ë¡œ ë°°ì¹˜ë¨ | height ì˜µì…˜ìœ¼ë¡œ í¬ê¸°ë¥¼ ì§€ì •í•´ì£¼ëŠ” ê²ƒë„ ê°€ëŠ¥ (+. width ì˜µì…˜ë„ ê°€ëŠ¥) | . | daily total bills per sex . fig = px.bar(bills_df, x='day', y='total_bill', color='sex', title='Total Bills per Day', category_orders={'day':['Thur', 'Fri', 'Sat', 'Sun']}, color_discrete_sequence=px.colors.qualitative.Pastel) fig.show() . | seabornì—ì„œëŠ” ì „ì²´ dataë¥¼ ë„£ì–´ì„œ bar graphë¥¼ ê·¸ë¦¬ë©´ defaultë¡œ í‰ê· ê°’ì„ ì‹œê°í™”í•´ì£¼ëŠ” ë°˜ë©´, plotly expressëŠ” ì „ì²´ dataì˜ ê·¸ëŒ€ë¡œ í•©ì‚°í•´ì„œ ì‹œê°í™”í•´ì¤€ë‹¤ | . | categoryë³„ë¡œ ì˜ì—­ì„ ë‚˜ëˆ„ì–´ ê·¸ë¦¬ê¸° . fig = px.bar(bills_df, x='day', y='total_bill', color='smoker', barmode='group', facet_col='sex', category_orders={'day':['Thur', 'Fri', 'Sat', 'Sun']}, color_discrete_sequence=px.colors.qualitative.Safe, template='plotly') fig.show() . | facet_col ì˜µì…˜ì„ í†µí•´ íŠ¹ì • ê·¸ë£¹ë³„ë¡œ ì˜ì—­ì„ ë‚˜ëˆ ì„œ ì‹œê°í™”í•  ìˆ˜ ìˆë‹¤ | template ì˜µì…˜ì„ í†µí•´ ê° ê·¸ë˜í”„ë³„ templateì„ ë³„ë„ë¡œ ì§€ì •í•  ìˆ˜ë„ ìˆë‹¤ (ì›ë˜ defaultëŠ” â€˜plotlyâ€™ì§€ë§Œ, ìœ„ì—ì„œ default templateì„ â€˜plotly_whiteâ€™ë¡œ ì§€ì •í•´ë‘ì—ˆìœ¼ë¯€ë¡œ, í•´ë‹¹ ê·¸ë˜í”„ì—ì„œëŠ” â€˜plotlyâ€™ templateì„ ì‚¬ìš©í•˜ê¸° ìœ„í•´ ì˜µì…˜ì„ ë³„ë„ë¡œ ì§€ì •) | . | . ",
    "url": "https://chaelist.github.io/docs/visualization/plotly/#bar-graph",
    "relUrl": "/docs/visualization/plotly/#bar-graph"
  },"174": {
    "doc": "Plotly",
    "title": "Scatter Plot",
    "content": ". | https://plotly.com/python/line-and-scatter/ | . iris_df = px.data.iris() iris_df.head() . | Â  | sepal_length | sepal_width | petal_length | petal_width | species | species_id | . | 0 | 5.1 | 3.5 | 1.4 | 0.2 | setosa | 1 | . | 1 | 4.9 | 3 | 1.4 | 0.2 | setosa | 1 | . | 2 | 4.7 | 3.2 | 1.3 | 0.2 | setosa | 1 | . | 3 | 4.6 | 3.1 | 1.5 | 0.2 | setosa | 1 | . | 4 | 5 | 3.6 | 1.4 | 0.2 | setosa | 1 | . | ê¸°ë³¸ scatter plot . fig = px.scatter(iris_df, x='sepal_width', y='sepal_length', color='species', color_discrete_sequence=px.colors.qualitative.Safe) fig.show() . | trendlineì„ í¬í•¨í•œ scatter plot . fig = px.scatter(bills_df, x='total_bill', y='tip', trendline='ols', color_discrete_sequence=px.colors.qualitative.Pastel1, trendline_color_override='gold') fig.show() . | trendline='ols' ì˜µì…˜ì„ í†µí•´ ì„ í˜•íšŒê·€ì„ ì„ í•¨ê»˜ ì‹œê°í™” | . | . Dot Plot . gapminder_df = px.data.gapminder() gapminder_df.head() . | Â  | country | continent | year | lifeExp | pop | gdpPercap | iso_alpha | iso_num | . | 0 | Afghanistan | Asia | 1952 | 28.801 | 8425333 | 779.445 | AFG | 4 | . | 1 | Afghanistan | Asia | 1957 | 30.332 | 9240934 | 820.853 | AFG | 4 | . | 2 | Afghanistan | Asia | 1962 | 31.997 | 10267083 | 853.101 | AFG | 4 | . | 3 | Afghanistan | Asia | 1967 | 34.02 | 11537966 | 836.197 | AFG | 4 | . | 4 | Afghanistan | Asia | 1972 | 36.088 | 13079460 | 739.981 | AFG | 4 | . â†’ scatter plotì— categorical axisë¥¼ ë„£ì–´ì„œ ì‹œê°í™” (dot plot) . fig = px.scatter(gapminder_df.query(\"continent=='Americas'\"), x='lifeExp', y='country', color='year', color_continuous_scale='Burgyl') fig.show() . | ì—°ì†ì ì¸ color paletteëŠ” color_continuous_scale ì˜µì…˜ìœ¼ë¡œ ì§€ì • | . Bubble Chart . : scatter plotì—ì„œ size optionì„ ì§€ì •í•´ ì‹œê°í™”í•˜ë©´ ëœë‹¤ . | 2007ë…„ì˜ gdpPercapê³¼ liefExpì˜ ê´€ê³„ë¥¼ ì‹œê°í™” (bubble size: population) . fig = px.scatter(gapminder_df.query(\"year == 2007\"), x='gdpPercap', y='lifeExp', size='pop', color='continent', hover_name='country') fig.show() . | hover_name ì˜µì…˜ì„ í†µí•´ ë§ˆìš°ìŠ¤ë¥¼ hoverí•  ë•Œ ìš°ì„ ì ìœ¼ë¡œ ë³´ì—¬ì§€ëŠ” ì •ë³´ë¥¼ ì§€ì • | . | xê°’ì„ log ë³€í™˜í•´ ì‹œê°í™” . fig = px.scatter(gapminder_df.query(\"year == 2007\"), x='gdpPercap', y='lifeExp', size='pop', color='continent', hover_name='country', log_x=True, size_max=60) fig.show() . | xê°’ê³¼ yê°’ì˜ ë” ëª…í™•í•œ ê´€ê³„ë¥¼ ì‹œê°í™”í•˜ê¸° ìœ„í•´ log_x=True ì˜µì…˜ì„ ì§€ì • | ë” bubbleì˜ sizeë¥¼ êµ¬ë¶„í•˜ê¸° ì‰½ë„ë¡ size_max=60 ì˜µì…˜ìœ¼ë¡œ bubble sizeë¥¼ ì¡°ì • | . | . ",
    "url": "https://chaelist.github.io/docs/visualization/plotly/#scatter-plot",
    "relUrl": "/docs/visualization/plotly/#scatter-plot"
  },"175": {
    "doc": "Plotly",
    "title": "Line Chart",
    "content": ". | https://plotly.com/python/line-charts/ | . fig = px.line(gapminder_df.query(\"continent == 'Oceania'\"), x='year', y='lifeExp', color='country', symbol='country', color_discrete_sequence=px.colors.qualitative.Pastel1) fig.show() . | symbol='country' ì˜µì…˜ì„ í†µí•´ ê° êµ­ê°€ë³„ markerì˜ ëª¨ì–‘ì„ ë‹¤ë¥´ê²Œ ì§€ì • | ê° ê·¸ë£¹ë³„ marker ëª¨ì–‘ì„ êµ¬ë¶„í•˜ì§€ ì•Šê³  ëª¨ë‘ ë™ì¼í•œ markerë¡œ í‘œì‹œí•˜ê³  ì‹¶ìœ¼ë©´ symbol ì˜µì…˜ ëŒ€ì‹  markers=True ì˜µì…˜ì„ ì¶”ê°€í•´ì£¼ë©´ ëœë‹¤ | . ",
    "url": "https://chaelist.github.io/docs/visualization/plotly/#line-chart",
    "relUrl": "/docs/visualization/plotly/#line-chart"
  },"176": {
    "doc": "Plotly",
    "title": "Pie Graph",
    "content": ". | https://plotly.com/python/pie-charts/ | . # 2007ë…„ gapminder ìˆ˜ì¹˜ ì¤‘, Asia ì§€ì—­ì˜ êµ­ê°€ë³„ population ë¹„ì¤‘ì„ ì‹œê°í™” temp_df = gapminder_df.query(\"year == 2007\").query(\"continent == 'Asia'\") # populationì´ ê°€ì¥ ë§ì€ top 15 êµ­ê°€ë¥¼ ì œì™¸í•˜ê³ ëŠ” ë‹¤ 'Other countries'ë¡œ ì²˜ë¦¬ temp_df.sort_values(by='pop', ascending=False, inplace=True) temp_df.iloc[15:, 0] = 'Other countries' fig = px.pie(temp_df, values='pop', names='country', color_discrete_sequence=px.colors.qualitative.Antique) fig.show() . Treemap . | https://plotly.com/python/treemaps/ | . temp_df = gapminder_df.query(\"year == 2007\") fig = px.treemap(temp_df, path=[px.Constant('world'), 'continent', 'country'], values='pop', color='lifeExp', color_continuous_scale='RdBu', color_continuous_midpoint=np.average(temp_df['lifeExp'], weights=temp_df['pop'])) fig.update_layout(margin = dict(t=50, l=25, r=25, b=25)) fig.show() . Sunburst Chart . | https://plotly.com/python/sunburst-charts/ | . fig = px.sunburst(bills_df, path=['day', 'time', 'sex'], values='tip', color='time', color_discrete_sequence=px.colors.qualitative.Pastel) fig.show() . ",
    "url": "https://chaelist.github.io/docs/visualization/plotly/#pie-graph",
    "relUrl": "/docs/visualization/plotly/#pie-graph"
  },"177": {
    "doc": "Plotly",
    "title": "Statistical Charts",
    "content": "Box Plot . | https://plotly.com/python/box-plots/ | . | ê¸°ë³¸ box plot . fig = px.box(bills_df, x='sex', y='total_bill', color='smoker', color_discrete_sequence=px.colors.qualitative.Pastel) fig.show() . | ê° pointë¥¼ í•¨ê»˜ ì‹œê°í™” . fig = px.box(bills_df, x='sex', y='total_bill', color='smoker', points='all', color_discrete_sequence=px.colors.qualitative.Pastel) fig.show() . | points='all' ì˜µì…˜ì„ ì§€ì •í•˜ë©´ box plot ì˜†ì— ëª¨ë“  pointë¥¼ í•¨ê»˜ ì‹œê°í™”í•´ì¤Œ (default: points='outliers') | . | . Strip Plot . | https://plotly.com/python/strip-charts/ | . | ê¸°ë³¸ strip plot . fig = px.strip(bills_df, x='sex', y='total_bill', color='smoker', color_discrete_sequence=px.colors.qualitative.Pastel) fig.show() . | categoryë³„ë¡œ ì„¸ë¶„í™”í•´ ì‹œê°í™” . fig = px.strip(bills_df, x='total_bill', y='time', color='sex', facet_col='day', category_orders={'day':['Thur', 'Fri', 'Sat', 'Sun']}, color_discrete_sequence=px.colors.qualitative.Safe, template='plotly') fig.show() . | . Violin Plot . | https://plotly.com/python/violin/ | . | ê¸°ë³¸ violin plot . fig = px.violin(bills_df, x='sex', y='total_bill', color='smoker', color_discrete_sequence=px.colors.qualitative.Pastel) fig.show() . | box, points ì˜µì…˜ ì§€ì • . fig = px.violin(bills_df, y='total_bill', color='sex', box=True, points='all', color_discrete_sequence=px.colors.qualitative.Pastel) fig.show() . | points='all' ì˜µì…˜ì„ ì§€ì •í•˜ë©´ violin plot ì˜†ì— ëª¨ë“  pointë¥¼ í•¨ê»˜ ì‹œê°í™”í•´ì¤Œ | box=True ì˜µì…˜ì„ ì§€ì •í•˜ë©´ violin plot ì•ˆì— box plotë¥¼ í•¨ê»˜ ì‹œê°í™”í•´ì¤Œ | . | . Histogram . | https://plotly.com/python/histograms/ | . | ê¸°ë³¸ histogram . fig = px.histogram(bills_df, x='total_bill', nbins=10) fig.show() . | nbins ì˜µì…˜ìœ¼ë¡œ number of binsë¥¼ ì¡°ì ˆ | . | categorical dataë¥¼ ë„£ìœ¼ë©´ count plotì²˜ëŸ¼ ì‘ìš© . fig = px.histogram(bills_df, x='day', category_orders={'day':['Thur', 'Fri', 'Sat', 'Sun']}) fig.show() . | xê°’ì— categorical dataë¥¼ ë„£ìœ¼ë©´ ê° ì¹´í…Œê³ ë¦¬ë³„ ìˆ˜ë¥¼ ì„¸ì–´ì„œ ì‹œê°í™”í•´ì¤Œ (count plot) | . | histogramê³¼ ê° ê°’ì˜ ë¶„í¬ë¥¼ í•¨ê»˜ ì‹œê°í™” . fig = px.histogram(bills_df, x='total_bill', y='tip', color='sex', marginal='box', color_discrete_sequence=px.colors.qualitative.Pastel) fig.show() . | marginal='box' ì˜µì…˜ì„ ì¶”ê°€í•˜ë©´ ê° ê°’ì˜ ë¶„í¬ë¥¼ box plotìœ¼ë¡œ í•¨ê»˜ ì‹œê°í™”í•´ì¤Œ (marginal: box, violin, rug ì¤‘ì— ì„ íƒ ê°€ëŠ¥) | ì°¸ê³ : Combined statistical representations | . | . +) ì°¸ê³ ìš© ë§í¬ë“¤ . | color_discrete_sequence | color_continuous_scale | colors in plotly express â€“ plotly.express.colors.sequential.swatches()ì™€ ê°™ì€ ì½”ë“œë¡œ ìƒ‰ ì¢…ë¥˜ í™•ì¸ ê°€ëŠ¥ | adjusting size | hovermode and hover labels | templates | image export / html export | plotly.express python api reference | . ",
    "url": "https://chaelist.github.io/docs/visualization/plotly/#statistical-charts",
    "relUrl": "/docs/visualization/plotly/#statistical-charts"
  },"178": {
    "doc": "ë°ì´í„° ì „ì²˜ë¦¬",
    "title": "ë°ì´í„° ì „ì²˜ë¦¬",
    "content": ". | Feature Scaling . | Min-Max Normalization (ìµœì†Œ-ìµœëŒ€ ì •ê·œí™”) | Standardization (í‘œì¤€í™”) | . | ì¹´í…Œê³ ë¦¬ ë³€ìˆ˜ ì¸ì½”ë”© . | One-hot Encoding | . | . ",
    "url": "https://chaelist.github.io/docs/ml_advanced/preprocessing/",
    "relUrl": "/docs/ml_advanced/preprocessing/"
  },"179": {
    "doc": "ë°ì´í„° ì „ì²˜ë¦¬",
    "title": "Feature Scaling",
    "content": ": ì…ë ¥ ë³€ìˆ˜(feature)ì˜ í¬ê¸°ë¥¼ ì¡°ì •(scale)í•´ì„œ ì¼ì • ë²”ìœ„ ë‚´ì— ë–¨ì–´ì§€ë„ë¡ ë°”ê¿”ì£¼ëŠ” ê²ƒ. | ì„œë¡œ ë‹¤ë¥¸ ë‹¨ìœ„ì™€ ë²”ìœ„ë¥¼ ê°–ëŠ” ì…ë ¥ë³€ìˆ˜ë“¤ì„ í™œìš©í•´ì„œ machine learningì„ í•  ë•Œ, ë” í° ê°’ë“¤ì„ ê°–ëŠ” ë³€ìˆ˜ê°€ ê°–ëŠ” ì˜í–¥ì´ ê³¼ë‹¤í•˜ê²Œ ë‚˜íƒ€ë‚˜ëŠ” ê²ƒì„ ë§‰ìœ¼ë ¤ë©´ Feature Scalingì„ í•´ì¤˜ì•¼ í•œë‹¤ . | ex) í‚¤ì™€ ëª¸ë¬´ê²Œ ë³€ìˆ˜ë¥¼ ì‚¬ìš©í•  ë•Œ, feature scalingì„ í•˜ì§€ ì•Šìœ¼ë©´ í‚¤ ë³€ìˆ˜ì˜ ì¤‘ìš”ì„±ì´ ê³¼ë‹¤í•˜ê²Œ ë‚˜íƒ€ë‚  ìˆ˜ ìˆë‹¤ | . | Feature Scalingì€ ê²½ì‚¬í•˜ê°•ë²•ì„ ë” ë¹¨ë¦¬ í•  ìˆ˜ ìˆë„ë¡ í•´ì¤€ë‹¤ . | ë” ë¹¨ë¦¬ ìµœì†Œì ì„ ì°¾ì„ ìˆ˜ ìˆê¸° ë•Œë¬¸ì— Maching Learningì—ì„œì˜ ëª¨ë¸ í•™ìŠµì´ ë¹¨ë¼ì§„ë‹¤ | . | . Min-Max Normalization (ìµœì†Œ-ìµœëŒ€ ì •ê·œí™”) . : ê°€ì¥ í”í•œ ì •ê·œí™” ë°©ë²•. ë°ì´í„°ì˜ í¬ê¸°ë¥¼ 0ê³¼ 1ì‚¬ì´ë¡œ ë°”ê¿”ì£¼ëŠ” ê²ƒ. (ìµœì†Ÿê°’ â†’ 0, ìµœëŒ“ê°’ â†’ 1) . | $ X_{new} = \\dfrac{X_{old} - X_{min}}{X_{max} - X_{min}} $ | ìƒˆë¡œìš´ ê°’ = (ì›ë˜ ê°’ - ìµœì†Ÿê°’) / (ìµœëŒ“ê°’ - ìµœì†Ÿê°’) | . import pandas as pd import numpy as np from sklearn.datasets import load_boston # boston ì§‘ê°’ ë°ì´í„° from sklearn import preprocessing . â£1. boston datasetì„ ë¶ˆëŸ¬ì˜´ . boston_dataset = load_boston() boston_data = pd.DataFrame(boston_dataset.data, columns=boston_dataset.feature_names) boston_data.head() . | Â  | CRIM | ZN | INDUS | CHAS | NOX | RM | AGE | DIS | RAD | TAX | PTRATIO | B | LSTAT | . | 0 | 0.00632 | 18 | 2.31 | 0 | 0.538 | 6.575 | 65.2 | 4.09 | 1 | 296 | 15.3 | 396.9 | 4.98 | . | 1 | 0.02731 | 0 | 7.07 | 0 | 0.469 | 6.421 | 78.9 | 4.9671 | 2 | 242 | 17.8 | 396.9 | 9.14 | . | 2 | 0.02729 | 0 | 7.07 | 0 | 0.469 | 7.185 | 61.1 | 4.9671 | 2 | 242 | 17.8 | 392.83 | 4.03 | . | 3 | 0.03237 | 0 | 2.18 | 0 | 0.458 | 6.998 | 45.8 | 6.0622 | 3 | 222 | 18.7 | 394.63 | 2.94 | . | 4 | 0.06905 | 0 | 2.18 | 0 | 0.458 | 7.147 | 54.2 | 6.0622 | 3 | 222 | 18.7 | 396.9 | 5.33 | . â£2. describe()ë¡œ ê° ë³€ìˆ˜ì˜ ë¶„í¬ë¥¼ í™•ì¸: maxê°’ê³¼ minê°’ì´ ì œê°ê°ì„ì„ í™•ì¸ ê°€ëŠ¥ . boston_data.describe() . | Â  | CRIM | ZN | INDUS | CHAS | NOX | RM | AGE | DIS | RAD | TAX | PTRATIO | B | LSTAT | . | count | 506 | 506 | 506 | 506 | 506 | 506 | 506 | 506 | 506 | 506 | 506 | 506 | 506 | . | mean | 3.61352 | 11.3636 | 11.1368 | 0.06917 | 0.554695 | 6.28463 | 68.5749 | 3.79504 | 9.54941 | 408.237 | 18.4555 | 356.674 | 12.6531 | . | std | 8.60155 | 23.3225 | 6.86035 | 0.253994 | 0.115878 | 0.702617 | 28.1489 | 2.10571 | 8.70726 | 168.537 | 2.16495 | 91.2949 | 7.14106 | . | min | 0.00632 | 0 | 0.46 | 0 | 0.385 | 3.561 | 2.9 | 1.1296 | 1 | 187 | 12.6 | 0.32 | 1.73 | . | 25% | 0.082045 | 0 | 5.19 | 0 | 0.449 | 5.8855 | 45.025 | 2.10018 | 4 | 279 | 17.4 | 375.377 | 6.95 | . | 50% | 0.25651 | 0 | 9.69 | 0 | 0.538 | 6.2085 | 77.5 | 3.20745 | 5 | 330 | 19.05 | 391.44 | 11.36 | . | 75% | 3.67708 | 12.5 | 18.1 | 0 | 0.624 | 6.6235 | 94.075 | 5.18843 | 24 | 666 | 20.2 | 396.225 | 16.955 | . | max | 88.9762 | 100 | 27.74 | 1 | 0.871 | 8.78 | 100 | 12.1265 | 24 | 711 | 22 | 396.9 | 37.97 | . â£3. MinMaxScaler()ë¡œ normalize: . # Min-Max Normalization scaler = preprocessing.MinMaxScaler() normalized_data = scaler.fit_transform(boston_data) # dfë¡œ ì •ë¦¬í•´ì„œ í™•ì¸ normalized_df = pd.DataFrame(normalized_data, columns=boston_dataset.feature_names) normalized_df.head() ## ë‹¤ 0~1 ì‚¬ì´ì˜ ê°’ìœ¼ë¡œ ë°”ë€Œì—ˆìŒ . | Â  | CRIM | ZN | INDUS | CHAS | NOX | RM | AGE | DIS | RAD | TAX | PTRATIO | B | LSTAT | . | 0 | 0 | 0.18 | 0.0678152 | 0 | 0.314815 | 0.577505 | 0.641607 | 0.269203 | 0 | 0.208015 | 0.287234 | 1 | 0.0896799 | . | 1 | 0.000235923 | 0 | 0.242302 | 0 | 0.17284 | 0.547998 | 0.782698 | 0.348962 | 0.0434783 | 0.104962 | 0.553191 | 1 | 0.20447 | . | 2 | 0.000235698 | 0 | 0.242302 | 0 | 0.17284 | 0.694386 | 0.599382 | 0.348962 | 0.0434783 | 0.104962 | 0.553191 | 0.989737 | 0.0634658 | . | 3 | 0.000292796 | 0 | 0.0630499 | 0 | 0.150206 | 0.658555 | 0.441813 | 0.448545 | 0.0869565 | 0.0667939 | 0.648936 | 0.994276 | 0.0333885 | . | 4 | 0.00070507 | 0 | 0.0630499 | 0 | 0.150206 | 0.687105 | 0.528321 | 0.448545 | 0.0869565 | 0.0667939 | 0.648936 | 1 | 0.0993377 | . â£4. ì˜ normalizeë˜ì—ˆëŠ”ì§€ min, maxê°’ì„ í™•ì¸ . normalized_df.describe() ## ëª¨ë“  ì—´ì´ min=0, max=1ë¡œ ì˜ ë°”ë€œ . | Â  | CRIM | ZN | INDUS | CHAS | NOX | RM | AGE | DIS | RAD | TAX | PTRATIO | B | LSTAT | . | count | 506 | 506 | 506 | 506 | 506 | 506 | 506 | 506 | 506 | 506 | 506 | 506 | 506 | . | mean | 0.0405441 | 0.113636 | 0.391378 | 0.06917 | 0.349167 | 0.521869 | 0.676364 | 0.242381 | 0.371713 | 0.422208 | 0.622929 | 0.898568 | 0.301409 | . | std | 0.0966793 | 0.233225 | 0.251479 | 0.253994 | 0.238431 | 0.134627 | 0.289896 | 0.191482 | 0.378576 | 0.321636 | 0.230313 | 0.230205 | 0.197049 | . | min | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . | 25% | 0.000851131 | 0 | 0.173387 | 0 | 0.131687 | 0.445392 | 0.433831 | 0.088259 | 0.130435 | 0.175573 | 0.510638 | 0.94573 | 0.14404 | . | 50% | 0.00281208 | 0 | 0.338343 | 0 | 0.314815 | 0.507281 | 0.76828 | 0.188949 | 0.173913 | 0.272901 | 0.68617 | 0.986232 | 0.265728 | . | 75% | 0.0412585 | 0.125 | 0.646628 | 0 | 0.49177 | 0.586798 | 0.93898 | 0.369088 | 1 | 0.914122 | 0.808511 | 0.998298 | 0.420116 | . | max | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | . Standardization (í‘œì¤€í™”) . : ë°ì´í„°ì˜ í‰ê· ì„ 0, í‘œì¤€í¸ì°¨ë¥¼ 1ë¡œ ë§ì¶°ì£¼ëŠ” ê²ƒ . | $ X_{new} = \\dfrac{X_{old} - \\bar{X}}{\\sigma} $ | ìƒˆë¡œìš´ ê°’ = (ì›ë˜ ê°’ - í‰ê· ) / í‘œì¤€í¸ì°¨ | í‘œì¤€í™”ë¥¼ í•œ ë°ì´í„°ë¥¼ ë³´í†µ z-scoreë¼ê³  í•œë‹¤ - ex) z-scoreê°€ 1.5ë©´ â€˜í‰ê· ê°’ë³´ë‹¤ 1.5 í‘œì¤€í¸ì°¨ë§Œí¼ í¬ë‹¤â€™ëŠ” ëœ» | . # ì†Œìˆ˜ì  2ë²ˆì§¸ ìë¦¬ê¹Œì§€ë§Œ ì¶œë ¥ë˜ë„ë¡ ì„¤ì • pd.set_option('display.float_format', lambda x: '%.2f' % x) # Standardization scaler = preprocessing.StandardScaler() # MinMaxScalerì™€ ë¹„êµí–ˆì„ ë•Œ, ì´ í•œ ì¤„ë§Œ ë°”ë€œ. standardized_data = scaler.fit_transform(boston_data) # ìœ„ì—ì„œ ì‚¬ìš©í–ˆë˜ boston_data dfë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš© standardized_df = pd.DataFrame(standardized_data, columns=boston_dataset.feature_names) standardized_df.describe() ## ëª¨ë“  ì—´ì´ í‰ê· ì€ 0, í‘œì¤€í¸ì°¨ëŠ” 1ì´ ë˜ë„ë¡ ì˜ í‘œì¤€í™”ë¨ . | Â  | CRIM | ZN | INDUS | CHAS | NOX | RM | AGE | DIS | RAD | TAX | PTRATIO | B | LSTAT | . | count | 506.00 | 506.00 | 506.00 | 506.00 | 506.00 | 506.00 | 506.00 | 506.00 | 506.00 | 506.00 | 506.00 | 506.00 | 506.00 | . | mean | -0.00 | -0.00 | -0.00 | -0.00 | -0.00 | -0.00 | -0.00 | -0.00 | -0.00 | -0.00 | -0.00 | -0.00 | -0.00 | . | std | 1.00 | 1.00 | 1.00 | 1.00 | 1.00 | 1.00 | 1.00 | 1.00 | 1.00 | 1.00 | 1.00 | 1.00 | 1.00 | . | min | -0.42 | -0.49 | -1.56 | -0.27 | -1.46588 | -3.88 | -2.34 | -1.27 | -0.98 | -1.31 | -2.71 | -3.91 | -1.53 | . | 25% | -0.41 | -0.49 | -0.87 | -0.27 | -0.91 | -0.57 | -0.84 | -0.81 | -0.64 | -0.77 | -0.49 | 0.21 | -0.80 | . | 50% | -0.39 | -0.49 | -0.21 | -0.27 | -0.14 | -0.11 | 0.32 | -0.28 | -0.52 | -0.46 | 0.27 | 0.38 | -0.18 | . | 75% | 0.01 | 0.05 | 1.02 | -0.27 | 0.60 | 0.48 | 0.91 | 0.66 | 1.66 | 1.53 | 0.81 | 0.43 | 0.60 | . | max | 9.93 | 3.80 | 2.42 | 3.66 | 2.73 | 3.56 | 1.12 | 3.96 | 1.66 | 1.80 | 1.64 | 0.44 | 3.55 | . ",
    "url": "https://chaelist.github.io/docs/ml_advanced/preprocessing/#feature-scaling",
    "relUrl": "/docs/ml_advanced/preprocessing/#feature-scaling"
  },"180": {
    "doc": "ë°ì´í„° ì „ì²˜ë¦¬",
    "title": "ì¹´í…Œê³ ë¦¬ ë³€ìˆ˜ ì¸ì½”ë”©",
    "content": ". | ë¬¸ìì—´ë¡œ êµ¬ì„±ëœ ì¹´í…Œê³ ë¦¬ ë³€ìˆ˜ëŠ” ìˆ«ìí˜•ìœ¼ë¡œ ë³€í™˜í•´ì„œ í‘œê¸°í•´ì•¼ í•™ìŠµí•  ìˆ˜ ìˆë‹¤ | Label Encodingê³¼ One-hot Encoding ë°©ì‹ì´ ê°€ëŠ¥ | *Label Encoding: ê³ ì–‘ì´ â†’ 1, ê°•ì•„ì§€ â†’ 2, ë„ˆêµ¬ë¦¬ â†’ 3 ì´ëŸ°ì‹ìœ¼ë¡œ ê° ì¹´í…Œê³ ë¦¬ë¥¼ ìˆ«ì ê°’ìœ¼ë¡œ ë³€í™˜í•´ì£¼ëŠ” ë°©ì‹. | í•˜ì§€ë§Œ ì´ë ‡ê²Œ í•˜ë©´ ì‹¤ì œë¡œëŠ” í¬ê³  ì‘ìŒì´ ì—†ëŠ” ë°ì´í„°ì¸ë° ML ì•Œê³ ë¦¬ì¦˜ì—ì„œ 1 &lt; 2 ì´ëŸ° ì‹ìœ¼ë¡œ ìˆ«ìì˜ í¬ê³  ì‘ìŒì— ë”°ë¼ ì¤‘ìš”ë„ê°€ ì¡´ì¬í•˜ëŠ” ê²ƒìœ¼ë¡œ ì¸ì‹ë  ê°€ëŠ¥ì„±ì´ ìˆìœ¼ë¯€ë¡œ ì¢‹ì€ ë°©ë²•ì€ ì•„ë‹ˆë‹¤. | . | . One-hot Encoding . | Label Encodingì˜ ë¬¸ì œì ì„ í•´ê²°í•´ì£¼ëŠ” ì¸ì½”ë”© ë°©ì‹. | 0ê³¼ 1ë¡œ ì´ë£¨ì–´ì§„ ë²¡í„°ë¡œ ê° ë³€ìˆ˜ë¥¼ í‘œí˜„í•´ì¤€ë‹¤ | ex) ê³ ì–‘ì´ â†’ 1 0 0, ê°•ì•„ì§€ â†’ 0 1 0, ë„ˆêµ¬ë¦¬ â†’ 0 0 1 ì´ëŸ°ì‹ìœ¼ë¡œ ë‚˜íƒ€ëƒ„ | sklearn.preprocessingì˜ OneHotEncoderë¥¼ ì‚¬ìš©í•´ë„ ë˜ê³ , pandasì˜ get_dummies() í•¨ìˆ˜ë¥¼ ì´ìš©í•´ë„ ì‰½ê²Œ í™œìš©í•  ìˆ˜ ìˆë‹¤ | . (ì¶œì²˜: towardsdatascience.com) . Â  . import pandas as pd titanic_df = pd.read_csv('data/titanic.csv') ## ë°ì´í„° ì¶œì²˜: kaggle titanic_df.head() . | Â  | PassengerId | Survived | Pclass | Name | Sex | Age | SibSp | Parch | Ticket | Fare | Cabin | Embarked | . | 0 | 1 | 0 | 3 | Braund, Mr. Owen Harris | male | 22 | 1 | 0 | A/5 21171 | 7.25 | nan | S | . | 1 | 2 | 1 | 1 | Cumings, Mrs. John Bradley (Florence Briggs Thayer) | female | 38 | 1 | 0 | PC 17599 | 71.2833 | C85 | C | . | 2 | 3 | 1 | 3 | Heikkinen, Miss. Laina | female | 26 | 0 | 0 | STON/O2. 3101282 | 7.925 | nan | S | . | 3 | 4 | 1 | 1 | Futrelle, Mrs. Jacques Heath (Lily May Peel) | female | 35 | 1 | 0 | 113803 | 53.1 | C123 | S | . | 4 | 5 | 0 | 3 | Allen, Mr. William Henry | male | 35 | 0 | 0 | 373450 | 8.05 | nan | S | . Â  . â£1. ì‚¬ìš©í•  ì—´ë§Œ ë”°ë¡œ ê°€ì ¸ì™€ì„œ ë”ë¯¸ë³€ìˆ˜í™” . titanic_sex_embarked = titanic_df[['Sex', 'Embarked']] ## ì‚¬ìš©í•  ì—´ë§Œ ë”°ë¡œ ê°€ì ¸ì˜´ titanic_sex_embarked.head() . | Â  | Sex | Embarked | . | 0 | male | S | . | 1 | female | C | . | 2 | female | S | . | 3 | female | S | . | 4 | male | S | . Â  . ## pandasì˜ get_dummies í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ë©´ ì‰½ê²Œ ë”ë¯¸ë³€ìˆ˜ë¥¼ ë§Œë“¤ ìˆ˜ ìˆë‹¤ one_hot_encoded_df = pd.get_dummies(titanic_sex_embarked) one_hot_encoded_df.head() . | Â  | Sex_female | Sex_male | Embarked_C | Embarked_Q | Embarked_S | . | 0 | 0 | 1 | 0 | 0 | 1 | . | 1 | 1 | 0 | 1 | 0 | 0 | . | 2 | 1 | 0 | 0 | 0 | 1 | . | 3 | 1 | 0 | 0 | 0 | 1 | . | 4 | 0 | 1 | 0 | 0 | 1 | . Â  . â£2. one-hot encodingí•  ì—´ì„ ë”°ë¡œ ì €ì¥í•˜ì§€ ì•Šê³ , ê¸°ì¡´ dfì—ì„œ íŠ¹ì • ë¶€ë¶„ë§Œ encoding . # columns=[]ë¡œ ë”ë¯¸ë³€ìˆ˜í™”í•´ì¤„ ì—´ì„ ì •í•´ì£¼ë©´ ëœë‹¤ one_hot_encoded_df = pd.get_dummies(data=titanic_df, columns=['Sex', 'Embarked']) one_hot_encoded_df.head() . | Â  | PassengerId | Survived | Pclass | Name | Age | SibSp | Parch | Ticket | Fare | Cabin | Sex_female | Sex_male | Embarked_C | Embarked_Q | Embarked_S | . | 0 | 1 | 0 | 3 | Braund, Mr. Owen Harris | 22 | 1 | 0 | A/5 21171 | 7.25 | nan | 0 | 1 | 0 | 0 | 1 | . | 1 | 2 | 1 | 1 | Cumings, Mrs. John Bradley (Florence Briggs Thayer) | 38 | 1 | 0 | PC 17599 | 71.2833 | C85 | 1 | 0 | 1 | 0 | 0 | . | 2 | 3 | 1 | 3 | Heikkinen, Miss. Laina | 26 | 0 | 0 | STON/O2. 3101282 | 7.925 | nan | 1 | 0 | 0 | 0 | 1 | . | 3 | 4 | 1 | 1 | Futrelle, Mrs. Jacques Heath (Lily May Peel) | 35 | 1 | 0 | 113803 | 53.1 | C123 | 1 | 0 | 0 | 0 | 1 | . | 4 | 5 | 0 | 3 | Allen, Mr. William Henry | 35 | 0 | 0 | 373450 | 8.05 | nan | 0 | 1 | 0 | 0 | 1 | . ",
    "url": "https://chaelist.github.io/docs/ml_advanced/preprocessing/#%EC%B9%B4%ED%85%8C%EA%B3%A0%EB%A6%AC-%EB%B3%80%EC%88%98-%EC%9D%B8%EC%BD%94%EB%94%A9",
    "relUrl": "/docs/ml_advanced/preprocessing/#ì¹´í…Œê³ ë¦¬-ë³€ìˆ˜-ì¸ì½”ë”©"
  },"181": {
    "doc": "Python ê¸°ì´ˆ",
    "title": "Python ê¸°ì´ˆ",
    "content": " ",
    "url": "https://chaelist.github.io/docs/python_basics",
    "relUrl": "/docs/python_basics"
  },"182": {
    "doc": "Regular Expressions",
    "title": "Regular Expressions",
    "content": ". | ì •ê·œ í‘œí˜„ì‹(Regular Expressions) . | [ ]: ë¬¸ì í´ë˜ìŠ¤(character class) | ìì£¼ ì‚¬ìš©í•˜ëŠ” ë¬¸ì í´ë˜ìŠ¤ | ê° í‘œí˜„ì‹ì˜ ì˜ë¯¸ | . | re ëª¨ë“ˆ . | re.search() | re.findall() | re.sub() | re.compile() | . | ì¶”ê°€ í™œìš© Tip | . ",
    "url": "https://chaelist.github.io/docs/data_handling/regular_expressions/",
    "relUrl": "/docs/data_handling/regular_expressions/"
  },"183": {
    "doc": "Regular Expressions",
    "title": "ì •ê·œ í‘œí˜„ì‹(Regular Expressions)",
    "content": ": ë©”íƒ€ ë¬¸ì(meta characters)ë¥¼ ì ì ˆíˆ í™œìš©í•´ ë³µì¡í•œ ë¬¸ìì—´ì„ ì²˜ë¦¬í•˜ëŠ” ê¸°ë²• . | ë©”íƒ€ ë¬¸ì: . ^ $ * + ? { } [ ] \\ | ( ) | . [ ]: ë¬¸ì í´ë˜ìŠ¤(character class) . : â€[ ] ì‚¬ì´ì˜ ë¬¸ìë“¤ê³¼ ë§¤ì¹˜â€ë¼ëŠ” ì˜ë¯¸ . | [abc]: â€œa, b, c ì¤‘ í•œ ê°œì˜ ë¬¸ìì™€ ë§¤ì¹˜â€ë¼ëŠ” ì˜ë¯¸ . | â€œaâ€ëŠ” ì •ê·œì‹ê³¼ ì¼ì¹˜í•˜ëŠ” ë¬¸ìì¸ â€œaâ€ê°€ ìˆìœ¼ë¯€ë¡œ ë§¤ì¹˜ë˜ëŠ” ë¶€ë¶„ì´ ì¡´ì¬ | â€œbeforeâ€ëŠ” ì •ê·œì‹ê³¼ ì¼ì¹˜í•˜ëŠ” ë¬¸ìì¸ â€œbâ€ê°€ ìˆìœ¼ë¯€ë¡œ ë§¤ì¹˜ë˜ëŠ” ë¶€ë¶„ì´ ì¡´ì¬ | â€œdudeâ€ëŠ” ì •ê·œì‹ê³¼ ì¼ì¹˜í•˜ëŠ” ë¬¸ì â€œaâ€, â€œbâ€, â€œcâ€ ì¤‘ ì–´ëŠ í•˜ë‚˜ë„ í¬í•¨í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ë§¤ì¹˜ë˜ì§€ ì•ŠìŒ | . | [ ] ì•ˆì˜ ë‘ ë¬¸ì ì‚¬ì´ì— í•˜ì´í”ˆ(-)ì„ ì´ìš©í•˜ë©´ ë‘ ë¬¸ì ì‚¬ì´ì˜ ë²”ìœ„(from - to)ë¥¼ ì˜ë¯¸ . | [a-z]: a~zì˜ ëª¨ë“  ì•ŒíŒŒë²³ | [0-5]: 012345 | [a-zA-Z]: ì•ŒíŒŒë²³ ëª¨ë‘ | [0-9]: ìˆ«ì ëª¨ë‘ | . | [ ] ì•ˆì— ^ë¥¼ ë„£ìœ¼ë©´, ë°˜ëŒ€(not)ì˜ ì˜ë¯¸ê°€ ë¨ . | [^0-9]: ìˆ«ìê°€ ì•„ë‹Œ ë¬¸ìë§Œ ë§¤ì¹˜ë¨ | . | . ìì£¼ ì‚¬ìš©í•˜ëŠ” ë¬¸ì í´ë˜ìŠ¤ . : ì•„ë˜ì™€ ê°™ì´ ìì£¼ ì‚¬ìš©í•˜ëŠ” ì •ê·œì‹ì€ ë³„ë„ì˜ í‘œê¸°ë²•ìœ¼ë¡œ í‘œí˜„ì´ ê°€ëŠ¥í•˜ë‹¤ . | \\d - ìˆ«ìì™€ ë§¤ì¹˜. [0-9]ì™€ ë™ì¼ | \\D - ìˆ«ìê°€ ì•„ë‹Œ ê²ƒê³¼ ë§¤ì¹˜. [^0-9]ì™€ ë™ì¼ | \\s - whitespace characterì™€ ë§¤ì¹˜, [ \\t\\n\\r\\f\\v]ì™€ ë™ì¼ (ë§¨ ì•ì˜ ë¹ˆ ì¹¸ì€ ê³µë°±(space)ë¥¼ ì˜ë¯¸) | \\S - non-whitespace characterê³¼ ë§¤ì¹˜, [^ \\t\\n\\r\\f\\v]ì™€ ë™ì¼ | \\w - alphanumeric(ë¬¸ì+ìˆ«ì) &amp; underscore(_)ì™€ ë§¤ì¹˜, [a-zA-Z0-9_]ì™€ ë™ì¼ | \\W - alphanumeric(ë¬¸ì+ìˆ«ì) &amp; underscore(_)ê°€ ì•„ë‹Œ ê²ƒê³¼ ë§¤ì¹˜, [^a-zA-Z0-9_]ì™€ ë™ì¼ | . (â€» ê°ê°, ëŒ€ë¬¸ìë¡œ ì‚¬ìš©ëœ ê²ƒì€ ì†Œë¬¸ìì˜ ë°˜ëŒ€) . ê° í‘œí˜„ì‹ì˜ ì˜ë¯¸ . | Dot(.): ì¤„ë°”ê¿ˆ ë¬¸ìì¸ \\nì„ ì œì™¸í•œ ëª¨ë“  ë¬¸ìì™€ ë§¤ì¹˜ë¨ì„ ì˜ë¯¸ . | a.b: â€œa + ëª¨ë“  ë¬¸ì + bâ€ë¼ëŠ” ëœ» . | â€œaabâ€, â€œa0bâ€ ëª¨ë‘ a.bì™€ ë§¤ì¹˜ëœë‹¤ | â€œabcâ€ëŠ” aì™€ b ì‚¬ì´ì— ì–´ë–¤ ë¬¸ìë„ ì—†ìœ¼ë¯€ë¡œ ë§¤ì¹˜ë˜ì§€ ì•ŠëŠ”ë‹¤ | . | cf) a[.]b: â€œa + . + bâ€ë¼ëŠ” ëœ». â€œa.bâ€ì™€ë§Œ ë§¤ì¹˜ëœë‹¤ | . | ë°˜ë³µ(*): 0ë²ˆ ì´ìƒ ë°˜ë³µëœë‹¤ëŠ” ì˜ë¯¸ . | ca*t: * ë°”ë¡œ ì•ì˜ ë¬¸ì â€˜aâ€™ê°€ 0ë²ˆ ì´ìƒ ë°˜ë³µëœë‹¤ëŠ” ëœ» (aëŠ” ì—†ì–´ë„ ë¨) . | â€œctâ€, â€œcatâ€, â€œcaaatâ€ ëª¨ë‘ ca*tì™€ ë§¤ì¹˜ëœë‹¤ (0ë²ˆë„ ê°€ëŠ¥) | . | . | ë°˜ë³µ(+): 1ë²ˆ ì´ìƒ ë°˜ë³µëœë‹¤ëŠ” ì˜ë¯¸ . | ca+t: + ë°”ë¡œ ì•ì˜ ë¬¸ì â€˜aâ€™ê°€ 1ë²ˆ ì´ìƒ ë°˜ë³µëœë‹¤ëŠ” ëœ» (ë¬´ì¡°ê±´ aê°€ ìˆì–´ì•¼ í•¨) . | â€œcatâ€, â€œcaaatâ€ì€ ca*tì™€ ë§¤ì¹˜, í•˜ì§€ë§Œ â€œctâ€ì²˜ëŸ¼ aê°€ í•œ ë²ˆë„ ì—†ëŠ” ê²ƒì€ ë§¤ì¹˜ë˜ì§€ ì•ŠëŠ”ë‹¤ | . | . | {m,n}: m~në²ˆ ë°˜ë³µëœë‹¤ëŠ” ì˜ë¯¸ (ë°˜ë³µ íšŸìˆ˜ë¥¼ ì œí•œ) . | ca{2,5}t: {m,n} ë°”ë¡œ ì•ì˜ ë¬¸ì aê°€ 2ë²ˆ~5ë²ˆ ë°˜ë³µëœë‹¤ëŠ” ëœ» . | â€œcaatâ€, â€œcaaatâ€, â€œcaaaatâ€, â€œcaaaaatâ€ ëª¨ë‘ ca{2,5}tì™€ ë§¤ì¹­ëœë‹¤ | . | . | ?: ìˆì–´ë„ ë˜ê³  ì—†ì–´ë„ ëœë‹¤ëŠ” ì˜ë¯¸ (0ë²ˆ or 1ë²ˆ ìˆìœ¼ë©´ ë¨) . | ab?c: â€œa + b(ìˆì–´ë„ ë˜ê³  ì—†ì–´ë„ ë¨) + câ€ë¼ëŠ” ëœ» . | â€œabcâ€, â€œacâ€ ëª¨ë‘ ë§¤ì¹˜ëœë‹¤ | â€œabbcâ€ ë“± bê°€ 2ë²ˆ ì´ìƒì´ë©´ ë§¤ì¹˜ë˜ì§€ ì•ŠëŠ”ë‹¤ | . | . | ^: Matches the beginning of a line . | ^Xë¼ê³  í•˜ë©´ Xë¡œ ì‹œì‘í•˜ëŠ” ì•„ì´ë“¤ë§Œ ë§¤ì¹˜ë¨ . | â€œXylophoneâ€ì€ ^Xì™€ ë§¤ì¹­ëœë‹¤ (case sensitiveí•˜ë¯€ë¡œ ë°˜ë“œì‹œ ëŒ€ë¬¸ì Xì—¬ì•¼ ë§¤ì¹­) | . | . | $: $: Matches the end of a line . | X$ë¼ê³  í•˜ë©´ Xë¡œ ëë‚˜ëŠ” ì•„ì´ë“¤ë§Œ ë§¤ì¹˜ë¨ . | â€œAdEXâ€ëŠ” X$ì™€ ë§¤ì¹­ëœë‹¤ | . | . | ( ): Indicates where string extraction is to start &amp; to end . | ^From (\\S+@\\S+)ë¼ê³  í•˜ë©´, â€˜Fromâ€™ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” \\S+@\\S+ë“¤ì„ ì°¾ì•„ì£¼ë˜, ( ) ì•ˆì˜ ë¶€ë¶„ë§Œ extract. | â€œFrom chaelist@github.comâ€ì´ë¼ëŠ” stringì´ ìˆë‹¤ë©´, ì´ ì¤‘ â€œchaelist@github.comâ€ë§Œ extract. | . | . | . ",
    "url": "https://chaelist.github.io/docs/data_handling/regular_expressions/#%EC%A0%95%EA%B7%9C-%ED%91%9C%ED%98%84%EC%8B%9Dregular-expressions",
    "relUrl": "/docs/data_handling/regular_expressions/#ì •ê·œ-í‘œí˜„ì‹regular-expressions"
  },"184": {
    "doc": "Regular Expressions",
    "title": "re ëª¨ë“ˆ",
    "content": ": íŒŒì´ì¬ì—ì„œ ì •ê·œ í‘œí˜„ì‹ì„ ì§€ì›í•˜ëŠ” re(regular expression) ëª¨ë“ˆ . re.search() . : íŠ¹ì • stringì´ í•´ë‹¹ regular expressionê³¼ ë§¤ì¹­ë˜ëŠ”ì§€ ì—¬ë¶€ì— ë”°ë¼ True/Falseë¥¼ return . | ifë¬¸ê³¼ í•¨ê»˜ ì‚¬ìš©í•´ì„œ ì›í•˜ëŠ” stringë“¤ë§Œ ì½ì–´ì˜¬ ë•Œ ì‚¬ìš©í•˜ë©´ ìœ ìš©í•¨ | . import re # importí•´ì¤˜ì•¼ ì‚¬ìš© ê°€ëŠ¥ lines = ['From chaelist@github.com', 'hi', 'From me', 'nice to meet you', 'From CYC'] for line in lines: if re.search('^From', line): ## if line.starswith('From'): ê³¼ ë™ì¼ print(line) . From chaelist@github.com From me From CYC . re.findall() . : í•´ë‹¹ regular expressionì— ë§¤ì¹­ë˜ëŠ” portions of stringì„ extract. | ë§¤ì¹˜ë˜ëŠ” ë¶€ë¶„ì„ ëª¨ë‘ ë¦¬ìŠ¤íŠ¸ë¡œ ì €ì¥í•´ return | . (â€» re.search() returns a True/False, re.findall() returns the matching strings) . x = 'My 2 favorite numbers are 19 and 42' y = re.findall('[0-9]+', x) # ìˆ«ì(0-9)ê°€ 1ë²ˆ ì´ìƒ ë°˜ë³µë˜ëŠ” ë¶€ë¶„ì„ ëª¨ë‘ ì°¾ìœ¼ë¯€ë¡œ, [2, 19, 42]ê°€ returnë¨ print(y) z = re.findall('[AEIOU]+', x) # A, E, I, O, U ì¤‘ í•˜ë‚˜ê°€ 1ë²ˆ ì´ìƒ ë°˜ë³µë˜ëŠ” ë¶€ë¶„ì„ ì°¾ëŠ”ë°, ì—†ìœ¼ë¯€ë¡œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ return print(z) . ['2', '19', '42'] [] . re.sub() . : íŒ¨í„´ì— ì¼ì¹˜ë˜ëŠ” ë¬¸ìì—´ì„ ë‹¤ë¥¸ ë¬¸ìì—´ë¡œ ë°”ê¿”ì¤€ë‹¤ . | re.sub(pattern, repl, string)ì˜ í˜•íƒœë¡œ ì‚¬ìš© | . # re.sub() ê°„ë‹¨í•œ ë²„ì „ print(re.sub('\\d{4}', 'XXXX', '010-1256-9999')) . 010-XXXX-XXXX . +) count ì¶”ê°€: . # re.sub(pattern, repl, string, count) print(re.sub(pattern='Hello', repl='Bye', count=2, string='Hello, Hello, Hello Everybody.')) ## ì¼ì¹˜í•˜ëŠ” ë¬¸ìì—´ì´ 3ê°œì´ì§€ë§Œ count=2ë¡œ í•œì •ë˜ì–´ ìˆìœ¼ë©´ ë”± 2ê°œê¹Œì§€ë§Œ ë³€í™˜ë¨ . Bye, Bye, Hello Everybody. +) re.subn(): re.sub()ì™€ ë§¤ìš° ìœ ì‚¬í•˜ì§€ë§Œ, ì¹˜í™˜ëœ ë¬¸ìì—´ê³¼ í•¨ê»˜ ì¹˜í™˜ëœ ê°œìˆ˜ë„ returní•´ì¤€ë‹¤ . print(re.subn('\\d{4}', 'XXXX', '010-1234-5678')) ## ì¹˜í™˜ ê²°ê³¼ì™€ ì¹˜í™˜ëœ ê°œìˆ˜ë¥¼ elementë¡œ í•˜ëŠ” tuple ë°˜í™˜ . ('010-XXXX-XXXX', 2) . re.compile() . : íŠ¹ì • ì •ê·œí‘œí˜„ì‹ì„ ì»´íŒŒì¼í•´ë‘ê³  ì‚¬ìš©í•˜ëŠ” ë°©ì‹ . x = 'My 2 favorite numbers are 19 and 42' y = re.findall('[0-9]+', x) print(y) p = re.compile('[0-9]+') # ì´ë ‡ê²Œ re.compileì„ í†µí•´ íŠ¹ì • ì •ê·œí‘œí˜„ì‹ì„ ì €ì¥í•´ë‘ê³ , m = p.findall(x) # ì—¬ê¸°ì— .findall('ë¬¸ìì—´')ì„ í•´ì¤˜ë„ re.findall('ì •ê·œí‘œí˜„ì‹', 'ë¬¸ìì—´')ê³¼ ë™ì¼. print(m) . ['2', '19', '42'] ['2', '19', '42'] . â€» ê²°ë¡ : ì•„ë˜ 1ë²ˆê³¼ 2ë²ˆì€ ì™„ì „íˆ ë™ì¼í•œ ê²°ê³¼ë¥¼ ë‚¸ë‹¤ . # 1ë²ˆ result = re.findall('ì •ê·œí‘œí˜„ì‹', 'ë¬¸ìì—´') # 2ë²ˆ p = re.compile('ì •ê·œí‘œí˜„ì‹') result = p.findall('ë¬¸ìì—´') . ",
    "url": "https://chaelist.github.io/docs/data_handling/regular_expressions/#re-%EB%AA%A8%EB%93%88",
    "relUrl": "/docs/data_handling/regular_expressions/#re-ëª¨ë“ˆ"
  },"185": {
    "doc": "Regular Expressions",
    "title": "ì¶”ê°€ í™œìš© Tip",
    "content": ". | ( ) ì‚¬ìš©í•´ì„œ ë” preciseí•˜ê²Œ ì°¾ê¸° x = 'From stephan.marquard@uct.ac.za Sat Jan 5 09:14:26 2008' y = re.findall('^From (\\S+@\\S+)', x) ## Fromìœ¼ë¡œ ì‹œì‘(^) &amp; space &amp; 1ê°œ ì´ìƒ(+)ì˜ Non-whitespace character(\\S) &amp; @ &amp; 1ê°œ ì´ìƒ(+)ì˜ Non-whitespace character(\\S) ## ì´ ì¤‘, () ì•ˆì— ìˆëŠ” ë¶€ë¶„ë§Œ ì¶”ì¶œí•œë‹¤. ('From 'ëŠ” ì¶”ì¶œX) print(y) . ['stephan.marquard@uct.ac.za'] . | ì£¼ì˜í•  ì : Greedy Matching . | The repeat characters (* and +) push outward in both directions (greedy) to match the largest possible string (*ë‚˜ + ê°™ì€ â€˜ë°˜ë³µâ€™ ë¬¸ìë¥¼ ì“°ë©´, ê°€ëŠ¥í•œ ê°€ì¥ í¬ê²Œ ë§¤ì¹­ë˜ë ¤ëŠ” ê²½í–¥ì´ ìˆë‹¤) | +?, *? ì´ëŸ° ì‹ìœ¼ë¡œ ?ë¥¼ ë’¤ì— ë¶™ì—¬ì£¼ë©´ non-greedy matchingì´ ê°€ëŠ¥ | . *Greedy Matching ì˜ˆì‹œ . x = 'From: Using the : character' y = re.findall('^F.+:', x) print(y) # From:ë„ ë§¤ì¹­ë˜ê³ , ì´ë¥¼ í¬í•¨í•˜ëŠ” From: Using the :ë„ ë§¤ì¹­ë  ë•Œ, ë²”ìœ„ê°€ ë” í° í›„ìë¡œ ë§¤ì¹­ë˜ê²Œ ë¨. ['From: Using the :'] . *Non-Greedy Matching ì˜ˆì‹œ . x = 'From: Using the : character' y = re.findall('^F.+?:', x) print(y) # +?ë¡œ í•´ì£¼ë©´ non-greedyí•œ ë§¤ì¹­ë°©ì‹ì´ë¯€ë¡œ, ê°€ì¥ ë¹¨ë¦¬ ì°¾ì•„ì§€ëŠ” From:ì„ returní•˜ê³  ëë‚´ê²Œ ë¨ . ['From:'] . | Escape Character (ì˜ˆì™¸ ë¬¸ì) . | ì •ê·œì‹ì—ì„œ ì“°ëŠ” ë¬¸ìì¸ë° ê·¸ëƒ¥ ì‹¤ì œ ê·¸ ëª¨ì–‘ ê·¸ëŒ€ë¡œì˜ ì˜ë¯¸ë¥¼ ë‹´ê³  ì‹¶ìœ¼ë©´, \\ë¥¼ ì•ì— ë¶™ì—¬ì£¼ë©´ ëœë‹¤ | ex) $(dollar sign) ê·¸ ìì²´ë¥¼ ì¨ì•¼ í•œë‹¤ë©´, \\$ ì´ë ‡ê²Œ í‘œì‹œ. | . x = 'We just received $10.00 for cookies.' y = re.findall('\\$[0-9]+', x) # \\$: ì‹¤ì œ $ ì‚¬ì¸ê³¼ ë§¤ì¹­ë˜ëŠ” ê²ƒì„ ì°¾ìœ¼ë¼ëŠ” ì˜ë¯¸ print(y) . ['$10'] . | ì •ê·œí‘œí˜„ì‹ì„ í…ŒìŠ¤íŠ¸í•  ìˆ˜ ìˆëŠ” ì‚¬ì´íŠ¸: https://regexr.com/ | . ",
    "url": "https://chaelist.github.io/docs/data_handling/regular_expressions/#%EC%B6%94%EA%B0%80-%ED%99%9C%EC%9A%A9-tip",
    "relUrl": "/docs/data_handling/regular_expressions/#ì¶”ê°€-í™œìš©-tip"
  },"186": {
    "doc": "Regularization",
    "title": "Regularization",
    "content": ". | í¸í–¥ê³¼ ë¶„ì‚° . | ê³¼ì í•©ê³¼ ê³¼ì†Œì í•© | Overfitting ë°©ì§€í•˜ê¸° | . | Overfitting ë¬¸ì œ ì²´í—˜ . | 6ì°¨í•­ ëª¨ë¸ ì¤€ë¹„ | í•™ìŠµ &amp; ê²°ê³¼ í™•ì¸ | . | Regularization (ê°€ì¤‘ì¹˜ ê·œì œ) . | L1 Regularization | L2 Regularization | Lasso Regression êµ¬í˜„ | . | . ",
    "url": "https://chaelist.github.io/docs/ml_advanced/regularization/",
    "relUrl": "/docs/ml_advanced/regularization/"
  },"187": {
    "doc": "Regularization",
    "title": "í¸í–¥ê³¼ ë¶„ì‚°",
    "content": ". | í¸í–¥(Bias) . | ëª¨ë¸ì´ ë„ˆë¬´ ê°„ë‹¨í•´ì„œ ë°ì´í„°ì˜ ê´€ê³„ë¥¼ ì˜ í•™ìŠµí•˜ì§€ ëª»í•˜ëŠ” ê²½ìš°, ëª¨ë¸ì˜ í¸í–¥(bias)ì´ ë†’ë‹¤ê³  í•¨ | ë†’ì€ ì°¨í•­ì˜ íšŒê·€ë¡œ training ë°ì´í„°ì— ê±°ì˜ ì™„ë²½íˆ ë§ì¶˜ ëª¨ë¸ì€ í¸í–¥ì´ ë‚®ì€ ëª¨ë¸ì´ë¼ê³  í•  ìˆ˜ ìˆìŒ | . | ë¶„ì‚°(Variance) . | ëª¨ë¸ì´ ì–¼ë§ˆë‚˜ ì¼ê´€ëœ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ëŠ”ì§€ë¥¼ ë¶„ì‚°(variance)ë¼ê³  í•¨ | ë‹¤ì–‘í•œ ë°ì´í„°ì…‹ ê°„ì— ì„±ëŠ¥ ì°¨ì´ê°€ ë§ì´ ë‚˜ë©´ ë¶„ì‚°ì´ ë†’ë‹¤ê³  í•˜ê³ , ì„±ëŠ¥ ì°¨ì´ê°€ ë³„ë¡œ ì—†ìœ¼ë©´ ë¶„ì‚°ì´ ë‚®ë‹¤ê³  í•¨ | ì§ì„  ëª¨ë¸ì€ ì–´ë–¤ ë°ì´í„°ì…‹ì— ì ìš©í•´ë„ ì„±ëŠ¥ì´ ë¹„ìŠ·í•˜ê²Œ ë‚˜ì˜´ (= ë¶„ì‚°ì´ ì‘ìŒ) vs ë³µì¡í•œ ê³¡ì„  ëª¨ë¸ì€ ë°ì´í„°ì…‹ì— ë”°ë¼ ì„±ëŠ¥ì˜ í¸ì°¨ê°€ í¼ (= ë¶„ì‚°ì´ í¼) | . | . ê³¼ì í•©ê³¼ ê³¼ì†Œì í•© . | ê³¼ì†Œì í•©(Underfit) . | í¸í–¥ì´ ë†’ê³  ë¶„ì‚°ì´ ë‚®ì€ ëª¨ë¸ì„ underfitë˜ì—ˆë‹¤ê³  í•¨ | ë„ˆë¬´ ë‹¨ìˆœí•œ ëª¨ë¸ì´ë¼ ê´€ê³„ë¥¼ ì œëŒ€ë¡œ í•™ìŠµí•˜ì§€ ëª»í•˜ëŠ” ê²½ìš°. | ëŒ€ì‹  ëª¨ë¸ì´ ê°„ë‹¨í•˜ê¸°ì— ì–´ë–¤ ë°ì´í„°ì— ì ìš©í•´ë„ ì¼ê´€ëœ ì„±ëŠ¥ì„ ë³´ì„ | . | ê³¼ì í•©(Overfit) . | í¸í–¥ì´ ë‚®ê³  ë¶„ì‚°ì´ ë†’ì€ ëª¨ë¸ì„ overfitë˜ì—ˆë‹¤ê³  í•¨ | training ë°ì´í„°ì˜ íŒ¨í„´ì„ í•™ìŠµí•˜ëŠ” ê²Œ ì•„ë‹ˆë¼ ê±°ì˜ ë°ì´í„° ìì²´ë¥¼ ì™¸ì›Œë²„ë¦¬ëŠ” ìˆ˜ì¤€ìœ¼ë¡œ ëª¨ë¸ì„ training ë°ì´í„°ì— ê±°ì˜ ì™„ë²½íˆ ë§ì¶”ëŠ” ê²½ìš°. | training ë°ì´í„°ì— ëŒ€í•œ ì„±ëŠ¥ì€ ì•„ì£¼ ë†’ì§€ë§Œ, ì²˜ìŒ ë³´ëŠ” test ë°ì´í„°ì— ëŒ€í•œ ì„±ëŠ¥ì€ ë§ì´ ë–¨ì–´ì§„ë‹¤ | . | . *í¸í–¥-ë¶„ì‚° íŠ¸ë ˆì´ë“œì˜¤í”„(Bias-Variance Tradeoff) : ì¼ë°˜ì ìœ¼ë¡œ, í¸í–¥ê³¼ ë¶„ì‚°ì€ í•˜ë‚˜ê°€ ì¤„ì–´ë“¤ë©´ ë‹¤ë¥¸ í•˜ë‚˜ëŠ” ëŠ˜ì–´ë‚˜ëŠ” tradeoff ê´€ê³„ â€» ê³¼ì†Œì í•©ê³¼ ê³¼ì í•© ì‚¬ì´ì˜ ì ë‹¹í•œ ë°¸ëŸ°ìŠ¤ë¥¼ ì°¾ì•„ë‚´ëŠ” ê²Œ ì¤‘ìš”í•˜ë‹¤! . Overfitting ë°©ì§€í•˜ê¸° . | ë…ë¦½ë³€ìˆ˜ ì¶”ê°€/ì œê±°: ê´€ê³„ê°€ ìˆëŠ” ì• ë“¤ì€ ì¶”ê°€, ê´€ê³„ê°€ ì—†ëŠ” ì• ë“¤ì€ ì œê±° . | ë³´í†µ, í•™ìŠµë°ì´í„°ê°€ ì•„ì£¼ ë§ì„ìˆ˜ë¡ ëª¨ë¸ì˜ ì„¤ëª…ë ¥ì´ ì¢‹ì•„ì§. í•˜ì§€ë§Œ yì™€ ìƒê´€ì—†ëŠ” ë…ë¦½ë³€ìˆ˜ê°€ ë§ì´ ë“¤ì–´ìˆìœ¼ë©´ ìƒˆë¡œìš´ dataë¥¼ ì˜ ì„¤ëª…í•˜ëŠ” ë° ë„ì›€ì´ ì•ˆë¨ | . | ë…ë¦½ë³€ìˆ˜ì™€ ì¢…ì†ë³€ìˆ˜ì˜ ê´€ê³„ë¥¼ ì˜ íŒŒì•… (ë¹„ì„ í˜•ê´€ê³„ ë“±ì„ ì˜ ê³ ë ¤í•´ì„œ ë°˜ì˜) | parameterì— penalty ì£¼ê¸° = Regularization | . ",
    "url": "https://chaelist.github.io/docs/ml_advanced/regularization/#%ED%8E%B8%ED%96%A5%EA%B3%BC-%EB%B6%84%EC%82%B0",
    "relUrl": "/docs/ml_advanced/regularization/#í¸í–¥ê³¼-ë¶„ì‚°"
  },"188": {
    "doc": "Regularization",
    "title": "Overfitting ë¬¸ì œ ì²´í—˜",
    "content": ": 6ì°¨í•­ì˜ ë³µì¡í•œ ëª¨ë¸ì„ í™œìš©í•´ Overfitting ë¬¸ì œë¥¼ ì²´í—˜í•´ë³¸ í›„, ì•„ë˜ì—ì„œ ê°™ì€ ì…ë ¥ë³€ìˆ˜ì— Regularizationì„ ì ìš©í•´ ê²°ê³¼ë¥¼ ë¹„êµí•´ë³¼ ì˜ˆì •. from sklearn.linear_model import LinearRegression from sklearn.model_selection import train_test_split from sklearn.metrics import mean_squared_error from sklearn.preprocessing import PolynomialFeatures from sklearn.datasets import load_boston # boston ì§‘ê°’ ë°ì´í„° from math import sqrt import numpy as np import pandas as pd . 6ì°¨í•­ ëª¨ë¸ ì¤€ë¹„ . boston_dataset = load_boston() X = pd.DataFrame(boston_dataset.data, columns=boston_dataset.feature_names) # ì…ë ¥ë³€ìˆ˜ë¥¼ 6ì°¨í•­ìœ¼ë¡œ ë°”ê¾¸ê¸° polynomial_transformer = PolynomialFeatures(6) # 6ì°¨í•­ ë³€í™˜ê¸° ì¤€ë¹„ polynomial_features = polynomial_transformer.fit_transform(X.values) features = polynomial_transformer.get_feature_names(X.columns) # ë°”ê¾¼ ê°’ë“¤ì„ ë‹¤ì‹œ Xë¡œ ì €ì¥ X = pd.DataFrame(polynomial_features, columns=features) X.head() . | Â  | 1 | CRIM | ZN | INDUS | CHAS | NOX | RM | AGE | DIS | RAD | â€¦ | B^4 LSTAT^2 | B^3 LSTAT^3 | B^2 LSTAT^4 | B LSTAT^5 | LSTAT^6 | . | 0 | 1 | 0.00632 | 18 | 2.31 | 0 | 0.538 | 6.575 | 65.2 | 4.09 | 1 | â€¦ | 6.15436e+11 | 7.72203e+09 | 9.68901e+07 | 1.2157e+06 | 15253.7 | . | 1 | 1 | 0.02731 | 0 | 7.07 | 0 | 0.469 | 6.421 | 78.9 | 4.9671 | 2 | â€¦ | 2.07308e+12 | 4.77399e+10 | 1.09938e+09 | 2.5317e+07 | 583012 | . | 2 | 1 | 0.02729 | 0 | 7.07 | 0 | 0.469 | 7.185 | 61.1 | 4.9671 | 2 | â€¦ | 3.86749e+11 | 3.96761e+09 | 4.07033e+07 | 417571 | 4283.81 | . | 3 | 1 | 0.03237 | 0 | 2.18 | 0 | 0.458 | 6.998 | 45.8 | 6.0622 | 3 | â€¦ | 2.09631e+11 | 1.56175e+09 | 1.16351e+07 | 86681.6 | 645.779 | . | 4 | 1 | 0.06905 | 0 | 2.18 | 0 | 0.458 | 7.147 | 54.2 | 6.0622 | 3 | â€¦ | 7.04983e+11 | 9.46727e+09 | 1.27137e+08 | 1.70733e+06 | 22927.8 | . 5 rows Ã— 27132 columns . Â  . # ëª©í‘œë³€ìˆ˜ë„ dataframeìœ¼ë¡œ ì •ë¦¬ y = pd.DataFrame(boston_dataset.target, columns=['MEDV']) y.head() . | Â  | MEDV | . | 0 | 24 | . | 1 | 21.6 | . | 2 | 34.7 | . | 3 | 33.4 | . | 4 | 36.2 | . í•™ìŠµ &amp; ê²°ê³¼ í™•ì¸ . X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=5) model = LinearRegression() model.fit(X_train, y_train) . LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False) . â†’ training dataì™€ test dataì—ì„œì˜ ì˜ˆì¸¡ ì„±ëŠ¥ ë¹„êµ: . rsquare = model.score(X_train, y_train) print(f'training setì—ì„œì˜ rìŠ¤í€˜ì–´: {rsquare :.3f}') rsquare = model.score(X_test, y_test) print(f'test setì—ì„œì˜ rìŠ¤í€˜ì–´: {rsquare :.3f}') . training setì—ì„œì˜ rìŠ¤í€˜ì–´: 1.000 test setì—ì„œì˜ rìŠ¤í€˜ì–´: -28657.989 . | training setì—ì„œëŠ” 100%ì˜ ì„¤ëª…ë ¥ì„ ê°–ëŠ” ë°˜ë©´, test setì€ í•˜ë‚˜ë„ ì„¤ëª…ì„ í•˜ì§€ ëª»í•œë‹¤. ê³¼ì í•©ëœ ê²ƒ! | . y_train_predict = model.predict(X_train) y_test_predict = model.predict(X_test) mse = mean_squared_error(y_train, y_train_predict) print(f'training setì—ì„œì˜ rmse: {sqrt(mse) :.3f}') mse = mean_squared_error(y_test, y_test_predict) print(f'test setì—ì„œì˜ rmse: {sqrt(mse) :.3f}') . training setì—ì„œì˜ rmse: 0.000 test setì—ì„œì˜ rmse: 1650.789 . | training setì—ì„œëŠ” í‰ê· ì œê³±ê·¼ì˜¤ì°¨ê°€ 0ì¸ ë°˜ë©´, test setì—ì„œëŠ” 1650ì´ ë„˜ëŠ” í° ê°’ì´ ë‚˜ì˜¨ë‹¤ | . ",
    "url": "https://chaelist.github.io/docs/ml_advanced/regularization/#overfitting-%EB%AC%B8%EC%A0%9C-%EC%B2%B4%ED%97%98",
    "relUrl": "/docs/ml_advanced/regularization/#overfitting-ë¬¸ì œ-ì²´í—˜"
  },"189": {
    "doc": "Regularization",
    "title": "Regularization (ê°€ì¤‘ì¹˜ ê·œì œ)",
    "content": ": ëª¨ë¸ì„ í•™ìŠµì‹œí‚¬ ë•Œ Î¸ê°’(coefficient; ë³€ìˆ˜ë³„ ê°€ì¤‘ì¹˜)ë“¤ì´ ë„ˆë¬´ ì»¤ì§€ëŠ” ê²ƒì„ ë°©ì§€í•´ì£¼ëŠ” ë°©ë²• (penaltyë¥¼ ë¶€ì—¬í•˜ëŠ” ê°œë…) . | ë°ì´í„°ì— ëŒ€í•œ ì˜¤ì°¨ì™€ Î¸ê°’ ì¤‘ ì–´ëŠ ê±¸ ì¤„ì´ëŠ” ê²ƒì´ ë” ì¤‘ìš”í• ì§€ë¥¼ ìƒìˆ˜ Î»(lambda)ì— ë”°ë¼ ê²°ì • (Î»ê°€ í´ìˆ˜ë¡ Î¸ê°’ì— ëŒ€í•œ ì œí•œì´ ì»¤ì§) | . L1 Regularization . | L1: Lasso. ê° parameterì˜ ì ˆëŒ€ê°’ì„ ì‚¬ìš© | ì†ì‹¤ í•¨ìˆ˜ J = í‰ê· ì œê³±ì˜¤ì°¨ + Î»|Î¸1| + Î»|Î¸2| + â€¦ + Î»|Î¸n| | L1 Regularizationì„ ì‚¬ìš©í•˜ëŠ” íšŒê·€ ëª¨ë¸ì„ Lasso ëª¨ë¸ì´ë¼ê³  í•¨ | â€» L1 Regularizationì€ ì—¬ëŸ¬ Î¸ê°’ì„ ì•„ì˜ˆ 0ìœ¼ë¡œ ë§Œë“¤ì–´ì¤€ë‹¤. (ëª¨ë¸ì— ì¤‘ìš”í•˜ì§€ ì•Šë‹¤ê³  ìƒê°ë˜ëŠ” ì†ì„±ë“¤ì„ ì•„ì˜ˆ 0ìœ¼ë¡œ ë§Œë“¤ì–´ ì—†ì• ì£¼ëŠ” íš¨ê³¼) . | L1 Regularizationì€ ì–´ë–¤ ëª¨ë¸ì— ì“°ì´ëŠ” ì†ì„±(ë³€ìˆ˜)ì˜ ê°œìˆ˜ë¥¼ ì¤„ì´ê³  ì‹¶ì„ ë•Œ ì‚¬ìš©ëœë‹¤. | ex) ì†ì„± 20ê°œë¡œ 2ì°¨ ë‹¤ì¤‘ ë‹¤í•­ íšŒê·€ ëª¨ë¸ì„ ë§Œë“¤ë©´ ì†ì„±ì€ ì´ 230ê°œê°€ ë¨ â†’ ì†ì„±ì´ ì´ë ‡ê²Œ ë§ìœ¼ë©´ ê³¼ì í•© ê°€ëŠ¥ì„±ì´ ë†’ì„ ë¿ë§Œì´ ì•„ë‹ˆë¼ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¬ ë•Œ ë§ì€ ìì›(RAM, ì‹œê°„ ë“±)ì„ ì†Œëª¨ë¨ â†’ L1 Regularizationì„ ì‚¬ìš©í•˜ë©´ í•™ìŠµì— ì‚¬ìš©ë˜ëŠ” ì†ì„±ì˜ ìˆ˜ë¥¼ ë§ì´ ì¤„ì¼ ìˆ˜ ìˆë‹¤ | . | . L2 Regularization . | L2: Ridge. ê° parameterì˜ ì œê³±ê°’ì„ ì‚¬ìš© | ì†ì‹¤ í•¨ìˆ˜ J = í‰ê· ì œê³±ì˜¤ì°¨ + Î»Î¸12 + Î»Î¸22 + â€¦ + Î»Î¸n2 | L2 Regularizationì„ ì‚¬ìš©í•˜ëŠ” íšŒê·€ ëª¨ë¸ì„ Ridge ëª¨ë¸ì´ë¼ê³  í•¨ | â€» L1ê³¼ ë‹¬ë¦¬ L2 Regularizationì€ Î¸ê°’ì„ ì•„ì˜ˆ 0ìœ¼ë¡œ ë§Œë“¤ì§€ëŠ” ì•Šê³ , ì¡°ê¸ˆì”© ê°€ì¤‘ì¹˜ë¥¼ ì¤„ì—¬ì£¼ê¸°ë§Œ í•œë‹¤. | ì†ì„±ì˜ ê°œìˆ˜ë¥¼ ì¤„ì¼ í•„ìš”ëŠ” ì—†ë‹¤ê³  ìƒê°ë˜ë©´ L2 Regularizationì„ ì“°ë©´ ë¨ | . | . Â  . *L1, L2 Regularization ì‚¬ìš©í•˜ê¸°: (Regularizationì€ ì†ì‹¤í•¨ìˆ˜ë¥¼ ìµœì†Œí™”í•˜ëŠ” ëª¨ë“  ì•Œê³ ë¦¬ì¦˜ì— ì ìš©í•  ìˆ˜ ìˆìŒ) . | ë‹¤ì¤‘íšŒê·€/ë‹¤í•­íšŒê·€ ëª¨ë¸: Linear Regression ëŒ€ì‹  Lasso ë˜ëŠ” Ridge ëª¨ë¸ì„ ì‚¬ìš©í•˜ë©´ ë¨ | Logistic Regression ëª¨ë¸: L2 Regularizationë¥¼ ì ìš©í•˜ëŠ” ê²ƒì´ default. | â€˜penaltyâ€™ë¼ëŠ” ì˜µì…”ë„ íŒŒë¼ë¯¸í„°ë¥¼ í†µí•´ ì–´ë–¤ Regularization ê¸°ë²•ì„ ì‚¬ìš©í• ì§€ ì •í•´ì¤„ ìˆ˜ ìˆìŒ LogisticRegression(penalty='none') # Regularization ì‚¬ìš© ì•ˆí•¨ LogisticRegression(penalty='l1') # L1 Regularization ì‚¬ìš© LogisticRegression(penalty='l2') # L2 Regularization ì‚¬ìš© LogisticRegression() # ìœ„ì™€ ë˜‘ê°™ìŒ: L2 Regularization ì‚¬ìš© . | . | . +) Deep Learningì—ì„œë„ regularizationì´ ì¤‘ìš”. Lasso Regression êµ¬í˜„ . from sklearn.linear_model import Lasso # ì•„ì˜ˆ Lasso Regression ëª¨ë¸ì´ ë”°ë¡œ ì œê³µëœë‹¤ from sklearn.model_selection import train_test_split from sklearn.metrics import mean_squared_error from math import sqrt import numpy as np import pandas as pd . *ìœ„ì˜ ê³¼ì í•© ë¬¸ì œ ì²´í—˜ì—ì„œ ì‚¬ìš©í•œ X, y ê·¸ëŒ€ë¡œ ì‚¬ìš© . X.head() . | Â  | 1 | CRIM | ZN | INDUS | CHAS | NOX | RM | AGE | DIS | RAD | â€¦ | B^4 LSTAT^2 | B^3 LSTAT^3 | B^2 LSTAT^4 | B LSTAT^5 | LSTAT^6 | . | 0 | 1 | 0.00632 | 18 | 2.31 | 0 | 0.538 | 6.575 | 65.2 | 4.09 | 1 | â€¦ | 6.15436e+11 | 7.72203e+09 | 9.68901e+07 | 1.2157e+06 | 15253.7 | . | 1 | 1 | 0.02731 | 0 | 7.07 | 0 | 0.469 | 6.421 | 78.9 | 4.9671 | 2 | â€¦ | 2.07308e+12 | 4.77399e+10 | 1.09938e+09 | 2.5317e+07 | 583012 | . | 2 | 1 | 0.02729 | 0 | 7.07 | 0 | 0.469 | 7.185 | 61.1 | 4.9671 | 2 | â€¦ | 3.86749e+11 | 3.96761e+09 | 4.07033e+07 | 417571 | 4283.81 | . | 3 | 1 | 0.03237 | 0 | 2.18 | 0 | 0.458 | 6.998 | 45.8 | 6.0622 | 3 | â€¦ | 2.09631e+11 | 1.56175e+09 | 1.16351e+07 | 86681.6 | 645.779 | . | 4 | 1 | 0.06905 | 0 | 2.18 | 0 | 0.458 | 7.147 | 54.2 | 6.0622 | 3 | â€¦ | 7.04983e+11 | 9.46727e+09 | 1.27137e+08 | 1.70733e+06 | 22927.8 | . 5 rows Ã— 27132 columns . Â  . y.head() . | Â  | MEDV | . | 0 | 24 | . | 1 | 21.6 | . | 2 | 34.7 | . | 3 | 33.4 | . | 4 | 36.2 | . Â  . X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=5) model = Lasso(alpha=0.001, max_iter=1000, normalize=True) model.fit(X_train, y_train) . Lasso(alpha=0.001, copy_X=True, fit_intercept=True, max_iter=1000, normalize=True, positive=False, precompute=False, random_state=None, selection='cyclic', tol=0.0001, warm_start=False) . | alphaê°€ Î»(lambda)ê°’ì„ ê²°ì •í•´ì£¼ëŠ” íŒŒë¼ë¯¸í„° | Lasso ëª¨ë¸ì€ ê²½ì‚¬í•˜ê°•ë²•ì„ ì‚¬ìš© â†’ max_iterë¡œ ê²½ì‚¬í•˜ê°•ë²•ì„ ìµœëŒ€ ëª‡ ë²ˆ í•  ì§€ ì„¤ì • | normalize=Trueë¡œ ì„¤ì •í•˜ë©´ ë°ì´í„°ë¥¼ 0~1 ì‚¬ì´ì˜ ê°’ìœ¼ë¡œ normalizeí•´ì¤€ë‹¤ | +) Lasso ë§ê³  Ridge ëª¨ë¸ì„ ì“°ê³  ì‹¶ë‹¤ë©´, Lasso ëŒ€ì‹  Ridgeë¥¼ importí•´ì„œ ì‚¬ìš©í•´ì£¼ë©´ ë¨ | . Â  . â†’ training dataì™€ test dataì—ì„œì˜ ì˜ˆì¸¡ ì„±ëŠ¥ ë¹„êµ: . rsquare = model.score(X_train, y_train) print(f'training setì—ì„œì˜ rìŠ¤í€˜ì–´: {rsquare :.3f}') rsquare = model.score(X_test, y_test) print(f'test setì—ì„œì˜ rìŠ¤í€˜ì–´: {rsquare :.3f}') . training setì—ì„œì˜ rìŠ¤í€˜ì–´: 0.945 test setì—ì„œì˜ rìŠ¤í€˜ì–´: 0.902 . y_train_predict = model.predict(X_train) y_test_predict = model.predict(X_test) mse = mean_squared_error(y_train, y_train_predict) print(f'training setì—ì„œì˜ rmse: {sqrt(mse) :.3f}') mse = mean_squared_error(y_test, y_test_predict) print(f'test setì—ì„œì˜ rmse: {sqrt(mse) :.3f}') . training setì—ì„œì˜ rmse: 2.099 test setì—ì„œì˜ rmse: 3.058 . | Regularizationìœ¼ë¡œ overfittingì„ ë°©ì§€í•´ì£¼ë‹ˆ, training setê³¼ test setì—ì„œ rìŠ¤í€˜ì–´ê°’ê³¼ í‰ê· ì œê³±ê·¼ì˜¤ì°¨ê°€ ë¹„ìŠ·í•˜ê²Œ ë§ì¶°ì§! | . ",
    "url": "https://chaelist.github.io/docs/ml_advanced/regularization/#regularization-%EA%B0%80%EC%A4%91%EC%B9%98-%EA%B7%9C%EC%A0%9C",
    "relUrl": "/docs/ml_advanced/regularization/#regularization-ê°€ì¤‘ì¹˜-ê·œì œ"
  },"190": {
    "doc": "Requests & BeautifulSoup",
    "title": "Requests &amp; BeautifulSoup",
    "content": ". | Requests . | Proxy ì‚¬ìš© | Header ì‚¬ìš© | . | BeautifulSoup . | ì§ì ‘ tag ì´ë¦„ í˜¸ì¶œ | find()ì™€ find_all() | tag ë‚´ element ì¶”ì¶œ | Nested Tags ì ‘ê·¼ | re.compile | . | BeautifulSoup: select() . | CSS ì„ íƒìë¡œ ì ‘ê·¼ | CSS ì„ íƒì ì¡°í•©í•˜ì—¬ ì‚¬ìš© | . | . ",
    "url": "https://chaelist.github.io/docs/webscraping/requests_beautifulsoup/#requests--beautifulsoup",
    "relUrl": "/docs/webscraping/requests_beautifulsoup/#requests--beautifulsoup"
  },"191": {
    "doc": "Requests & BeautifulSoup",
    "title": "Requests",
    "content": "*ì—­í• : web communication. ì„œë²„ì—ì„œ (raw) data (=source codes)ë¥¼ ë°›ì•„ì˜¨ë‹¤. import requests # importí•´ì„œ ì‚¬ìš© url = 'https://www.imdb.com/title/tt0805647/?ref_=vp_vi_tt' r = requests.get(url) ## sends request message &amp; recieves source codes &amp; returns variables print(r.text) ## .textë¡œ ê°„ë‹¨íˆ ë¶ˆëŸ¬ì˜¨ source code ì¶œë ¥ . &lt;!DOCTYPE html&gt; &lt;html xmlns:og=\"http://ogp.me/ns#\" xmlns:fb=\"http://www.facebook.com/2008/fbml\"&gt; &lt;head&gt; &lt;meta charset=\"utf-8\"&gt; &lt;meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\"&gt; &lt;meta name=\"apple-itunes-app\" content=\"app-id=342792525, app-argument=imdb:///title/tt0805647?src=mdot\"&gt; (ìƒëµ) . +) headerë„ ë³„ë„ë¡œ ê°€ì ¸ì˜¬ ìˆ˜ ìˆìŒ . r.headers . {'Content-Type': 'text/html;charset=UTF-8', 'Transfer-Encoding': 'chunked', 'Connection': 'keep-alive', 'Server': 'Server', 'Date': 'Mon, 21 Dec 2020 12:27:34 GMT', 'x-amz-rid': 'H9QPTE15DWZ8GAJ3F3ZG', 'Set-Cookie': 'uu=BCYhjgIkRCdAZD3iSQrRQiy0YbMFhLNwk8zxrf0rHF5X3OWIKROLTsDOoCR5TLd5OyQ0OMF32iae%0D%0A1Y0pLcflXX3sTAoF6tCuyAmbPBhAOsNT3-PBrLCEP09Zd7kJEDY89VtMKo_UdYkrDrA6PrC082Jb%0D%0AzA%0D%0A; Domain=.imdb.com; Expires=Sat, 08-Jan-2089 15:41:41 GMT; Path=/; Secure, session-id=000-0000000-0000000; Domain=.imdb.com; Expires=Sat, 08-Jan-2089 15:41:41 GMT; Path=/; Secure, session-id-time=2239273654; Domain=.imdb.com; Expires=Sat, 08-Jan-2089 15:41:41 GMT; Path=/; Secure', 'X-Frame-Options': 'SAMEORIGIN', 'Content-Security-Policy': \"frame-ancestors 'self' imdb.com *.imdb.com *.media-imdb.com withoutabox.com *.withoutabox.com amazon.com *.amazon.com amazon.co.uk *.amazon.co.uk amazon.de *.amazon.de translate.google.com images.google.com www.google.com www.google.co.uk search.aol.com bing.com www.bing.com\", 'Content-Language': 'en-US', 'Strict-Transport-Security': 'max-age=47474747; includeSubDomains; preload', 'Vary': 'Content-Type,Accept-Encoding,X-Amzn-CDN-Cache,X-Amzn-AX-Treatment,User-Agent', 'X-Cache': 'Miss from cloudfront', 'Via': '1.1 214d8a3cdb14de6b0331d1f72902cc67.cloudfront.net (CloudFront)', 'X-Amz-Cf-Pop': 'HKG60-C1', 'X-Amz-Cf-Id': '-_bmiycwCogkwse1lduiRQkvFpgQb2evcKf0DlKRZKSiJwYrPimtfw=='} . Proxy ì‚¬ìš© . : proxyë¥¼ ì‚¬ìš©í•˜ë©´ êµ­ê°€ë¥¼ ìš°íšŒí•˜ì—¬ ì ‘ì†í•  ìˆ˜ ìˆë‹¤ (íŠ¹ì • êµ­ê°€ì—ì„œë§Œ ì ‘ì† ê°€ëŠ¥í•œ ì‚¬ì´íŠ¸ì— ì ‘ì†í•˜ê¸°) . # Proxy ì‚¬ìš© ì˜ˆì‹œ ì½”ë“œ proxyDict = { \"http\" : \"http://proxy_example:8080\", \"https\" : \"https://proxy_example:8080\" } r = requests.get(url, proxies=proxyDict) . Header ì‚¬ìš© . : headers ì˜µì…˜ì„ ì‚¬ìš©í•˜ë©´ ì‚¬ëŒì¸ ì²™ ì ‘ì†í•  ìˆ˜ ìˆë‹¤ (scraper ì°¨ë‹¨í•˜ëŠ” ì‚¬ì´íŠ¸ì— ì ‘ì†í•˜ê¸°) . | User AgentëŠ” ìì‹ ì´ ì‚¬ìš©í•˜ëŠ” ë¸Œë¼ìš°ì €ì—ì„œ ì •ë³´ë¥¼ ê°€ì ¸ì™€ë„ ë˜ê³ , fake user agentë¥¼ ë°›ì•„ì™€ì„œ ì‚¬ìš©í•´ë„ ëœë‹¤. | . # Header ì‚¬ìš© ì˜ˆì‹œ ì½”ë“œ headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'} r = requests.get(url, headers=headers) . +) fake user agent ì‰½ê²Œ ê°€ì ¸ì™€ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ . import requests from fake_useragent import UserAgent # importí•´ì„œ ì‚¬ìš© ua = UserAgent() headers = { 'User-Agent': ua.random, } resp = requests.get(url, headers = headers) . ",
    "url": "https://chaelist.github.io/docs/webscraping/requests_beautifulsoup/#requests",
    "relUrl": "/docs/webscraping/requests_beautifulsoup/#requests"
  },"192": {
    "doc": "Requests & BeautifulSoup",
    "title": "BeautifulSoup",
    "content": "*ì—­í• : information extraction. ì›í•˜ëŠ” ì •ë³´ë¥¼ ë‹´ì€ tagë¥¼ ì°¾ì•„ì„œ, ê·¸ ì•ˆì˜ textë¥¼ extractí•´ì¤€ë‹¤ . | ë³´í†µ Requestsë¡œ ì›¹ ìƒì˜ source codeë¥¼ ë°›ì•„ì˜¨ í›„ì—, BeautifulSoupë¥¼ í†µí•´ ì›í•˜ëŠ” ì •ë³´ë¥¼ ì¶”ì¶œí•œë‹¤ . import requests from bs4 import BeautifulSoup url = 'https://www.imdb.com/title/tt0805647/?ref_=vp_vi_tt' r = requests.get(url) # requestsê°€ urlì˜ source codeë¥¼ ë°›ì•„ì˜¨ í›„, soup = BeautifulSoup(r.text, 'lxml') # BeautifulSoupë¡œ ê° tagë¥¼ ì ‘ê·¼í•  ìˆ˜ ìˆê²Œ ì¤€ë¹„ . | parserì˜ ì¢…ë¥˜: . | html.parser: ê°ì¢… ê¸°ëŠ¥ ì™„ë¹„, ì ì ˆí•œ ì†ë„, ê´€ëŒ€í•¨(lenient) | lxml: ì•„ì£¼ ë¹ ë¦„, ê´€ëŒ€í•¨ | html5lib: ì•„ì£¼ ê´€ëŒ€í•¨ (ì›¹ ë¸Œë¼ìš°ì € ë°©ì‹ìœ¼ë¡œ í˜ì´ì§€ë¥¼ í•´ì„í•¨), í•˜ì§€ë§Œ ì†ë„ê°€ ë§¤ìš° ëŠë¦¼ | . | parserì— ë”°ë¼ ì½ì–´ì˜¤ëŠ” ë°©ì‹ì´ ì¡°ê¸ˆì”© ë‹¤ë¥´ê¸°ì—, parserë¥¼ ë³€ê²½í•˜ë©´ í¬ë¡¤ë§ì´ ê°€ëŠ¥í•´ì§€ëŠ” ê²½ìš°ë„ ìˆë‹¤. | . ì§ì ‘ tag ì´ë¦„ í˜¸ì¶œ . # ì‹¤ìŠµìš©ìœ¼ë¡œ ì œì‘ëœ html code. (Requestsë¡œ ì›¹ ìƒì˜ source codeë¥¼ ë°›ì•„ì˜¤ëŠ” ë‹¨ê³„ë¥¼ ëŒ€ì²´) html_code = \"\"\"&lt;!DOCTYPE html&gt; &lt;html&gt; &lt;head&gt; &lt;title&gt; Cities and Locations &lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;span class=\"city1\"&gt; &lt;a href = \"http://english.seoul.go.kr/\"&gt;Seoul&lt;/a&gt; &lt;p class = \"location\"&gt; South Korea &lt;/p&gt; &lt;a href = \"https://en.wikipedia.org/wiki/Seoul\"&gt;Wikipedia - Seoul &lt;/p&gt; &lt;/span&gt; &lt;span class=\"city2\"&gt; &lt;a href = \"hhttp://www1.nyc.gov/\"&gt;New York City&lt;/a&gt; &lt;p class = \"location western\"&gt; U.S.A. &lt;/p&gt; &lt;a href = \"https://en.wikipedia.org/wiki/New_York_City\"&gt;Wikipedia - NY City &lt;/p&gt; &lt;/span&gt; &lt;span class=\"city3\"&gt; &lt;a href = \"http://www.ebeijing.gov.cn/\"&gt;Beijing&lt;/a&gt; &lt;p class = \"location\"&gt; China &lt;/p&gt; &lt;a href = \"https://en.wikipedia.org/wiki/Beijing\"&gt;Wikipedia - Beijing &lt;/p&gt; &lt;/span&gt; &lt;/body&gt; &lt;/html&gt;\"\"\" # BeautifulSoup íƒ€ì…ìœ¼ë¡œ ë³€í™˜ soup = BeautifulSoup(html_code, 'html.parser') . | title tagì— ì ‘ê·¼í•˜ê¸° . soup.title . &lt;title&gt; Cities and Locations &lt;/title&gt; . | tagì˜ textë§Œ ì¶”ì¶œí•˜ê¸° . soup.title.text.strip() ## .textë§Œ í•´ì¤˜ë„ ë˜ì§€ë§Œ, ì•ë’¤ì— spaceê°€ ìˆì„ ìˆ˜ ìˆì–´ì„œ ë³´í†µì€ strip()ì„ í•¨ê»˜ ì¨ì¤€ë‹¤. Cities and Locations . | . cf) ì—¬ëŸ¬ ê°œì˜ ê°™ì€ tagê°€ ìˆëŠ” ê²½ìš°: . | ì§ì ‘ tag ì´ë¦„ì„ í˜¸ì¶œí•´ ë¶ˆëŸ¬ì˜¤ëŠ” ê²½ìš°ëŠ”, ê°™ì€ tagê°€ ë‘ ê°œ ì´ìƒì´ì—¬ë„ ë§¨ ì²˜ìŒ tag í•˜ë‚˜ë§Œ ë¶ˆëŸ¬ì˜¨ë‹¤ . soup.p . &lt;p class=\"location\"&gt; South Korea &lt;/p&gt; . | . find()ì™€ find_all() . : html tag ê¸°ë°˜ ì„ íƒì . | find(): ì¡°ê±´ì„ ì¶©ì¡±í•˜ëŠ” ê²ƒ ì¤‘ ê°€ì¥ ì²˜ìŒì˜ tag í•˜ë‚˜ë§Œ ê°€ì ¸ì˜´ | find_all(): ì¡°ê±´ì„ ì¶©ì¡±í•˜ëŠ” ëª¨ë“  tagë¥¼ list í˜•íƒœë¡œ ë‹¤ ë¶ˆëŸ¬ì˜´ | . | find() soup.find('p') # ë§¨ ì²˜ìŒ p tagë§Œ ë°˜í™˜. soup.pë‘ ë™ì¼. &lt;p class=\"location\"&gt; South Korea &lt;/p&gt; . | find_all() soup.find_all('p') # list í˜•íƒœë¡œ ëª¨ë“  p tag ëª¨ë‘ ë°˜í™˜ . [&lt;p class=\"location\"&gt; South Korea &lt;/p&gt;, &lt;p class=\"location western\"&gt; U.S.A. &lt;/p&gt;, &lt;p class=\"location\"&gt; China &lt;/p&gt;] . â†’ list í˜•íƒœë¡œ ë°˜í™˜ë˜ë©´, indexingì„ í†µí•´ textë¥¼ ì¶”ì¶œí•˜ë©´ ëœë‹¤ . soup.find_all('p')[2].text.strip() . China . | íŠ¹ì • attributeë¥¼ ê°–ëŠ” tag ì°¾ê¸° soup.find('p', attrs={'class':'western'}) # 'western'ì´ë¼ëŠ” 'class'ë¥¼ ê°€ì§„ p tag ì°¾ê¸° . &lt;p class=\"location western\"&gt; U.S.A. &lt;/p&gt; . | soup.find('p', attrs={'class':'western'})ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì¨ë„ ë™ì¼: soup.find('p', 'western'), soup.find('p', class_='western') (í¸í•œëŒ€ë¡œ ì¤„ì—¬ì„œ ì¨ë„ ëœë‹¤) | . | . tag ë‚´ element ì¶”ì¶œ . | .get() í•¨ìˆ˜ soup.find('a') # a tagì— ì ‘ê·¼ . &lt;a href=\"http://english.seoul.go.kr/\"&gt;Seoul&lt;/a&gt; . â†’ a tagì˜ â€˜hrefâ€™ì—ì„œ url ì •ë³´ ì¶”ì¶œí•˜ê¸° . soup.find('a').get('href') . http://english.seoul.go.kr/ . | [ ] ë°©ì‹ soup.find('a')['href'] ## ì´ë ‡ê²Œ ì ‘ê·¼í•˜ëŠ” ê²ƒë„ ê°€ëŠ¥ . http://english.seoul.go.kr/ . | . Nested Tags ì ‘ê·¼ . | a tag can be a parent / child / sibling of another tag | íŠ¹ì • parent tagì— ë¨¼ì € ì ‘ê·¼í•œ í›„, child tagì— ì ‘ê·¼í•˜ë©´ ë” ì •êµí•œ ì ‘ê·¼ì´ ê°€ëŠ¥í•˜ë‹¤ | . city3_info = soup.find_all('span', 'city3')[0] # parent tag city3_info . &lt;span class=\"city3\"&gt; &lt;a href=\"http://www.ebeijing.gov.cn/\"&gt;Beijing&lt;/a&gt; &lt;p class=\"location\"&gt; China &lt;/p&gt; &lt;a href=\"https://en.wikipedia.org/wiki/Beijing\"&gt;Wikipedia - Beijing &lt;/a&gt;&lt;/span&gt; . â†’ í•´ë‹¹ parent tag ì•„ë˜ì— ìˆëŠ” child tagì— ì ‘ê·¼í•´ì„œ ì •ë³´ ì¶”ì¶œ . city3_info.a.get('href') . http://www.ebeijing.gov.cn/ . re.compile . : ì •ê·œí‘œí˜„ì‹ì„ í™œìš©í•˜ë©´ íŠ¹ì • ê¸€ìë¥¼ í¬í•¨í•˜ëŠ” tagë¥¼ ëª¨ë‘ ì°¾ì•„ì˜¤ëŠ” ê²ƒì´ ê°€ëŠ¥ . import re tags = soup.find_all('span', attrs={'class':re.compile('city')}) # ì´ë ‡ê²Œ í•˜ë©´ class attributeì— 'city'ê°€ í¬í•¨ëœ span tagë¥¼ ë‹¤ ì°¾ëŠ”ë‹¤ ## &lt;span class='city1'&gt;, &lt;span class='city2'&gt;, &lt;span class='city3'&gt; ë“±ì„ ë‹¤ ì°¾ì•„ì˜´! tags . [&lt;span class=\"city1\"&gt; &lt;a href=\"http://english.seoul.go.kr/\"&gt;Seoul&lt;/a&gt; &lt;p class=\"location\"&gt; South Korea &lt;/p&gt; &lt;a href=\"https://en.wikipedia.org/wiki/Seoul\"&gt;Wikipedia - Seoul &lt;/a&gt;&lt;/span&gt;, &lt;span class=\"city2\"&gt; &lt;a href=\"hhttp://www1.nyc.gov/\"&gt;New York City&lt;/a&gt; &lt;p class=\"location western\"&gt; U.S.A. &lt;/p&gt; &lt;a href=\"https://en.wikipedia.org/wiki/New_York_City\"&gt;Wikipedia - NY City &lt;/a&gt;&lt;/span&gt;, &lt;span class=\"city3\"&gt; &lt;a href=\"http://www.ebeijing.gov.cn/\"&gt;Beijing&lt;/a&gt; &lt;p class=\"location\"&gt; China &lt;/p&gt; &lt;a href=\"https://en.wikipedia.org/wiki/Beijing\"&gt;Wikipedia - Beijing &lt;/a&gt;&lt;/span&gt;] . ",
    "url": "https://chaelist.github.io/docs/webscraping/requests_beautifulsoup/#beautifulsoup",
    "relUrl": "/docs/webscraping/requests_beautifulsoup/#beautifulsoup"
  },"193": {
    "doc": "Requests & BeautifulSoup",
    "title": "BeautifulSoup: select()",
    "content": ": CSS ê¸°ë°˜ ì„ íƒì . | select(): find_all()ê³¼ ìœ ì‚¬. ì¡°ê±´ì„ ì¶©ì¡±í•˜ëŠ” ëª¨ë“  tagë¥¼ list í˜•íƒœë¡œ ë‹¤ ë¶ˆëŸ¬ì˜´ | select_one(): find()ì™€ ìœ ì‚¬. ì¡°ê±´ì„ ì¶©ì¡±í•˜ëŠ” ê²ƒ ì¤‘ ê°€ì¥ ì²˜ìŒì˜ tag í•˜ë‚˜ë§Œ ê°€ì ¸ì˜´ | . â€» íƒœê·¸ ì´ë¦„, ì†ì„±, ì†ì„±ê°’ì„ íŠ¹ì •í•˜ëŠ” ë°©ì‹ì€ find()ì™€ ê°™ë‹¤. í•˜ì§€ë§Œ select()ëŠ” ì´ ì™¸ì—ë„ ë‹¤ì–‘í•œ ì„ íƒì(selector)ë¥¼ ê°–ê¸° ë•Œë¬¸ì— ì—¬ëŸ¬ ìš”ì†Œë¥¼ ì¡°í•©í•˜ì—¬ íƒœê·¸ë¥¼ íŠ¹ì •í•˜ê¸° ì‰½ë‹¤ëŠ” ì¥ì ì´ ìˆë‹¤ (more flexible!) . html_code = \"\"\"&lt;!DOCTYPE html&gt; &lt;html&gt; &lt;head&gt; &lt;title&gt;Sample Website&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;h2&gt;HTML ì—°ìŠµ!&lt;/h2&gt; &lt;p&gt;ì´ê²ƒì€ ì²« ë²ˆì§¸ ë¬¸ë‹¨ì…ë‹ˆë‹¤.&lt;/p&gt; &lt;p&gt;ì´ê²ƒì€ ë‘ ë²ˆì§¸ ë¬¸ë‹¨ì…ë‹ˆë‹¤!&lt;/p&gt; &lt;ul&gt; &lt;li id=\"coffee\"&gt;ì»¤í”¼&lt;/li&gt; &lt;li class=\"green-tea\"&gt;ë…¹ì°¨&lt;/li&gt; &lt;li class=\"green-tea favorite\"&gt;Sencha Green Tea&lt;/li&gt; &lt;li class=\"milk\"&gt;ìš°ìœ &lt;/li&gt; &lt;/ul&gt; &lt;img src='https://i.imgur.com/bY0l0PC.jpg' alt=\"coffee\"/&gt; &lt;img src='https://i.imgur.com/fvJLWdV.jpg' alt=\"green-tea\"/&gt; &lt;img src='https://i.imgur.com/rNOIbNt.jpg' alt=\"milk\"/&gt; &lt;/body&gt; &lt;/html&gt;\"\"\" # BeautifulSoup íƒ€ì…ìœ¼ë¡œ ë³€í™˜ soup = BeautifulSoup(html_code, 'html.parser') . | html tagë¡œ ì„ íƒ # soup.select() li_tags = soup.select('li') # &lt;li&gt; íƒœê·¸ë§Œ ëª¨ì•„ì„œ ë¦¬ìŠ¤íŠ¸ë¡œ ì¶œë ¥ li_tags . [&lt;li id=\"coffee\"&gt;ì»¤í”¼&lt;/li&gt;, &lt;li class=\"green-tea\"&gt;ë…¹ì°¨&lt;/li&gt;, &lt;li class=\"green-tea favorite\"&gt;Sencha Green Tea&lt;/li&gt;, &lt;li class=\"milk\"&gt;ìš°ìœ &lt;/li&gt;] . â†’ textë§Œ ì¶”ì¶œ . li_tags[0].text # find()ì—ì„œì™€ ë™ì¼í•˜ê²Œ .textë¥¼ ì‚¬ìš© ## í•˜ë‚˜ë§Œ ì¶”ì¶œí•˜ê³  ì‹¶ì„ ë•ŒëŠ” soup.select_one('li').textë¼ê³  í•´ë„ ë™ì¼ . ì»¤í”¼ . | tag ë‚´ element ì¶”ì¶œ img_tags = soup.select('img') img_tags . [&lt;img alt=\"coffee\" src=\"https://i.imgur.com/bY0l0PC.jpg\"/&gt;, &lt;img alt=\"green-tea\" src=\"https://i.imgur.com/fvJLWdV.jpg\"/&gt;, &lt;img alt=\"milk\" src=\"https://i.imgur.com/rNOIbNt.jpg\"/&gt;] . â†’ find()ì—ì„œì™€ ë™ì¼í•˜ê²Œ, []ì´ë‚˜ .get()ì„ í™œìš©í•˜ë©´ tag ë‚´ element ì¶”ì¶œ ê°€ëŠ¥ . img_tags[0]['src'] ## img_tags[0].get('src')ì™€ ë™ì¼ . https://i.imgur.com/bY0l0PC.jpg . | . CSS ì„ íƒìë¡œ ì ‘ê·¼ . | íŠ¹ì • idì˜ íƒœê·¸ ì„ íƒ . | #ìœ¼ë¡œ ì ‘ê·¼ - ex) idê°€ coffeeì¸ íƒœê·¸: #coffee | . soup.select('#coffee') #soup.find('#coffee')ëŠ” ì„±ë¦½X . [&lt;li id=\"coffee\"&gt;ì»¤í”¼&lt;/li&gt;] . | íŠ¹ì • classì˜ íƒœê·¸ ì„ íƒ . | .ìœ¼ë¡œ ì ‘ê·¼ - ex) classê°€ â€˜green-teaâ€™ì¸ íƒœê·¸: .green-tea | . soup.select('.green-tea') . [&lt;li class=\"green-tea\"&gt;ë…¹ì°¨&lt;/li&gt;, &lt;li class=\"green-tea favorite\"&gt;Sencha Green Tea&lt;/li&gt;] . | ì†ì„±ì˜ ì´ë¦„/ê°’ìœ¼ë¡œ íƒœê·¸ ì„ íƒ . | [name=\"value\"] í˜•ì‹ìœ¼ë¡œ ì ‘ê·¼ | ex) alt ì†ì„±ì˜ ê°’ì´ â€œgreen-teaâ€ì¸ íƒœê·¸: [alt=\"green-tea\"] | ex) href ì†ì„±ì˜ ê°’ì´ â€œhttps://chaelist.github.io/â€ì¸ íƒœê·¸: [href=\"https://chaelist.github.io/\"] | . soup.select('[alt=\"green-tea\"]') . [&lt;img alt=\"green-tea\" src=\"https://i.imgur.com/fvJLWdV.jpg\"/&gt;] . | . CSS ì„ íƒì ì¡°í•©í•˜ì—¬ ì‚¬ìš© . : ì•„ë˜ì™€ ê°™ì´ ë‹¤ì–‘í•œ ì¡°í•©ìœ¼ë¡œ ì ‘ê·¼í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì´ CSS ê¸°ë°˜ ì„ íƒìì¸ select()ì˜ ì¥ì ! . | OR ì—°ì‚° . | , ì‚¬ìš© â†’ ë‘ CSS ì„ íƒì ì¤‘ í•˜ë‚˜ë¼ë„ í•´ë‹¹ë˜ë©´ ì„ íƒ | . ex1) . # idê°€ coffeeì´ê±°ë‚˜ classê°€ milkì¸ íƒœê·¸ soup.select('#coffee, .milk') . [&lt;li id=\"coffee\"&gt;ì»¤í”¼&lt;/li&gt;, &lt;li class=\"milk\"&gt;ìš°ìœ &lt;/li&gt;] . ex2) . # ëª¨ë“  p íƒœê·¸ì™€ ëª¨ë“  li íƒœê·¸ soup.select('p, li') . [&lt;p&gt;ì´ê²ƒì€ ì²« ë²ˆì§¸ ë¬¸ë‹¨ì…ë‹ˆë‹¤.&lt;/p&gt;, &lt;p&gt;ì´ê²ƒì€ ë‘ ë²ˆì§¸ ë¬¸ë‹¨ì…ë‹ˆë‹¤!&lt;/p&gt;, &lt;li id=\"coffee\"&gt;ì»¤í”¼&lt;/li&gt;, &lt;li class=\"green-tea favorite\"&gt;ë…¹ì°¨&lt;/li&gt;, &lt;li class=\"milk\"&gt;ìš°ìœ &lt;/li&gt;] . | AND ì—°ì‚° . | ë‘ CSS ì„ íƒìë¥¼ ë¶™ì—¬ì”€ â†’ ë‘ CSS ì„ íƒì ëª¨ë‘ í•´ë‹¹ë˜ëŠ” ìš”ì†Œë§Œ ì„ íƒ | . ex1) . # green-tea í´ë˜ìŠ¤ì™€ favorite í´ë˜ìŠ¤ë¥¼ ëª¨ë‘ ê°€ì§„ íƒœê·¸ soup.select('.green-tea.favorite') . [&lt;li class=\"green-tea favorite\"&gt;Sencha Green Tea&lt;/li&gt;] . ex2) . # favoite í´ë˜ìŠ¤ë¥¼ ê°€ì§„ li íƒœê·¸ soup.select('li.favorite') . [&lt;li class=\"green-tea favorite\"&gt;Sencha Green Tea&lt;/li&gt;] . | Nested Tags ì ‘ê·¼ (Descendant Combinator) . | ë‘ CSS ì„ íƒìë¥¼ ë„ì–´ì”€ â†’ ì²« ë²ˆì§¸ CSS ì„ íƒìë¡œ ì„ íƒëœ íƒœê·¸ ë‚´ë¶€ì—ì„œ ë‘ ë²ˆì§¸ CSS ì„ íƒìë¥¼ ì°¾ëŠ”ë‹¤ | . # ul íƒœê·¸ ì•„ë˜ì— ìˆëŠ” ìì† íƒœê·¸ ì¤‘, milkë¼ëŠ” classë¥¼ ê°€ì§„ íƒœê·¸ soup.select('ul .milk') . [&lt;li class=\"milk\"&gt;ìš°ìœ &lt;/li&gt;] . cf) .class1 .class2ì™€ .class1 &gt; .class2ì˜ ì°¨ì´: . | .class1 .class2: class1 í´ë˜ìŠ¤ë¥¼ ê°€ì§„ íƒœê·¸ì˜ â€œìì†â€ ì¤‘ì— class2 í´ë˜ìŠ¤ë¥¼ ê°€ì§„ ëª¨ë“  íƒœê·¸ë¥¼ ì˜ë¯¸ | .class1 &gt; .class2: class1 í´ë˜ìŠ¤ë¥¼ ê°€ì§„ íƒœê·¸ì˜ â€œìì‹â€ ì¤‘ì— class2 í´ë˜ìŠ¤ë¥¼ ê°€ì§„ ëª¨ë“  íƒœê·¸ë¥¼ ì˜ë¯¸ | â€œìì†â€: ë¶€ëª¨ ìš”ì†Œì— í¬í•¨ëœ ëª¨ë“  í•˜ìœ„ ìš”ì†Œë¥¼ ì˜ë¯¸ vs â€œìì‹â€: ë¶€ëª¨ì˜ ë°”ë¡œ ì•„ë˜ ìì‹ ìš”ì†Œë§Œì„ ì˜ë¯¸ | . Â  . | í˜•ì œ íƒœê·¸ ì ‘ê·¼ (Sibling Combinator) . | ~ ì‚¬ìš© â†’ íŠ¹ì • íƒœê·¸ ë’¤ì— ë‚˜ì˜¤ëŠ” í˜•ì œ íƒœê·¸ë¥¼ ì„ íƒ | í˜•ì œ íƒœê·¸: parent-child ê´€ê³„ê°€ ì•„ë‹Œ, ê°™ì€ depthë¥¼ ê°€ì§„ íƒœê·¸ (ë³‘ë ¬ì  ë‚˜ì—´) | . ex1) . # green-teaë¼ëŠ” classë¥¼ ê°€ì§„ íƒœê·¸ ë’¤ì— ë‚˜ì˜¤ëŠ” green-teaë¼ëŠ” classë¥¼ ê°€ì§„ íƒœê·¸ soup.select('.green-tea ~ .green-tea') . [&lt;li class=\"green-tea favorite\"&gt;Sencha Green Tea&lt;/li&gt;] . ex2) . # ul íƒœê·¸ ë’¤ì— ë‚˜ì˜¤ëŠ” img íƒœê·¸ (ë’¤ì— ë‚˜ì˜¤ëŠ” ëª¨ë“  img íƒœê·¸) soup.select('ul ~ img') . [&lt;img alt=\"coffee\" src=\"https://i.imgur.com/bY0l0PC.jpg\"/&gt;, &lt;img alt=\"green-tea\" src=\"https://i.imgur.com/fvJLWdV.jpg\"/&gt;, &lt;img alt=\"milk\" src=\"https://i.imgur.com/rNOIbNt.jpg\"/&gt;] . cf) ì¸ì ‘í˜•ì œ ì„ íƒì +: íŠ¹ì • íƒœê·¸ ë°”ë¡œ ë’¤ì— ë‚˜ì˜¤ëŠ” í˜•ì œ íƒœê·¸ë¥¼ ì„ íƒ. | ~ì™€ ìœ ì‚¬í•˜ë‚˜, ë°”ë¡œ ë’¤ì— ìˆëŠ” 1ê°œë§Œ ì„ íƒí•œë‹¤ëŠ” ì ì—ì„œ ì°¨ì´ # ~ ëŒ€ì‹  +ë¥¼ ì‚¬ìš©í•˜ë©´ ë°”ë¡œ ë’¤ì— ë‚˜ì˜¤ëŠ” íƒœê·¸ 1ê°œë§Œ ì°¾ì•„ì¤Œ soup.select('ul + img') . [&lt;img alt=\"coffee\" src=\"https://i.imgur.com/bY0l0PC.jpg\"/&gt;] . | . | . ",
    "url": "https://chaelist.github.io/docs/webscraping/requests_beautifulsoup/#beautifulsoup-select",
    "relUrl": "/docs/webscraping/requests_beautifulsoup/#beautifulsoup-select"
  },"194": {
    "doc": "Requests & BeautifulSoup",
    "title": "Requests & BeautifulSoup",
    "content": " ",
    "url": "https://chaelist.github.io/docs/webscraping/requests_beautifulsoup/",
    "relUrl": "/docs/webscraping/requests_beautifulsoup/"
  },"195": {
    "doc": "Seaborn",
    "title": "Seaborn",
    "content": ". | ê¸°ë³¸ ê·¸ë˜í”„ . | Bar Plot, Count Plot | Line Plot | Scatter Plot | . | KDE Plot . | ê¸°ë³¸ KDE Plot | histplot() | violinplot() | KDE Plot; ë“±ê³ ì„  ê·¸ë˜í”„ | . | LM Plot | Joint Plot | Box Plot | Catplot | Heatmap | Pairplot | . *Seaborn: Python data visualization library based on matplotlib . ",
    "url": "https://chaelist.github.io/docs/visualization/seaborn/",
    "relUrl": "/docs/visualization/seaborn/"
  },"196": {
    "doc": "Seaborn",
    "title": "ê¸°ë³¸ ê·¸ë˜í”„",
    "content": "Bar Plot, Count Plot . import pandas as pd import seaborn as sns # importí•´ì•¼ ì‚¬ìš© ê°€ëŠ¥; ë³´í†µ snsë¡œ ì¤„ì—¬ì„œ import exam_df = pd.read_csv('data/exam.csv') ## ë°ì´í„° ì¶œì²˜: codeit exam_df.head() . | Â  | gender | race/ethnicity | parental level of education | lunch | test preparation course | math score | reading score | writing score | . | 0 | female | group B | bachelorâ€™s degree | standard | none | 72 | 72 | 74 | . | 1 | female | group C | some college | standard | completed | 69 | 90 | 88 | . | 2 | female | group B | masterâ€™s degree | standard | none | 90 | 95 | 93 | . | 3 | male | group A | associateâ€™s degree | free/reduced | none | 47 | 57 | 44 | . | 4 | male | group C | some college | standard | none | 76 | 78 | 75 | . | barplot sns.barplot(data=exam_df, x=\"race/ethnicity\", y=\"math score\", palette='Blues_d'); # ci=95ê°€ default (ci: Confidence Interval. ì‹ ë¢°êµ¬ê°„) . | barplotì˜ ê¸°ë³¸ estimatorëŠ” np.mean (í‰ê· ê°’). ë³„ë„ë¡œ ì„¤ì •í•´ì£¼ì§€ ì•Šìœ¼ë©´ defaultë¡œ ê° ì¹´í…Œê³ ë¦¬ì˜ í‰ê· ê°’ì„ ì‹œê°í™”í•´ì¤€ë‹¤ | estimatorëŠ” np.sum, np.count ë“±ìœ¼ë¡œ ë‹¤ì–‘í•˜ê²Œ ë³€ê²½ ê°€ëŠ¥ | . +) hue ì˜µì…˜ ì§€ì •: . sns.barplot(data=exam_df, x=\"race/ethnicity\", y=\"math score\", hue='gender', palette='Set3'); . | countplot: yê°’ì—ëŠ” ìë™ìœ¼ë¡œ countê°€ ë“¤ì–´ê°€ëŠ” bar plot # ê° race/ethnicity ê·¸ë£¹ì— ì†í•œ í•™ìƒ ìˆ˜ë¥¼ ë¹„êµ sns.countplot(data=exam_df, x=\"race/ethnicity\", color='skyblue'); . | . Line Plot . import pandas as pd import seaborn as sns flights_df = sns.load_dataset(\"flights\") flights_df.head() . | Â  | year | month | passengers | . | 0 | 1949 | Jan | 112 | . | 1 | 1949 | Feb | 118 | . | 2 | 1949 | Mar | 132 | . | 3 | 1949 | Apr | 129 | . | 4 | 1949 | May | 121 | . | ì—°ë„ë³„ ì›”í‰ê·  ìŠ¹ê°ìˆ˜ ì¶”ì´ sns.lineplot(data=flights_df, x='year', y='passengers', color='skyblue'); # í‰ê· ê°’ì„ lineìœ¼ë¡œ ê·¸ë ¤ì£¼ê³ , 95% confidence intervalì„ í•¨ê»˜ í‘œì‹œ . | ì—°ë„ë³„ ì´ ìŠ¹ê°ìˆ˜ ì¶”ì´ . flights_groupby = flights_df.groupby('year')[['passengers']].sum().reset_index() sns.lineplot(data=flights_groupby, x='year', y='passengers', color='skyblue'); . +) ì•„ì˜ˆ estimator=â€™sumâ€™ìœ¼ë¡œ ì„¤ì •í•´ì„œ ì‹œê°í™”í•˜ëŠ” ê²ƒë„ ê°€ëŠ¥ . # ì—°ë„ë³„ ì´ ìŠ¹ê°ìˆ˜ ì¶”ì´ (groupbyë¥¼ ê±°ì³ì„œ ê·¸ë¦¬ëŠ” ê²ƒê³¼ ë™ì¼í•œ í˜•íƒœë¡œ ì‹œê°í™” ê°€ëŠ¥) sns.lineplot(data=flights_df, x='year', y='passengers', color='skyblue', estimator='sum', ci=None); . | estimatorëŠ” â€˜sumâ€™, â€˜minâ€™, â€˜countâ€™ ë“± ë‹¤ì–‘í•˜ê²Œ ì„¤ì • ê°€ëŠ¥ | . | ì—°ë„ë³„, ì›”ë³„ ìŠ¹ê°ìˆ˜ ì¶”ì´ . sns.lineplot(data=flights_df, x='year', y='passengers', hue='month', palette='pastel'); . +) ì•„ì˜ˆ pivotëœ ë°ì´í„°ë¥¼ í†µì§¸ë¡œ ë„£ì–´ì„œ ê·¸ë¦¬ëŠ” ê²ƒë„ ê°€ëŠ¥: . flights_groupby2 = flights_df.pivot('year', 'month', 'passengers') sns.lineplot(data=flights_groupby2, palette='pastel'); . | . Scatter Plot . sns.scatterplot(data=exam_df, x='math score', y='reading score', color='skyblue'); . +) hue, style ë“±ì˜ ì˜µì…˜ìœ¼ë¡œ êµ¬ë¶„í•´ì„œ ì‹œê°í™”: . import matplotlib.pyplot as plt plt.figure(figsize=(10, 6)) sns.scatterplot(data=exam_df, x='math score', y='reading score', hue='gender', style='race/ethnicity', palette='Blues_d'); # +) size='lunch' ì´ëŸ° ì‹ìœ¼ë¡œ ì ì˜ sizeì— ë”°ë¥¸ êµ¬ë¶„ë„ ì¶”ê°€ ê°€ëŠ¥ . ",
    "url": "https://chaelist.github.io/docs/visualization/seaborn/#%EA%B8%B0%EB%B3%B8-%EA%B7%B8%EB%9E%98%ED%94%84",
    "relUrl": "/docs/visualization/seaborn/#ê¸°ë³¸-ê·¸ë˜í”„"
  },"197": {
    "doc": "Seaborn",
    "title": "KDE Plot",
    "content": ": ëŒ€ì²´ë¡œ ì„¸ìƒì—ì„œ ì¼ì–´ë‚˜ëŠ” ëŒ€ë¶€ë¶„ì˜ ì¼ë“¤ì€ ë¹„ìŠ·í•œ í™•ë¥ ë°€ë„í•¨ìˆ˜(PDF)ì˜ ìƒê¹€ìƒˆë¥¼ ê°–ëŠ”ë‹¤. ê·¸ëŸ°ë°, ìš°ë¦¬ëŠ” ë¬´í•œê°œì˜ ë°ì´í„°ë¥¼ êµ¬í•  ìˆ˜ ì—†ê¸°ì—, ìš°ë¦¬ê°€ êµ¬í•œ ë°ì´í„°ë¡œ ê·¸ë˜í”„ë¥¼ ê·¸ë ¤ë³´ë©´ ë§¤ë„ëŸ¬ìš´ í™•ë¥ ë°€ë„í•¨ìˆ˜ì˜ ëª¨ì–‘ì´ ë‚˜ì˜¤ì§€ ì•ŠëŠ”ë‹¤. â†’ ì´ë¥¼ í•´ê²°í•˜ëŠ” ê²ƒì´ KDE(Kernel Density Estimation). KDEë¥¼ ì‚¬ìš©í•˜ë©´ ìš°ë¦¬ê°€ êµ¬í•œ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì–´ëŠ ì •ë„ ì¶”ì¸¡ì„ í•´ì„œ, ì‹¤ì œ ë¶„í¬ì— ê°€ê¹ê²Œ ë§¤ë„ëŸ¬ìš´ ê·¸ë˜í”„ë¥¼ ê·¸ë ¤ì¤€ë‹¤. +) PDF: Probability Density Function (í™•ë¥ ë°€ë„í•¨ìˆ˜) . | PDF ì•„ë˜ì˜ ëª¨ë“  ë©´ì ì„ ë”í•˜ë©´ 1ì´ë‹¤. (100%) | PDFë¥¼ í™œìš©í•´ì„œ ê°’ë“¤ì´ ì–´ë–»ê²Œ ë¶„í¬ë˜ì–´ ìˆëŠ”ì§€ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤ | íŠ¹ì • êµ¬ê°„ì˜ ë©´ì ì´ ê³§ ê°’ì´ ê·¸ êµ¬ê°„ì— ì†í•  í™•ë¥ ì´ë‹¤ | íŠ¹ì • â€˜ì â€™ì˜ í™•ë¥ ì„ ë¬´ì¡°ê±´ 0ì´ë‹¤. (ë©´ì ì´ 0ì´ë¯€ë¡œ) | . ê¸°ë³¸ KDE Plot . body_df = pd.read_csv('data/body.csv') ## ë°ì´í„° ì¶œì²˜: codeit body_df.head() . | Â  | Number | Height | Weight | . | 0 | 1 | 176 | 85.2 | . | 1 | 2 | 175.3 | 67.7 | . | 2 | 3 | 168.6 | 75.2 | . | 3 | 4 | 168.1 | 67.1 | . | 4 | 5 | 175.3 | 63 | . â†’ í‚¤ ë°ì´í„°ë¥¼ ìˆœì„œëŒ€ë¡œ ì •ë ¬ . | value_counts()ë¡œ ê° í•­ëª©ì˜ ê°œìˆ˜ íŒŒì•… | sort_index()ë¡œ ìˆœì„œëŒ€ë¡œ ì •ë ¬ | . body_df['Height'].value_counts().sort_index() . 154.4 1 155.5 1 157.4 1 157.8 1 158.0 1 .. 190.3 1 191.2 1 191.8 1 192.4 1 193.1 1 Name: Height, Length: 262, dtype: int64 . | â€˜Heightâ€™ ë°ì´í„° ë¶„í¬ë¥¼ ê·¸ëŒ€ë¡œ ê·¸ë˜í”„ë¡œ ê·¸ë ¤ë³´ê¸° body_df['Height'].value_counts().sort_index().plot() ## ì´ë ‡ê²Œ ìš°ë¦¬ê°€ ê°€ì§„ ë°ì´í„°ë¡œë§Œ ê·¸ë˜í”„ë¥¼ ê·¸ë¦¬ë©´ ë³´ê¸° ë¶ˆí¸í•˜ê²Œ ë‚˜ì˜¨ë‹¤ . | Seabornì˜ KED Plot ê¸°ëŠ¥ìœ¼ë¡œ ë§¤ë„ëŸ¬ìš´ í™•ë¥ ë°€ë„í•¨ìˆ˜ ê·¸ë¦¬ê¸° sns.kdeplot(body_df['Height']) . | bandwidth ì¡°ì ˆí•˜ê¸° sns.kdeplot(body_df['Height'], bw_adjust=0.5); # bwëŠ” bandwidthì˜ ì•½ì . | bw_adjustë¡œ ì–¼ë§ˆë‚˜ ë§¤ë„ëŸ½ê²Œ í™•ë¥ ë°€ë„í•¨ìˆ˜ë¥¼ ì¡°ì ˆí•  ê²ƒì¸ì§€ ì„ íƒ. bwê°’ì´ í´ìˆ˜ë¡ ë§¤ë„ëŸ½ë‹¤. | í•˜ì§€ë§Œ bwê°’ì´ ë¬´ì¡°ê±´ í´ìˆ˜ë¡ ì¢‹ì€ ê²ƒë§Œì€ ì•„ë‹ˆë‹¤. ì ì ˆíˆ ì¸ì‚¬ì´íŠ¸ë¥¼ ì–»ì„ ìˆ˜ ìˆëŠ” ë§Œí¼ìœ¼ë¡œ ì¡°ì ˆ. | **bw_adjustë¥¼ ì„¤ì •í•˜ì§€ ì•Šìœ¼ë©´, Seabornì´ ì ë‹¹í•œ ê°’ìœ¼ë¡œ ì•Œì•„ì„œ ê³¨ë¼ì¤€ë‹¤. | . | . histplot() . | ê¸°ë³¸ íˆìŠ¤í† ê·¸ë¨ . sns.histplot(body_df['Height'], bins=15); . | íˆìŠ¤í† ê·¸ë¨ê³¼ KDE Plotì„ í•œ ë²ˆì— í‘œí˜„ . sns.histplot(body_df['Height'], kde=True, stat='density', linewidth=0, bins=15); . | kde=True, stat='density', linewidth=0 ì˜µì…˜ì„ ì¶”ê°€í•´ì£¼ë©´ deprecated ê¸°ëŠ¥ì¸ â€˜seaborn.distplotâ€™ê³¼ ë™ì¼í•˜ê²Œ ì‹œê°í™” ê°€ëŠ¥ | stat='density'ë¼ê³  ë³„ë„ë¡œ ì„¤ì •í•´ì£¼ì§€ ì•Šìœ¼ë©´ yëŠ” ê·¸ëƒ¥ â€˜Countâ€™ê°€ ë¨ | linewidth=0ì€ kde ê³¡ì„ ì„ ì˜ ë³´ì´ê²Œ í•˜ê¸° ìœ„í•´ histogramì˜ í…Œë‘ë¦¬ì„ ì„ ì—†ì• ì£¼ëŠ” ì—­í•  | . | . violinplot() . : KDE Plotì„ ì–‘ ì˜†ìœ¼ë¡œ ëŒ€ì¹­ìœ¼ë¡œ ê·¸ë ¤ë‘” ëª¨ì–‘ì´ë‘ ë™ì¼. box plotê³¼ ìœ ì‚¬í•˜ê²Œ, ê°’ì˜ ë¶„í¬ë¥¼ íŒŒì•… ê°€ëŠ¥. sns.violinplot(y=body_df['Height']) . KDE Plot; ë“±ê³ ì„  ê·¸ë˜í”„ . :KDE Plot 2ê°œë¥¼ í•©ì³ì„œ ë“±ê³ ì„  í˜•íƒœë¡œ ë‚˜íƒ€ë‚¸ ê·¸ë˜í”„. sns.kdeplot(body_df['Height'], body_df['Weight']) . | body_df ìë£Œì—ì„œì˜ Heightì™€ Weightì˜ ì—°ê´€ì„±ê³¼ í•¨ê»˜, ê°ê°ì˜ ë¶„í¬ ëª¨ì–‘ë„ ë³¼ ìˆ˜ ìˆëŠ” ê·¸ë˜í”„. | Heightì˜ KDE Plotê³¼ Weightì˜ KEP Plotì„ í•¨ì³ì„œ, 3D í˜•íƒœì˜ ì‚°ìœ¼ë¡œ ë‚˜íƒ€ë‚¸ ê²ƒì„ ìœ„ì—ì„œ ë°”ë¼ë³¸ ëª¨ì–‘. â†’ ë“±ê³ ì„ ì²˜ëŸ¼ í‘œí˜„ë¨ | ì•„ë˜ì— ê°ê° ê·¸ë ¤ë‘” KDE Plotì„ ë³´ê³  ë¨¸ë¦¿ì†ìœ¼ë¡œ í•©ì³ë³´ë©´, ìœ„ì™€ ê°™ì€ ë“±ê³ ì„ ì´ ì´í•´ê°€ ë  ê²ƒì´ë‹¤ | . sns.kdeplot(body_df['Height']) . sns.kdeplot(body_df['Weight']) . ",
    "url": "https://chaelist.github.io/docs/visualization/seaborn/#kde-plot",
    "relUrl": "/docs/visualization/seaborn/#kde-plot"
  },"198": {
    "doc": "Seaborn",
    "title": "LM Plot",
    "content": ": LMì€ Linear Modelì˜ ì•½ì. ì‚°ì ë„ì™€ Regression Line(íšŒê·€ì„ )ì„ í•¨ê»˜ ê·¸ë ¤ì¤€ë‹¤. (lineplot + scatterplot) . body_df.head() . | Â  | Number | Height | Weight | . | 0 | 1 | 176 | 85.2 | . | 1 | 2 | 175.3 | 67.7 | . | 2 | 3 | 168.6 | 75.2 | . | 3 | 4 | 168.1 | 67.1 | . | 4 | 5 | 175.3 | 63 | . | ê¸°ë³¸ lmplot ê·¸ë¦¬ê¸° sns.set_style('darkgrid') ## ì´ë ‡ê²Œ gridì˜ styleë„ ì§€ì • ê°€ëŠ¥ sns.lmplot(data=body_df, x='Height', y='Weight') . | ci (Confidence Interval, ì‹ ë¢°êµ¬ê°„) ì¡°ì ˆ . | lmplotì˜ default settingì€ ci=95 (95% ì‹ ë¢°êµ¬ê°„ í‘œì‹œ) â†’ ci=Noneìœ¼ë¡œ êº¼ë²„ë¦¬ê±°ë‚˜, ci=80 ì´ëŸ° ì‹ìœ¼ë¡œ ë³€ê²½í•  ìˆ˜ ìˆë‹¤ | . sns.set_style('ticks') ## style: dict, None, or one of {darkgrid, whitegrid, dark, white, ticks} sns.lmplot(data=body_df, x='Height', y='Weight', ci=None) . | hue ì˜µì…˜ ì§€ì • . exam_df.head() . | Â  | gender | race/ethnicity | parental level of education | lunch | test preparation course | math score | reading score | writing score | . | 0 | female | group B | bachelorâ€™s degree | standard | none | 72 | 72 | 74 | . | 1 | female | group C | some college | standard | completed | 69 | 90 | 88 | . | 2 | female | group B | masterâ€™s degree | standard | none | 90 | 95 | 93 | . | 3 | male | group A | associateâ€™s degree | free/reduced | none | 47 | 57 | 44 | . | 4 | male | group C | some college | standard | none | 76 | 78 | 75 | . â†’ hue ì˜µì…˜ì„ ì‚¬ìš©í•´ â€˜genderâ€™ ë³„ë¡œ ë°ì´í„° ë¹„êµí•´ë³´ê¸° . sns.lmplot(data=exam_df, x='math score', y='writing score', hue='gender', palette='Set2', height=4); ## palette(ê·¸ë˜í”„ê°€ ê·¸ë ¤ì§€ëŠ” ì»¬ëŸ¬ íŒ”ë ˆíŠ¸ë¥¼ ê²°ì •), height(ê·¸ë˜í”„ í¬ê¸°ë¥¼ ê²°ì •)ë„ ì§€ì • ê°€ëŠ¥. | . ",
    "url": "https://chaelist.github.io/docs/visualization/seaborn/#lm-plot",
    "relUrl": "/docs/visualization/seaborn/#lm-plot"
  },"199": {
    "doc": "Seaborn",
    "title": "Joint Plot",
    "content": ". | 2ì°¨ì› ì‹¤ìˆ˜í˜• ë°ì´í„°ëŠ” jointplotì„ í™œìš©í•˜ë©´ ë‹¤ê°ë„ë¡œ ì‚´í”¼ê¸° ìš©ì´í•˜ë‹¤ | jointplotì„ ì‚¬ìš©í•˜ë©´ scatterplot + ê° ë³€ìˆ˜ì˜ íˆìŠ¤í† ê·¸ë¨ì„ í•¨ê»˜ ê·¸ë ¤ì¤€ë‹¤ | kind=â€™kdeâ€™ì´ë©´ ì»¤ë„ ë°€ë„ íˆìŠ¤í† ê·¸ë¨ì„ ê·¸ë¦°ë‹¤ (ë“±ê³ ì„  ëª¨ì–‘ ê·¸ë˜í”„ + ê° ë³€ìˆ˜ì˜ KDE Plot) | . | ê¸°ë³¸ jointplot: scatterplot + ë³€ìˆ˜ë³„ íˆìŠ¤í† ê·¸ë¨ sns.jointplot(data=body_df, x='Height', y='Weight') . | KDE jointplot: ë“±ê³ ì„  ê·¸ë˜í”„ + ë³€ìˆ˜ë³„ KDE Plot sns.jointplot(data=body_df, x='Height', y='Weight', kind='kde') . | . ",
    "url": "https://chaelist.github.io/docs/visualization/seaborn/#joint-plot",
    "relUrl": "/docs/visualization/seaborn/#joint-plot"
  },"200": {
    "doc": "Seaborn",
    "title": "Box Plot",
    "content": ": Seabornì„ í™œìš©í•˜ë©´ boxplotì„ ë” ë‹¤ì–‘í•˜ê²Œ ê·¸ë¦´ ìˆ˜ ìˆë‹¤ . cf) pandas ë‚´ì¥ ê¸°ëŠ¥ìœ¼ë¡œ ê·¸ë¦´ ìˆ˜ ìˆëŠ” boxplotì€ ì´ ì •ë„ . exam_df.plot(kind='box', y='math score'); . * Seabornì„ í™œìš©í•˜ë©´ xì¶•, hueë„ ì§€ì •í•´ì„œ ê°ê° ë¹„êµí•´ ë³¼ ìˆ˜ ìˆë‹¤. sns.boxplot(data=exam_df, x='test preparation course', y='math score', hue='gender', palette='Set3') . | showfliers=False ì˜µì…˜ì„ ì¶”ê°€í•˜ë©´ outlierë¥¼ ì œì™¸í•˜ê³  ì‹œê°í™”í•  ìˆ˜ ìˆë‹¤ sns.boxplot(data=exam_df, x='test preparation course', y='math score', hue='gender', palette='Set3', showfliers=False) . | . +) ì•„ë˜ì—ì„œ ë°°ìš¸ catplotì„ í™œìš©í•´ë„ boxplotê³¼ ê±°ì˜ ë™ì¼í•˜ê²Œ í‘œí˜„ ê°€ëŠ¥ . sns.catplot(data=exam_df, x='test preparation course', y='math score', hue='gender', kind='box') . | catplot(kind=â€™boxâ€™) ì—­ì‹œ ë§ˆì°¬ê°€ì§€ë¡œ showfliers=False ì˜µì…˜ì„ ì¶”ê°€í•  ìˆ˜ ìˆë‹¤ sns.catplot(data=exam_df, x='test preparation course', y='math score', hue='gender', kind='box', showfliers=False) . | . ",
    "url": "https://chaelist.github.io/docs/visualization/seaborn/#box-plot",
    "relUrl": "/docs/visualization/seaborn/#box-plot"
  },"201": {
    "doc": "Seaborn",
    "title": "Catplot",
    "content": ": ì¹´í…Œê³ ë¦¬ë³„ ë¹„êµë¥¼ ìœ„í•œ ì‹œê°í™”ì— ì í•© . laptops_df = pd.read_csv('data/laptops.csv') ## ë°ì´í„° ì¶œì²˜: codeit laptops_df.head() . | Â  | brand | model | ram | hd_type | hd_size | screen_size | price | processor_brand | processor_model | clock_speed | graphic_card_brand | graphic_card_size | os | weight | comments | . | 0 | Dell | Inspiron 15-3567 | 4 | hdd | 1024 | 15.6 | 40000 | intel | i5 | 2.5 | intel | NaN | linux | 2.5 | NaN | . | 1 | Apple | MacBook Air | 8 | ssd | 128 | 13.3 | 55499 | intel | i5 | 1.8 | intel | 2 | mac | 1.35 | NaN | . | 2 | Apple | MacBook Air | 8 | ssd | 256 | 13.3 | 71500 | intel | i5 | 1.8 | intel | 2 | mac | 1.35 | NaN | . | 3 | Apple | MacBook Pro | 8 | ssd | 128 | 13.3 | 96890 | intel | i5 | 2.3 | intel | 2 | mac | 3.02 | NaN | . | 4 | Apple | MacBook Pro | 8 | ssd | 256 | 13.3 | 112666 | intel | i5 | 2.3 | intel | 2 | mac | 3.02 | NaN | . â†’ â€˜osâ€™ì— ëª‡ ê°œì˜ uniqueí•œ ê°’ì´ ìˆë‚˜ í™•ì¸ . laptops_df['os'].unique() . array(['linux', 'mac', 'windows'], dtype=object) . â†’ osë³„ ê°€ê²© ë¹„êµ: . | box plotìœ¼ë¡œ ë¹„êµ sns.catplot(data=laptops_df, x='os', y='price', kind='box'); . | violin plotìœ¼ë¡œ ë¹„êµ sns.catplot(data=laptops_df, x='os', y='price', kind='violin') . | strip plotìœ¼ë¡œ ë¹„êµ: default option. kind= ì•ˆì“°ë©´ ìë™ìœ¼ë¡œ strip plotìœ¼ë¡œ ê·¸ë ¤ì§ sns.catplot(data=laptops_df, x='os', y='price', kind='strip') . | ì´ ê²½ìš°ì—ëŠ” ê° í•­ëª©ì˜ ë¶„í¬ ë¿ ì•„ë‹ˆë¼, ì–´ë–¤ í•­ëª©ì— ë°ì´í„°ê°€ ë” ë§ì€ì§€ë„ ì•Œë©´ ì¢‹ê¸° ë•Œë¬¸ì— strip plotì´ ê°€ì¥ ì í•©í•´ ë³´ì¸ë‹¤. | windowsê°€ ë°ì´í„°ê°€ ê°€ì¥ ë§ê³ , macì€ ëª‡ ê°œ ì—†ëŠ”ë° ë§ì´ ë¶„ì‚°ë˜ì–´ ìˆë‹¤ëŠ” ê²ƒì„ í•œ ëˆˆì— ì•Œ ìˆ˜ ìˆë‹¤. | . | ì¶”ê°€) í”„ë¡œì„¸ì„œ ë¸Œëœë“œì— ë”°ë¼ ìƒ‰ ë‚˜ëˆ ì£¼ê¸° laptops_df['processor_brand'].unique() # ëª‡ ê°œì˜ uniqueí•œ ê°’ì´ ìˆë‚˜ í™•ì¸ . array(['intel', 'amd'], dtype=object) . â†’ hue= ì˜µì…˜ì„ ì‚¬ìš©í•´ í”„ë¡œì„¸ì„œ ë¸Œëœë“œë³„ë¡œ êµ¬ë¶„ . sns.catplot(data=laptops_df, x='os', y='price', kind='strip', hue='processor_brand') . | amd í”„ë¡œì„¸ì„œë¥¼ ì‚¬ìš©í•œ ë…¸íŠ¸ë¶ì€ ëª‡ ê°œ ì—†ê³ , ëŒ€ì²´ë¡œ ì €ë ´í•œ ì¶•ì— ì†í•œë‹¤ëŠ” ê±¸ ì•Œ ìˆ˜ ìˆë‹¤ | . | swarm plotìœ¼ë¡œ ë¹„êµ: ê°’ì´ ë­‰ì³ìˆëŠ” ë¶€ë¶„ì˜ ë°ì´í„°ê°€ í¼ì³ì ¸ì„œ ë³´ì—¬ì§„ë‹¤ sns.catplot(data=laptops_df, x='os', y='price', kind='swarm', hue='processor_brand'); . | . ",
    "url": "https://chaelist.github.io/docs/visualization/seaborn/#catplot",
    "relUrl": "/docs/visualization/seaborn/#catplot"
  },"202": {
    "doc": "Seaborn",
    "title": "Heatmap",
    "content": ". | ë³€ìˆ˜ê°„ ìƒê´€ê³„ìˆ˜ ì‹œê°í™” . exam_df.head() . | Â  | gender | race/ethnicity | parental level of education | lunch | test preparation course | math score | reading score | writing score | . | 0 | female | group B | bachelorâ€™s degree | standard | none | 72 | 72 | 74 | . | 1 | female | group C | some college | standard | completed | 69 | 90 | 88 | . | 2 | female | group B | masterâ€™s degree | standard | none | 90 | 95 | 93 | . | 3 | male | group A | associateâ€™s degree | free/reduced | none | 47 | 57 | 44 | . | 4 | male | group C | some college | standard | none | 76 | 78 | 75 | . â†’ DatFrameì˜ corr() ë©”ì†Œë“œë¥¼ ì‚¬ìš©í•´ ìˆ«ìí˜• ë³€ìˆ˜ê°„ì˜ ìƒê´€ê³„ìˆ˜ í™•ì¸ . exam_df.corr() . | Â  | math score | reading score | writing score | . | math score | 1 | 0.81758 | 0.802642 | . | reading score | 0.81758 | 1 | 0.954598 | . | writing score | 0.802642 | 0.954598 | 1 | . â†’ Seabornì˜ heatmap() ë©”ì†Œë“œë¥¼ ì‚¬ìš©í•´ ìƒê´€ê³„ìˆ˜ë¥¼ í•œëˆˆì— ì‹œê°í™” . sns.heatmap(exam_df.corr()) . +) annot=True ì˜µì…˜ì„ ì¶”ê°€í•´ì£¼ë©´ ìˆ«ìë„ í•¨ê»˜ í™•ì¸ ê°€ëŠ¥ . sns.heatmap(exam_df.corr(), annot=True) . | ì—°ë„ë³„, ì›”ë³„ í•­ê³µê¸° ìŠ¹ê°ìˆ˜ ì¶”ì´ë¥¼ Heatmapìœ¼ë¡œ ì‹œê°í™” . ## Seaborn ì œê³µ flights ë°ì´í„°ì…‹ (ì—°ë„ ë° ì›”ë³„ í•­ê³µê¸° ìŠ¹ê°ìˆ˜ë¥¼ ê¸°ë¡í•œ ë°ì´í„°ì…‹) flights = sns.load_dataset('flights') flights.head() . | Â  | year | month | passengers | . | 0 | 1949 | Jan | 112 | . | 1 | 1949 | Feb | 118 | . | 2 | 1949 | Mar | 132 | . | 3 | 1949 | Apr | 129 | . | 4 | 1949 | May | 121 | . â†’ í–‰: ì—°ë„, ì—´: ì›” ê¸°ì¤€ìœ¼ë¡œ ë°ì´í„° ì¬êµ¬ì„± . flights_pivot = flights.pivot('month', 'year', 'passengers') flights_pivot.head() . | month | 1949 | 1950 | 1951 | 1952 | 1953 | 1954 | 1955 | 1956 | 1957 | 1958 | 1959 | 1960 | . | Jan | 112 | 115 | 145 | 171 | 196 | 204 | 242 | 284 | 315 | 340 | 360 | 417 | . | Feb | 118 | 126 | 150 | 180 | 196 | 188 | 233 | 277 | 301 | 318 | 342 | 391 | . | Mar | 132 | 141 | 178 | 193 | 236 | 235 | 267 | 317 | 356 | 362 | 406 | 419 | . | Apr | 129 | 135 | 163 | 181 | 235 | 227 | 269 | 313 | 348 | 348 | 396 | 461 | . | May | 121 | 125 | 172 | 183 | 229 | 234 | 270 | 318 | 355 | 363 | 420 | 472 | . â†’ Heatmapìœ¼ë¡œ ì‹œê°í™” . sns.heatmap(flights_pivot, annot=True, fmt='d', cmap=\"YlGnBu\") # annot=Trueë¡œ í•˜ë©´ ì•ˆì— ìˆ˜ì¹˜ê°€ í•¨ê»˜ ì“°ì—¬ì§ # fmt(formatëŠ” ì•ˆì— ì“°ì—¬ì§€ëŠ” ìˆ˜ì¹˜ì˜ í˜•íƒœë¥¼ ê²°ì •. 'd'ëŠ” ì •ìˆ˜í˜•. '.1f'ëŠ” ì†Œìˆ«ì  í•œìë¦¬ê¹Œì§€ # cmapìœ¼ë¡œ colormap ë³€ê²½ ê°€ëŠ¥ . | í•´ê°€ ì§€ë‚ ìˆ˜ë¡ ì—¬ë¦„ ì¤‘ì‹¬ìœ¼ë¡œ ìŠ¹ê°ìˆ˜ê°€ ì ì  ë§ì•„ì§ì„ í•œëˆˆì— í™•ì¸ ê°€ëŠ¥. | . | . ",
    "url": "https://chaelist.github.io/docs/visualization/seaborn/#heatmap",
    "relUrl": "/docs/visualization/seaborn/#heatmap"
  },"203": {
    "doc": "Seaborn",
    "title": "Pairplot",
    "content": ": 3ì°¨ì› ì´ìƒì˜ ì‹¤ìˆ˜í˜• ë°ì´í„°ë¥¼ ë‹¤ê°ë„ë¡œ ì‚´í´ë³¼ ìˆ˜ ìˆìŒ . ## Seaborn ì œê³µ iris ë°ì´í„°ì…‹ iris = sns.load_dataset('iris') iris.head() . | Â  | sepal_length | sepal_width | petal_length | petal_width | species | . | 0 | 5.1 | 3.5 | 1.4 | 0.2 | setosa | . | 1 | 4.9 | 3 | 1.4 | 0.2 | setosa | . | 2 | 4.7 | 3.2 | 1.3 | 0.2 | setosa | . | 3 | 4.6 | 3.1 | 1.5 | 0.2 | setosa | . | 4 | 5 | 3.6 | 1.4 | 0.2 | setosa | . â†’ 4ê°œ ë³€ìˆ˜(ê½ƒì/ê½ƒë°›ì¹¨ì˜ ë„ˆë¹„/í­) ì¤‘ ì–´ëŠ ë³€ìˆ˜ê°€ ì¢…ì„ êµ¬ë¶„í•˜ëŠ” ë° ë„ì›€ì´ ë  ì§€ pariplotìœ¼ë¡œ í™•ì¸ . sns.pairplot(iris, hue='species') plt.show() . ",
    "url": "https://chaelist.github.io/docs/visualization/seaborn/#pairplot",
    "relUrl": "/docs/visualization/seaborn/#pairplot"
  },"204": {
    "doc": "ë°ì´í„° ì¡°íšŒ ì‹¬í™”",
    "title": "ë°ì´í„° ì¡°íšŒ ì‹¬í™”",
    "content": ". | ì§‘ê³„/ì‚°ìˆ  í•¨ìˆ˜ . | ì§‘ê³„ í•¨ìˆ˜ | ì‚°ìˆ  í•¨ìˆ˜ | . | ë¬¸ìì—´ ê°€ê³µ í•¨ìˆ˜ | Nullê°’ ë‹¤ë£¨ê¸° &amp; ì¤‘ë³µ ì œê±° . | COALESCE(): NULLê°’ ëŒ€ì²´ | DISTINCT(): ì¤‘ë³µ ì œê±° í™•ì¸ | . | ì»¬ëŸ¼ ê°„ ê³„ì‚°/ì—°ê²° | CASE: ì¡°ê±´ë¬¸ìœ¼ë¡œ ê°€ê³µ | GROUP BY: ê·¸ë£¨í•‘í•´ì„œ ë³´ê¸° . | HAVING | WITH ROLLUP | . | SELECTë¬¸: ê° ì ˆì˜ ì‘ì„± ìˆœì„œ | . ",
    "url": "https://chaelist.github.io/docs/sql/select_advanced/",
    "relUrl": "/docs/sql/select_advanced/"
  },"205": {
    "doc": "ë°ì´í„° ì¡°íšŒ ì‹¬í™”",
    "title": "ì§‘ê³„/ì‚°ìˆ  í•¨ìˆ˜",
    "content": "ì§‘ê³„ í•¨ìˆ˜ . : íŠ¹ì • ì¹¼ëŸ¼ì˜ ì—¬ëŸ¬ row ê°’ë“¤ì„ ë™ì‹œì— ê³ ë ¤í•´ì„œ ì‹¤í–‰ë˜ëŠ” í•¨ìˆ˜. ì•„ë˜ í•¨ìˆ˜ë“¤ì€ ê³„ì‚°í•  ë•Œ nullê°’ì€ ê³ ë ¤í•˜ì§€ ì•Šê³  ê³„ì‚°ëœë‹¤. (nullê°’ì€ ì œì™¸í•˜ê³  ê³„ì‚°) . | COUNT -- email ì¹¼ëŸ¼ì— ì¡´ì¬í•˜ëŠ” nullì„ ì œì™¸í•œ ê°’ì˜ ìˆ˜ë¥¼ êµ¬í•´ì¤€ë‹¤ SELECT COUNT(email) FROM tablename; . | ë‹¤ë§Œ, íŠ¹ì • ì¹¼ëŸ¼ì— COUNT()ë¥¼ ì ìš©í•˜ë©´ nullê°’ì´ ì œì™¸ëœ ê°œìˆ˜ë¥¼ êµ¬í•´ì£¼ê¸° ë•Œë¬¸ì—, ì „ì²´ í…Œì´ë¸”ì˜ row ìˆ˜ë¥¼ êµ¬í•´ì£¼ë ¤ë©´ ì•„ë˜ì™€ ê°™ì´ ì „ì²´ì— COUNT()ë¥¼ ì ìš©í•´ì£¼ëŠ” ê²ƒì´ ì¢‹ë‹¤ SELECT COUNT(*) FROM tablename; -- ì „ì²´ í…Œì´ë¸”ì˜ row ìˆ˜ êµ¬í•˜ê¸° . | . | MAX -- í‚¤ì˜ ìµœëŒ“ê°’ êµ¬í•˜ê¸° SELECT MAX(height) FROM tablename; . | +) GREATEST(ì¹¼ëŸ¼1, ì¹¼ëŸ¼2, ì¹¼ëŸ¼3): ê° rowì—ì„œ, ê°€ì¥ í° ê°’ì„ ê°€ì§„ ì¹¼ëŸ¼ì˜ ê°’ì„ ì°¾ì•„ì¤Œ. (cf. MAXëŠ” íŠ¹ì • ì¹¼ëŸ¼ ë‚´ì˜ ìµœëŒ“ê°’ì„ ê³„ì‚°) | . | MIN -- í‚¤ì˜ ìµœì†Ÿê°’ êµ¬í•˜ê¸° SELECT MIN(height) FROM tablename; . | +) LEAST(ì¹¼ëŸ¼1, ì¹¼ëŸ¼2, ì¹¼ëŸ¼3): ê° rowì—ì„œ, ê°€ì¥ ì‘ì€ ê°’ì„ ê°€ì§„ ì¹¼ëŸ¼ì˜ ê°’ì„ ì°¾ì•„ì¤Œ. (cf. MINëŠ” íŠ¹ì • ì¹¼ëŸ¼ ë‚´ì˜ ìµœì†Ÿê°’ì„ ê³„ì‚°) | . | AVG -- í‚¤ì˜ í‰ê· ê°’ êµ¬í•˜ê¸° SELECT AVG(height) FROM tablename; . | SUM -- ë‚˜ì´ì˜ ì´í•© êµ¬í•˜ê¸° SELECT SUM(age) FROM tablename; . | STD - ë‚˜ì´ì˜ í‘œì¤€í¸ì°¨ êµ¬í•˜ê¸° SELECT STD(age) FROM tablename; . | . ì‚°ìˆ  í•¨ìˆ˜ . : íŠ¹ì • ì¹¼ëŸ¼ì˜ ê° rowë§ˆë‹¤ ì‹¤í–‰ë˜ëŠ” í•¨ìˆ˜ . | ABS: ì ˆëŒ€ê°’ì„ êµ¬í•˜ëŠ” í•¨ìˆ˜ | SQRT: ì œê³±ê·¼ì„ êµ¬í•˜ëŠ” í•¨ìˆ˜ | CEIL: ì˜¬ë¦¼ì„ í•´ì£¼ëŠ” í•¨ìˆ˜ . | ex) 178.4ë¼ëŠ” ë°ì´í„° â†’ CEIL(height)ë¡œ ì¶œë ¥í•˜ë©´ 179ë¼ê³  ì¶œë ¥ë¨ | . | FLOOR: ë‚´ë¦¼ì„ í•´ì£¼ëŠ” í•¨ìˆ˜ | ROUND: ë°˜ì˜¬ë¦¼ì„ í•´ì£¼ëŠ” í•¨ìˆ˜ . | ex) ROUND(ê°’, 1)ì´ë¼ê³  í•˜ë©´ ì†Œìˆ˜ì  ì²«ë²ˆì§¸ ìë¦¬ê¹Œì§€ ë°˜ì˜¬ë¦¼í•´ì„œ ì¶œë ¥ | +) ROUNDëŠ” ì†Œìˆ˜ì  ë‘˜ì§¸ìë¦¬ì˜ ìˆ˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë°˜ì˜¬ë¦¼ì„ í•´ì£¼ëŠ” ë°˜ë©´, TRUNCATE(ê°’, 1)ì´ë¼ê³  í•˜ë©´ ì†Œìˆ˜ì  ì²«ë²ˆì§¸ ìë¦¬ê¹Œì§€ ë‚´ë¦¼ì„ í•´ì„œ ì¶œë ¥í•´ì¤€ë‹¤ (ê·¸ëƒ¥ ë‹¨ìˆœí•˜ê²Œ ë‘˜ì§¸ìë¦¬ë¶€í„°ëŠ” ë²„ë¦¼) | . | POW: ì œê³±ì„ í•´ì£¼ëŠ” í•¨ìˆ˜ . | ex) POW(ê°’, 2)ë¼ê³  í•˜ë©´ ì œê³±, POW(ê°’, 3)ì´ë¼ê³  í•˜ë³€ ì„¸ì œê³± | . | . +) ê·¸ ì™¸ ë‹¤ì–‘í•œ ì‚°ìˆ  í•¨ìˆ˜: https://dev.mysql.com/doc/refman/8.0/en/mathematical-functions.html . ",
    "url": "https://chaelist.github.io/docs/sql/select_advanced/#%EC%A7%91%EA%B3%84%EC%82%B0%EC%88%A0-%ED%95%A8%EC%88%98",
    "relUrl": "/docs/sql/select_advanced/#ì§‘ê³„ì‚°ìˆ -í•¨ìˆ˜"
  },"206": {
    "doc": "ë°ì´í„° ì¡°íšŒ ì‹¬í™”",
    "title": "ë¬¸ìì—´ ê°€ê³µ í•¨ìˆ˜",
    "content": ". | SUBSTRING(ì¹¼ëŸ¼, m, n): íŠ¹ì • ì¹¼ëŸ¼ì˜ më²ˆì§¸ ìœ„ì¹˜ë¶€í„° nê°œì˜ ê¸€ìë§Œ ì¶”ì¶œ SELECT SUBSTRING(address, 3, 3) FROM tablename; -- 'ì„œìš¸íŠ¹ë³„ì‹œ ì¤‘êµ¬ ì„œì†Œë¬¸ë¡œ 00'ì™€ ê°™ì€ ì£¼ì†Œ â†’ 'íŠ¹ë³„ì‹œ'ë¼ê³  ì¶œë ¥ë¨ (3ë²ˆì§¸ ìœ„ì¹˜ì—ì„œë¶€í„° 3ê°œì˜ ê¸€ìë§Œ ì¶”ì¶œ) . | LEFT, RIGHT: ì¢Œìš° ëª‡ ê°œì˜ ê¸€ìë§Œ ì˜ë¼ì„œ ì¶”ì¶œí•´ì£¼ëŠ” í•¨ìˆ˜ . -- address ì¤‘ ì™¼ìª½ìœ¼ë¡œë¶€í„° 2ê°œì˜ ê¸€ìë§Œ ì˜ë¼ì„œ ì¶”ì¶œ (ex. 'ì„œìš¸íŠ¹ë³„ì‹œ ì‹ ì´Œë¡œ'ë¼ë©´ 'ì„œìš¸'ë§Œ ì¶”ì¶œ) SELECT LEFT(address, 2) FROM tablename; . -- address ì¤‘ ì˜¤ë¥¸ìª½ìœ¼ë¡œë¶€í„° 3ê°œì˜ ê¸€ìë§Œ ì˜ë¼ì„œ ì¶”ì¶œ (ex. 'ì„œìš¸íŠ¹ë³„ì‹œ ì‹ ì´Œë¡œ'ë¼ë©´ 'ì‹ ì´Œë¡œ'ë§Œ ì¶”ì¶œ) SELECT RIGHT(address, 3) FROM tablename; . | LENGTH(): ë¬¸ìì—´ì˜ ê¸¸ì´ë¥¼ êµ¬í•´ì¤Œ SELECT LENGTH(address) FROM tablename; -- `ì•ˆë“œë¡œë©”ë‹¤ 128í–‰ì„±`ì´ë¼ëŠ” ì£¼ì†Œ â†’ LENGHT()ì˜ ê²°ê³¼ëŠ” 25 . | â€» ë‹¤ë§Œ, LENGTH() í•¨ìˆ˜ëŠ” ë¬¸ìì˜ Byteê¸¸ì´ë¥¼ ê°€ì ¸ì˜¤ê¸° ë•Œë¬¸ì— í•œê¸€ì˜ ê²½ìš° ì •í™•í•œ ê¸¸ì´ë¥¼ ì•Œê¸° ì–´ë µë‹¤ | ëŒ€ì‹  CHAR_LENGTH() í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ë©´ ëª‡ ê°œì˜ ê¸€ìê°€ ì‚¬ìš©ë˜ì—ˆëŠ”ì§€ íŒŒì•… ê°€ëŠ¥. (ê³µë°±ë„ í¬í•¨í•´ ê³„ì‚°) SELECT CHAR_LENGTH(address) FROM tablename; -- `ì•ˆë“œë¡œë©”ë‹¤ 128í–‰ì„±`ì´ë¼ëŠ” ì£¼ì†Œ â†’ CHAR_LENGTH()ì˜ ê²°ê³¼ëŠ” 11 . | . | UPPER, LOWER -- ëª¨ë“  ë¬¸ìë¥¼ ëŒ€ë¬¸ìë¡œ ë³€í™˜ SELECT UPPER(address) FROM tablename; . -- ëª¨ë“  ë¬¸ìë¥¼ ì†Œë¬¸ìë¡œ ë³€í™˜ SELECT LOWER(address) FROM tablename; . | LPAD, RPAD: ë¬¸ìì—´ì˜ ì™¼ìª½(LPAD) ë˜ëŠ” ì˜¤ë¥¸ìª½(RPAD)ì„ íŠ¹ì • ë¬¸ìì—´ë¡œ ì±„ì›Œì£¼ëŠ” í•¨ìˆ˜ (*ë³´í†µ ì–´ë–¤ ìˆ«ìì˜ ìë¦¬ìˆ˜ë¥¼ ë§ì¶œ ë•Œ ìì£¼ ì‚¬ìš©) . -- age ì»¬ëŸ¼ì˜ ê°’ì„, ì™¼ìª½ì— ë¬¸ì 0ì„ ë¶™ì—¬ì„œ ì´ 10ìë¦¬ë¡œ ë§Œë“¤ì–´ ì¤€ë‹¤ SELECT email, LPAD(age, 10, '0') FROM tablename; -- ex) 28 â†’ 0000000028, 101 â†’ 0000000101 . -- age ì»¬ëŸ¼ì˜ ê°’ì„, ì˜¤ë¥¸ìª½ì— !ë¥¼ ë¶™ì—¬ì„œ ì´ 5ìë¦¬ë¡œ ë§Œë“¤ì–´ ì¤€ë‹¤ SELECT email, RPAD(age, 5, '!') FROM tablename; -- ex) 28 â†’ 28!!!, 101 â†’ 101!! . | â€» ì‚¬ì‹¤ age ì¹¼ëŸ¼ì€ INT íƒ€ì…ì´ì§€ë§Œ, ìˆ«ìë„ ë¬¸ìì—´ í•¨ìˆ˜ ì•ˆì— ì¸ìë¡œ ë„£ì–´ì£¼ë©´ ê·¸ ê°’ì´ ìë™ìœ¼ë¡œ ë¬¸ìì—´ë¡œ í˜• ë³€í™˜ì´ ë˜ì–´ ê³„ì‚°ëœë‹¤ | . | TRIM, LTRIM, RTRIM: ë¬¸ìì—´ ì–‘ ëì— ì¡´ì¬í•˜ëŠ” ê³µë°±ì„ ì œê±°í•˜ëŠ” í•¨ìˆ˜ . | LTRIM(word): ì™¼ìª½ ê³µë°± ì‚­ì œ | RTRIM(word): ì˜¤ë¥¸ìª½ ê³µë°± ì‚­ì œ | TRIM(word): ì™¼ìª½, ì˜¤ë¥¸ìª½ ì–‘ìª½ ê³µë°± ëª¨ë‘ ì‚­ì œ | . | REPLACE(ì¹¼ëŸ¼, â€˜ë¬¸ì1â€™, â€˜ë¬¸ì2â€™): íŠ¹ì • ì¹¼ëŸ¼ì—ì„œ ë¬¸ì1ì„ ë¬¸ì2ë¡œ ëª¨ë‘ ëŒ€ì²´í•´ì¤Œ -- addressì—ì„œ 'ì„œìš¸'ì´ë¼ëŠ” ëª¨ë“  ë¬¸ìë¥¼ 'ë¶€ì‚°'ìœ¼ë¡œ ë³€ê²½í•´ì„œ ì¶œë ¥ SELECT REPLACE(address, 'ì„œìš¸', 'ë¶€ì‚°') FROM tablename; . | +) INT íƒ€ì…ì—ë„ ê·¸ëŒ€ë¡œ ì ìš© ê°€ëŠ¥: -- ì •ìˆ˜í˜• íƒ€ì…ì¸ 'salary'ì—ì„œ ìˆ«ì ì¤‘ê°„ ì¤‘ê°„ì— ë“¤ì–´ê°€ëŠ” 0ì„ ëª¨ë‘ ì‚­ì œí•´ì„œ ì¶œë ¥ SELECT REPLACE(salary, '0', '') FROM tablename; . | . | . ",
    "url": "https://chaelist.github.io/docs/sql/select_advanced/#%EB%AC%B8%EC%9E%90%EC%97%B4-%EA%B0%80%EA%B3%B5-%ED%95%A8%EC%88%98",
    "relUrl": "/docs/sql/select_advanced/#ë¬¸ìì—´-ê°€ê³µ-í•¨ìˆ˜"
  },"207": {
    "doc": "ë°ì´í„° ì¡°íšŒ ì‹¬í™”",
    "title": "Nullê°’ ë‹¤ë£¨ê¸° &amp; ì¤‘ë³µ ì œê±°",
    "content": ". | Nullê°’ë§Œ / Nullê°’ ì œì™¸í•˜ê³  ì¶œë ¥ . | NULLì€ ì–´ë–¤ ê°’ë„ ì•„ë‹ˆê¸°ì—, = NULL, != NULL ì´ë ‡ê²ŒëŠ” í‘œí˜„í•  ìˆ˜ ì—†ê³ , IS NULL í˜¹ì€ IS NOT NULLì´ë¼ê³  í‘œí˜„í•´ì•¼ í•¨ | . -- address ì¹¼ëŸ¼ì´ nullê°’ì¸ ë°ì´í„°ë§Œ ì¡°íšŒ SELECT * FROM tablename WHERE address IS NULL; . -- address ì¹¼ëŸ¼ì´ nullê°’ì´ ì•„ë‹Œ ë°ì´í„°ë§Œ ì¡°íšŒ SELECT * FROM tablename WHERE address IS NOT NULL; . -- address, height, weight ì„¸ ì¹¼ëŸ¼ ì¤‘ í•˜ë‚˜ë¼ë„ nullê°’ì´ ìˆëŠ” ë°ì´í„° ì¡°íšŒ SELECT * FROM tablename WHERE address IS NULL OR height IS NULL OR weight IS NULL; . | . COALESCE(): NULLê°’ ëŒ€ì²´ . | COALESCE(ê°’1, ê°’2): nullì´ ì•„ë‹ˆë©´ ê°’1ì„, nullì´ë©´ ê°’2ë¥¼ ëŒë ¤ì¤Œ SELECT COALESCE(height, '####'), COALESCE(weight, '---'), COALESCE(address, '@@@') FROM tablename; -- height ì¹¼ëŸ¼ì˜ null ë¶€ë¶„ì€ ####ê°€, weight ì¹¼ëŸ¼ì€ ---, addressëŠ” @@@ê°€ ì¶œë ¥ë¨ . | . +) NULLê°’ì„ ë³€í™˜í•˜ëŠ” ë‹¤ì–‘í•œ ë°©ë²• . | COALESCE(height, 'N/A') â€“ height ì¹¼ëŸ¼ì˜ nullê°’ì„ â€˜N/Aâ€™ë¼ëŠ” textë¡œ ì±„ì›Œì¤Œ | COALESCE(height, weight * 2.3, 'N/A') â€“ height ì¹¼ëŸ¼ì˜ nullê°’ì„ ìš°ì„  weightì— 2.3ì„ ê³±í•œ ê°’ìœ¼ë¡œ ë„£ì–´ì£¼ê³ , ë§Œì•½ weight ì¹¼ëŸ¼ë„ NULLì´ë©´ â€˜N/Aâ€™ë¡œ ì±„ì›Œì¤Œ | IFNULL(height, 'N/A') â€“ height ì¹¼ëŸ¼ì˜ nullê°’ì„ â€˜N/Aâ€™ë¼ëŠ” textë¡œ ì±„ì›Œì¤Œ . | COALESCE(height, â€˜N/Aâ€™)ì™€ ê°™ì§€ë§Œ, IFNULLì€ mysqlì—ì„œë§Œ ì§€ì›í•˜ëŠ” í•¨ìˆ˜ì´ê³  COALESCEëŠ” SQL í‘œì¤€ í•¨ìˆ˜ì´ë‹¤ | . | IF(height IS NOT NULL, height, 'N/A') â€“ heightê°€ NULLì´ ì•„ë‹ˆë©´ heightê°’ì„ returní•˜ê³ , NULLì´ë©´ â€˜N/Aâ€™ë¼ëŠ” textë¥¼ return . | IF(ì¡°ê±´ì‹, ê²°ê³¼1, ê²°ê³¼2): ì¡°ê±´ì‹ì´ Trueë©´ ê²°ê³¼1ì„, Falseë©´ ê²°ê³¼2ë¥¼ returní•˜ë¼ëŠ” êµ¬ë¬¸ | IF êµ¬ë¬¸ì€ NULLê°’ ë°˜í™˜ ì´ì™¸ì˜ ë‹¤ì–‘í•œ ì¡°ê±´ì—ë„ ì‘ìš©í•  ìˆ˜ ìˆë‹¤ | . | . DISTINCT(): ì¤‘ë³µ ì œê±° í™•ì¸ . | DISTINCT í•¨ìˆ˜: Uniqueí•œ ê°’ë§Œ í™•ì¸í•˜ê²Œ í•´ì£¼ëŠ” í•¨ìˆ˜ -- gender ì¹¼ëŸ¼ì˜ ê³ ìœ ê°’ë§Œ í™•ì¸ SELECT DISTINCT(gender) FROM tablename; . -- address ì¹¼ëŸ¼ì˜ ê°€ì¥ ì• ë‘ ê¸€ì(ex. ê²½ê¸°, ì„œìš¸, ì „ë¼)ë§Œ ì¶”ì¶œí•´ì„œ ê³ ìœ ê°’ë§Œ í™•ì¸ SELECT DISTINCT(SUBSTRING(address, 1, 2)) FROM tablename; . | +) ê³ ìœ ê°’ ê°œìˆ˜ êµ¬í•˜ê¸° -- gender ì¹¼ëŸ¼ì— ìˆëŠ” ê³ ìœ ê°’ì˜ ê°œìˆ˜ë¥¼ ì„¸ì–´ì¤Œ SELECT COUNT(DISTINCT(gender)) FROM tablename; . | . ",
    "url": "https://chaelist.github.io/docs/sql/select_advanced/#null%EA%B0%92-%EB%8B%A4%EB%A3%A8%EA%B8%B0--%EC%A4%91%EB%B3%B5-%EC%A0%9C%EA%B1%B0",
    "relUrl": "/docs/sql/select_advanced/#nullê°’-ë‹¤ë£¨ê¸°--ì¤‘ë³µ-ì œê±°"
  },"208": {
    "doc": "ë°ì´í„° ì¡°íšŒ ì‹¬í™”",
    "title": "ì»¬ëŸ¼ ê°„ ê³„ì‚°/ì—°ê²°",
    "content": ". | ì»¬ëŸ¼ ì‚°ìˆ ì—°ì‚°: + (ë”í•˜ê¸°), - (ë¹¼ê¸°), * (ê³±í•˜ê¸°), / (ë‚˜ëˆ„ê¸°), % (ë‚˜ë¨¸ì§€ êµ¬í•˜ê¸°) -- ì´ë©”ì¼, í‚¤, ëª¸ë¬´ê²Œ, BMI (ëª¸ë¬´ê²Œ(kg) / í‚¤(m)^2)ë¥¼ ì¶œë ¥ -- BMI ê³„ì‚°í•  ë•Œ í‚¤ëŠ” ë³´í†µ më‹¨ìœ„ë¡œ ë„£ìœ¼ë¯€ë¡œ 100ì„ ë‚˜ëˆ ì„œ ì œê³±í•´ì¤Œ SELECT email, height, weight, weight / ((height/100) * (height/100)) FROM tablename; . | ì—°ì‚°ì‹œ, í•œ ì»¬ëŸ¼ì´ë¼ë„ NULLì´ë©´ ê³„ì‚°ì‹ì˜ ê²°ê³¼ë„ NULL | . | alias ë¶™ì´ê¸°: â€˜ASâ€™ ì‚¬ìš© . | ì›ë˜ì˜ ì»¬ëŸ¼ ì´ë¦„ì„ ë‹¤ë¥¸ ì´ë¦„ìœ¼ë¡œ êµì²´í•´ì„œ ë³´ì—¬ì£¼ëŠ” ê²ƒ. | ë‹¨ìˆœíˆ ê³„ì‚°ì‹ë§Œ ë³´ì´ëŠ” ê²ƒë³´ë‹¤, ë³´ë‹¤ ì§ê´€ì ì¸ aliasë¥¼ ë¶™ì—¬ì„œ ë³´ì—¬ì£¼ë©´ ì´í•´í•˜ê¸° ì‰½ë‹¤ | SELECT column AS aliasì™€ ê°™ì€ ì‹ìœ¼ë¡œ ASë¥¼ ì‚¬ìš©í•˜ê±°ë‚˜, SELECT column aliasì™€ ê°™ì´ ë¹ˆì¹¸ì„ í•œ ì¹¸ ë‘ê³  ë’¤ì— ì¨ì¤˜ë„ ëœë‹¤ (í•˜ì§€ë§Œ ASë¥¼ ì“°ëŠ” ìŠµê´€ì„ ë“¤ì´ë©´ ì¡°ê¸ˆ ë” ì§ê´€ì ìœ¼ë¡œ ì´í•´í•˜ê¸° ì‰¬ìš´ ì½”ë“œê°€ ëœë‹¤) | . SELECT email, height, weight, weight / ((height/100) * (height/100)) AS BMI FROM tablename; -- ì´ë ‡ê²Œ ì ìœ¼ë©´ weight / ((height/100) * (height/100)) ê³„ì‚° ê²°ê³¼ ì¹¼ëŸ¼ì€ 'BMI'ë¼ëŠ” ì´ë¦„ìœ¼ë¡œ ì¶œë ¥ë¨ . | CONCAT: ì—¬ëŸ¬ ì»¬ëŸ¼ì„ ì›í•˜ëŠ” í˜•ì‹ìœ¼ë¡œ ì—°ê²°í•´ ì¶œë ¥ -- '165.5cm, 67.3kg'ì™€ ê°™ì€ í˜•ì‹ìœ¼ë¡œ, heightì™€ weight ì»¬ëŸ¼ì´ ê²°í•©ë˜ì–´ ì¶œë ¥ë¨ SELECT CONCAT(height, 'cm, ', weight, 'kg') AS 'í‚¤ì™€ ëª¸ë¬´ê²Œ' FROM tablename; . | cf) â€˜í‚¤ì™€ ëª¸ë¬´ê²Œâ€™ì²˜ëŸ¼ aliasì— ë¹ˆì¹¸ì´ ìˆì„ ê²½ìš°, â€˜â€˜ë‚˜ â€œâ€œë¡œ ê°ì‹¸ì„œ êµ¬ë¶„í•´ì¤˜ì•¼ í•œë‹¤ | . | . ",
    "url": "https://chaelist.github.io/docs/sql/select_advanced/#%EC%BB%AC%EB%9F%BC-%EA%B0%84-%EA%B3%84%EC%82%B0%EC%97%B0%EA%B2%B0",
    "relUrl": "/docs/sql/select_advanced/#ì»¬ëŸ¼-ê°„-ê³„ì‚°ì—°ê²°"
  },"209": {
    "doc": "ë°ì´í„° ì¡°íšŒ ì‹¬í™”",
    "title": "CASE: ì¡°ê±´ë¬¸ìœ¼ë¡œ ê°€ê³µ",
    "content": ". | ë‹¨ìˆœ CASE í•¨ìˆ˜: CASE ë’¤ì— ì»¬ëŸ¼ì„ ì ê³ , ê·¸ ì»¬ëŸ¼ì˜ ê°’ê³¼ ì–´ë–¤ íŠ¹ì •í•œ ê°’ì´ ê°™ì€ì§€(=) ë¹„êµ . | êµ¬ì¡°: (WHEN ~ THEN ~ ì€ ì›í•˜ëŠ” ë§Œí¼ ì‚¬ìš© ê°€ëŠ¥) CASE ì¹¼ëŸ¼ ì´ë¦„ WHEN ê°’ THEN ì¶œë ¥í•  ë‚´ìš© ELSE ì¶œë ¥í•  ë‚´ìš© END . | ì˜ˆì‹œ: -- age ê°’ì´ 29ë©´ 'ìŠ¤ë¬¼ ì•„í™‰'ìœ¼ë¡œ, 30ì´ë©´ 'ì„œë¥¸'ìœ¼ë¡œ ë°”ê¿”ì„œ ì¶œë ¥ SELECT email, CASE age WHEN 29 THEN â€˜ìŠ¤ë¬¼ ì•„í™‰â€™ WHEN 30 THEN â€˜ì„œë¥¸â€™ ELSE age END FROM tablename; . | . | ê²€ìƒ‰ CASE í•¨ìˆ˜: íŠ¹ì • ì¡°ê±´ì— ë”°ë¼ íŠ¹ì • ê°’ì„ ëŒë ¤ì¤€ë‹¤. (ë‹¨ìˆœ CASE í•¨ìˆ˜ë³´ë‹¤ ë‹¤ì–‘í•œ ì—°ì‚°ì´ ê°€ëŠ¥) . | êµ¬ì¡°: (WHEN ~ THEN ~ ì€ ì›í•˜ëŠ” ë§Œí¼ ì‚¬ìš© ê°€ëŠ¥) . CASE WHEN ì¡°ê±´ THEN ì¶œë ¥í•  ë‚´ìš© ELSE ì¶œë ¥í•  ë‚´ìš© END . | ì˜ˆì‹œ: . SELECT CONCAT(height, 'cm, ', weight, 'kg') AS 'í‚¤ì™€ ëª¸ë¬´ê²Œ', weight / ((height/100) * (height/100)) AS BMI, (CASE WHEN weight IS NULL or height IS NULL THEN 'ë¹„ë§Œ ì—¬ë¶€ ì•Œ ìˆ˜ ì—†ìŒ' WHEN weight / ((height/100) * (height/100)) &gt;= 25 THEN 'ê³¼ì²´ì¤‘' WHEN weight / ((height/100) * (height/100)) &gt;= 18.25 AND weight / ((height/100) * (height/100)) &lt; 25 THEN 'ì •ìƒ' ELSE 'ì €ì²´ì¤‘' END) AS obesity_check FROM tablename ORDER BY obesity_check ASC; . | BMIì— ë”°ë¼ â€˜ê³¼ì²´ì¤‘â€™ , â€˜ì •ìƒâ€™, â€˜ì €ì²´ì¤‘â€™ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ê²°ê³¼ë¥¼ ë³´ì—¬ì£¼ëŠ” ì¹¼ëŸ¼ì„ ìƒˆë¡œ ìƒì„±í•´ ì¶œë ¥ | +) CASEì˜ ê²½ìš°, aliasë¥¼ ë¶™ì—¬ì£¼ëŠ” ê²Œ ì¢‹ë‹¤ (ì´ ê²½ìš°ëŠ” obesity_checkë¼ëŠ” ì´ë¦„ìœ¼ë¡œ ì¶œë ¥ë¨) | +) ORDER BY obesity_check ASCë„ ë¶™ì—¬ì¤¬ìœ¼ë¯€ë¡œ obesity_check ì¹¼ëŸ¼ì„ ê¸°ì¤€ìœ¼ë¡œ ì˜¤ë¦„ì°¨ìˆœ ì •ë ¬ë˜ì–´ ì¶œë ¥ | . | . +) ìœ„ SQLë¬¸ì„ ì„œë¸Œì¿¼ë¦¬ë¥¼ í™œìš©í•´ ë” ê°„ê²°í•˜ê²Œ í‘œí˜„í•˜ê¸°: . SELECT email, BMI, (CASE WHEN weight IS NULL OR height IS NULL THEN 'ë¹„ë§Œ ì—¬ë¶€ ì•Œ ìˆ˜ ì—†ìŒ' WHEN BMI &gt;= 25 THEN 'ê³¼ì²´ì¤‘' WHEN BMI &gt;= 18.5 AND BMI &lt; 25 THEN 'ì •ìƒ' ELSE 'ì €ì²´ì¤‘' END) AS obesity_check FROM (SELECT *, weight/((height/100) * (height/100)) AS BMI FROM tablename) AS subquery_for_BMI ORDER BY obesity_check ASC; . ",
    "url": "https://chaelist.github.io/docs/sql/select_advanced/#case-%EC%A1%B0%EA%B1%B4%EB%AC%B8%EC%9C%BC%EB%A1%9C-%EA%B0%80%EA%B3%B5",
    "relUrl": "/docs/sql/select_advanced/#case-ì¡°ê±´ë¬¸ìœ¼ë¡œ-ê°€ê³µ"
  },"210": {
    "doc": "ë°ì´í„° ì¡°íšŒ ì‹¬í™”",
    "title": "GROUP BY: ê·¸ë£¨í•‘í•´ì„œ ë³´ê¸°",
    "content": ". | ì§‘ê³„í•¨ìˆ˜(COUNT, AVG, MIN, MAX ë“±)ê³¼ í•¨ê»˜ ì‚¬ìš© . | â€»ê·œì¹™: GROUP BYë¥¼ ì‚¬ìš©í•  ë•ŒëŠ”, SELECT ì ˆì— (1) GROUP BY ë’¤ì— ë‚˜ì˜¤ëŠ” ì¹¼ëŸ¼ë“¤ ë˜ëŠ” (2) ì§‘ê³„í•¨ìˆ˜ë§Œ ì“¸ ìˆ˜ ìˆë‹¤! | . -- ê° genderë³„ íšŒì› ìˆ˜ ì¡°íšŒ (m: 15, f: 9) ex) SELECT gender, COUNT(*) FROM tablename GROUP BY gender; -- ê° genderë³„ í‰ê·  í‚¤ë¥¼ ì¡°íšŒ (m: 176.55, f:173.7) ex) SELECT gender, AVG(height) FROM tablename GROUP BY gender; -- ê° genderë³„ ìµœì†Œ ëª¸ë¬´ê²Œë¥¼ ì¡°íšŒ (m: 58, f: 47) SELECT gender, MIN(weight) FROM tablename GROUP BY gender; . | ê¸°ì¡´ ì»¬ëŸ¼ì´ ì•„ë‹Œ, ìƒˆë¡œìš´ ê¸°ì¤€ì„ ë§Œë“¤ì–´ ê·¸ë£¨í•‘í•˜ëŠ” ê²ƒë„ ê°€ëŠ¥ -- ì§€ì—­ë³„(ex. ì„œìš¸, ê²½ê¸°, ëŒ€ì „) íšŒì› ìˆ˜ í™•ì¸ -- addressì˜ ë§¨ ì²«ê¸€ì 2ê°œë§Œ ì¶”ì¶œí•´ì„œ í™œìš© (ex. ì„œìš¸) SELECT SUBSTRING(address, 1, 2) AS region, COUNT(*) FROM tablename GROUP BY SUBSTRING(address, 1, 2); . | 2ê°€ì§€ ì´ìƒì˜ ê¸°ì¤€ìœ¼ë¡œë„ ê·¸ë£¨í•‘ ê°€ëŠ¥ -- ì§€ì—­ë³„*ì„±ë³„ íšŒì› ìˆ˜ í™•ì¸ SELECT SUBSTRING(address, 1, 2) AS region, gender, COUNT(*) FROM tablename GROUP BY SUBSTRING(address, 1, 2), gender; . | ê·¸ë£¨í•‘ì„ í•˜ê³  ë‚˜ë©´, ê¹”ë”í•˜ê²Œ ì •ë ¬ì„ í•´ì£¼ëŠ” ê²Œ ì¢‹ë‹¤ SELECT SUBSTRING(address, 1, 2) as region, gender, COUNT(*) FROM tablename GROUP BY region, -- MySQLì˜ ê²½ìš°, GROUP BY ë’¤ì— ë°”ë¡œ aliasë¥¼ í™œìš©í•˜ëŠ” ê²ƒë„ ê°€ëŠ¥ gender ORDER BY region ASC, gender DESC ; . | . HAVING . : ê·¸ë£¨í•‘ í›„, íŠ¹ì • ì¡°ê±´ì˜ ë°ì´í„°ë§Œ í™•ì¸ . SELECT SUBSTRING(address, 1, 2) as region, gender, COUNT(*) FROM tablename GROUP BY region, gender HAVING region = 'ì„œìš¸' ORDER BY region ASC, gender DESC; -- â†’ ì´ë ‡ê²Œ í•˜ë©´ regionì´ ì„œìš¸ì¸ ë°ì´í„°ë§Œ ì¡°íšŒë¨ -- MySQLì˜ ê²½ìš°, HAVING ë’¤ì— ë°”ë¡œ aliasë¥¼ í™œìš© ê°€ëŠ¥ . â€» WHEREì™€ HAVINGì˜ ì°¨ì´: . | WHEREëŠ” í…Œì´ë¸”ì—ì„œ ë§¨ ì²˜ìŒ rowë“¤ì„ ì¡°íšŒí•  ë•Œ ì¡°ê±´ì„ ì„¤ì •í•˜ê¸° ìœ„í•œ êµ¬ë¬¸ì´ê³ , | HAVINGì€ ì´ë¯¸ ì¡°íšŒëœ rowë“¤ì„ ê·¸ë£¨í•‘í–ˆì„ ë•Œ ê·¸ ê·¸ë£¹ë“¤ ë‚´ì—ì„œ ë‹¤ì‹œ í•„í„°ë§í•  ë•Œ ì“°ëŠ” êµ¬ë¬¸ | . WITH ROLLUP . : ë¶€ë¶„ ì´ê³„ë¥¼ í¬í•¨í•´ ê·¸ë£¨í•‘í•˜ê¸° . SELECT SUBSTRING(address, 1, 2) as region, gender, COUNT(*) FROM tablename GROUP BY region, gender WITH ROLLUP ORDER BY region ASC, gender DESC; . | ì´ë ‡ê²Œ WITH ROLLUPì´ ë“¤ì–´ê°€ë©´, ì„¸ë¶€ ê·¸ë£¹ë“¤ì„ ì¢€ ë” í° ë‹¨ìœ„ì˜ ê·¸ë£¹ìœ¼ë¡œ ì¤‘ê°„ì¤‘ê°„ì— í•©ì³ì¤€ë‹¤. | ex) ì„±*ì—°ë ¹ êµì°¨ ë°ì´í„°ë¼ê³  í•˜ë©´, ê·¸ëƒ¥ ê·¸ë£¨í•‘í•˜ë©´ â€˜ì—¬ì„±-10ëŒ€â€™, â€˜ì—¬ì„±-20ëŒ€â€™ ë“±ì˜ ê·¸ë£¹ë§Œ ìƒì„±ë˜ëŠ” í•œí¸, WITH ROLLUPì´ ë“¤ì–´ê°€ë©´ â€˜NULL-10ëŒ€â€™ (ì„±ë³„ì „ì²´-10ëŒ€ë¥¼ ì˜ë¯¸) ì´ëŸ° ì‹ìœ¼ë¡œ ë¶€ë¶„ì´ê³„ ê°’ ìƒì„±ë¨ | . | WITH ROLLUPì€ GROUP BY ë’¤ì— ë‚˜ì˜¤ëŠ” ê·¸ë£¨í•‘ ê¸°ì¤€ì˜ ìˆœì„œì— ë§ì¶°ì„œ ê³„ì¸µì ì¸ ë¶€ë¶„ ì´ê³„ë¥¼ ë³´ì—¬ì¤Œ . | ex) GROUP BY gender, age ìˆœì„œë©´ â€˜ì—¬ì„±-NULLâ€™(=ì—¬ì„±-ë‚˜ì´ì „ì²´)ëŠ” ì¶œë ¥ë˜ì§€ë§Œ, â€˜NULL-ì—¬ì„±â€™(=ë‚˜ì´ì „ì²´-ì—¬ì„±)ì€ ì¶œë ¥ë˜ì§€ ì•ŠëŠ”ë‹¤ | . | . +) â€˜Nullê°’ì„ ë‚˜íƒ€ë‚´ëŠ” NULLâ€™ê³¼ â€˜WITH ROLLUPì˜ ê²°ê³¼ë¡œ ë¶€ë¶„ ì´ê³„ë¥¼ í‘œí˜„í•˜ê¸° ìœ„í•´ ì“°ì¸ NULLâ€™ êµ¬ë¶„: . | GROUPING() í•¨ìˆ˜ ì‚¬ìš©: . | ê·¸ ì¸ìë¥¼ ê·¸ë£¨í•‘ ê¸°ì¤€ì—ì„œ ê³ ë ¤í•˜ì§€ ì•Šì€ ë¶€ë¶„ ì´ê³„ì¸ ê²½ìš°ì— 1ì„ ë¦¬í„´, ê·¸ë ‡ì§€ ì•Šì€ ê²½ìš° 0ì„ ë¦¬í„´. (ê°™ì€ NULLì´ë”ë¼ë„ ì›ë˜ NULLì´ ìˆë˜ ê³³ì€ 0ì´ ì¶œë ¥ë˜ê³ , ë¶€ë¶„ ì´ê³„ë¥¼ ë‚˜íƒ€ë‚´ê¸° ìœ„í•´ NULLì´ ì“°ì¸ ê³³ì€ 1ì´ ì¶œë ¥ë¨) | . SELECT YEAR(sign_up_day) AS s_year, gender, SUBSTRING(address, 1, 2) as region, GROUPING(YEAR(sign_up_day)), GROUPING(gender), GROUPING(SUBSTRING(address, 1, 2)), COUNT(*) FROM tablename GROUP BY s_year, gender, region WITH ROLLUP ORDER BY s_year DESC; . | . ",
    "url": "https://chaelist.github.io/docs/sql/select_advanced/#group-by-%EA%B7%B8%EB%A3%A8%ED%95%91%ED%95%B4%EC%84%9C-%EB%B3%B4%EA%B8%B0",
    "relUrl": "/docs/sql/select_advanced/#group-by-ê·¸ë£¨í•‘í•´ì„œ-ë³´ê¸°"
  },"211": {
    "doc": "ë°ì´í„° ì¡°íšŒ ì‹¬í™”",
    "title": "SELECTë¬¸: ê° ì ˆì˜ ì‘ì„± ìˆœì„œ",
    "content": "(+. í•´ì„/ì‹¤í–‰ë˜ëŠ” ìˆœì„œë„ ë³„ë„ë¡œ í‘œì‹œ) . | SELECT: ëª¨ë“  ì»¬ëŸ¼ ë˜ëŠ” íŠ¹ì • ì»¬ëŸ¼ë“¤ì„ ì¡°íšŒ (í•´ì„ ìˆœì„œ: 5) . | SELECT ì ˆì—ì„œ ì»¬ëŸ¼ ì´ë¦„ì— aliasë¥¼ ë¶™ì¸ ê²Œ ìˆë‹¤ë©´, ì´ ì´í›„ ë‹¨ê³„(ORDER BY, LIMIT)ë¶€í„°ëŠ” í•´ë‹¹ aliasë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤ | MySQLì—ì„œëŠ” GROUP BYë‚˜ HAVINGì—ì„œë„ SELECTì ˆì—ì„œ ë¶™ì¸ aliasë¥¼ ì‚¬ìš© ê°€ëŠ¥ | . | FROM: ì–´ëŠ í…Œì´ë¸”ì„ ëŒ€ìƒìœ¼ë¡œ í•  ì§€ ê²°ì • (í•´ì„ ìˆœì„œ: 1) | WHERE: í•´ë‹¹ í…Œì´ë¸”ì—ì„œ íŠ¹ì • ì¡°ê±´(ë“¤)ì„ ë§Œì¡±í•˜ëŠ” rowë“¤ë§Œ ì„ ë³„ (í•´ì„ ìˆœì„œ: 2) | GROUP BY: rowë“¤ì„ ê·¸ë£¨í•‘ ê¸°ì¤€ì— ë”°ë¼ ê·¸ë£¨í•‘ (í•´ì„ ìˆœì„œ: 3) | HAVING: ê·¸ë£¨í•‘ ì‘ì—… í›„ ìƒì„±ëœ ì—¬ëŸ¬ ê·¸ë£¹ ì¤‘ì—ì„œ, íŠ¹ì • ì¡°ê±´(ë“¤)ì„ ë§Œì¡±í•˜ëŠ” ê·¸ë£¹ë§Œ ì„ ë³„ (í•´ì„ ìˆœì„œ: 4) | ORDER BY: ê° rowë¥¼ íŠ¹ì • ê¸°ì¤€ì— ë”°ë¼ ì •ë ¬ (í•´ì„ ìˆœì„œ: 6) | LIMIT: ì´ì „ ë‹¨ê³„ê¹Œì§€ ì¡°íšŒëœ rowë“¤ ì¤‘ ì¼ë¶€ rowë§Œ ì„ ë³„í•´ì„œ ì¶œë ¥ (í•´ì„ ìˆœì„œ: 7) | . +) ê·¸ ì™¸ ì‘ì„± ìˆœì„œ: https://dev.mysql.com/doc/refman/8.0/en/select.html . ",
    "url": "https://chaelist.github.io/docs/sql/select_advanced/#select%EB%AC%B8-%EA%B0%81-%EC%A0%88%EC%9D%98-%EC%9E%91%EC%84%B1-%EC%88%9C%EC%84%9C",
    "relUrl": "/docs/sql/select_advanced/#selectë¬¸-ê°-ì ˆì˜-ì‘ì„±-ìˆœì„œ"
  },"212": {
    "doc": "ë°ì´í„° ì¡°íšŒ ê¸°ì´ˆ",
    "title": "ë°ì´í„° ì¡°íšŒ ê¸°ì´ˆ",
    "content": ". | SQL ê¸°ì´ˆ . | DB, í…Œì´ë¸” êµ¬ì¡° íŒŒì•…í•˜ê¸° | SQLë¬¸ ì‘ì„± í˜•ì‹ | SELECT, FROM | . | WHERE: ì¡°ê±´ ê±¸ê¸° . | ì¡°ê±´ í‘œí˜„ ë°©ì‹ | AND, OR ì—°ì‚° | . | DATE ë°ì´í„° íƒ€ì… ë‹¤ë£¨ê¸° | ORDER BY: ë°ì´í„° ì •ë ¬ . | CAST(data AS íƒ€ì…) | . | LIMIT: ë°ì´í„° ì¼ë¶€ë§Œ ì¶œë ¥ | . ",
    "url": "https://chaelist.github.io/docs/sql/select_basics/",
    "relUrl": "/docs/sql/select_basics/"
  },"213": {
    "doc": "ë°ì´í„° ì¡°íšŒ ê¸°ì´ˆ",
    "title": "SQL ê¸°ì´ˆ",
    "content": "DB, í…Œì´ë¸” êµ¬ì¡° íŒŒì•…í•˜ê¸° . | ì¡´ì¬í•˜ëŠ” ë°ì´í„°ë² ì´ìŠ¤ íŒŒì•… SHOW DATABASES; . | information_schema / mysql / performance_schema / sysëŠ” MySQLì´ë¼ëŠ” DBMSì˜ êµ¬ë™ì„ ìœ„í•´ ì¡´ì¬í•˜ëŠ” ê¸°ë³¸ ë°ì´í„°ë² ì´ìŠ¤ë“¤ | . | ì¡´ì¬í•˜ëŠ” í…Œì´ë¸” íŒŒì•… SHOW FULL TABLES IN ë°ì´í„°ë² ì´ìŠ¤ëª…; . | ê° í…Œì´ë¸”ì˜ ì´ë¦„ê³¼ í•¨ê»˜ Table_typeì„ íŒŒì•…í•  ìˆ˜ ìˆë‹¤ (BASE TABLE / VIEW) | cf) ê·¸ëƒ¥ SHOW TABLESë¼ê³ ë§Œ í•˜ë©´ type ì •ë³´ ì—†ì´ ê° í…Œì´ë¸”ì˜ ì´ë¦„ë§Œ ì¶œë ¥ë¨ | . | í…Œì´ë¸”ì˜ êµ¬ì¡° íŒŒì•… DESCRIBE í…Œì´ë¸”ëª…; . | í…Œì´ë¸”ì˜ ì»¬ëŸ¼ êµ¬ì¡°, ê° ì»¬ëŸ¼ì˜ ë°ì´í„° íƒ€ì…, ì†ì„±ì„ í™•ì¸ ê°€ëŠ¥ | DESC í…Œì´ë¸”ëª…;ì´ë¼ê³  ì¤„ì—¬ì„œ ì…ë ¥í•´ë„ ë™ì¼ | . ex) Â  . | Field : ì»¬ëŸ¼ì˜ ì´ë¦„ | Type : ì»¬ëŸ¼ì˜ ë°ì´í„° íƒ€ì… | Null : ì»¬ëŸ¼ì˜ Null ì†ì„± ìœ ë¬´ | Key : Primary Key, Unique ì†ì„± ì—¬ë¶€ | Default : ì»¬ëŸ¼ì˜ ê¸°ë³¸ê°’ | Extra : AUTO_INCREMENT ë“±ì˜ ê¸°íƒ€ ì†ì„± | . | Foreign Key ê´€ê³„ íŒŒì•… . | Foreign Key ê´€ê³„ë¥¼ íŒŒì•…í•´ë‘ë©´, ë°ì´í„°ë² ì´ìŠ¤ê°€ ì–´ë–»ê²Œ ì„¤ê³„ë˜ì—ˆëŠ”ì§€, ê° ë°ì´í„°ê°€ ì–´ë–¤ ê´€ê³„ë¥¼ ê°–ëŠ”ì§€ íŒŒì•…í•˜ëŠ” ë°ì— ë„ì›€ì´ ëœë‹¤ | ì•„ë˜ëŠ” MySQLì—ì„œ ì‚¬ìš©ë˜ëŠ” ì½”ë“œ. Foreign Key ì •ë³´ë¥¼ ì¡°íšŒí•˜ëŠ” SQLë¬¸ì€ DBMSë§ˆë‹¤ ì°¨ì´ê°€ ìˆìœ¼ë¯€ë¡œ ìœ ì˜! | . SELECT i.TABLE_SCHEMA, i.TABLE_NAME, i.CONSTRAINT_TYPE, i.CONSTRAINT_NAME, k.REFERENCED_TABLE_NAME, k.REFERENCED_COLUMN_NAME FROM information_schema.TABLE_CONSTRAINTS i LEFT JOIN information_schema.KEY_COLUMN_USAGE k ON i.CONSTRAINT_NAME = k.CONSTRAINT_NAME WHERE i.CONSTRAINT_TYPE = 'FOREIGN KEY'; . | . SQLë¬¸ ì‘ì„± í˜•ì‹ . | SQL ë¬¸ ëì—ëŠ” í•­ìƒ ì„¸ë¯¸ì½œë¡ (;)ì„ ì¨ì¤˜ì•¼í•œë‹¤ | SQL ë¬¸ ì•ˆì—ëŠ” ê³µë°±/ì¤„ë°”ê¿ˆ ë“±ì„ ììœ ë¡­ê²Œ ë„£ì„ ìˆ˜ ìˆë‹¤ (ì‹¤í–‰ì—ëŠ” ì˜í–¥ ì—†ìŒ. ê°€ë…ì„±ì„ ìœ„í•œ ê²ƒ) | SELECT, FROM ë“± ê¸°ë³¸ ì˜ˆì•½ì–´ë“¤ì€ ëŒ€ë¬¸ìë¡œ ì“°ëŠ” ê²Œ ì¢‹ë‹¤ (ëŒ€ì†Œë¬¸ì ì—¬ë¶€ëŠ” ì‹¤í–‰ì—ëŠ” ì˜í–¥ ì—†ìŒ.) | ì‚¬ìš©í•  ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ëª…ì‹œí•´ì£¼ëŠ” ë°©ë²•: . | ë°ì´í„° ì¡°íšŒì‹œ SELECT * FROM dbname.tablename ì´ë ‡ê²Œ ë°ì´í„°ë² ì´ìŠ¤ëª…ë„ í•¨ê»˜ ì ì–´ì¤€ë‹¤ | ì™¼ìª½ SCHEMAS íŒ¨ë„ ë¶€ë¶„ì—ì„œ ì‚¬ìš©í•  ë°ì´í„°ë² ì´ìŠ¤ë¥¼ í´ë¦­í•´ í™œì„±í™”í•´ë‘”ë‹¤ | SQLë¬¸ ë§¨ ì²˜ìŒì— USE dbname;ì´ë¼ê³  ëª…ì‹œí•´ë‘ê³  ì‹œì‘í•œë‹¤ | . | . SELECT, FROM . : ë³´í†µ SELECT ëŒ€ìƒ FROM í…Œì´ë¸”ëª…ì˜ êµ¬ì¡°ê°€ SQL ë°ì´í„° ì¡°íšŒì˜ ê¸°ì´ˆ . | SELECT: í…Œì´ë¸”ì˜ ì–´ë–¤ ì¹¼ëŸ¼ì„ ì¡°íšŒí• ì§€ . | *: í•´ë‹¹ í…Œì´ë¸”ì˜ ëª¨ë“  ì¹¼ëŸ¼ì„ ë‹¤ ì¡°íšŒí•˜ê² ë‹¤ëŠ” ì˜ë¯¸ SELECT * FROM tablename; . | íŠ¹ì • ì¹¼ëŸ¼ëª…ì„ ë„£ì–´ì£¼ë©´ í•´ë‹¹ ì¹¼ëŸ¼ë“¤ë§Œ ì¡°íšŒ SELECT column1, column2 FROM tablename; . | . | FROM: ì–´ë–¤ í…Œì´ë¸”ì˜ ë°ì´í„°ë¥¼ ì¡°íšŒí• ì§€ . | ë°ì´í„°ë² ì´ìŠ¤ëª….í…Œì´ë¸”ëª…ì˜ êµ¬ì¡°ë¡œ ì ì–´ì¤˜ë„ ë˜ê³ , (ê°™ì€ í…Œì´ë¸”ëª…ì´ ì—¬ëŸ¬ DBì— ì¡´ì¬í•œë‹¤ë©´ ì´ëŸ° í˜•íƒœë¡œ ì ì–´ì£¼ëŠ” ê²Œ ì¢‹ë‹¤) SELECT * FROM dbname.tablename; . | ì–´ë–¤ ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì‚¬ìš©í•˜ëŠ”ì§€ ëª…í™•í•œ ê²½ìš°ë¼ë©´ í…Œì´ë¸”ëª…ë§Œ ì ì–´ì¤˜ë„ ëœë‹¤ SELECT * FROM tablename . | . | . ",
    "url": "https://chaelist.github.io/docs/sql/select_basics/#sql-%EA%B8%B0%EC%B4%88",
    "relUrl": "/docs/sql/select_basics/#sql-ê¸°ì´ˆ"
  },"214": {
    "doc": "ë°ì´í„° ì¡°íšŒ ê¸°ì´ˆ",
    "title": "WHERE: ì¡°ê±´ ê±¸ê¸°",
    "content": ". | WHEREë¡œ íŠ¹ì • ì¡°ê±´ì˜ ë°ì´í„°ë§Œ ì¡°íšŒí•  ìˆ˜ ìˆë‹¤ | . ì¡°ê±´ í‘œí˜„ ë°©ì‹ . | ê¸°ì´ˆ . | =: ê°™ìŒ -- â€˜ageâ€™ ì¹¼ëŸ¼ì˜ ê°’ì´ 27ì¸ í–‰(row)ë§Œ ì¡°íšŒ SELECT * FROM tablename WHERE age = 27; . | !=, &gt;&lt;: ê°™ì§€ ì•ŠìŒ -- ì„±ë³„ì´ â€˜mâ€™ì´ ì•„ë‹Œ = ì—¬ì„±ì¸ íšŒì› ë°ì´í„°ë§Œ ì¡°íšŒ SELECT * FROM tablename WHERE gender != â€˜mâ€™; . | IN: ( ) ì•ˆì˜ ì—¬ëŸ¬ ê°’ë“¤ ì¤‘ í•´ë‹¹í•˜ëŠ” ê°’ì´ ìˆëŠ” rowë“¤ë§Œ ì„ íƒ -- ë‚˜ì´ê°€ 20ì´ë‚˜ 30ì¸ íšŒì› ë°ì´í„°ë§Œ ì¡°íšŒ ex) SELECT * FROM tablename WHERE age IN (20, 30); . | NOT IN: ( ) ì•ˆì˜ ì—¬ëŸ¬ ê°’ë“¤ ì¤‘ ì–´ëŠ ê²ƒê³¼ë„ ì¼ì¹˜í•˜ì§€ ì•ŠëŠ” rowë§Œ ì„ íƒ -- ë‚˜ì´ê°€ 20ì´ë‚˜ 30ì´ ì•„ë‹Œ íšŒì› ë°ì´í„°ë§Œ ì¡°íšŒ ex) SELECT * FROM tablename WHERE age NOT IN (20, 30); . | . | ìˆ«ìí˜• ì¹¼ëŸ¼ (INT, DOUBLE ë“±) . | ë¶€ë“±í˜¸ ì´ìš© (&gt;, &lt;, &gt;=, &lt;=) -- ë‚˜ì´ê°€ 27 ì´ìƒì¸ íšŒì› ë°ì´í„°ë§Œ ì¡°íšŒ SELECT * FROM tablename WHERE age &gt;= 27; . | BETWEEN a AND b: aì™€ b ì‚¬ì´ì˜ ê°’ì„ ê°–ëŠ” ë°ì´í„°ë§Œ ì¡°íšŒ (ì–‘ ë(a, b)ë„ í¬í•¨) -- 30ëŒ€ íšŒì› ë°ì´í„°ë§Œ ì¡°íšŒ SELECT * FROM tablename WHERE age BETWEEN 30 AND 39; . | NOTì„ ë¶™ì´ë©´ í•´ë‹¹ ì¡°ê±´ì„ ì œì™¸í•˜ê³  ì¡°íšŒë¨ -- 30ëŒ€ê°€ ì•„ë‹Œ íšŒì› ë°ì´í„°ë§Œ ì¡°íšŒ SELECT * FROM tablename WHERE age NOT BETWEEN 30 AND 39; . | . | ë‚ ì§œí˜• ì¹¼ëŸ¼ (DATE, DATETIME ë“±) . | ìˆ«ìí˜•ì²˜ëŸ¼, ë¶€ë“±í˜¸ë‚˜ BETWEEN ì‚¬ìš© ê°€ëŠ¥ SELECT * FROM tablename WHERE sign_up_day &gt; '2019-01-01'; . SELECT * FROM tablename WHERE sign_up_day BETWEEN '2018-01-01' AND '2018-12-31'; . | . | ë¬¸ìí˜• ì¹¼ëŸ¼ (TEXT ë“±) . | LIKE â€˜íŒ¨í„´â€™ ë§¤ì¹­ -- addressê°€ â€˜ì„œìš¸â€™ë¡œ ì‹œì‘í•˜ëŠ” ëª¨ë“  í–‰ê³¼ ë§¤ì¹­ SELECT * FROM tablename WHERE address LIKE 'ì„œìš¸%'; . -- addressì— â€˜ê³ ì–‘ì‹œâ€™ê°€ í¬í•¨ë˜ì–´ ìˆëŠ” ëª¨ë“  í–‰ê³¼ ë§¤ì¹­ SELECT * FROM tablename WHERE address LIKE '%ê³ ì–‘ì‹œ%'; . | +) NOT LIKE ë§¤ì¹­ë„ ê°€ëŠ¥ -- addressê°€ â€˜ì„œìš¸â€™ë¡œ ì‹œì‘í•˜ì§€ ì•ŠëŠ” ëª¨ë“  í–‰ê³¼ ë§¤ì¹­ SELECT * FROM tablename WHERE address NOT LIKE 'ì„œìš¸%'; . | %: ì„ì˜ì˜ ê¸¸ì´ë¥¼ ê°€ì§„ ë¬¸ìì—´ì„ ë‚˜íƒ€ëƒ„ (0ìë„ í¬í•¨) | _: ë¬¸ì í•˜ë‚˜ë¥¼ ì˜ë¯¸ -- ì´ë©”ì¼ ì£¼ì†Œê°€ cë¡œ ì‹œì‘í•˜ê³ , ê·¸ ë’¤ì— ì„ì˜ì˜ ë¬¸ì 5ê°œê°€ ì˜¨ í›„, @ê°€ ë¶™ê³  ê·¸ ë’¤ì— ë¬´ì–¸ê°€ ì„ì˜ì˜ ê¸¸ì´ë¥¼ ê°€ì§„ ë¬¸ìì—´ì´ ë¶™ëŠ”ë‹¤ëŠ” ì˜ë¯¸ SELECT * FROM tablename WHERE email LIKE â€˜c_____@%â€™; -- candy@google.com ë“±ì˜ ì´ë©”ì¼ ì£¼ì†Œê°€ í•´ë‹¹ íŒ¨í„´ê³¼ ë§¤ì¹­ë¨ . | . +) ë¬¸ìí˜•ì¡°íšŒ Escaping: \\(backslash)ë¥¼ ì‚¬ìš© . | ex) \\% â€“ ì‹¤ì œ %(í¼ì„¼íŠ¸ ê¸°í˜¸)ë¥¼ í¬í•¨í•œ ë¬¸ìë¥¼ ì°¾ì„ ìˆ˜ ìˆìŒ | â€˜(ì‘ì€ ë”°ì˜´í‘œ), _(ì–¸ë”ë°”) ,â€(í° ë”°ì˜´í‘œ)ë„ \\ë¥¼ í™œìš©í•´ escape | . +) ëŒ€ì†Œë¬¸ì êµ¬ë¶„í•´ì„œ ì¡°íšŒí•˜ê¸° . | Table collation ì„¤ì •ì´ ci(Case Insensitive) ì„¤ì •ìœ¼ë¡œ ë˜ì–´ ìˆìœ¼ë©´, ë¬¸ìí˜• ë°ì´í„°ë¥¼ ì¡°íšŒí•  ë•Œ gë‚˜ Gê°€ ë˜‘ê°™ì´ ê°„ì£¼ë¨. | ì´ëŸ´ ë•Œ gì™€ Gë¥¼ êµ¬ë¶„í•´ì„œ ì¡°íšŒí•˜ê³  ì‹¶ìœ¼ë©´, BINARYë¼ê³  ì¨ì£¼ë©´ ëœë‹¤ -- ì†Œë¬¸ì gê°€ í¬í•¨ëœ ë¬¸ìì—´ë§Œ ì¡°íšŒ SELECT * FROM tablename WHERE sentence LIKE BINARY â€˜%g%â€™; . -- ëŒ€ë¬¸ì Gê°€ í¬í•¨ëœ ë¬¸ìì—´ë§Œ ì¡°íšŒ ex) SELECT * FROM tablename WHERE sentence LIKE BINARY â€˜%G%â€™; . | . | ANY, ALL . | ANY: ì—¬ëŸ¬ ì¡°ê±´ ì¤‘ í•˜ë‚˜ë¼ë„ ë§Œì¡±ë˜ë©´ TRUEê°€ ë°˜í™˜ë¨ (â€˜SOMEâ€™ì„ ì‚¬ìš©í•´ë„ ë™ì¼) SELECT * FROM tablename WHERE view_count &gt; ANY(150000, 250000, 300000); -- 150000, 250000, 300000 ì¤‘ í•˜ë‚˜ë³´ë‹¤ í° view_countë¥¼ ê°€ì§€ê³  ìˆìœ¼ë©´ ì¶œë ¥ë¨ -- (ì‚¬ì‹¤ ì´ ê²½ìš°ëŠ” ê·¸ëƒ¥ MINì„ ì‚¬ìš©í•˜ëŠ” ê²ƒê³¼ ë™ì¼) . | ALL: ì—¬ëŸ¬ ì¡°ê±´ ëª¨ë‘ê°€ ë§Œì¡±ë˜ë©´ TRUEê°€ ë°˜í™˜ë¨ SELECT * FROM tablename WHERE view_count &gt; ALL(150000, 250000, 300000); -- 150000, 250000, 300000 ì¤‘ ëª¨ë‘ë³´ë‹¤ í° view_countë¥¼ ê°€ì§€ê³  ìˆìœ¼ë©´ ì¶œë ¥ë¨ -- (ì‚¬ì‹¤ ì´ ê²½ìš°ëŠ” ê·¸ëƒ¥ MAXë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒê³¼ ë™ì¼) . | . | . AND, OR ì—°ì‚° . : ì—¬ëŸ¬ ê°œì˜ ì¡°ê±´ ê±¸ê¸° . | AND . | ex) ë‚¨ì„±ì´ë©´ì„œ ì£¼ì†Œê°€ ì„œìš¸ë¡œ ì‹œì‘í•˜ëŠ” íšŒì› ë°ì´í„° ì¡°íšŒ SELECT * FROM tablename WHERE gender = â€˜mâ€™ AND address LIKE â€˜ì„œìš¸%; . | ex) ì¡°ê±´ 3ê°œë¥¼ ì—°ê²°: ë‚¨ì„±ì´ë©´ì„œ ì£¼ì†Œê°€ ì„œìš¸ì´ê³  ë‚˜ì´ê°€ 25~29ì¸ íšŒì› ë°ì´í„° ì¡°íšŒ SELECT * FROM tablename WHERE gender = â€˜mâ€™ AND address LIKE â€˜ì„œìš¸% AND age BETWEEN 25 and 29; . | . | OR . | ex) ë´„(3~5ì›”) í˜¹ì€ ê°€ì„(9~11ì›”)ì— ê°€ì…í•œ íšŒì› ë°ì´í„° ì¡°íšŒ SELECT * FROM tablename WHERE MONTH(sign_up_day) BETWEEN 3 AND 5 OR MONTH(sign_up_day) BEWEEN 9 AND 11; . | ì£¼ì˜ì‚¬í•­: WHERE id = 1 OR id = 2ì™€ ê°™ì´ ê°™ì€ ì¹¼ëŸ¼ì—ì„œì˜ ì¡°ê±´ì„ í•¨ê»˜ ì ëŠ” ê²½ìš°, WHERE id = 1 OR 2 ì´ë ‡ê²Œ ì ìœ¼ë©´ ì•ˆëœë‹¤! (ë‘ ì¡°ê±´ì˜ ì¹¼ëŸ¼ì´ ê°™ë”ë¼ë„ ë‹¤ ì ì–´ì¤˜ì•¼ í•¨) | +) ì‚¬ì‹¤, WHERE id = 1 OR id = 2 ê°™ì€ ê²½ìš°ëŠ” WHERE id IN (1, 2) ë¼ê³  í‘œí˜„í•˜ë©´ ë” ì§§ê²Œ ê°€ëŠ¥ | . | ANDì™€ OR ë³µí•© . | ex) 180 ì´ìƒì¸ ë‚¨ì íšŒì›, ë˜ëŠ” 170 ì´ìƒì¸ ì—¬ì íšŒì› ë°ì´í„° ì¡°íšŒ SELECT * FROM tablename WHERE (gender = 'm' AND height &gt;= 180) OR (gender = 'f' AND height &gt;= 170); . | ANDì™€ ORë¥¼ ì„ì–´ì„œ ì‚¬ìš©í•  ê²½ìš°, ì–´ë–¤ ë¶€ë¶„ì„ ë¨¼ì € ê³ ë ¤í•´ì•¼ í•˜ëŠ”ì§€ ()ë¡œ í‘œì‹œí•´ì£¼ëŠ” ê²Œ ì¢‹ë‹¤! | ()ë¡œ ìš°ì„ ìˆœìœ„ë¥¼ í‘œì‹œí•˜ì§€ ì•Šìœ¼ë©´, ANDê°€ ORë³´ë‹¤ ìš°ì„ ìˆœìœ„ê°€ ë†’ê²Œ ê°„ì£¼ëœë‹¤. (AND ë¶€ë¶„ì´ ë¨¼ì € ì‹¤í–‰ë¨) | í•˜ì§€ë§Œ ê·¸ëƒ¥ ANDì´ë“  ORì´ë“  ë¨¼ì € ì‹¤í–‰ë˜ì–´ì•¼ í•˜ëŠ” ë¶€ë¶„ì— ()ë¥¼ ì”Œì›Œ í‘œì‹œí•´ì£¼ëŠ” ìŠµê´€ì„ ë“¤ì´ë©´ ë” ì§ê´€ì ìœ¼ë¡œ ì´í•´í•˜ê¸° ì‰¬ìš´ ì½”ë“œê°€ ëœë‹¤ | . | . ",
    "url": "https://chaelist.github.io/docs/sql/select_basics/#where-%EC%A1%B0%EA%B1%B4-%EA%B1%B8%EA%B8%B0",
    "relUrl": "/docs/sql/select_basics/#where-ì¡°ê±´-ê±¸ê¸°"
  },"215": {
    "doc": "ë°ì´í„° ì¡°íšŒ ê¸°ì´ˆ",
    "title": "DATE ë°ì´í„° íƒ€ì… ë‹¤ë£¨ê¸°",
    "content": ". | YEAR, MONTH, DAYOFMONTH, DAYOFWEEK . | YEAR í•¨ìˆ˜: ë‚ ì§œì—ì„œ ì—°ë„ë§Œ ì¶”ì¶œ -- 1992ë…„ì— íƒœì–´ë‚œ íšŒì› ë°ì´í„°ë§Œ ì¡°íšŒ SELECT * FROM tablename WHERE YEAR(birthday) = â€˜1992â€™; . | MONTH í•¨ìˆ˜: ë‚ ì§œì—ì„œ ì›”ë§Œ ì¶”ì¶œ -- ì—¬ë¦„(6, 7, 8ì›”)ì— ê°€ì…í•œ íšŒì› ë°ì´í„°ë§Œ ì¡°íšŒ SELECT * FROM tablename WHERE MONTH(sign_up_day) IN (6, 7, 8); . | DAYOFMONTH í•¨ìˆ˜: ë‚ ì§œì—ì„œ ì¼ë§Œ ì¶”ì¶œ -- ê° ë‹¬ì˜ í›„ë°˜ë¶€(15ì¼~31ì¼)ì— ê°€ì…í•œ íšŒì› ë°ì´í„°ë§Œ ì¡°íšŒ SELECT * FROM tablename WHERE DAYOFMONTH(sign_up_day) BETWEEN 15 AND 31; . | DAYOFWEEK í•¨ìˆ˜: ë‚ ì§œì—ì„œ ìš”ì¼ì„ ì¶”ì¶œ -- í† ìš”ì¼ì— ê°€ì…í•œ íšŒì› ë°ì´í„°ë§Œ ì¡°íšŒ (1: ì¼ìš”ì¼, 2: ì›”ìš”ì¼, ..., 7: í† ìš”ì¼) SELECT * FROM tablename WHERE DAYOFWEEK(sign_up_day) = 7; . | . | ë‚ ì§œ ê°„ ì°¨ì´ êµ¬í•˜ê¸°: DATEDIFF . | DATEDIFF(ë‚ ì§œ a, ë‚ ì§œ b) â†’ â€˜ë‚ ì§œ a - ë‚ ì§œ bâ€™ë¥¼ í•´ì„œ ê·¸ ì°¨ì´ ì¼ìˆ˜ë¥¼ ì•Œë ¤ì¤€ë‹¤ | ex) DATEDIFF(â€™2018-01-05â€™, â€™2018-01-03â€™)ì˜ ê°’ì€ 2 -- ê°€ì…í•œ ë‚ ë¡œë¶€í„° 2019.01.01ê¹Œì§€ì˜ ê¸°ê°„ì„ í•¨ê»˜ ì¡°íšŒ SELECT email, sign_up_day, DATEDIFF(sign_up_day, â€˜2019-01-01â€™) FROM tablename; . | +) CURDATE(): ì˜¤ëŠ˜ ë‚ ì§œë¥¼ êµ¬í•˜ëŠ” í•¨ìˆ˜ -- ê°€ì…í•œ ë‚ ë¡œë¶€í„° ì˜¤ëŠ˜ ë‚ ì§œê¹Œì§€ì˜ ê¸°ê°„ì„ í•¨ê»˜ ì¡°íšŒ SELECT email, sign_up_day, DATEDIFF(sign_up_day, CURDATE()) FROM tablename; . | +) DATEDIFF(ë‚ ì§œ a, ë‚ ì§œ b) / 365 ì´ë ‡ê²Œ ê³„ì‚°í•˜ë©´ ê¸°ê°„ì„ â€˜ì—°(year)â€™ ë‹¨ìœ„ë¡œ í™•ì¸í•  ìˆ˜ ìˆìŒ | +) DATEDIFF(ë‚ ì§œ a, ë‚ ì§œ b)ëŠ” ë‚ ì§œ a - ë‚ ì§œ bì™€ ë™ì¼ (ì–´ëŠ ë‚ ì§œê°€ ë” í°ì§€ ì•„ëŠ” ê²½ìš°, ë‹¨ìˆœ ë¹¼ê¸°ë¡œë„ ê³„ì‚° ê°€ëŠ¥) | . | DATE_FORMAT(datetime, â€˜custom_formatâ€™) . | datetime í¬ë§·ì˜ ë‚ ì§œë¥¼ ì›í•˜ëŠ” í˜•íƒœë¡œ ì¶œë ¥í•´ì¤Œ -- ë¡œê·¸ì¸ ë‚ ì§œë¥¼ '2022-03-04'ì™€ ê°™ì€ í¬ë§·ìœ¼ë¡œ ì¶œë ¥ SELECT DATE_FORMAT(login_date, '%Y-%m-%d') FROM tablename; . | . | STR_TO_DATE(date_string, â€˜assigned_formatâ€™) . | ë¬¸ìì—´ í¬ë§·ìœ¼ë¡œ ë‹´ê¸´ ë‚ ì§œë¥¼ datetime í¬ë§·ìœ¼ë¡œ ë³€í™˜í•´ì¤€ë‹¤. ë¬¸ìì—´ ë‚ ì§œê°€ ì–´ë–¤ í˜•íƒœë¡œ ë‹´ê²¨ìˆëŠ”ì§€ ê·¸ í˜•ì‹ì„ ì˜ ì ì–´ì¤˜ì•¼ í•¨. -- '20220304'ì™€ ê°™ì€ í˜•ì‹ì˜ ë¬¸ìì—´ë¡œ ë‹´ê²¨ ìˆë˜ ë¡œê·¸ì¸ ë‚ ì§œë¥¼ datetimeìœ¼ë¡œ ë°”ê¿”ì„œ ì¶œë ¥ SELECT STR_TO_DATE(login_date_str, '%Y%m%d') FROM tablename; . | . | . ",
    "url": "https://chaelist.github.io/docs/sql/select_basics/#date-%EB%8D%B0%EC%9D%B4%ED%84%B0-%ED%83%80%EC%9E%85-%EB%8B%A4%EB%A3%A8%EA%B8%B0",
    "relUrl": "/docs/sql/select_basics/#date-ë°ì´í„°-íƒ€ì…-ë‹¤ë£¨ê¸°"
  },"216": {
    "doc": "ë°ì´í„° ì¡°íšŒ ê¸°ì´ˆ",
    "title": "ORDER BY: ë°ì´í„° ì •ë ¬",
    "content": ". | ì˜¤ë¦„ì°¨ìˆœ ì •ë ¬: ASC -- í‚¤ê°€ ì‘ì€ íšŒì›ë¶€í„° ìˆœì„œëŒ€ë¡œ ì¶œë ¥ SELECT * FROM tablename ORDER BY height ASC; . | +) ì•„ë˜ì™€ ê°™ì´ ASCë¥¼ ì•ˆì¨ì¤˜ë„ ASCê°€ defaultì´ê¸° ë•Œë¬¸ì— ë˜‘ê°™ì´ ì˜¤ë¦„ì°¨ìˆœ ì •ë ¬ë¡œ ì‹¤í–‰ëœë‹¤ (ê·¸ë ‡ì§€ë§Œ ë‚˜ì¤‘ì— ë³´ê³  ë¹¨ë¦¬ ì´í•´í•˜ê¸° ìœ„í•´ì„  ASCë¥¼ ì¨ì£¼ëŠ” ê²Œ ë” ì¢‹ìŒ) SELECT * FROM tablename ORDER BY height; . | . | ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬: DESC -- í‚¤ê°€ í° íšŒì›ë¶€í„° ìˆœì„œëŒ€ë¡œ ì¶œë ¥ SELECT * FROM tablename ORDER BY height DESC; . | +) nullê°’ì€ ê°€ì¥ ì‘ì€ ê°’ìœ¼ë¡œ ì·¨ê¸‰ë¨ | +) TEXT íƒ€ì… ì¹¼ëŸ¼ì˜ ê²½ìš°, ASCìœ¼ë¡œ ì •ë ¬í•˜ë©´ ì•ŒíŒŒë²³ ìˆœì„œëŒ€ë¡œ ì •ë ¬ë¨ . | +) ì—¬ëŸ¬ ê¸°ì¤€ì„ ë‘ê³  ì •ë ¬: â€˜ORDER BY ê¸°ì¤€1, ê¸°ì¤€2â€™ ì´ë ‡ê²Œ ì“°ë©´ ê¸°ì¤€1ë¶€í„° ìš°ì„  ì •ë ¬ë¨ -- ê°€ì… ì—°ë„ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ìš°ì„  ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬í•œ í›„, ê°€ì… ì—°ë„ê°€ ê°™ì€ íšŒì›ë“¤ì€ ì´ë©”ì¼ ê¸°ì¤€ìœ¼ë¡œ ì˜¤ë¦„ì°¨ìˆœ ì •ë ¬ë¨ ex) SELECT sign_up_day, email FROM tablename ORDER BY YEAR(sign_up_day) DESC, email ASC; . | . CAST(data AS íƒ€ì…) . : ë°ì´í„° íƒ€ì… ì¼ì‹œì ìœ¼ë¡œ ë°”ê¿”ì£¼ê¸° . | TEXT íƒ€ì… ì¹¼ëŸ¼ì— ìˆëŠ” ìˆ«ìê°’ì„ ì •ë ¬í•˜ë©´, 120 &lt; 19 &lt; 230 &lt; 27ì˜ ìˆœì„œë¡œ ì˜¤ë¦„ì°¨ìˆœ ì •ë ¬ë¨. (TEXT íƒ€ì…ì€ INT íƒ€ì…ê³¼ ë‹¬ë¦¬, ì²«ë²ˆì§¸ ìë¦¬ë¶€í„° í•œ ë¬¸ì í•œë¬¸ìì”© ë¹„êµí•´ì„œ ì •ë ¬í•˜ëŠ” ê²ƒì´ê¸° ë•Œë¬¸) | TEXT íƒ€ì…ì¸ ì¹¼ëŸ¼ì— ìˆëŠ” ìˆ«ìê°’ì„ ìˆ«ìí˜•ì²˜ëŸ¼ ì •ë ¬í•´ì£¼ê³  ì‹¶ìœ¼ë©´, CAST í•¨ìˆ˜ë¡œ ë°ì´í„° íƒ€ì…ì„ ì¼ì‹œì ìœ¼ë¡œ ë³€ê²½í•´ì£¼ë©´ ëœë‹¤! SELECT * FROM tablename ORDER BY CAST(data AS signed) ASC; -- ì´ë ‡ê²Œ ì¨ì£¼ë©´ 19 &lt; 27 &lt; 120 &lt; 230 ì´ë ‡ê²Œ ì˜ ì •ë ¬ë¨ . | signed: ì–‘ìˆ˜ì™€ ìŒìˆ˜ë¥¼ í¬í•¨í•œ ëª¨ë“  ì •ìˆ˜ë¥¼ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆëŠ” ë°ì´í„° íƒ€ì…. | +) decimal: ì†Œìˆ˜ì ì´ ìˆëŠ” ìˆ˜ë¥¼ ë‚˜íƒ€ë‚´ëŠ” íƒ€ì… | . | . ",
    "url": "https://chaelist.github.io/docs/sql/select_basics/#order-by-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%A0%95%EB%A0%AC",
    "relUrl": "/docs/sql/select_basics/#order-by-ë°ì´í„°-ì •ë ¬"
  },"217": {
    "doc": "ë°ì´í„° ì¡°íšŒ ê¸°ì´ˆ",
    "title": "LIMIT: ë°ì´í„° ì¼ë¶€ë§Œ ì¶œë ¥",
    "content": ". | â€˜LIMIT nâ€™: ë§¨ ì²˜ìŒë¶€í„° nê°œì˜ ë°ì´í„°ë§Œ ì¶œë ¥ -- ê°€ì¥ ìµœê·¼ì— ê°€ì…í•œ íšŒì› 10ëª…ì˜ ë°ì´í„°ë§Œ ì¶œë ¥ SELECT * FROM tablename ORDER BY sign_up_day DESC LIMIT 10; . | â€˜LIMIT m, nâ€™: m+1ë²ˆì§¸ ë°ì´í„°ë¶€í„° nê°œì˜ ë°ì´í„°ë§Œ ì¶œë ¥ (0, 1, 2, â€¦ , m ì´ëŸ° ì‹ìœ¼ë¡œ í–‰ì„ ì„¸ëŠ” ê²ƒì´ë¼, mì€ m+1ë²ˆì§¸ë¥¼ ì˜ë¯¸) -- ê°€ì…ì¼ìê°€ ê°€ì¥ ìµœê·¼ì¸ íšŒì›ë¶€í„° ì •ë ¬ëœ ìƒíƒœì—ì„œ, 9ë²ˆì§¸ &amp; 10ë²ˆì§¸ rowì˜ ë°ì´í„°ë§Œ ì¶œë ¥ -- (indexê°€ 8, 9ì¸ row) SELECT * FROM tablename ORDER BY sign_up_day DESC LIMIT 8, 2; . | . ",
    "url": "https://chaelist.github.io/docs/sql/select_basics/#limit-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%9D%BC%EB%B6%80%EB%A7%8C-%EC%B6%9C%EB%A0%A5",
    "relUrl": "/docs/sql/select_basics/#limit-ë°ì´í„°-ì¼ë¶€ë§Œ-ì¶œë ¥"
  },"218": {
    "doc": "Selenium",
    "title": "Selenium",
    "content": ". | Selenium ê¸°ì´ˆ . | driver ì‹¤í–‰ ë° ì¢…ë£Œ | ì›¹ë¸Œë¼ìš°ì € ì œì–´í•˜ê¸° | find_element(s) | click() | send_keys() | . | Selenium ì¶”ê°€ ê¸°ëŠ¥ë“¤ . | ë¡œë”© ëŒ€ê¸° (Wait) | iframe ì† ìš”ì†Œ ë‹¤ë£¨ê¸° | Select ìš”ì†Œ ë‹¤ë£¨ê¸° | Scroll Down (Keys í™œìš©) | Headless ëª¨ë“œë¡œ ì´ìš© | . | window(ì°½) ì œì–´ . | ì°½ í¬ê¸° ì¡°ì ˆ ë° ìŠ¤í¬ë¦°ìƒ· | ì›¹ ë¸Œë¼ìš°ì € ì°½ ì—¬ëŸ¬ ê°œ ë‹¤ë£¨ê¸° | . | . *Selenium: ì›¹ ë¸Œë¼ìš°ì €ë¥¼ ìë™í™”í•˜ëŠ” ë„êµ¬. ì›¹ ìë™ í…ŒìŠ¤íŠ¸, ë°ì´í„° ìˆ˜ì§‘ ë“±ì— ì‚¬ìš© . ",
    "url": "https://chaelist.github.io/docs/webscraping/selenium/",
    "relUrl": "/docs/webscraping/selenium/"
  },"219": {
    "doc": "Selenium",
    "title": "Selenium ê¸°ì´ˆ",
    "content": "*Selenium ì‚¬ìš©ì„ ìœ„í•œ ì¤€ë¹„ ì‚¬í•­: . | pipì´ë‚˜ condaë¥¼ í™œìš©í•´ì„œ installí•´ì¤˜ì•¼ í•œë‹¤ | driverë¥¼ ë‹¤ìš´ë°›ì•„ì•¼ í•¨ (í¬ë¡¬ë“œë¼ì´ë²„ ë‹¤ìš´) | . driver ì‹¤í–‰ ë° ì¢…ë£Œ . | driver ì‹¤í–‰ . | ì•„ë˜ ì½”ë“œëŒ€ë¡œ ì‹¤í–‰í•˜ë©´, chromedriverê°€ ì¼œì§„ë‹¤ | . from selenium import webdriver # importí•´ì¤˜ì•¼ ì‚¬ìš© ê°€ëŠ¥ driver = webdriver.Chrome('driver/chromedriver.exe') ## driver ê²½ë¡œ ì ì–´ì£¼ê¸° # driverê°€ ê°™ì€ íŒŒì¼ì— ìˆìœ¼ë©´ ê·¸ëƒ¥ ì´ë ‡ê²Œë§Œ ì¨ë„ ëœë‹¤: `driver = webdriver.Chrome()` . | URL ì ‘ì†í•˜ê¸° driver.get('https://chaelist.github.io/') # chromedriverê°€ í•´ë‹¹ urlì— ì ‘ì†ëœë‹¤ . | driver ì¢…ë£Œ . | driverë¡œ í•„ìš”í•œ ì‘ì—…ì„ ëª¨ë‘ ìˆ˜í–‰í•œ í›„ì—ëŠ” ì¢…ë£Œí•˜ëŠ” ê²ƒì´ ì¢‹ë‹¤ | . driver.quit() ## ì°½ì´ í•˜ë‚˜ë§Œ ì—´ë ¤ ìˆë‹¤ë©´ `driver.close()`ì´ë¼ê³  í•´ë„ ë¨ . | . ì›¹ë¸Œë¼ìš°ì € ì œì–´í•˜ê¸° . | í˜„ì¬ url ê°€ì ¸ì˜¤ê¸° driver.current_url . | ë’¤ë¡œê°€ê¸°(ì´ì „ í˜ì´ì§€ë¡œ ì´ë™) driver.back() . | ì•ìœ¼ë¡œê°€ê¸°(ì´í›„ í˜ì´ì§€ë¡œ ì´ë™) driver.forward() . | ìƒˆë¡œê³ ì¹¨í•˜ê¸° driver.refresh() . | . find_element(s) . | find_element ê³„ì—´ì€ ì²«ë²ˆì§¸ë¡œ ì°¾ì•„ì§€ëŠ” ìš”ì†Œ í•˜ë‚˜ë§Œì„ ë°˜í™˜ | find_elements ê³„ì—´ì€ list í˜•íƒœë¡œ ì°¾ì•„ì§€ëŠ” ëª¨ë“  ìš”ì†Œë¥¼ ë°˜í™˜ | . | find_element(s)_by_css_selector element = driver.find_element_by_css_selector('summary.fs-5') # &lt;summary class=\"fs-5\"&gt; íƒœê·¸ . | elementë¥¼ ë°›ì•„ì™€ì„œ í´ë¦­í•˜ê±°ë‚˜, ê°’ì„ ì…ë ¥í•˜ê±°ë‚˜, í•„ìš”í•œ textë¥¼ ì¶”ì¶œí•˜ë©´ ëœë‹¤ element.text ## .textë¥¼ í•´ì£¼ë©´ í•´ë‹¹ elementì— ë“¤ì–´ìˆëŠ” textë¥¼ ì¶”ì¶œí•  ìˆ˜ ìˆìŒ . 'Python ê¸°ì´ˆ' . | cf) element.get_attribute('innerHTML')ë„ element.textì™€ ê°™ì´ ì•ˆì— ìˆëŠ” textë¥¼ ì¶”ì¶œ | cf)element.get_attribute('outerHTML'): íƒœê·¸ì™€ textë¥¼ ëª¨ë‘ ë³´ì—¬ì¤Œ element.get_attribute('outerHTML') . '&lt;summary class=\"fs-5 fw-500\"&gt; Python ê¸°ì´ˆ &lt;/summary&gt;' . | . +) ì¼ì¹˜í•˜ëŠ” ëª¨ë“  ìš”ì†Œë¥¼ listë¡œ ë°›ì•„ì˜¤ê¸° . element_list = driver.find_elements_by_css_selector('summary.fs-5') [element.text for element in element_list] # listì˜ ê° ìš”ì†Œì— ì ‘ê·¼í•´ textë¥¼ ì¶”ì¶œ . ['Python ê¸°ì´ˆ', 'Data Handling', 'Numpy', 'Pandas', 'ë°ì´í„° ì‹œê°í™”', 'Web Scraping'] . | find_element(s)_by_tag_name element = driver.find_element_by_tag_name('footer') # &lt;footer&gt; íƒœê·¸ . | find_element(s)_by_class_name element = driver.find_element_by_class_name('main') # &lt;div class=\"main\"&gt; íƒœê·¸ . | find_element_by_id . | idëŠ” ëŠ˜ í•˜ë‚˜ë¿ì´ê¸°ì—, find_element_by_idëŠ” ì¡´ì¬í•˜ì§€ ì•ŠëŠ”ë‹¤ | . element = driver.find_element_by_id('contents') # &lt;h1 id=\"contents\"&gt; íƒœê·¸ . | find_element(s)_by_xpath . | xpath(XML Path Language)ë¥¼ í†µí•´ íŠ¹ì • elementë¥¼ ì ‘ê·¼í•  ìˆ˜ ìˆë‹¤ | ê°œë°œì ëª¨ë“œ(ctrl+shift+i)ì—ì„œ íŠ¹ì • íƒœê·¸ì˜ xpathë¥¼ ì‰½ê²Œ ë³µì‚¬ ê°€ëŠ¥. | ë³´í†µ, ë³µì‚¬í•´ ì˜¨ xpathë¥¼ â€œ\"â€xpathâ€â€â€ ì´ë ‡ê²Œ ì ì–´ì¤€ë‹¤ (í”íˆ xpath ì•ˆì— ë”°ì˜´í‘œê°€ ë“¤ì–´ê°€ê¸°ì—, â€œâ€â€ ì•ˆì— ë„£ì–´ì£¼ëŠ” ê²ƒ) | . element = driver.find_element_by_xpath(\"\"\"//*[@id=\"main-content\"]/footer/p\"\"\") . | . click() . | element.click(): í•´ë‹¹ elementë¥¼ í´ë¦­í•˜ëŠ” íš¨ê³¼ | . # naver.comì—ì„œ [NAVER ë¡œê·¸ì¸] ë²„íŠ¼ ëˆ„ë¥´ê¸° driver = webdriver.Chrome('driver/chromedriver.exe') driver.get('https://www.naver.com/') driver.find_element_by_css_selector(\"a.link_login\").click() . send_keys() . | element.send_keys(): í•´ë‹¹ elementì— ê°’ì„ ì…ë ¥í•˜ëŠ” íš¨ê³¼ | . # (ìœ„ ì½”ë“œì—ì„œ ì—°ê²°) IDë¥¼ ì…ë ¥í•˜ëŠ” ë¶€ë¶„ì„ ì°¾ì•„ì„œ, 'Your ID'ë¼ëŠ” ê°’ì„ ì…ë ¥í•˜ê¸° driver.find_element_by_css_selector(\"#id\").send_keys('Your ID') . â†’ ë„¤ì´ë²„ ë¡œê·¸ì¸ ìë™í™” ì½”ë“œ FULL . driver = webdriver.Chrome('driver/chromedriver.exe') driver.get('https://www.naver.com/') driver.find_element_by_css_selector(\"a.link_login\").click() # [NAVER ë¡œê·¸ì¸] ë§í¬ ëˆ„ë¥´ê¸° driver.find_element_by_css_selector(\"#id\").send_keys('Your ID') # IDë¥¼ ì…ë ¥ driver.find_element_by_css_selector(\"#pw\").send_keys('Your Password') # Passwordë¥¼ ì…ë ¥ driver.find_element_by_id(\"log.login\").click() # [ë¡œê·¸ì¸] ë²„íŠ¼ ëˆ„ë¥´ê¸° . ",
    "url": "https://chaelist.github.io/docs/webscraping/selenium/#selenium-%EA%B8%B0%EC%B4%88",
    "relUrl": "/docs/webscraping/selenium/#selenium-ê¸°ì´ˆ"
  },"220": {
    "doc": "Selenium",
    "title": "Selenium ì¶”ê°€ ê¸°ëŠ¥ë“¤",
    "content": "ë¡œë”© ëŒ€ê¸° (Wait) . Seleniumì€ ì›¹ê³¼ ì§ì ‘ ìƒí˜¸ì‘ìš©í•˜ê¸° ë•Œë¬¸ì—, ì›¹ì˜ ìš”ì†Œë“¤ì´ ë¡œë”©ë  ë•Œê¹Œì§€ ê¸°ë‹¤ë¦¬ì§€ ì•Šìœ¼ë©´ errorê°€ ë‚  ë•Œê°€ ìˆë‹¤. ì›¹ ìš”ì†Œë“¤ì´ ë¡œë”©ë  ë•Œê¹Œì§€ ê¸°ë‹¤ë ¤ì£¼ëŠ” ê²ƒì„ Waitì´ë¼ê³  í•œë‹¤. | Implicit Wait . | ì²˜ìŒì— í•œ ë²ˆ ì„¤ì •í•´ ì£¼ë©´, ì°¾ìœ¼ë ¤ëŠ” ì›¹ ìš”ì†Œê°€ ì—†ì„ ë•Œë§ˆë‹¤ ìµœëŒ€ Xì´ˆë¥¼ ì•”ë¬µì ìœ¼ë¡œ ê¸°ë‹¤ë ¤ ì¤€ë‹¤ | ë§Œì•½ ì„¤ì •í•œ ì‹œê°„ë³´ë‹¤ ë¹¨ë¦¬ ìš”ì†Œê°€ ì°¾ì•„ì§€ë©´, ë” ê¸°ë‹¤ë¦¬ì§€ ì•Šê³  ë‹¤ìŒ ì½”ë“œë¡œ ë„˜ì–´ê°„ë‹¤ | ex) driver.implicitly_wait(3): ì›¹ ìš”ì†Œê°€ ì¡´ì¬í•  ë•Œê¹Œì§€ (ì°¾ì•„ì§ˆ ë•Œê¹Œì§€) ìµœëŒ€ 3ì´ˆë¥¼ ê¸°ë‹¤ë ¤ì¤€ë‹¤ | . driver = webdriver.Chrome('driver/chromedriver.exe') driver.implicitly_wait(3) # ì´ë ‡ê²Œ driverì— ë§¨ ì²˜ìŒ í•œ ë²ˆë§Œ ì„¤ì •í•´ë‘ë©´ ëœë‹¤ driver.get('https://www.naver.com/') driver.find_element_by_css_selector(\"a.link_login\").click() # [NAVER ë¡œê·¸ì¸] ë²„íŠ¼ ëˆ„ë¥´ê¸° . | time.sleep() . | time.sleep()ë„ ì½”ë“œ ì§„í–‰ì„ ë©ˆì¶°ì„œ ë¡œë”©ì„ ê¸°ë‹¤ë ¤ ì¤„ ìˆ˜ ìˆëŠ” ë°©ë²• ì¤‘ í•˜ë‚˜. | time.sleep(3)ì„ ì½”ë“œ ì¤‘ê°„ì— ë„£ì–´ì£¼ë©´, ì •í™•íˆ 3ì´ˆê°„ ë©ˆì·„ë‹¤ê°€ ì•„ë˜ ì½”ë“œë¡œ ë„˜ì–´ê°„ë‹¤ | . import time # importí•´ì¤˜ì•¼ ì‚¬ìš© ê°€ëŠ¥ driver = webdriver.Chrome('driver/chromedriver.exe') driver.get('https://www.naver.com/') time.sleep(3) # í˜ì´ì§€ê°€ ë¡œë”©ë˜ê¸¸ 3ì´ˆ ë™ì•ˆ ê¸°ë‹¤ë ¸ë‹¤ê°€ ì•„ë˜ ì½”ë“œë¡œ ë„˜ì–´ê°„ë‹¤ driver.find_element_by_css_selector(\"a.link_login\").click() # [NAVER ë¡œê·¸ì¸] ë²„íŠ¼ ëˆ„ë¥´ê¸° . â€»implicitly_wait(3)ì€ ì°¾ê³ ì í•˜ëŠ” ìš”ì†Œê°€ 1ì´ˆë§Œì— ì°¾ì•„ì§€ë©´ ë” ê¸°ë‹¤ë¦¬ì§€ ì•Šê³  ì§„í–‰ë˜ì§€ë§Œ, time.sleep(3)ì€ ë°˜ë“œì‹œ 3ì´ˆë¥¼ ëª¨ë‘ ë©ˆì·„ë‹¤ê°€ ì§„í–‰ë˜ê¸°ì— implictly_wait(3)ì´ ë” íš¨ìœ¨ì ì¸ ê²½ìš°ê°€ ë§ë‹¤ . | Explicit Wait . | íŠ¹ì • elementì— ëŒ€í•´ ì–´ë–¤ ìƒíƒœê°€ ë  ë•Œê¹Œì§€ ìµœëŒ€ ì–¼ë§ˆë‚˜ ê¸°ë‹¤ë ¤ì¤„ ê²ƒì¸ì§€ë¥¼ ì¼ì¼ì´ ëª…ì‹œí•´ì£¼ëŠ” ê²ƒ. | Implicit Waitê³¼ Explicit Waitì„ ë™ì‹œì— ì‚¬ìš©í•˜ëŠ” ê²ƒì€ ê¶Œì¥ë˜ì§€ ì•ŠëŠ”ë‹¤. | . ## ì•„ë˜ 3ê°€ì§€ë¥¼ ì¶”ê°€ë¡œ importí•´ì¤˜ì•¼ í•œë‹¤ from selenium.webdriver.common.by import By from selenium.webdriver.support.ui import WebDriverWait from selenium.webdriver.support import expected_conditions as EC driver = webdriver.Chrome('driver/chromedriver.exe') driver.get('https://chaelist.github.io/') # &lt;summary class=\"fs-5\"&gt; íƒœê·¸ê°€ ì¡´ì¬í•  ë•Œê¹Œì§€ ìµœëŒ€ 5ì´ˆë¥¼ ê¸°ë‹¤ë ¤ì„œ elementë¥¼ ë°›ì•„ì˜¨ë‹¤ element = WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.CSS_SELECTOR, \"summary.fs-5\"))) element.click() # ìœ„ì—ì„œ ë°›ì•„ì˜¨ elementë¥¼ í´ë¦­ . *ë‹¤ì–‘í•œ waitì¡°ê±´ë“¤* . | presence_of_element_located(): elementê°€ ì¡´ì¬í•  ë•Œê¹Œì§€ ê¸°ë‹¤ë¦¼ . | íŠ¹ì • ê°’ì„ ì¶”ì¶œí•´ì˜¤ëŠ” ëª©ì ì´ë¼ë©´ presenceë§Œ ì²´í¬í•´ë„ ê´œì°®ë‹¤ | . | visibility_of_element_located(): elementê°€ ì‹¤ì œë¡œ ë³´ì´ëŠ” ìƒíƒœê°€ ë  ë•Œê¹Œì§€ ê¸°ë‹¤ë¦¼ . | elementì™€ interactí•´ì„œ ë°˜ì‘ì„ ì²´í¬í•˜ëŠ” ë“±ì˜ ëª©ì ì´ë¼ë©´ visibilityê¹Œì§€ ì²´í¬í•´ì£¼ëŠ” ê²Œ ì¢‹ë‹¤ | . | element_to_be_clickable(): elementê°€ í´ë¦­ ê°€ëŠ¥í•œ ìƒíƒœê°€ ë  ë•Œê¹Œì§€ ê¸°ë‹¤ë¦¼ . | elementë¥¼ ì°¾ì•„ì„œ í´ë¦­í•˜ë ¤ëŠ” ëª©ì ì´ë¼ë©´ clickableí•œì§€ ì²´í¬í•´ì£¼ëŠ” ê²Œ ì¢‹ë‹¤ | . | invisibility_of_element_located(): elementê°€ ì•ˆ ë³´ì¼ ë•Œê¹Œì§€ ê¸°ë‹¤ë¦¼. | text_to_be_present_in_element(): element ì•ˆì— í…ìŠ¤íŠ¸ê°€ ë¡œë”©ë  ë•Œê¹Œì§€ ê¸°ë‹¤ë¦¼. | text_to_be_present_in_element(locator, í™•ì¸í•˜ê³ ì í•˜ëŠ” text) ì´ë ‡ê²Œ ë‘ ê°œì˜ ê°’ì„ ë„£ì–´ì¤˜ì•¼ í•œë‹¤ | . | . â†’ ë„¤ì´ë²„ ë¡œê·¸ì¸ ìë™í™” ì½”ë“œ ê° ì¤„ì— Explicit Wait ì‚¬ìš©í•´ë³´ê¸° . from selenium import webdriver from selenium.webdriver.common.by import By from selenium.webdriver.support.ui import WebDriverWait from selenium.webdriver.support import expected_conditions as EC driver = webdriver.Chrome('driver/chromedriver.exe') driver.get('https://www.naver.com/') wait = WebDriverWait(driver, 3) # ì•„ë˜ì—ì„œ ëª¨ë‘ ê°™ì€ ì‹œê°„(3ì´ˆ)ì„ ì„¤ì •í•´ì¤„ ê±°ë©´ ì´ë ‡ê²Œ ë¯¸ë¦¬ ì €ì¥í•´ë‘¬ë„ ëœë‹¤ # [NAVER ë¡œê·¸ì¸] ë§í¬ê°€ clickable ìƒíƒœê°€ ë  ë•Œê¹Œì§€ ìµœëŒ€ 3ì´ˆ ê¸°ë‹¤ë ¤ì„œ í´ë¦­í•˜ê¸° login_link = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, 'a.link_login'))) login_link.click() # ID ì…ë ¥ì°½ì´ visibleí•  ë•Œê¹Œì§€ ìµœëŒ€ 3ì´ˆ ê¸°ë‹¤ë ¤ì„œ ID ì…ë ¥í•˜ê¸° id_box = wait.until(EC.visibility_of_element_located((By.CSS_SELECTOR, '#id'))) id_box.send_keys('Your ID') # IDë¥¼ ì…ë ¥ # Password ì…ë ¥ì°½ì´ visibleí•  ë•Œê¹Œì§€ ìµœëŒ€ 3ì´ˆ ê¸°ë‹¤ë ¤ì„œ Password ì…ë ¥í•˜ê¸° password_box = wait.until(EC.visibility_of_element_located((By.CSS_SELECTOR, '#pw'))) password_box.send_keys('Your Password') # [ë¡œê·¸ì¸] ë²„íŠ¼ì´ clickable ìƒíƒœê°€ ë  ë•Œê¹Œì§€ ìµœëŒ€ 3ì´ˆ ê¸°ë‹¤ë ¤ì„œ í´ë¦­í•˜ê¸° login_button = wait.until(EC.element_to_be_clickable((By.ID, 'log.login'))) login_button.click() # [ë¡œê·¸ì¸] ë²„íŠ¼ ëˆ„ë¥´ê¸° . | . iframe ì† ìš”ì†Œ ë‹¤ë£¨ê¸° . | íƒœê·¸: HTML ë¬¸ì„œ ì•ˆì— ë˜ ë‹¤ë¥¸ HTML ë¬¸ì„œë¥¼ ì‚½ì…í•  ë•Œ ì‚¬ìš©í•˜ëŠ” íƒœê·¸. | í”íˆ ê´‘ê³  ê°™ì€ ì„œë“œíŒŒí‹° ì½˜í…ì¸ ë¥¼ í•˜ë‚˜ì˜ iframe ì•ˆì— ë‹´ëŠ”ë‹¤ | . | iframe íƒœê·¸ëŠ” ì‹¤ì œ HTML ë¬¸ì„œë¥¼ ë‹´ê³  ìˆëŠ” ê²ƒì´ ì•„ë‹ˆë¼ HTML ë¬¸ì„œë¥¼ ì°¸ì¡°í•˜ê¸° ë•Œë¬¸ì—, iframe ì•ˆì— ìˆëŠ” elementê°€ ê°œë°œìë„êµ¬ì—ì„œëŠ” ì°¾ì•„ì§„ë‹¤ í•´ë„ Seleniumìœ¼ë¡œ ì°¾ìœ¼ë ¤ê³  í•˜ë©´ ì‹¤íŒ¨í•œë‹¤ . ex) naver.comì˜ ì•„ë˜ â€˜íŠ¸ë Œë“œì‡¼í•‘â€™ ì˜ì—­ì€ í•˜ë‚˜ì˜ iframe ì•ˆì— ë‹´ê²¨ ìˆë‹¤ . â†’ li.goods_item (ê´‘ê³  ì•„ì´í…œ ë¦¬ìŠ¤íŠ¸)ì€ iframe ì•ˆì— ë“¤ì–´ìˆê¸°ì—, ì•„ë˜ì™€ ê°™ì´ ì½”ë“œë¥¼ ì“°ë©´ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ë‹¤ . # ì›í•˜ëŠ” ê²°ê³¼ê°€ ë‚˜ì˜¤ì§€ ì•ŠëŠ” ì½”ë“œ driver = webdriver.Chrome('driver/chromedriver.exe') driver.get('https://www.naver.com/') driver.implicitly_wait(3) element_list = driver.find_elements_by_css_selector('li.goods_item') [element.text for element in element_list] . [] . | iframeì— ìˆëŠ” ì½”ë“œë¥¼ ì ‘ê·¼í•˜ë ¤ë©´ iframeìœ¼ë¡œ ì´ë™í•´ì•¼ í•œë‹¤ â€“ driver.switch_to.frame()ì„ ì‚¬ìš©í•˜ë©´ í”„ë ˆì„ìœ¼ë¡œ ì´ë™í•˜ëŠ” ê²ƒì´ ê°€ëŠ¥ . *li.goods_item (ê´‘ê³  ì•„ì´í…œ ë¦¬ìŠ¤íŠ¸)ê°€ ë‹´ê¸´ iframeì€ &lt;iframe id=\"shopcast_iframe\"&gt; . â†’ í•´ë‹¹ iframeìœ¼ë¡œ ì´ë™í•˜ë ¤ë©´ ë‹¤ìŒê³¼ ê°™ì€ ì½”ë“œ ì¤‘ í•˜ë‚˜ë¥¼ ì¨ì£¼ë©´ ëœë‹¤ . # 1) ì›¹ ìš”ì†Œ íŒŒë¼ë¯¸í„°ë¡œ ì‚¬ìš© driver.switch_to.frame(driver.find_element_by_css_selector('#shopcast_iframe') # 2) iframeì˜ id ë˜ëŠ” name ì†ì„±ê°’ ì‚¬ìš© driver.switch_to.frame('shopcast_iframe') # 3)iframe ì¸ë±ìŠ¤ ì‚¬ìš© driver.switch_to.frame(2) ##ì°¾ì€ iframe íƒœê·¸ê°€ 3ë²ˆì§¸ iframeì´ì—¬ì„œ . | ìµœì¢…: #shopcast_iframe iframeìœ¼ë¡œ ì´ë™í•´ì„œ li.goods_item elementì— ì ‘ê·¼, text ê°€ì ¸ì˜¤ê¸° . driver = webdriver.Chrome('driver/chromedriver.exe') driver.get('https://www.naver.com/') time.sleep(1) # 'mainFrame'ìœ¼ë¡œ ì´ë™ driver.switch_to.frame('shopcast_iframe') element_list = driver.find_elements_by_css_selector('li.goods_item') [element.text for element in element_list] # ì•„ë˜ textëŠ” ê´‘ê³ ê°€ ê·¸ë•Œê·¸ë•Œ ë‹¬ë¼ì§€ë¯€ë¡œ ê³„ì† ê°€ì ¸ì˜¬ë•Œë§ˆë‹¤ ë°”ë€œ. ['ì˜¬ê²¨ìš¸ ìµœê°•í•œíŒŒ~\\ní¬ê·¼í•˜ê³ ë”°ëœ»í•œì˜·', 'ì‚´ê· ìˆ˜ê°€ëŒ€ì‹ í•´ìš”\\në³€ê¸°ì‚´ê· ì²­ì†Œê¹Œì§€', 'ì„œë‘˜ëŸ¬ìš”~ìµœëŒ€80%\\nê³ ë¯¼í•˜ë©´ ë†“ì³ìš”!', 'ì´ë ‡ê²Œ ì‰¬ì›Œë„ë¼?\\nì™„ì „~ì¤‘ë…ëì–´!', 'ì´ë ‡ê²Œ ë§›ìˆëŠ”ë°\\nì™œë„ˆë§Œëª°ë¼?', 'ìµœëŒ€ 50% SALE\\nì‹œì¦Œì˜¤í”„ ì§„í–‰ì¤‘!', 'ë¶ê·¹ë°œí•œíŒŒ ì¶”ìœ„~\\nBESTë¡±íŒ¨ë”©64%â†“', 'ë¯¸ë ¤í•œ ë””ìì¸~\\ní•œëˆˆì— ë°˜í–ˆì–´!', 'ìì‹ ìˆëŠ” í€„ë¦¬í‹°\\nê·€ê±¸ì´ ë‹¹ì¼ë°œì†¡', 'ìš´ë™ë³µ ì¼ìƒë³µOK\\nê²¨ìš¸ì´ˆíŠ¹ê°€ì„¸ì¼', 'ë¹„ì‹¸ê²Œì‚¬ì§€ë§ˆ~\\ní€„ë¦¬í‹° ë¹„êµë¶ˆê°€!', 'KF94 100ì¥ìŸì—¬\\nì¥ë‹¹200ì›ëŒ€ íŠ¹ê°€', 'ê·¸ìœ½í•œ ë¶„ìœ„ê¸° ì•„ì´íŒ”ë ˆíŠ¸', 'ëª©ìš•íƒ•ëŒ€ì‹  ì´ê±°! ì§€ê¸ˆ3+1'] . | . +) Nested iframes . | â€˜Aâ€™ iframe ì•ˆì— â€˜Bâ€™ iframeì´ ë“¤ì–´ìˆë‹¤ë©´, ë©”ì¸ í˜ì´ì§€ì—ì„œ ë°”ë¡œ Bë¡œ ë“¤ì–´ê°ˆ ìˆ˜ëŠ” ì—†ë‹¤. ì•„ë˜ì™€ ê°™ì´ ìˆœì„œëŒ€ë¡œ switchí•´ì¤˜ì•¼ í•¨. | . driver.get('URL') driver.switch_to.frame('A') driver.switch_to.frame('B') . +) iframeì—ì„œ ë°–ìœ¼ë¡œ / ìƒìœ„ iframeìœ¼ë¡œ ë‚˜ì˜¤ê¸° . # ìƒìœ„ iframeìœ¼ë¡œ ì´ë™ driver.switch_to.parent_frame() # ì›¹ í˜ì´ì§€ / ìµœìƒìœ„ í”„ë ˆì„ / ë©”ì¸ í”„ë ˆì„ìœ¼ë¡œ ì´ë™ driver.switch_to.default_content() . Select ìš”ì†Œ ë‹¤ë£¨ê¸° . *Selectìš”ì†Œ: ë“œë¡­ë‹¤ìš´ ë©”ë‰´ì—ì„œ ì—¬ëŸ¬ ì˜µì…˜ ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒí•˜ëŠ” í˜•íƒœì˜ ì›¹ ìš”ì†Œ (ì•„ë˜ ì˜ˆì‹œ) . | Select ìš”ì†ŒëŠ” select íƒœê·¸ë¡œ ë¼ìˆê³ , ì•ˆì— ìˆëŠ” ì˜µì…˜ë“¤ì€ option íƒœê·¸ë¡œ ë˜ì–´ ìˆë‹¤. | select íƒœê·¸ ì˜ˆì‹œ: &lt;select id='cityCode' name='cityCode' tabindex=\"3\"&gt; | option íƒœê·¸ ì˜ˆì‹œ: &lt;option value=\"1100\" &gt;ì„œìš¸íŠ¹ë³„ì‹œ&lt;/option&gt; | . | . *Select ìš”ì†ŒëŠ” ê° elementë¥¼ ì°¾ì•„ì„œ í´ë¦­í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ë‹¤ë¤„ë„ ë˜ì§€ë§Œ, ì•„ë˜ì™€ ê°™ì´ Seleniumì˜ â€˜Selectâ€™ë¥¼ ì‚¬ìš©í•˜ë©´ í›¨ì”¬ ê°„ë‹¨í•˜ê²Œ ë‹¤ë£° ìˆ˜ ìˆë‹¤. from selenium.webdriver.support.ui import Select # Select íˆ´ì„ importí•´ì¤€ë‹¤ driver = webdriver.Chrome('driver/chromedriver.exe') driver.get('http://info.nec.go.kr/main/showDocument.xhtml?electionId=0020200415&amp;topMenuId=CP&amp;secondMenuId=CPRI03') driver.implicitly_wait(3) # 'êµ­íšŒì˜ì›ì„ ê±°'íƒ­ í´ë¦­ driver.find_element_by_css_selector('#electionId2').click() # select ì›¹ ìš”ì†Œë¥¼ ì°¾ì•„ì„œ Select ì•ˆì— ë„£ì–´ ì¤€ë‹¤ cityCode_select = Select(driver.find_element_by_css_selector('#cityCode')) # &lt;select id='cityCode'&gt; íƒœê·¸ # 'ì„œìš¸íŠ¹ë³„ì‹œ' option ì„ íƒ cityCode_select.select_by_visible_text('ì„œìš¸íŠ¹ë³„ì‹œ') . cf) Select ìš”ì†Œì—ì„œ optionì„ ì„ íƒí•˜ëŠ” ë‹¤ì–‘í•œ ë°©ë²• . # 1) ì˜µì…˜ ì´ë¦„ìœ¼ë¡œ ì„ íƒ (ì›¹ì‚¬ì´íŠ¸ì—ì„œ ë³´ì´ëŠ” ì˜µì…˜ ì´ë¦„) cityCode_select.select_by_visible_text('ì„œìš¸íŠ¹ë³„ì‹œ') # 2) ì˜µì…˜ì˜ valueë¡œ ì„ íƒ ('ì„œìš¸íŠ¹ë³„ì‹œ' ì˜µì…˜ì˜ valueëŠ” 1100) cityCode_select.select_by_value('1100') # 3) ì˜µì…˜ì˜ ì¸ë±ìŠ¤ë¡œ ì„ íƒ ('ì„œìš¸íŠ¹ë³„ì‹œ'ëŠ” ë‘ ë²ˆì§¸ ì˜µì…˜) cityCode_select.select_by_index(1) . Scroll Down (Keys í™œìš©) . scroll downí•´ì¤˜ì•¼ë§Œ ì•„ë˜ì— ìˆëŠ” elementê°€ ë¡œë”©ë˜ëŠ” ê²½ìš°ê°€ ë§ë‹¤. | 100ë²ˆ scroll downí•˜ê¸° from selenium.webdriver.common.keys import Keys ## Keysë¥¼ importí•´ì¤€ë‹¤ body = driver.find_element_by_tag_name(\"body\") # &lt;body&gt; element ì €ì¥í•´ë‘ê¸° pagedowns = 1 while pagedowns &lt; 100: body.send_keys(Keys.PAGE_DOWN) #scroll down ### ---ì‘ì—…--- ### pagedowns += 1 driver.quit() . | íŠ¹ì • ì¡°ê±´ì´ ë§Œì¡±ë  ë•Œê¹Œì§€ scroll down #--ì˜ˆì‹œ1---------------------------------------------- body = driver.find_element_by_tag_name(\"body\") month = 0 while int(month) &lt; 10: ## 1ì›”~9ì›”ê¹Œì§€ë§Œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ë ¤ëŠ” ê²½ìš° body.send_keys(Keys.PAGE_DOWN) #scroll down ### ---ì‘ì—…--- ### #--ì˜ˆì‹œ2---------------------------------------------- body = driver.find_element_by_tag_name(\"body\") app_list = [] while 'ì• ë‹ˆíŒ¡' not in app_list: ## 'ì• ë‹ˆíŒ¡' ë°ì´í„°ê°€ ìˆ˜ì§‘ë  ë•Œê¹Œì§€ scroll down body.send_keys(Keys.PAGE_DOWN) #scroll down ### ---ì‘ì—…--- ### . | . Headless ëª¨ë“œë¡œ ì´ìš© . *Headless ëª¨ë“œ: ì›¹ ë¸Œë¼ìš°ì €ê°€ ëˆˆì— ë³´ì´ì§€ ì•ŠëŠ” ìƒíƒœë¡œ, ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰ë˜ëŠ” ëª¨ë“œ . | Headless ëª¨ë“œë¥¼ ì‚¬ìš©í•˜ë©´ ì»´í“¨í„°ì˜ ìì›(CPU, RAM ë“±)ì´ ëœ ì†Œëª¨ë˜ê³ , ë” ë¹ ë¥´ê²Œ ë™ì‘í•œë‹¤ | . from selenium.webdriver.chrome.options import Options # Optionsë¥¼ ë³„ë„ë¡œ import options = Options() options.add_argument(\"--headless\") ## headless ëª¨ë“œë¡œ ì„¤ì • options.add_argument(\"window-size=1920,1080\") ## ë¸Œë¼ìš°ì € ì°½ í¬ê¸° ì„¤ì •(1920x1080) - ë„ˆë¬´ í¬ê¸°ê°€ ì‘ìœ¼ë©´ ë¡œë”©ì´ ì˜ ì•ˆë  ìˆ˜ ìˆë‹¤ driver = webdriver.Chrome('driver/chromedriver.exe', options=options) ## -- ì‘ì—… -- ## . ",
    "url": "https://chaelist.github.io/docs/webscraping/selenium/#selenium-%EC%B6%94%EA%B0%80-%EA%B8%B0%EB%8A%A5%EB%93%A4",
    "relUrl": "/docs/webscraping/selenium/#selenium-ì¶”ê°€-ê¸°ëŠ¥ë“¤"
  },"221": {
    "doc": "Selenium",
    "title": "window(ì°½) ì œì–´",
    "content": "ì°½ í¬ê¸° ì¡°ì ˆ ë° ìŠ¤í¬ë¦°ìƒ· . | í’€ìŠ¤í¬ë¦°ìœ¼ë¡œ ë³´ê¸° driver.fullscreen_window() . | ì°½ í¬ê¸° ìµœëŒ€í™”(â–¡) driver.maximize_window() . | ì°½ í¬ê¸° ìµœì†Œí™”(-) driver.minimize_window() . | ì°½ ì½”ê¸° ì¡°ì ˆ driver.set_window_size(800, 600) # ì°½ í¬ê¸° (800, 600)ìœ¼ë¡œ ì„¤ì • . | ìŠ¤í¬ë¦°ìƒ· ì €ì¥ driver.get_screenshot_as_file('image.png') # image.pngë¼ëŠ” íŒŒì¼ì— ìŠ¤í¬ë¦°ìƒ· ì €ì¥ ## driver.save_screenshot('image.png)ë¼ê³  í•´ë„ ë™ì¼í•œ íš¨ê³¼ . +) íŠ¹ì • ë¶€ë¶„ë§Œ ìº¡ì²˜í•˜ê¸° . # ì˜ˆì‹œ: naver.com ë©”ì¸ ë°°ë„ˆ ê´‘ê³  ë¶€ë¶„ë§Œ ìº¡ì²˜í•˜ê¸° driver = webdriver.Chrome('driver/chromedriver.exe') driver.get('https://www.naver.com/') element = driver.find_element_by_css_selector('#veta_top') # ìº¡ì²˜í•  elementë¥¼ ì„ íƒí•´ì¤€ë‹¤ element_png = element.screenshot_as_png with open(\"naver_main_banner.png\", \"wb\") as file: ## naver_main_baneer.pngë¼ëŠ” íŒŒì¼ì— ì €ì¥ file.write(element_png) driver.quit() . | . ì›¹ ë¸Œë¼ìš°ì € ì°½ ì—¬ëŸ¬ ê°œ ë‹¤ë£¨ê¸° . : ì›¹ì‚¬ì´íŠ¸ì—ì„œ ë§í¬ë¥¼ í´ë¦­í•˜ë©´ ìƒˆë¡œìš´ ì°½ì— ì›¹ì‚¬ì´íŠ¸ê°€ ì—´ë¦¬ëŠ” ê²½ìš°ì— ìœ ìš©. ex) ì¿ íŒ¡ì˜ ê²½ìš°, ìƒí’ˆì„ í´ë¦­í•˜ë©´ ìƒí’ˆì˜ ìƒì„¸ í˜ì´ì§€ê°€ ìƒˆë¡œìš´ ì°½ìœ¼ë¡œ ì—´ë¦°ë‹¤. | ì¿ íŒ¡ â€˜ì»¤í”¼â€™ ê²€ìƒ‰ ê²°ê³¼ í˜ì´ì§€ ì ‘ì† â†’ ì²«ë²ˆì§¸ item í´ë¦­ import time from selenium import webdriver driver = webdriver.Chrome('driver/chromedriver.exe') driver.implicitly_wait(3) # ì¿ íŒ¡ 'ì»¤í”¼' ê²€ìƒ‰ ê²°ê³¼ í˜ì´ì§€ ì ‘ì† driver.get('https://www.coupang.com/np/search?component=&amp;q=%EC%BB%A4%ED%94%BC&amp;channel=user') time.sleep(1) # ì²«ë²ˆì§¸ itemì„ ì°¾ì•„ì„œ í´ë¦­ products = driver.find_elements_by_css_selector('li.search-product') products[0].click() . â†’ ì²« ë²ˆì§¸ ì•„ì´í…œì— ëŒ€í•œ ìƒì„¸ í˜ì´ì§€ê°€ ìƒˆë¡œìš´ íƒ­ì— ì—´ë¦°ë‹¤. | â€» í•˜ì§€ë§Œ í˜„ì¬ ì›¹ ë¸Œë¼ìš°ì €ì—ì„œ ì—´ë¦° íƒ­ê³¼, Seleniumì´ í¬ì»¤ìŠ¤ëœ íƒ­ì€ ë‹¤ë¥´ë‹¤ | Seleniumì´ í¬ì»¤ìŠ¤ëœ íƒ­ì€, Selenium ì›¹ ë“œë¼ì´ë²„ê°€ í˜„ì¬ â€˜ë‹¤ë£¨ê³  ìˆëŠ”â€™ íƒ­ìœ¼ë¡œ, ì§ì ‘ ì„¤ì •í•´ì¤˜ì•¼ ë°”ë€ë‹¤. | ê·¸ë ‡ê¸° ë•Œë¬¸ì— ì•„ë˜ì™€ ê°™ì´ ë‘ë²ˆì§¸ itemì„ í´ë¦­í•˜ëŠ” ì½”ë“œë¥¼ ì¨ì¤˜ë„ ìì—°ìŠ¤ëŸ½ê²Œ ì‹¤í–‰ë˜ì–´ ìƒˆë¡œìš´ íƒ­ì´ 1ê°œ ë” ì—´ë¦°ë‹¤. (íƒ­ ì´ 3ê°œ) | . products[1].click() . | Seleniumì´ í¬ì»¤ìŠ¤ëœ íƒ­ ë°”ê¾¸ê¸°: driver.switch_to.window(driver.window_handles[i]) # ì—´ë ¤ ìˆëŠ” íƒ­ ë¦¬ìŠ¤íŠ¸ ì¡°íšŒ print(driver.window_handles) # í˜„ì¬ í¬ì»¤ìŠ¤ëœ íƒ­ -- ì•„ì§ íƒ­ì„ ë°”ê¿”ì£¼ì§€ ì•Šì•˜ê¸° ë•Œë¬¸ì— ì²˜ìŒ .get()ìœ¼ë¡œ ì ‘ì†í•œ íƒ­ (=ì²«ë²ˆì§¸ íƒ­) print(driver.current_window_handle) # ë‘ ë²ˆì§¸ íƒ­ìœ¼ë¡œ ë°”ê¿”ì£¼ê¸° driver.switch_to.window(driver.window_handles[1]) # í˜„ì¬ í¬ì»¤ìŠ¤ëœ íƒ­ -- ì´ì œ ë‘ë²ˆì§¸ íƒ­ìœ¼ë¡œ ë°”ë€Œì—ˆìŒì„ í™•ì¸ ê°€ëŠ¥ print(driver.current_window_handle) . ['CDwindow-9C390D79522E1EAE4B301DD80A05D3FA', 'CDwindow-83BB35AAF356770405A2644797340DE6', 'CDwindow-E2CFF69328DFC8E29C4DF6DA6F16967F'] CDwindow-9C390D79522E1EAE4B301DD80A05D3FA CDwindow-83BB35AAF356770405A2644797340DE6 . | driver.closeë¡œ í˜„ì¬ í¬ì»¤ìŠ¤ëœ íƒ­ ë‹«ê¸° . | ë” ì´ìƒ ì•ˆì“°ëŠ” íƒ­ì„ ê³„ì† ì—´ì–´ë‘ë©´ ìì›ì´ ì†Œëª¨ë˜ê¸°ì— ê·¸ë•Œê·¸ë•Œ ë‹«ì•„ì£¼ëŠ” ê²ƒì´ ì¢‹ë‹¤. | . driver.close() # íƒ­ ë‹«ê¸° driver.switch_to.window(driver.window_handles[0]) # í¬ì»¤ìŠ¤ëœ íƒ­ì´ ì—†ì–´ì¡Œìœ¼ë¯€ë¡œ, ë‹¤ì‹œ ì²«ë²ˆì§¸ íƒ­ìœ¼ë¡œ í¬ì»¤ìŠ¤ë¥¼ ì´ë™ì‹œí‚¨ë‹¤ . | . ",
    "url": "https://chaelist.github.io/docs/webscraping/selenium/#window%EC%B0%BD-%EC%A0%9C%EC%96%B4",
    "relUrl": "/docs/webscraping/selenium/#windowì°½-ì œì–´"
  },"222": {
    "doc": "Semantic Network Analysis",
    "title": "Semantic Network Analysis",
    "content": ". | Semantic NA: English . | ì „ì²˜ë¦¬ (Preprocessing) | Semantic Network í˜•ì„± | ë„¤íŠ¸ì›Œí¬ ì‹œê°í™” | Centrality ê³„ì‚° | . | Semantic NA: í•œê¸€ . | ì „ì²˜ë¦¬ (Preprocessing) | Semantic Network í˜•ì„± | ë„¤íŠ¸ì›Œí¬ ì‹œê°í™” | Centrality ê³„ì‚° | . | . *Semantic Network Analysis: ì–¸ì–´ ë„¤íŠ¸ì›Œí¬ ë¶„ì„. íŠ¹ì • ë¬¸ì„œ ë‚´ ë‹¨ì–´ë“¤ ì‚¬ì´ì˜ ê´€ê³„ë¥¼ ì‹œê°í™”í•´ ë‚´ìš©ì„ í•œ ëˆˆì— íŒŒì•…í•  ìˆ˜ ìˆê²Œ í•œë‹¤. ",
    "url": "https://chaelist.github.io/docs/network_analysis/semantic_network/",
    "relUrl": "/docs/network_analysis/semantic_network/"
  },"223": {
    "doc": "Semantic Network Analysis",
    "title": "Semantic NA: English",
    "content": ". | New York Times ê¸°ì‚¬ë¥¼ ì˜ˆì‹œë¡œ í™œìš© | ê¸°ì‚¬ ë‚´ìš© ë° ë‹¨ì–´ ì‚¬ì´ì˜ ê´€ê³„ë¥¼ í•œ ëˆˆì— íŒŒì•…í•˜ëŠ” ê²ƒì´ ëª©ì  | . # ë¶„ì„ì— ì‚¬ìš©í•  text ì¤€ë¹„ import requests from bs4 import BeautifulSoup url = 'https://www.nytimes.com/2017/06/12/well/live/having-friends-is-good-for-you.html' r = requests.get(url) soup = BeautifulSoup(r.text, 'lxml') title = soup.title.text.strip() content = soup.find('section', attrs={'name':'articleBody'}).text content . 'Hurray for the HotBlack Coffee cafe in Toronto for declining to offer Wi-Fi to its customers. There are other such cafes, to be sure, including seven of the eight New York City locations of CafÃ© Grumpy.But itâ€™s HotBlackâ€™s reason for the electronic blackout that is cause for hosannas. As its president, Jimson Bienenstock, explained, his aim is to get customers to talk with one another instead of being buried in their portable devices.â€œItâ€™s about creating a social vibe,â€ he told a New York Times reporter. â€œWeâ€™re a vehicle for human interaction, otherwise itâ€™s just a commodity.â€ (ìƒëµ) . ì „ì²˜ë¦¬ (Preprocessing) . *ì˜ì–´ ì „ì²˜ë¦¬ì— ëŒ€í•œ ìì„¸í•œ ì„¤ëª… . | Text Cleaning import re filtered_content = re.sub('[^.,?!\\s\\w]','',content) # ì •ê·œí‘œí˜„ì‹ re í™œìš©. filtered_content = filtered_content.replace('Mr.', 'Mr').replace('Dr.', 'Dr') # ì¶”ê°€ë¡œ ë” ì œê±° . | Case Conversion filtered_content = filtered_content.lower() . | Tokenization import nltk word_tokens = nltk.word_tokenize(filtered_content) . | POS tagging tokens_pos = nltk.pos_tag(word_tokens) . | Select Noun words NN_words = [] for word, pos in tokens_pos: if 'NN' in pos: NN_words.append(word) . | Lemmatization # nltkì˜ WordNetLemmatizerì„ ì´ìš© wlem = nltk.WordNetLemmatizer() lemmatized_words = [] for word in NN_words: new_word = wlem.lemmatize(word) lemmatized_words.append(new_word) . | Stopwords removal # 1ì°¨ì ìœ¼ë¡œ nltkì—ì„œ ì œê³µí•˜ëŠ” ë¶ˆìš©ì–´ì‚¬ì „ì„ ì´ìš©í•´ì„œ ë¶ˆìš©ì–´ë¥¼ ì œê±° from nltk.corpus import stopwords stopwords_list = stopwords.words('english') #nltkì—ì„œ ì œê³µí•˜ëŠ” ë¶ˆìš©ì–´ì‚¬ì „ ì´ìš© unique_NN_words = set(lemmatized_words) #setì„ ì‚¬ìš©í•´ ì¤‘ë³µ ì œê±° final_NN_words = lemmatized_words for word in unique_NN_words: if word in stopwords_list: while word in final_NN_words: final_NN_words.remove(word) # ì•„ë˜ì™€ ê°™ì´ ì¶”ê°€ë¡œ ì§ì ‘ ë§Œë“  ë¶ˆìš©ì–´ì‚¬ì „ì„ ì´ìš©í•´ ë¶ˆìš©ì–´ ì œê±° customized_stopwords = ['be', 'today', 'yesterday', 'new', 'york', 'time'] # ì§ì ‘ ë§Œë“  ë¶ˆìš©ì–´ ì‚¬ì „ unique_NN_words1 = set(final_NN_words) for word in unique_NN_words1: if word in customized_stopwords: while word in final_NN_words: final_NN_words.remove(word) ## final_NN_words ì¶œë ¥í•´ë³´ê¸° print(final_NN_words) . ['hurray', 'hotblack', 'coffee', 'cafe', 'toronto', 'wifi', 'customer', 'cafe', 'city', 'location', 'cafÃ©', 'grumpy.but', 'hotblacks', 'reason', 'blackout', 'cause', 'hosanna', 'president', 'jimson', 'bienenstock', 'aim', 'customer', 'devices.its', 'vibe', 'vehicle', 'interaction', 'commodity.what', 'idea', 'bienenstock', 'science', 'decade', 'interaction', 'contributor', 'health', 'evidence', 'value', 'connection', 'experience', 'morning', 'walk', 'woman', 'swim', 'locker', 'room', 'ymca', 'use', 'device', 'locker', 'room', 'experience', 'friend', 'share', 'joy', 'sorrow', 'woman', 'problem', 'board', 'advice', 'counsel', 'laugh', 'day.and', 'study', 'life.as', 'harvard', 'health', 'watch', 'dozen', 'study', 'people', 'relationship', 'family', 'friend', 'community', 'health', 'problem', 'longer.in', 'study', 'men', 'woman', 'county', 'calif.', 'berkman', 'syme', 'people', 'others', 'nineyear', 'study', 'people', 'tie', 'john', 'robbins', 'book', 'health', 'longevity', 'difference', 'survival', 'people', 'age', 'gender', 'health', 'practice', 'health', 'status', 'fact', 'researcher', 'tie', 'lifestyle', 'smoking', 'obesity', 'lack', 'exercise', 'tie', 'living', 'habit', 'mr', 'robbins', 'people', 'lifestyle', 'tie', 'all.in', 'study', 'journal', 'medicine', 'researcher', 'health', 'insurance', 'plan', 'men', 'heart', 'attack', 'connection', 'people', 'quarter', 'risk', 'death', 'year', 'connectedness.researchers', 'duke', 'university', 'center', 'tie', 'death', 'people', 'condition', 'brummett', (ìƒëµ)] . | . Semantic Network í˜•ì„± . | Frequency Analysis . | ë‹¨ì–´ì˜ ë¹ˆë„ë¥¼ íŒŒì•…í•˜ê³ , ê°€ì¥ ë§ì´ ë‚˜ì˜¤ëŠ” ë‹¨ì–´ 10ê°œë¥¼ ì¶”ì¶œ | . from collections import Counter c = Counter(final_NN_words) print(c.most_common(10)) # ê°€ì¥ ë¹ˆë²ˆí•˜ê²Œ ë‚˜ì˜¤ëŠ” 10ê°œì˜ ë‹¨ì–´ ì¶œë ¥ # ê°€ì¥ ë§ì´ ë‚˜ì˜¤ëŠ” ë‹¨ì–´ 10ê°œ ì €ì¥ list_of_words = [] for word, count in c.most_common(10): list_of_words.append(word) print(list_of_words) . [('health', 11), ('people', 10), ('study', 6), ('tie', 6), ('researcher', 6), ('interaction', 5), ('friend', 4), ('others', 4), ('exercise', 4), ('connection', 3)] ['health', 'people', 'study', 'tie', 'researcher', 'interaction', 'friend', 'others', 'exercise', 'connection'] . | ì›ë³¸ text ë¬¸ì¥ ë‹¨ìœ„ë¡œ ìª¼ê°œê¸° # ìœ„ì—ì„œ ë§Œë“¤ì–´ë‘ì—ˆë˜ filtered_contentì„ í™œìš©í•´ì„œ ë¬¸ì¥ì„ ìª¼ê°¬. (., ?, ! ë“± ë¬¸ì¥ êµ¬ë¶„ì ê¼­ í•„ìš”) sentences = filtered_content.split('.\\n') sentences1 = [] sentences2 = [] sentences3 = [] for sentence in sentences: sentences1.extend(sentence.split('. ')) for sentence in sentences1: sentences2.extend(sentence.split('!')) for sentence in sentences2: sentences3.extend(sentence.split('?')) article_sentences = sentences3 print(article_sentences) ## ê° ë¬¸ì¥ì„ elementë¡œ ë‹´ê³  ìˆëŠ” list . ['hurray for the hotblack coffee cafe in toronto for declining to offer wifi to its customers', 'there are other such cafes, to be sure, including seven of the eight new york city locations of cafÃ© grumpy.but its hotblacks reason for the electronic blackout that is cause for hosannas', 'as its president, jimson bienenstock, explained, his aim is to get customers to talk with one another instead of being buried in their portable devices.its about creating a social vibe, he told a new york times reporter', 'were a vehicle for human interaction, otherwise its just a commodity.what a novel idea', ' perhaps mr bienenstock instinctively knows what medical science has been increasingly demonstrating for decades social interaction is a critically important contributor to good health and longevity.personally, i dont need researchbased evidence to appreciate the value of making and maintaining social connections', 'i experience it daily during my morning walk with up to three women, then before and after my swim in the locker room of the ymca where the use of electronic devices is not allowed.the locker room experience has been surprisingly rewarding', 'ive made many new friends with whom i can share both joys and sorrows', (ìƒëµ)] . | ë‹¨ì–´ë“¤ì„ nodeë¡œ ìƒì„± # ê°€ì¥ ë§ì´ ì¶œí˜„í•˜ëŠ” 10ê°œì˜ ëª…ì‚¬ ë‹¨ì–´ë“¤ì„ nodeë¡œ í•˜ëŠ” network ìƒì„± import networkx as nx G = nx.Graph() # undirected graph ìƒì„± G.add_nodes_from(list_of_words) # node ìƒì„± (ê°€ì¥ ë§ì•˜ë˜ ëª…ì‚¬ ë‹¨ì–´ 10ê°œ) print(G.nodes()) # nodes print(G.edges()) # edge, ì¦‰ node ê°„ì˜ ê´€ê³„ëŠ” ì•„ì§ ì—†ëŠ” ìƒí™© . ['health', 'people', 'study', 'tie', 'researcher', 'interaction', 'friend', 'others', 'exercise', 'connection'] [] . | edge(=tie) ë§Œë“¤ê¸° import itertools for sentence in article_sentences: # ê° ë¬¸ì¥ì„ elementë¡œ ë‹´ê³  ìˆëŠ” list sentence = sentence.lower() word_tokens = nltk.word_tokenize(sentence) tokens_pos = nltk.pos_tag(word_tokens) NN_words = [] for word, pos in tokens_pos: if 'NN' in pos: NN_words.append(word) wlem = nltk.WordNetLemmatizer() lemmatized_words = [] for word in NN_words: new_word = wlem.lemmatize(word) lemmatized_words.append(new_word) selected_words = [] for word in lemmatized_words: if word in list_of_words: selected_words.append(word) # ë¹ˆë„ top 10ì— í¬í•¨ëœ ë‹¨ì–´ë§Œ append selected_words = set(selected_words) # ì¤‘ë³µì„ ì œê±°í•˜ê¸° ìœ„í•´ set(ì§‘í•©ìë£Œí˜•)ìœ¼ë¡œ ë³€í™˜ for pair in list(itertools.combinations(list(selected_words), 2)): # itertools.combinations: selected_words ë¦¬ìŠ¤íŠ¸ì—ì„œ 2ê°œì”© ê³¨ë¼ ì¡°í•©ì„ ë§Œë“¤ì–´ì¤€ë‹¤ if pair in G.edges(): weight = G[pair[0]][pair[1]]['weight'] weight += 1 G[pair[0]][pair[1]]['weight'] = weight else: G.add_edge(pair[0], pair[1], weight=1) # ìƒì„±ëœ edge í™•ì¸í•´ë³´ê¸° print(nx.get_edge_attributes(G, 'weight')) . {('health', 'connection'): 2, ('health', 'interaction'): 3, ('health', 'people'): 4, ('health', 'friend'): 1, ('health', 'study'): 3, ('health', 'tie'): 3, ('health', 'others'): 1, ('health', 'researcher'): 3, ('health', 'exercise'): 1, ('people', 'friend'): 1, ('people', 'study'): 4, ('people', 'tie'): 3, ('people', 'others'): 3, ('people', 'connection'): 2, ('people', 'researcher'): 3, ('people', 'interaction'): 1, ('study', 'friend'): 1, ('study', 'tie'): 2, ('study', 'others'): 2, ('study', 'connection'): 1, ('study', 'researcher'): 1, ('tie', 'others'): 1, ('tie', 'exercise'): 1, ('tie', 'researcher'): 3, ('tie', 'connection'): 1, ('tie', 'interaction'): 1, ('researcher', 'exercise'): 2, ('researcher', 'connection'): 2, ('researcher', 'interaction'): 2, ('researcher', 'others'): 1, ('interaction', 'connection'): 1, ('interaction', 'exercise'): 1, ('friend', 'exercise'): 1, ('others', 'connection'): 1} . | . +) itertools: íš¨ìœ¨ì ì¸ ë°˜ë³µì„ ê°€ëŠ¥í•˜ê²Œ í•˜ëŠ” library . | itertools.permutations(list, num): ë¦¬ìŠ¤íŠ¸ì˜ ì›ì†Œë“¤ì„ (num)ê°œì”© ê³¨ë¼ ìˆœì—´ì„ ë§Œë“¤ì–´ì¤€ë‹¤ | itertools.combinations(list, num): ë¦¬ìŠ¤íŠ¸ì˜ ì›ì†Œë“¤ì„ (num)ê°œì”© ê³¨ë¼ ì¡°í•©ì„ ë§Œë“¤ì–´ì¤€ë‹¤ . | â€» ì¡°í•©ì€ ìˆœì—´ê³¼ ë‹¬ë¦¬ ìˆœì„œê°€ ì—†ìŒ | . | . &gt;&gt; ì˜ˆì‹œ: . alphabets = ['A', 'B', 'C'] print(list(itertools.permutations(alphabets, 2))) . [('A', 'B'), ('A', 'C'), ('B', 'A'), ('B', 'C'), ('C', 'A'), ('C', 'B')] . print(list(itertools.combinations(alphabets, 2))) # ì¡°í•©ì€ ìˆœì„œê°€ ì¤‘ìš”ì¹˜ ì•Šê¸°ì—, ('A', 'B')ì™€ ('B', 'A')ë¥¼ ê°™ì€ ê²ƒìœ¼ë¡œ ê°„ì£¼, í•œ ê°œë§Œ ë§Œë“ ë‹¤. [('A', 'B'), ('A', 'C'), ('B', 'C')] . ë„¤íŠ¸ì›Œí¬ ì‹œê°í™” . | networkxì˜ ë‹¤ì–‘í•œ layout: https://networkx.org/documentation/stable/reference/drawing.html#layout | . import matplotlib.pyplot as plt ## ë…¸ë“œì˜ degreeì— ë”°ë¼ color ë‹¤ë¥´ê²Œ ì„¤ì •í•˜ê¸° color_map = [] for node in G: if G.degree(node) &gt;= 8: # ì¤‘ìš”í•œ ë…¸ë“œ (degreeê°€ 8 ì´ìƒ) color_map.append('pink') else: color_map.append('beige') plt.figure(figsize=(8, 6)) # size ì„¤ì • pos = nx.spring_layout(G, scale=0.2) # spring layout ì‚¬ìš©, ê¸€ì”¨ê°€ ì˜ë¦¬ëŠ” ê²ƒì„ ë°©ì§€í•˜ê¸° ìœ„í•´ scale=0.2ë¡œ ì¡°ì • nx.draw_networkx(G, pos, node_color=color_map, edge_color='grey') plt.axis('off') # turn off axis plt.show() . â†’ ì‚¬ëŒë“¤ ê°„ì˜ ì¹œêµ¬ ê´€ê³„ê°€ ê±´ê°•ì— ì¤‘ìš”í•˜ë‹¤ëŠ” ì—°êµ¬ì— ëŒ€í•œ ë‚´ìš©ì´ë¼ê³  ìœ ì¶” ê°€ëŠ¥. +) circular layoutìœ¼ë¡œë„ ì‹œê°í™”í•´ë³´ê¸° . plt.figure(figsize=(8, 7)) pos = nx.circular_layout(G, scale=0.2) # circular layout ëª¨ì–‘ nx.draw_networkx(G, pos, node_color=color_map, edge_color='grey') # ìœ„ì—ì„œ ì§€ì •í•œ color_map ê·¸ëŒ€ë¡œ ì‚¬ìš© plt.axis('off') # turn off axis plt.show() . Centrality ê³„ì‚° . : Centralityë¥¼ ê³„ì‚°í•´ ë‹¨ì–´ì˜ ì¤‘ìš”ë„ íŒë‹¨í•˜ê¸° . | ì˜ˆì‹œë¡œ Degree Centralityë§Œ ê³„ì‚°í•´ ë´„ (+. ë‹¤ì–‘í•œ Centrality ê³„ì‚°ë²•) | . # degree centrality print(nx.degree_centrality(G)) . {'health': 1.0, 'people': 0.8888888888888888, 'study': 0.7777777777777777, 'tie': 0.8888888888888888, 'researcher': 0.8888888888888888, 'interaction': 0.6666666666666666, 'friend': 0.4444444444444444, 'others': 0.6666666666666666, 'exercise': 0.5555555555555556, 'connection': 0.7777777777777777} . ",
    "url": "https://chaelist.github.io/docs/network_analysis/semantic_network/#semantic-na-english",
    "relUrl": "/docs/network_analysis/semantic_network/#semantic-na-english"
  },"224": {
    "doc": "Semantic Network Analysis",
    "title": "Semantic NA: í•œê¸€",
    "content": ". | ì¤‘ì•™ì¼ë³´(joins.com) ê¸°ì‚¬ë¥¼ ì˜ˆì‹œë¡œ í™œìš© | . # ë¶„ì„ì— ì‚¬ìš©í•  text ì¤€ë¹„ import requests from bs4 import BeautifulSoup url = 'https://news.joins.com/article/23904235' r = requests.get(url) soup = BeautifulSoup(r.text, 'lxml') title = soup.select_one('#article_title').text.strip() content = soup.select_one('#article_body').text.strip() print(content) . ê°•ì°¬ìˆ˜ í™˜ê²½ì „ë¬¸ê¸°ì íƒë°° ë…¸ë™ìë“¤ì´ ì‡ë”°ë¼ ìˆ¨ì§€ê³  ìˆë‹¤. ì „êµ­ íƒë°°ì—°ëŒ€ ë…¸ì¡° ë“±ì— ë”°ë¥´ë©´ ì´ë‹¬ì—ë§Œ CJëŒ€í•œí†µìš´ ì„œìš¸ ê°•ë¶ì§€ì , í•œì§„íƒë°° ì„œìš¸ ë™ëŒ€ë¬¸ì§€ì‚¬, ê²½ë¶ ì¹ ê³¡ ì¿ íŒ¡ ë¬¼ë¥˜ì„¼í„°ì—ì„œ ì¼í–ˆë˜ ë…¸ë™ìê°€ ì—°ë‹¬ì•„ ìˆ¨ì¡Œë‹¤. ë°°ë‹¬ ë¬¼ëŸ‰ ê¸‰ì¦ìœ¼ë¡œ ì¸í•œ ê³¼ë¡œì‚¬ë¼ëŠ” ì§€ì ì´ ìŸì•„ì§„ë‹¤. ì‹¤ì œë¡œ êµ­í† êµí†µë¶€ê°€ êµ­íšŒì— ì œì¶œí•œ ìë£Œì— ë”°ë¥´ë©´, 2020ë…„ 6ì›” ìƒí™œë¬¼ë¥˜ íƒë°° ë¬¼ë™ëŸ‰ì€ 2ì–µ 9340ë§Œ ê°œë¡œ ì§€ë‚œí•´ 6ì›”ë³´ë‹¤ 36.3% ëŠ˜ì—ˆë‹¤. ì‹ ì¢… ì½”ë¡œë‚˜ë°”ì´ëŸ¬ìŠ¤ ê°ì—¼ì¦(ì½”ë¡œë‚˜19) íƒ“ì— ì‹œë¯¼ë“¤ì´ ì™¸ì¶œê³¼ ë§¤ì¥ ì‡¼í•‘ì„ êº¼ë¦° íƒ“ì´ë‹¤. 1íšŒìš© ë§ˆìŠ¤í¬ ì¬ì§ˆì´ í”Œë¼ìŠ¤í‹±ë¨¼ì§€ ì¹˜ì†Ÿìœ¼ë©´ ìŒì‹ ë°°ë‹¬ë„ ëŠ˜ì–´ë‚™ë™ê°• ë¬¼ê³ ê¸°ì—ë„ ë¯¸ì„¸í”Œë¼ìŠ¤í‹±ë‹¤íšŒìš©ê¸° ì‚¬ìš© ë“± ëŒ€ì•ˆëª¨ë¸ í•„ìš” ë°°ë‹¬ìŒì‹ ì£¼ë¬¸ë„ ëŠ˜ì—ˆë‹¤. ê³µì •ê±°ë˜ìœ„ì›íšŒì— ë”°ë¥´ë©´ ì˜¬í•´ êµ­ë‚´ ë°°ë‹¬ ìŒì‹ ì‹œì¥ ê·œëª¨ëŠ” ëŒ€ëµ 20ì¡°ì›ìœ¼ë¡œ, ì§€ë‚œí•´ë³´ë‹¤ 17%ê°€ëŸ‰ ì¦ê°€í•  ê²ƒìœ¼ë¡œ ì „ë§ëœë‹¤. ë‹¹ì¥ì€ ë…¸ë™ìì˜ ì—´ì•…í•œ ìƒí™©ì´ ë¬¸ì œì§€ë§Œ, ê²¹ê²¹ì´ í¬ì¥ëœ íƒë°° ìƒìì™€ í”Œë¼ìŠ¤í‹± ìš©ê¸°ê°€ ê°€ë“ ë“  ë°°ë‹¬ ìŒì‹ì€ ì‹¬ê°í•œ í™˜ê²½ë¬¸ì œì´ê¸°ë„ í•˜ë‹¤. ì½”ë¡œë‚˜19 ëŒ€ì‘ ê³¼ì •ì—ì„œë„ í”Œë¼ìŠ¤í‹± ì“°ë ˆê¸°ëŠ” ìŸì•„ì§„ë‹¤. ë§ˆìŠ¤í¬ë„ í”Œë¼ìŠ¤í‹±ìœ¼ë¡œ ë§Œë“¤ì–´ì§„ë‹¤. ì„¸ê³„ë³´ê±´ê¸°êµ¬(WHO)ëŠ” ì½”ë¡œë‚˜19 ëŒ€ì‘ì„ ìœ„í•´ ì„¸ê³„ì ìœ¼ë¡œ ë§¤ë‹¬ 8900ë§Œ ê°œì˜ ì˜ë£Œìš© ë§ˆìŠ¤í¬ì™€ 7600ë§Œ ê°œì˜ ê²€ì‚¬ìš© ì¥ê°‘ì´ í•„ìš”í•  ê²ƒìœ¼ë¡œ ì¶”ì •í–ˆë‹¤. ì¼ë°˜ ì‹œë¯¼ì´ ì‚¬ìš©í•˜ëŠ” ê²ƒì€ ì œì™¸í•œ ìˆ˜ì¹˜ë‹¤. êµ­ë‚´ì—ì„œë§Œ ë§¤ì£¼ 2ì–µì¥ì˜ ë§ˆìŠ¤í¬ë¥¼ ìƒì‚°í•œë‹¤. ê²½ê¸°ë„ ìˆ˜ì›ì‹œ ìì›ìˆœí™˜ì„¼í„°ì— ìŒ“ì¸ í”Œë¼ìŠ¤í‹± ì¬í™œìš© ì“°ë ˆê¸°. ìµœê·¼ íƒë°°ì™€ ìŒì‹ ë°°ë‹¬ì´ ëŠ˜ë©´ì„œ í”Œë¼ìŠ¤í‹± ì“°ë ˆê¸°ë„ í¬ê²Œ ëŠ˜ì—ˆë‹¤. [ì—°í•©ë‰´ìŠ¤] ìµœê·¼ì—ëŠ” ë¯¸ì„¸ë¨¼ì§€ ì˜¤ì—¼ì´ í”Œë¼ìŠ¤í‹± ì“°ë ˆê¸°ë¥¼ ëŠ˜ë¦°ë‹¤ëŠ” ì—°êµ¬ ê²°ê³¼ë„ ë‚˜ì™”ë‹¤. ì‹±ê°€í¬ë¥´ ì—°êµ¬íŒ€ì´ ì§€ë‚œ 22ì¼ â€˜ë„¤ì´ì²˜ ì¸ê°„ í–‰ë™â€™ ì €ë„ì— ë°œí‘œí•œ ë…¼ë¬¸ì´ë‹¤. ë² ì´ì§• ë“± ì¤‘êµ­ 3ê°œ ë„ì‹œì—ì„œ ì‹¤ì‹œí•œ ì¡°ì‚¬ì—ì„œ ë¯¸ì„¸ë¨¼ì§€ ë†ë„ê°€ ã¥ë‹¹ 100ã(ë§ˆì´í¬ë¡œê·¸ë¨) ìƒìŠ¹í•˜ë©´, ì‚¬ë¬´ì‹¤ ë…¸ë™ìì˜ ìŒì‹ ë°°ë‹¬ ì£¼ë¬¸ì´ 43%ë‚˜ ëŠ˜ì—ˆë‹¤. ì¤‘êµ­ ì „ì—­ìœ¼ë¡œ í™•ëŒ€í•˜ë©´ ë°°ë‹¬ ìŒì‹ì´ 260ë§Œ ê°œ ëŠ˜ì–´ë‚œë‹¤ëŠ” ì˜ë¯¸ë‹¤. (ìƒëµ) . ì „ì²˜ë¦¬ (Preprocessing) . *í•œê¸€ ì „ì²˜ë¦¬ì— ëŒ€í•œ ìì„¸í•œ ì„¤ëª… . | Text Cleaning import re cleaned_content = re.sub('[^,.?!\\w\\s]','', content) # ì •ê·œí‘œí˜„ì‹ ì‚¬ìš© cleaned_content = cleaned_content.replace('ê°•ì°¬ìˆ˜', '').replace('ì—°í•©ë‰´ìŠ¤', '').replace('í™˜ê²½ì „ë¬¸ê¸°ì', '') # ì¶”ê°€ë¡œ ë” ì œê±° . | Tokenization + Lemmatization + POS tagging from konlpy.tag import Kkma kkma = Kkma() NN_words = [] kkma_pos = kkma.pos(cleaned_content) for word, pos in kkma_pos: if 'NN' in pos: NN_words.append(word) . ['íƒë°°', 'ë…¸ë™ì', 'ì „êµ­', 'íƒë°°', 'ì—°ëŒ€', 'ë…¸ì¡°', 'ë“±', 'ì´ë‹¬', 'í†µìš´', 'ì„œìš¸', 'ê°•ë¶', 'ì§€ì ', 'í•œì§„', 'íƒë°°', 'ì„œìš¸', 'ë™ëŒ€ë¬¸', 'ì§€ì‚¬', 'ê²½ë¶', 'ê³¡', 'ë¬¼ë¥˜', 'ì„¼í„°', 'ë…¸ë™ì', 'ë°°ë‹¬', 'ë¬¼ëŸ‰', 'ê¸‰ì¦', 'ê³¼ë¡œ', 'ì‚¬', 'ì§€ì ', 'êµ­í† ', 'êµí†µë¶€', 'êµ­íšŒ', 'ì œì¶œ', (ìƒëµ)] . | Stopwords ì œê±° customized_stopwords = ['ê²ƒ', 'ë“±', 'íƒ“', 'ë°”', 'ìš©', 'ë…„', 'ê°œ', 'ë‹¹', 'ë©´', 'ë§'] unique_NN_words = set(NN_words) for word in unique_NN_words: if word in customized_stopwords: while word in NN_words: NN_words.remove(word) . | . Semantic Network í˜•ì„± . | ë‹¨ì–´ì˜ ë¹ˆë„ íŒŒì•… from collections import Counter c = Counter(NN_words) print(c.most_common(20)) # ê°€ì¥ ë¹ˆë²ˆí•˜ê²Œ ë‚˜ì˜¤ëŠ” 20ê°œì˜ ë‹¨ì–´ ì¶œë ¥ # ê°€ì¥ ë§ì´ ë‚˜ì˜¤ëŠ” ë‹¨ì–´ 20ê°œ ì €ì¥ list_of_words = [] for word, count in c.most_common(20): list_of_words.append(word) . [('í”Œë¼ìŠ¤í‹±', 28), ('ë¯¸ì„¸', 13), ('ì“°ë ˆê¸°', 12), ('ë°°ë‹¬', 8), ('ì—°êµ¬', 8), ('ìŒì‹', 7), ('í†¤', 7), ('íƒë°°', 6), ('íŒ€', 6), ('ì½”ë¡œë‚˜', 5), ('ìƒì‚°', 5), ('ë°œí‘œ', 5), ('ë…¼ë¬¸', 5), ('ë…¸ë™ì', 4), ('ë§ˆìŠ¤í¬', 4), ('ë¨¼ì§€', 4), ('ë¬¼ê³ ê¸°', 4), ('ìµœê·¼', 4), ('ë°”ë‹¤', 4), ('ì§€ë‚œí•´', 3)] . | ì›ë³¸ text ë¬¸ì¥ ë‹¨ìœ„ë¡œ ìª¼ê°œê¸° sentences = cleaned_content.split('.\\n') sentences1 = [] sentences2 = [] sentences3 = [] for sentence in sentences: sentences1.extend(sentence.strip().split('. ')) for sentence in sentences1: sentences2.extend(sentence.strip().split('!')) for sentence in sentences2: sentences3.extend(sentence.strip().split('?')) article_sentences = sentences3 print(article_sentences) . ['íƒë°° ë…¸ë™ìë“¤ì´ ì‡ë”°ë¼ ìˆ¨ì§€ê³  ìˆë‹¤', 'ì „êµ­ íƒë°°ì—°ëŒ€ ë…¸ì¡° ë“±ì— ë”°ë¥´ë©´ ì´ë‹¬ì—ë§Œ CJëŒ€í•œí†µìš´ ì„œìš¸ ê°•ë¶ì§€ì , í•œì§„íƒë°° ì„œìš¸ ë™ëŒ€ë¬¸ì§€ì‚¬, ê²½ë¶ ì¹ ê³¡ ì¿ íŒ¡ ë¬¼ë¥˜ì„¼í„°ì—ì„œ ì¼í–ˆë˜ ë…¸ë™ìê°€ ì—°ë‹¬ì•„ ìˆ¨ì¡Œë‹¤', 'ë°°ë‹¬ ë¬¼ëŸ‰ ê¸‰ì¦ìœ¼ë¡œ ì¸í•œ ê³¼ë¡œì‚¬ë¼ëŠ” ì§€ì ì´ ìŸì•„ì§„ë‹¤', 'ì‹¤ì œë¡œ êµ­í† êµí†µë¶€ê°€ êµ­íšŒì— ì œì¶œí•œ ìë£Œì— ë”°ë¥´ë©´, 2020ë…„ 6ì›” ìƒí™œë¬¼ë¥˜ íƒë°° ë¬¼ë™ëŸ‰ì€ 2ì–µ 9340ë§Œ ê°œë¡œ ì§€ë‚œí•´ 6ì›”ë³´ë‹¤ 36.3 ëŠ˜ì—ˆë‹¤', 'ì‹ ì¢… ì½”ë¡œë‚˜ë°”ì´ëŸ¬ìŠ¤ ê°ì—¼ì¦ì½”ë¡œë‚˜19 íƒ“ì— ì‹œë¯¼ë“¤ì´ ì™¸ì¶œê³¼ ë§¤ì¥ ì‡¼í•‘ì„ êº¼ë¦° íƒ“ì´ë‹¤', '1íšŒìš© ë§ˆìŠ¤í¬ ì¬ì§ˆì´ í”Œë¼ìŠ¤í‹±ë¨¼ì§€ ì¹˜ì†Ÿìœ¼ë©´ ìŒì‹ ë°°ë‹¬ë„ ëŠ˜ì–´ë‚™ë™ê°• ë¬¼ê³ ê¸°ì—ë„ ë¯¸ì„¸í”Œë¼ìŠ¤í‹±ë‹¤íšŒìš©ê¸° ì‚¬ìš© ë“± ëŒ€ì•ˆëª¨ë¸ í•„ìš” ë°°ë‹¬ìŒì‹ ì£¼ë¬¸ë„ ëŠ˜ì—ˆë‹¤', 'ê³µì •ê±°ë˜ìœ„ì›íšŒì— ë”°ë¥´ë©´ ì˜¬í•´ êµ­ë‚´ ë°°ë‹¬ ìŒì‹ ì‹œì¥ ê·œëª¨ëŠ” ëŒ€ëµ 20ì¡°ì›ìœ¼ë¡œ, ì§€ë‚œí•´ë³´ë‹¤ 17ê°€ëŸ‰ ì¦ê°€í•  ê²ƒìœ¼ë¡œ ì „ë§ëœë‹¤', 'ë‹¹ì¥ì€ ë…¸ë™ìì˜ ì—´ì•…í•œ ìƒí™©ì´ ë¬¸ì œì§€ë§Œ, ê²¹ê²¹ì´ í¬ì¥ëœ íƒë°° ìƒìì™€ í”Œë¼ìŠ¤í‹± ìš©ê¸°ê°€ ê°€ë“ ë“  ë°°ë‹¬ ìŒì‹ì€ ì‹¬ê°í•œ í™˜ê²½ë¬¸ì œì´ê¸°ë„ í•˜ë‹¤', 'ì½”ë¡œë‚˜19 ëŒ€ì‘ ê³¼ì •ì—ì„œë„ í”Œë¼ìŠ¤í‹± ì“°ë ˆê¸°ëŠ” ìŸì•„ì§„ë‹¤', 'ë§ˆìŠ¤í¬ë„ í”Œë¼ìŠ¤í‹±ìœ¼ë¡œ ë§Œë“¤ì–´ì§„ë‹¤', 'ì„¸ê³„ë³´ê±´ê¸°êµ¬WHOëŠ” ì½”ë¡œë‚˜19 ëŒ€ì‘ì„ ìœ„í•´ ì„¸ê³„ì ìœ¼ë¡œ ë§¤ë‹¬ 8900ë§Œ ê°œì˜ ì˜ë£Œìš© ë§ˆìŠ¤í¬ì™€ 7600ë§Œ ê°œì˜ ê²€ì‚¬ìš© ì¥ê°‘ì´ í•„ìš”í•  ê²ƒìœ¼ë¡œ ì¶”ì •í–ˆë‹¤', 'ì¼ë°˜ ì‹œë¯¼ì´ ì‚¬ìš©í•˜ëŠ” ê²ƒì€ ì œì™¸í•œ ìˆ˜ì¹˜ë‹¤', 'êµ­ë‚´ì—ì„œë§Œ ë§¤ì£¼ 2ì–µì¥ì˜ ë§ˆìŠ¤í¬ë¥¼ ìƒì‚°í•œë‹¤', 'ê²½ê¸°ë„ ìˆ˜ì›ì‹œ ìì›ìˆœí™˜ì„¼í„°ì— ìŒ“ì¸ í”Œë¼ìŠ¤í‹± ì¬í™œìš© ì“°ë ˆê¸°', 'ìµœê·¼ íƒë°°ì™€ ìŒì‹ ë°°ë‹¬ì´ ëŠ˜ë©´ì„œ í”Œë¼ìŠ¤í‹± ì“°ë ˆê¸°ë„ í¬ê²Œ ëŠ˜ì—ˆë‹¤', (ìƒëµ)] . | ë‹¨ì–´ë“¤ì„ nodeë¡œ ìƒì„± # ê°€ì¥ ë§ì´ ì¶œí˜„í•˜ëŠ” 20ê°œì˜ ëª…ì‚¬ ë‹¨ì–´ë“¤ì— ëŒ€í•´ì„œ ë„¤íŠ¸ì›Œí¬ ìƒì„±í•˜ê¸° import networkx as nx G = nx.Graph() G.add_nodes_from(list_of_words) # node ìƒì„± (ê°€ì¥ ë§ì•˜ë˜ ëª…ì‚¬ ë‹¨ì–´ 20ê°œ) print(G.nodes()) # nodes print(G.edges()) # edge, ì¦‰ node ê°„ì˜ ê´€ê³„ëŠ” ì•„ì§ ì—†ëŠ” ìƒí™© . ['í”Œë¼ìŠ¤í‹±', 'ë¯¸ì„¸', 'ì“°ë ˆê¸°', 'ë°°ë‹¬', 'ì—°êµ¬', 'ìŒì‹', 'í†¤', 'íƒë°°', 'íŒ€', 'ì½”ë¡œë‚˜', 'ìƒì‚°', 'ë°œí‘œ', 'ë…¼ë¬¸', 'ë…¸ë™ì', 'ë§ˆìŠ¤í¬', 'ë¨¼ì§€', 'ë¬¼ê³ ê¸°', 'ìµœê·¼', 'ë°”ë‹¤', 'ì§€ë‚œí•´'] [] . | edge(=tie) ë§Œë“¤ê¸° import itertools for sentence in article_sentences: selected_words = [] NN_words = [] kkma_pos = kkma.pos(sentence) for word, pos in kkma_pos: if 'NN' in pos: NN_words.append(word) for word in NN_words: if word in list_of_words: selected_words.append(word) selected_words = set(selected_words) for pair in list(itertools.combinations(list(selected_words), 2)): if pair in G.edges(): weight = G[pair[0]][pair[1]]['weight'] weight += 1 G[pair[0]][pair[1]]['weight'] = weight else: G.add_edge(pair[0], pair[1], weight=1 ) # ìƒì„±ëœ edge í™•ì¸í•´ë³´ê¸° print(nx.get_edge_attributes(G, 'weight')) . {('í”Œë¼ìŠ¤í‹±', 'ìŒì‹'): 3, ('í”Œë¼ìŠ¤í‹±', 'ë¯¸ì„¸'): 9, ('í”Œë¼ìŠ¤í‹±', 'ë°°ë‹¬'): 3, ('í”Œë¼ìŠ¤í‹±', 'ë¨¼ì§€'): 2, ('í”Œë¼ìŠ¤í‹±', 'ë¬¼ê³ ê¸°'): 2, ('í”Œë¼ìŠ¤í‹±', 'ë§ˆìŠ¤í¬'): 2, ('í”Œë¼ìŠ¤í‹±', 'ë…¸ë™ì'): 1, ('í”Œë¼ìŠ¤í‹±', 'íƒë°°'): 2, ('í”Œë¼ìŠ¤í‹±', 'ì“°ë ˆê¸°'): 11, ('í”Œë¼ìŠ¤í‹±', 'ì½”ë¡œë‚˜'): 1, ('í”Œë¼ìŠ¤í‹±', 'ìµœê·¼'): 4, ('í”Œë¼ìŠ¤í‹±', 'ì—°êµ¬'): 6, ('í”Œë¼ìŠ¤í‹±', 'íŒ€'): 4, ('í”Œë¼ìŠ¤í‹±', 'ìƒì‚°'): 3, ('í”Œë¼ìŠ¤í‹±', 'í†¤'): 3, ('í”Œë¼ìŠ¤í‹±', 'ë°œí‘œ'): 3, ('í”Œë¼ìŠ¤í‹±', 'ë…¼ë¬¸'): 3, ('í”Œë¼ìŠ¤í‹±', 'ì§€ë‚œí•´'): 1, ('í”Œë¼ìŠ¤í‹±', 'ë°”ë‹¤'): 1, ('ë¯¸ì„¸', 'ìŒì‹'): 2, ('ë¯¸ì„¸', 'ë°°ë‹¬'): 2, ('ë¯¸ì„¸', 'ë¨¼ì§€'): 4, ('ë¯¸ì„¸', 'ë¬¼ê³ ê¸°'): 2, ('ë¯¸ì„¸', 'ë§ˆìŠ¤í¬'): 1, ('ë¯¸ì„¸', 'ìµœê·¼'): 1, ('ë¯¸ì„¸', 'ì“°ë ˆê¸°'): 2, ('ë¯¸ì„¸', 'ì—°êµ¬'): 4, ('ë¯¸ì„¸', 'ë…¸ë™ì'): 1, ('ë¯¸ì„¸', 'ì½”ë¡œë‚˜'): 1, ('ë¯¸ì„¸', 'ë°”ë‹¤'): 1, ('ë¯¸ì„¸', 'íŒ€'): 2, ('ë¯¸ì„¸', 'ìƒì‚°'): 1, ('ë¯¸ì„¸', 'í†¤'): 1, ('ë¯¸ì„¸', 'ë°œí‘œ'): 1, ('ë¯¸ì„¸', 'ë…¼ë¬¸'): 1, ('ë¯¸ì„¸', 'ì§€ë‚œí•´'): 1, ('ì“°ë ˆê¸°', 'ì½”ë¡œë‚˜'): 1, ('ì“°ë ˆê¸°', 'ìµœê·¼'): 2, ('ì“°ë ˆê¸°', 'ìŒì‹'): 1, ('ì“°ë ˆê¸°', 'ë°°ë‹¬'): 1, ('ì“°ë ˆê¸°', 'íƒë°°'): 1, ('ì“°ë ˆê¸°', 'ì—°êµ¬'): 2, ('ì“°ë ˆê¸°', 'ë¨¼ì§€'): 1, ('ì“°ë ˆê¸°', 'ë°”ë‹¤'): 2, ('ì“°ë ˆê¸°', 'íŒ€'): 1, ('ì“°ë ˆê¸°', 'í†¤'): 1, ('ì“°ë ˆê¸°', 'ë°œí‘œ'): 1, ('ì“°ë ˆê¸°', 'ë…¼ë¬¸'): 1, ('ì“°ë ˆê¸°', 'ë¬¼ê³ ê¸°'): 1, ('ë°°ë‹¬', 'ìŒì‹'): 6, ('ë°°ë‹¬', 'ë¨¼ì§€'): 2, ('ë°°ë‹¬', 'ë¬¼ê³ ê¸°'): 1, ('ë°°ë‹¬', 'ë§ˆìŠ¤í¬'): 1, ('ë°°ë‹¬', 'ì§€ë‚œí•´'): 1, ('ë°°ë‹¬', 'ë…¸ë™ì'): 2, ('ë°°ë‹¬', 'íƒë°°'): 2, ('ë°°ë‹¬', 'ìµœê·¼'): 1, ('ì—°êµ¬', 'ìµœê·¼'): 3, ('ì—°êµ¬', 'ë¨¼ì§€'): 1, ('ì—°êµ¬', 'ë…¼ë¬¸'): 5, (ìƒëµ)} . | . ë„¤íŠ¸ì›Œí¬ ì‹œê°í™” . import matplotlib.pyplot as plt from matplotlib import font_manager as fm ## í•œê¸€ë¡œ ì‹œê°í™”í•  ê²½ìš°, ì•„ë˜ì™€ ê°™ì´ fontë¥¼ ê°€ì ¸ì™€ì¤˜ì•¼ í•¨ path = 'c:/Windows/Fonts/malgun.ttf' font_name = fm.FontProperties(fname=path).get_name() ## ë…¸ë“œì˜ degreeì— ë”°ë¼ color ë‹¤ë¥´ê²Œ ì„¤ì •í•˜ê¸° color_map = [] for node in G: if G.degree(node) &gt;= 15: # ì¤‘ìš”í•œ ë…¸ë“œ (degreeê°€ 15 ì´ìƒ) color_map.append('pink') else: color_map.append('beige') plt.figure(figsize=(9, 7)) pos = nx.spring_layout(G) # spring layout ì‚¬ìš© nx.draw_networkx(G, pos, node_color=color_map, edge_color='grey', font_family=font_name) plt.axis('off') # turn off axis plt.show() . â†’ í”Œë¼ìŠ¤í‹± ì“°ë ˆê¸°, íŠ¹íˆ ë¯¸ì„¸ í”Œë¼ìŠ¤í‹±ìœ¼ë¡œ ì¸í•œ ë¬¸ì œì— ê´€í•œ ë‚´ìš©ì´ë¼ê³  ìœ ì¶” ê°€ëŠ¥. +) circular layoutìœ¼ë¡œë„ ì‹œê°í™”í•´ë³´ê¸° . | nodeê°€ ë§ì€ ê²½ìš°, circular layoutìœ¼ë¡œ ì‹œê°í™”í•˜ë©´ ê°œë³„ nodeì™€ edgeë¥¼ ë” ì‰½ê²Œ ì‹ë³„ ê°€ëŠ¥ | . plt.figure(figsize=(9, 7)) pos = nx.circular_layout(G) nx.draw_networkx(G, pos, node_color=color_map, edge_color='grey', font_family=font_name) plt.axis('off') plt.show() . Centrality ê³„ì‚° . : Centralityë¥¼ ê³„ì‚°í•´ ë‹¨ì–´ì˜ ì¤‘ìš”ë„ íŒë‹¨í•˜ê¸° . # degree centrality print(nx.degree_centrality(G)) . {'í”Œë¼ìŠ¤í‹±': 1.0, 'ë¯¸ì„¸': 0.9473684210526315, 'ì“°ë ˆê¸°': 0.7894736842105263, 'ë°°ë‹¬': 0.5789473684210527, 'ì—°êµ¬': 0.631578947368421, 'ìŒì‹': 0.5789473684210527, 'í†¤': 0.5263157894736842, 'íƒë°°': 0.3684210526315789, 'íŒ€': 0.5789473684210527, 'ì½”ë¡œë‚˜': 0.2631578947368421, 'ìƒì‚°': 0.5263157894736842, 'ë°œí‘œ': 0.5263157894736842, 'ë…¼ë¬¸': 0.5263157894736842, 'ë…¸ë™ì': 0.3157894736842105, 'ë§ˆìŠ¤í¬': 0.42105263157894735, 'ë¨¼ì§€': 0.5789473684210527, 'ë¬¼ê³ ê¸°': 0.42105263157894735, 'ìµœê·¼': 0.6842105263157894, 'ë°”ë‹¤': 0.5263157894736842, 'ì§€ë‚œí•´': 0.3684210526315789} . ",
    "url": "https://chaelist.github.io/docs/network_analysis/semantic_network/#semantic-na-%ED%95%9C%EA%B8%80",
    "relUrl": "/docs/network_analysis/semantic_network/#semantic-na-í•œê¸€"
  },"225": {
    "doc": "ì˜í™” ë¦¬ë·° ê°ì„± ë¶„ì„",
    "title": "ì˜í™” ë¦¬ë·° ê°ì„± ë¶„ì„",
    "content": ". | ê°ì„±ë¶„ì„(Sentiment Analysis)ì´ë€? | ì˜í™” ë¦¬ë·° ê°ì„± ë¶„ì„: ì¤€ë¹„ . | ì˜í™” í‰ì  - ë¦¬ë·° ë°ì´í„° ì¤€ë¹„ | í•™ìŠµìš© ë°ì´í„°ë¡œ ê°€ê³µ | . | Logistic Regressionìœ¼ë¡œ í•™ìŠµ . | train_test_split &amp; vectorí™” | í•™ìŠµ &amp; test data ì˜ˆì¸¡ | ì˜ˆì¸¡ì— ì¤‘ìš”í•œ ì—­í• ì„ í•˜ëŠ” ë‹¨ì–´ë“¤ í™•ì¸ | ìƒˆë¡œìš´ ëŒ“ê¸€ì˜ ê¸ì •/ë¶€ì • ì˜ˆì¸¡í•´ë³´ê¸° | . | . ",
    "url": "https://chaelist.github.io/docs/ml_application/sentiment_analysis/",
    "relUrl": "/docs/ml_application/sentiment_analysis/"
  },"226": {
    "doc": "ì˜í™” ë¦¬ë·° ê°ì„± ë¶„ì„",
    "title": "ê°ì„±ë¶„ì„(Sentiment Analysis)ì´ë€?",
    "content": ": íŠ¹ì • ì£¼ì œì— ëŒ€í•œ ê¸€ì˜ ê°ì„±/íƒœë„ë¥¼ íŒŒì•…í•˜ëŠ” ê²ƒ (ê¸ì • / ë¶€ì •) . | ML-based (ê¸°ê³„ í•™ìŠµ ê¸°ë°˜) . | Supervised learning. (training data: ê¸ì •/ë¶€ì •ì˜ labelì´ ìˆëŠ” text ë°ì´í„°) | ë‹¨ì : ìœ ì‚¬í•œ domainì˜ í•™ìŠµ ë°ì´í„°ë¥¼ ì¤€ë¹„í•´ì•¼ í•˜ëŠ”ë°, ê¸ì •-ë¶€ì •ì´ ëª…í™•íˆ ë“œëŸ¬ë‚˜ ìˆëŠ” í•™ìŠµ ë°ì´í„°ë¥¼ ë§Œë“¤ê¸°ê°€ ì–´ë µë‹¤. | . | Lexicon-based (ê°ì„±ì–´ ì‚¬ì „ ê¸°ë°˜) . | Lexicon(ê°ì„±ì–´ ì‚¬ì „)ì— ë‹¨ì–´ë³„ë¡œ ê°ì„± ì ìˆ˜ê°€ ë“¤ì–´ìˆë‹¤ (ê¸ì •ì ì¸ ë‹¨ì–´ëŠ” + ê°’, ë¶€ì •ì ì¸ ë‹¨ì–´ëŠ” - ê°’) | ì‚¬ì „ì„ ë°”íƒ•ìœ¼ë¡œ textì˜ ì „ì²´ì ì¸ ê°ì„± ì ìˆ˜ë¥¼ ë§¤ê¸°ê³ , ê°’ì´ 0ë³´ë‹¤ í¬ë©´ ê¸ì • / ì‘ìœ¼ë©´ ë¶€ì •ìœ¼ë¡œ í•´ì„ | ë‹¨ì : ì‚¬ì „ì„ ë§Œë“œëŠ” ê²ƒ ìì²´ê°€ ì–´ë µê³ , ì‚¬ì „ì´ ìˆë‹¤ê³  í•´ë„ ì´ê±¸ ë¬¸ì„œì— ì ìš©í•´ì„œ ì •í™•í•œ ê°ì„±ì ìˆ˜ë¥¼ ê³„ì‚°í•˜ëŠ” ê²Œ ì‰½ì§€ë§Œì€ ì•Šë‹¤. (ex. the movie was not fun â†’ funì€ ê¸ì •ì´ì§€ë§Œ not funì€ ë¶€ì •) | . | . ",
    "url": "https://chaelist.github.io/docs/ml_application/sentiment_analysis/#%EA%B0%90%EC%84%B1%EB%B6%84%EC%84%9Dsentiment-analysis%EC%9D%B4%EB%9E%80",
    "relUrl": "/docs/ml_application/sentiment_analysis/#ê°ì„±ë¶„ì„sentiment-analysisì´ë€"
  },"227": {
    "doc": "ì˜í™” ë¦¬ë·° ê°ì„± ë¶„ì„",
    "title": "ì˜í™” ë¦¬ë·° ê°ì„± ë¶„ì„: ì¤€ë¹„",
    "content": ": Machine Learning ê¸°ë°˜ ê°ì„± ë¶„ì„ì„ ìœ„í•´, ê¸ì •/ë¶€ì •ì´ êµ¬ë¶„ë˜ëŠ” ë°ì´í„°ì¸ ì˜í™” í‰ì  &amp; ë¦¬ë·° ë°ì´í„°ë¥¼ ìˆ˜ì§‘ . # ì‚¬ìš©í•  libraryë¥¼ ë¨¼ì € ëª¨ë‘ import import pandas as pd import numpy as np import konlpy from sklearn.feature_extraction.text import CountVectorizer # tf-idf ë°©ì‹ì„ ì‚¬ìš©í•˜ë ¤ë©´ ëŒ€ì‹  TfidfVectorizerë¥¼ import from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression from sklearn.metrics import accuracy_score . ì˜í™” í‰ì  - ë¦¬ë·° ë°ì´í„° ì¤€ë¹„ . Â  . â£1. ë„¤ì´ë²„ ì˜í™” â€˜í˜„ì¬ ìƒì˜ ì˜í™”â€™ 100ê°œì˜ í‰ì -ë¦¬ë·° ë°ì´í„° ìˆ˜ì§‘ (2021.03.30 ê¸°ì¤€) . | https://movie.naver.com/movie/running/current.nhn | . ## 'ë„¤ì´ë²„ ì˜í™”'ì—ì„œ ì§ì ‘ ìˆ˜ì§‘í•´ ì˜¨ ë°ì´í„°ë¥¼ dataframeìœ¼ë¡œ ì •ë¦¬í•´ ë‘  reviews_df.head() . | Â  | movie_num | movie_title | star | comment | . | 0 | 191637 | ê³ ì§ˆë¼ VS. ì½© | 10 | ì‹œë¦¬ì¦ˆ ì¤‘ ìµœê³ ì˜€ë˜ê±° ê°™ì•„ìš”;; ê·¹ì¥ê°€ì„œ ë³´ê¸¸ ì˜í•œë“¯ | . | 1 | 191637 | ê³ ì§ˆë¼ VS. ì½© | 9 | ì‹œì›ì‹œì›í•˜ê²Œ ë‘ë“œë ¤ íŒ¨ê³  ë§ëŠ” ì˜í™”. ì¢…ì¢… ë´ì¤˜ì•¼ ë§ˆìŒì´ ìƒì¾Œí•´ì§„ë‹¤. | . | 2 | 191637 | ê³ ì§ˆë¼ VS. ì½© | 10 | We need Kong, The world needs him. | . | 3 | 191637 | ê³ ì§ˆë¼ VS. ì½© | 10 | ë°©ì‚¬ëŠ¥ ì™•ë„ë§ˆë±€ê³¼ ì¸ë¥˜ ì¡°ìƒ ê²© ì—ì´í”„ì˜ í™˜ìƒì ì¸ ì¢Œì¶©ìš°ëŒ ì•¡ì…˜ ì½œë¼ë³´ | . | 4 | 191637 | ê³ ì§ˆë¼ VS. ì½© | 10 | ì½© ì í”„í•˜ê³  ê³ ì§ˆë¼ ìˆ˜ì˜í•˜ê³  ì „íˆ¬ì”¬ ë‚˜ì˜¬ë•Œë§ˆë‹¤ í¬ë””ë¡œ ë³´ë‹ˆê¹Œ ã„¹ã…‡ ì¡´ì¼ì„ã…‹ã…‹ ë‹˜ë“¤ì•„ ê¼­ ë‚˜ë¯¿ê³  í¬ë””ë¡œ ë³´ì…”ë¼ | . Â  . reviews_df.info() . &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 170388 entries, 0 to 170387 Data columns (total 4 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 movie_num 170388 non-null int64 1 movie_title 170388 non-null object 2 star 170388 non-null int64 3 comment 170388 non-null object dtypes: int64(2), object(2) . â£2. commentì— ë‚´ìš©ì´ ì—†ëŠ” í–‰ íŒŒì•…í•´ì„œ ì‚­ì œ . | í‰ì ë§Œ ìˆê³  ì—°ê²°ëœ textê°€ ì—†ë‹¤ë©´ ê°ì„± ë¶„ì„ì— ì‚¬ìš©í•  ìˆ˜ ì—†ìŒ | . # commentì— ë‚´ìš©ì´ ì—†ëŠ” í–‰ íŒŒì•… print(len(reviews_df[reviews_df['comment'] == ''])) # commentì— ë‚´ìš©ì´ ì—†ëŠ” í–‰ì€ ì§€ì›Œì„œ ìƒˆ dfì— ì €ì¥ reviews_with_comment_df = reviews_df[reviews_df['comment'] != ''] reviews_with_comment_df.reset_index(drop=True, inplace=True) print(len(reviews_with_comment_df)) # commentê°€ ìˆëŠ” ë¦¬ë·°ì˜ ìˆ˜ . 5370 165018 . í•™ìŠµìš© ë°ì´í„°ë¡œ ê°€ê³µ . | í‰ì  8 ì´ìƒ í˜¹ì€ 3 ì´í•˜ë§Œ ì €ì¥ (8 ì´ìƒ: ê¸ì •ì , 3 ì´í•˜: ë¶€ì •ì ) | ê° textë¥¼ tokenizeí•œ í›„, ë™ì‚¬, í˜•ìš©ì‚¬, ëª…ì‚¬ë§Œ ì €ì¥ (konlpyì˜ Okt ì‚¬ìš©) | . # í…ìŠ¤íŠ¸ë¥¼ tokenizeí•´ì„œ adjective, verb, nounë§Œ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜ def tokenize_korean_text(text): text_filtered = re.sub('[^,.?!\\w\\s]','', text) okt = konlpy.tag.Okt() Okt_morphs = okt.pos(text_filtered) words = [] for word, pos in Okt_morphs: if pos == 'Adjective' or pos == 'Verb' or pos == 'Noun': words.append(word) words_str = ' '.join(words) return words_str . X_texts = [] y = [] for star, comment in zip(reviews_with_comment_df['star'], reviews_with_comment_df['comment']): if 4 &lt;= star &lt;= 7: continue # í‰ì ì´ 4~7ì¸ ì˜í™”ëŠ” ì• ë§¤í•˜ê¸° ë•Œë¬¸ì— í•™ìŠµë°ì´í„°ë¡œ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ tokenized_comment = tokenize_korean_text(comment) # ìœ„ì—ì„œ ë§Œë“¤ì—ˆë˜ í•¨ìˆ˜ë¡œ comment ìª¼ê°œê¸° X_texts.append(tokenized_comment) y.append(1 if star &gt; 7 else -1) # í‰ì ì´ 8 ì´ìƒì´ë©´(8,9,10) ê°’ì„ 1ë¡œ ì§€ì • (positive) # í‰ì ì´ 3 ì´í•˜ì´ë©´(1,2,3) ê°’ì„ -1ë¡œ ì§€ì • (negative) print(f'ì›ë˜ text ìˆ˜: {len(reviews_with_comment_df)}') print(f'í‰ì  3 ì´í•˜ í˜¹ì€ 8 ì´ìƒì¸ text ìˆ˜: {len(X_texts)}') print(X_texts[:5]) . ì›ë˜ text ìˆ˜: 165018 í‰ì  3 ì´í•˜ í˜¹ì€ 8 ì´ìƒì¸ text ìˆ˜: 149373 ['ì‹œë¦¬ì¦ˆ ì¤‘ ìµœê³  ì˜€ë˜ê±° ê°™ì•„ìš” ì¥ê°€ ë³´ê¸¸ í•œë“¯', 'ì‹œì›ì‹œì›í•˜ê²Œ ë‘ë“œë ¤ íŒ¨ ë§ëŠ” ì˜í™” ì¢…ì¢… ë´ì¤˜ì•¼ ë§ˆìŒ ìƒì¾Œí•´ì§„ë‹¤', '', 'ë°©ì‚¬ ëŠ¥ ì™• ë„ë§ˆë±€ ì¸ë¥˜ ì¡°ìƒ ê²© ì´í”„ í™˜ìƒ ì¢Œì¶©ìš°ëŒ ì•¡ì…˜ ì½œë¼ë³´', 'ì½© ì í”„ ê³ ì§ˆë¼ ìˆ˜ì˜ ì „íˆ¬ì”¬ ë‚˜ì˜¬ ë•Œ í¬ë”” ë³´ ì¡´ì¼ì„ ë‹˜ë“¤ ê¼­ ë‚˜ ë¯¿ê³  í¬ë”” ë³´ì…”ë¼'] . ",
    "url": "https://chaelist.github.io/docs/ml_application/sentiment_analysis/#%EC%98%81%ED%99%94-%EB%A6%AC%EB%B7%B0-%EA%B0%90%EC%84%B1-%EB%B6%84%EC%84%9D-%EC%A4%80%EB%B9%84",
    "relUrl": "/docs/ml_application/sentiment_analysis/#ì˜í™”-ë¦¬ë·°-ê°ì„±-ë¶„ì„-ì¤€ë¹„"
  },"228": {
    "doc": "ì˜í™” ë¦¬ë·° ê°ì„± ë¶„ì„",
    "title": "Logistic Regressionìœ¼ë¡œ í•™ìŠµ",
    "content": "train_test_split &amp; vectorí™” . # train_test_split X_train_texts, X_test_texts, y_train, y_test = train_test_split(X_texts, y, test_size=0.2, random_state=0) . # CountVectorizerë¡œ vectorí™” tf_vectorizer = CountVectorizer(min_df=1, ngram_range=(1,1)) X_train_tf = tf_vectorizer.fit_transform(X_train_texts) # training dataì— ë§ê²Œ fit &amp; training dataë¥¼ transform X_test_tf = tf_vectorizer.transform(X_test_texts) # test dataë¥¼ transform vocablist = [word for word, number in sorted(tf_vectorizer.vocabulary_.items(), key=lambda x:x[1])] # ë‹¨ì–´ë“¤ì„ ë²ˆí˜¸ ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœìœ¼ë¡œ ì €ì¥ . *CountVectorizer: ê° ë‹¨ì–´ì˜ frequencyë¥¼ ê¸°ë°˜ìœ¼ë¡œ textë¥¼ vectorí™”. | ngram_range: (1, 1) means only unigrams, (1, 2) means unigrams and bigrams, and (2, 2) means only bigrams. (default: (1,1)) | min_df: ignore terms that have a document frequency strictly lower than the given threshold. (default: 1) . | float ê°’ì„ ì“°ë©´: documentì— ì–´ëŠ ì •ë„ ë¹„ìœ¨ ì•„ë˜ë¡œ ë“¤ì–´ ìˆìœ¼ë©´ ë¬´ì‹œí•œë‹¤ëŠ” ëœ» | int ê°’ì„ ì“°ë©´ (ex. 1): ì ˆëŒ€ì ìœ¼ë¡œ ëª‡ ë²ˆ ì•„ë˜ë¡œ í¬í•¨ëœ ë‹¨ì–´ë©´ ë¬´ì‹œí•œë‹¤ëŠ” ëœ» (1ì´ë©´ 1ë²ˆë„ ì•ˆ ë‚˜ì˜¨ ë‹¨ì–´ëŠ” ë¬´ì‹œí•œë‹¤ëŠ” ëœ») | . | tf_vectorizer.fit_transform(X_train_texts): . | fit: training dataì— ìˆëŠ” ê° ë‹¨ì–´ë³„ë¡œ ë²ˆí˜¸ê°€ ë¶™ì–´ì„œ ì €ì¥ë¨ | transform: training dataì˜ ê° ê°’ë§ˆë‹¤, ì–´ë–¤ ë‹¨ì–´ê°€ ëª‡ ë²ˆ ë‚˜ì˜¤ëŠ”ì§€ ë°ì´í„°ê°€ ì €ì¥ë¨ â€“ ex) (0, 2442) 1ì´ë¼ê³  í•˜ë©´ 0ë²ˆì§¸ ë³€ìˆ˜ì— 2442ë²ˆ ë‹¨ì–´ê°€ 1ë²ˆ ë‚˜ì˜¨ë‹¤ëŠ” ëœ» | . | tf_vectorizer.transform(X_train_texts): . | test dataì˜ ê° ê°’ë§ˆë‹¤, ì–´ë–¤ ë‹¨ì–´ê°€ ëª‡ ë²ˆ ë‚˜ì˜¤ëŠ”ì§€ì˜ ë°ì´í„°ê°€ ì €ì¥ë¨ (ë‹¨, fit_transformì´ ì•„ë‹ˆë¯€ë¡œ, ì•ì„œ training dataì— ìˆë˜ ë‹¨ì–´ë“¤ì— í•œí•´ì„œ ë¶„ì„ë¨) | . | . ## í™•ì¸í•´ë³´ê¸° print(X_train_tf[:1], '\\n') print(X_test_tf[:1], '\\n') print(vocablist[:3]) . (0, 48016) 1 (0, 30089) 2 (0, 1146) 1 (0, 68104) 1 (0, 57682) 1 (0, 1277) 1 (0, 34835) 1 (0, 51567) 1 (0, 172) 1 (0, 37975) 1 (0, 1485) 1 (0, 15929) 1 (0, 64573) 1 (0, 66001) 1 ['ê°€ê°€', 'ê°€ê°', 'ê°€ê°'] . +) Text Vectorization: TFì™€ TF-IDF ë°©ì‹ . | TF (Term Frequency) . | text ë‚´ ë‹¨ì–´ë³„ frequencyë¥¼ ê¸°ë°˜ìœ¼ë¡œ vectorë¥¼ í˜•ì„± | . (+. ìœ„ì™€ ê°™ì´, ê° ë¬¸ì„œì˜ ë‹¨ì–´ë¥¼ ë‚˜íƒ€ë‚´ëŠ” vectorë“¤ë¡œ ë§Œë“  í–‰ë ¬ì„ DTM(Document Term Matrix)ì´ë¼ê³  í•¨) . | TF-IDF (Term Frequency - Inverse Document Frequency) . | íŠ¹ì • ë‹¨ì–´ê°€ íŠ¹ì • ë¬¸ì„œì˜ uniquenessë¥¼ ì–¼ë§ˆë‚˜ ë‚˜íƒ€ë‚´ëŠ”ê°€ë¥¼ ê³„ì‚°í•˜ê¸° ìœ„í•´ ì‚¬ìš© | TF-IDFê°€ ë†’ì„ìˆ˜ë¡ í•´ë‹¹ ë‹¨ì–´ëŠ” ë‹¤ë¥¸ ë¬¸ì„œì—ì„œëŠ” ì ê²Œ ì‚¬ìš©ë˜ê³ , í•´ë‹¹ ë¬¸ì„œì—ì„œ ë§ì´ ì‚¬ìš©ë˜ê³  ìˆë‹¤ëŠ” ëœ»ìœ¼ë¡œ, í•´ë‹¹ ë‹¨ì–´ê°€ í•´ë‹¹ ë¬¸ì„œì˜ uniquenessë¥¼ ë§ì´ ë‚˜íƒ€ë‚¸ë‹¤ê³  ë³¼ ìˆ˜ ìˆìŒ. | . (ì¶œì²˜: bloter) . | . í•™ìŠµ &amp; test data ì˜ˆì¸¡ . model = LogisticRegression(C=0.1, penalty='l2', random_state=0) model.fit(X_train_tf, y_train) # í•™ìŠµ . LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True, intercept_scaling=1, l1_ratio=None, max_iter=100, multi_class='auto', n_jobs=None, penalty='l2', random_state=0, solver='lbfgs', tol=0.0001, verbose=0, warm_start=False) . â†’ Test data ì˜ˆì¸¡: . y_test_pred = model.predict(X_test_tf) print('Misclassified samples: {} out of {}'.format((y_test_pred != y_test).sum(), len(y_test))) print('Accuracy: {:.2f}'.format(accuracy_score(y_test, y_test_pred))) # model.score(X_test_tf, y_test)ë¡œ ê³„ì‚°í•´ë„ ë¨ . Misclassified samples: 2490 out of 29875 Accuracy: 0.92 . | 92%ë¥¼ ì˜ ì˜ˆì¸¡í•¨! | . ì˜ˆì¸¡ì— ì¤‘ìš”í•œ ì—­í• ì„ í•˜ëŠ” ë‹¨ì–´ë“¤ í™•ì¸ . coefficients = model.coef_.tolist() sorted_coefficients = sorted(enumerate(coefficients[0]), key=lambda x:x[1], reverse=True) # coefficients(ê³„ìˆ˜)ê°€ í° ê°’ë¶€í„° ë‚´ë¦¼ì°¨ìˆœìœ¼ë¡œ ì •ë ¬ print('ê¸ì •ì ì¸ ë‹¨ì–´ Top 10 (ë†’ì€ í‰ì ê³¼ ìƒê´€ê´€ê³„ê°€ ê°•í•œ ë‹¨ì–´ë“¤)') for word_num, coef in sorted_coefficients[:10]: print('{0:}({1:.3f})'.format(vocablist[word_num], coef)) print('\\në¶€ì •ì ì¸ ë‹¨ì–´ Top 10 (ë‚®ì€ í‰ì ê³¼ ìƒê´€ê´€ê³„ê°€ ê°•í•œ ë‹¨ì–´ë“¤)') for word_num, coef in sorted_coefficients[-10:]: print('{0:}({1:.3f})'.format(vocablist[word_num], coef)) . ê¸ì •ì ì¸ ë‹¨ì–´ Top 10 (ë†’ì€ í‰ì ê³¼ ìƒê´€ê´€ê³„ê°€ ê°•í•œ ë‹¨ì–´ë“¤) ìµœê³ (2.163) ì—¬ìš´(1.829) ì¢‹ì•˜ìŠµë‹ˆë‹¤(1.626) ì¬ë°Œì–´ìš”(1.617) ì¢‹ì•˜ì–´ìš”(1.524) ì¬ë°Œì—ˆì–´ìš”(1.456) ì¬ë°Œê²Œ(1.435) ì¢‹ì•„ìš”(1.389) ë´¤ì–´ìš”(1.330) ìµœê³ ë‹¤(1.317) ë¶€ì •ì ì¸ ë‹¨ì–´ Top 10 (ë‚®ì€ í‰ì ê³¼ ìƒê´€ê´€ê³„ê°€ ê°•í•œ ë‹¨ì–´ë“¤) ì¬ë¯¸ì—†ë‹¤(-1.818) ì¬ë¯¸ì—†ì–´ìš”(-1.836) ê²Œì´(-1.842) ì¬ë¯¸ì—†ìŒ(-1.936) ì“°ë ˆê¸°(-2.012) ì•„ê¹ë‹¤(-2.045) ë¯¸í™”(-2.196) ë˜¥ê¼¬(-2.597) ë…¸ì¼(-2.748) ìµœì•…(-2.767) . ìƒˆë¡œìš´ ëŒ“ê¸€ì˜ ê¸ì •/ë¶€ì • ì˜ˆì¸¡í•´ë³´ê¸° . : í•™ìŠµì— í¬í•¨ë˜ì§€ ì•Šì•˜ë˜ ì˜í™”ì¸ â€˜ì¸í„´â€˜ì—ì„œ ëŒ“ê¸€ì„ ê°€ì ¸ì™€ì„œ í…ŒìŠ¤íŠ¸ . # ê¸ì •/ë¶€ì • í…ŒìŠ¤íŠ¸ìš© í•¨ìˆ˜ ìƒì„± def guess_good_or_bad(text): text_filtered = text.replace('.', '').replace(',','').replace(\"'\",\"\").replace('Â·', ' ').replace('=','') okt = konlpy.tag.Okt() Okt_morphs = okt.pos(text_filtered) words = [] for word, pos in Okt_morphs: if pos == 'Adjective' or pos == 'Verb' or pos == 'Noun': words.append(word) words_str = ' '.join(words) new_text_tf = tf_vectorizer.transform([words_str]) if model.predict(new_text_tf) == 1: print('ê¸ì •') else: print('ë¶€ì •') . â†’ ë¦¬ë·° 3ê°œë¥¼ ê°ê° í…ŒìŠ¤íŠ¸: . guess_good_or_bad('ë¡œë²„íŠ¸ ë“œë‹ˆë¡œê°€ ì²«ì¶œê·¼ ì „ë‚ ë°¤ì— (ë‹¤ìŒë‚  ì…ì„) ì˜·ê³¼ ì‹ ë°œì„ ì¤€ë¹„í•˜ëŠ” ì¥ë©´ì´ ì™œê·¸ë ‡ê²Œ ê°€ìŠ´ì°¡í•˜ë˜ì§€..ì™ ì§€ ê°€ìŠ´ì„¤ë ˆê³  ê¸´ì¥ë˜ê³ , ê°€ê¸° ì‹«ì—ˆë˜ ì˜¨ê°– ê°ì •ë“¤ì´ 15ë…„ë§Œì— ë‹¤ì‹œ ê¸°ì–µì´ ë‚˜ë”êµ°ìš” ì¬ë°Œê³  ë”°ëœ»í•œ ì˜í™”ì…ë‹ˆë‹¤~') . ê¸ì • . guess_good_or_bad('ì‹œì‘í•˜ê³  ëª‡ë¶„ê°„ì€ ì¬ë°Œì„ê±°ê°™ì•˜ë‹¤. í•˜ì§€ë§Œ ê³„ì† ì§„í–‰ë ìˆ˜ë¡ ë­”ê°€ ì´ìƒí•¨ê³¼ ëœ¬ê¸ˆì—†ìŒê³¼ ã…ˆê°™ìŒì„ ëŠê¼ˆë‹¤. ë‚œì¤‘ì— ê°ë…ì´ ì—¬ìì¸ê²ƒì„ ì•Œê³  ë‚©ë“ì´ ê°€ë²„ë ¸ë‹¤.') . ë¶€ì • . guess_good_or_bad('ìš¸ì»¥... í•œ ì˜í™”...') . ê¸ì • . ",
    "url": "https://chaelist.github.io/docs/ml_application/sentiment_analysis/#logistic-regression%EC%9C%BC%EB%A1%9C-%ED%95%99%EC%8A%B5",
    "relUrl": "/docs/ml_application/sentiment_analysis/#logistic-regressionìœ¼ë¡œ-í•™ìŠµ"
  },"229": {
    "doc": "Social Network Analysis",
    "title": "Social Network Analysis",
    "content": ". | Harry Potter Network Analysis . | Network ë°ì´í„° ì¤€ë¹„ | ì¸ë¬¼ ê°„ ê´€ê³„ Graph ìƒì„± | Circos Plotìœ¼ë¡œ ì‹œê°í™” | ê°€ì¥ ê°€ê¹Œìš´ ì¸ë¬¼ 10ëª… ì°¾ê¸° | . | ì¸ë¬¼ì˜ ì¤‘ìš”ë„ ë³€í™” í™•ì¸ (HP 1~7) . | ì¸ë¬¼ì˜ ì¤‘ìš”ë„ ë³€í™”: Degree Centrality | ì¸ë¬¼ì˜ ì¤‘ìš”ë„ ë³€í™”: Betweenness Centrality | ì¸ë¬¼ì˜ ì¤‘ìš”ë„ ë³€í™”: Eigenvector Centrality | ì¸ë¬¼ì˜ ì¤‘ìš”ë„ ë³€í™”: PageRank | Correlation between different measures | . | . *Social Network Analysis: ì‚¬ëŒë“¤ë¡œ ì´ë£¨ì–´ì§„ ë„¤íŠ¸ì›Œí¬, ì¦‰ ì‚¬ëŒë“¤ ê°„ì˜ ê´€ê³„ë¥¼ ë¶„ì„. ",
    "url": "https://chaelist.github.io/docs/network_analysis/social_network/",
    "relUrl": "/docs/network_analysis/social_network/"
  },"230": {
    "doc": "Social Network Analysis",
    "title": "Harry Potter Network Analysis",
    "content": ": ì˜ˆì‹œë¡œ, ì†Œì„¤ &lt; Harry Potter &gt; ì‹œë¦¬ì¦ˆ ì† ì¸ë¬¼ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ Social Networkë¥¼ êµ¬í˜„í•´ ë´„. (â†’ ì‹¤ì œ ì‚¬ëŒë“¤ ì‚¬ì´ì˜ ê´€ê³„ì—ë„ ìœ ì‚¬í•œ ë¶„ì„ì„ ì ìš© ê°€ëŠ¥) . Network ë°ì´í„° ì¤€ë¹„ . | bookwarm libraryë¥¼ ì‚¬ìš©: ì±… textë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¸ë¬¼ ê°„ ê´€ê³„ ë°ì´í„°ë¥¼ êµ¬ì„±. | source, target, value(=weight) 3ê°œì˜ ì¹¼ëŸ¼ìœ¼ë¡œ êµ¬ì„±. (ë°©í–¥ì„±ì€ ì—†ì§€ë§Œ í¸ì˜ìƒ sourceì™€ targetìœ¼ë¡œ ë‚˜ëˆ”) | ìš°ì„ ì€ 1ê¶Œ &lt; Harry Potter and the Philosopherâ€™s Stone &gt; ë°ì´í„°ë§Œ ì‚¬ìš© | . import pandas as pd # ë¯¸ë¦¬ bookwarm libraryë¥¼ ì‚¬ìš©í•´ì„œ êµ¬ì¶•í•´ ë†“ì€ ì¸ë¬¼ ê°„ ê´€ê³„ ë°ì´í„° book1 = pd.read_csv('harrypotter/network_HarryPotter1.csv', index_col=0) book1.head() . | Â  | source | target | value | . | 0 | vernon | petunia | 24 | . | 1 | vernon | dudley | 29 | . | 2 | vernon | harry | 75 | . | 3 | vernon | hagrid | 6 | . | 4 | vernon | marge | 21 | . ì¸ë¬¼ ê°„ ê´€ê³„ Graph ìƒì„± . import networkx as nx import matplotlib.pyplot as plt # Create an empty graph object G = nx.Graph() # Iterate through the DataFrame &amp; add edges for _, edge in book1.iterrows(): G.add_edge(edge['source'], edge['target'], weight=edge['value']) nx.draw_networkx(G) plt.axis('off') # turn off axis plt.show() . | â€» ì´ë ‡ê²Œ nodeê°€ ë„ˆë¬´ ë§ì€ ê²½ìš° ë­‰ì¹œ ë¶€ë¶„ì´ ì˜ ì•ˆ ë³´ì¸ë‹¤ | . Circos Plotìœ¼ë¡œ ì‹œê°í™” . | nxvizì˜ Circos Plotì„ ì‚¬ìš©: nodeê°€ ë§ì•„ graphê°€ ë­‰ì¹˜ëŠ” í˜„ìƒì„ í•´ê²° | +) ê° nodeì˜ Degree Centrality ê³„ì‚°, Centralityì— ë”°ë¼ ìƒ‰ì„ ë‹¤ë¥´ê²Œ í•´ì„œ ê·¸ë¦¬ê¸° | +) Centrality ê°’ì— ë”°ë¼ nodeë¥¼ ì •ë ¬í•´ì„œ ì¤‘ìš”ë„ ë†’ì€ node ìˆœìœ¼ë¡œ íŒë³„ ê°€ëŠ¥í•˜ê²Œ í•˜ê¸° | . # degree centrality ê³„ì‚° deg_cen = nx.degree_centrality(G) # ê° nodeì— 'centrality' attribute ë„£ì–´ì£¼ê¸° for node in G.nodes(): G.nodes[node]['centrality'] = deg_cen[node] # centrality ê°’ì˜ í¬ê¸°ë¥¼ ìƒ‰ìœ¼ë¡œ êµ¬ë¶„í•´ circos plot ê·¸ë¦¬ê¸° import nxviz as nv c = nv.CircosPlot(G, node_labels=True, node_label_layout=\"rotation\", # nodeë¼ë¦¬ ì„œë¡œ ê°€ë¦¬ì§€ ì•Šë„ë¡ labelë¥¼ rotate node_color='centrality', # centrality ê°’ì„ ê¸°ë°˜ìœ¼ë¡œ ìƒ‰ì„ êµ¬ë¶„ node_order='centrality', # centrality ê°’ ìˆœì„œëŒ€ë¡œ ì •ë ¬ figsize=(12,12)) c.draw() . ê°€ì¥ ê°€ê¹Œìš´ ì¸ë¬¼ 10ëª… ì°¾ê¸° . : íŠ¹ì • nodeì™€ ì—°ê²°ëœ edgeì˜ weightê°€ í° ì¸ë¬¼ ìˆœì„œëŒ€ë¡œ ì •ë ¬í•´ ì°¾ê¸° . # ë„¤íŠ¸ì›Œí¬ G ì•ˆì—ì„œ íŠ¹ì • ë…¸ë“œ(name) ì‚¬ì´ì˜ weightê°€ í° neighbor ìˆœì„œëŒ€ë¡œ 10ëª… ì¶œë ¥í•˜ëŠ” í•¨ìˆ˜ def find_ten_closest(G, name): neighbors_list = list(G.neighbors(f\"{name}\")) temp_dict = {} for n in neighbors_list: weight = G.edges[f\"{name}\", n]['weight'] temp_dict[n] = weight sorted_list = sorted(temp_dict.items(), key=(lambda x:x[1]), reverse=True) for n, w in sorted_list[0:10]: # 1~10ìœ„ print(f'{n}: {w}') # harryì™€ ê°€ì¥ ê°€ê¹Œìš´ ì¸ë¬¼ top10ê³¼ weight ì¶œë ¥ - 1ê¶Œ ê¸°ì¤€ find_ten_closest(G, 'harry') . ron: 367 hagrid: 183 hermione: 177 snape: 94 draco: 90 quirrell: 87 dudley: 84 dumbledore: 83 vernon: 75 neville: 66 . ",
    "url": "https://chaelist.github.io/docs/network_analysis/social_network/#harry-potter-network-analysis",
    "relUrl": "/docs/network_analysis/social_network/#harry-potter-network-analysis"
  },"231": {
    "doc": "Social Network Analysis",
    "title": "ì¸ë¬¼ì˜ ì¤‘ìš”ë„ ë³€í™” í™•ì¸ (HP 1~7)",
    "content": "import pandas as pd import networkx as nx # ë¯¸ë¦¬ bookwarm libraryë¥¼ ì‚¬ìš©í•´ì„œ ë§Œë“¤ì–´ ë‘” ì¸ë¬¼ ê°„ ê´€ê³„ ë°ì´í„°: 1ê¶Œ~7ê¶Œ book1 = pd.read_csv('harrypotter/network_HarryPotter1.csv', index_col=0) book2 = pd.read_csv('harrypotter/network_HarryPotter2.csv', index_col=0) book3 = pd.read_csv('harrypotter/network_HarryPotter3.csv', index_col=0) book4 = pd.read_csv('harrypotter/network_HarryPotter4.csv', index_col=0) book5 = pd.read_csv('harrypotter/network_HarryPotter5.csv', index_col=0) book6 = pd.read_csv('harrypotter/network_HarryPotter6.csv', index_col=0) book7 = pd.read_csv('harrypotter/network_HarryPotter7.csv', index_col=0) # ê° bookìœ¼ë¡œë¶€í„° ë„¤íŠ¸ì›Œí¬ë¥¼ ë§Œë“¤ì–´ G_books listì— append books = [book1, book2, book3, book4, book5, book6, book7] G_books = [] for book in books: G_book = nx.Graph() for _, edge in book.iterrows(): G_book.add_edge(edge['source'], edge['target'], weight=edge['value']) G_books.append(G_book) . ì¸ë¬¼ì˜ ì¤‘ìš”ë„ ë³€í™”: Degree Centrality . # Creating a list of degree centrality of all the books evol = [nx.degree_centrality(G) for G in G_books] # Creating a DataFrame from the list of degree centralities in all the books index = ['Book1', 'Book2', 'Book3', 'Book4', 'Book5', 'Book6', 'Book7'] degree_evol_df = pd.DataFrame.from_records(evol, index=index).fillna(0) # ëª¨ë“  ì±…ì— ëª¨ë“  ìºë¦­í„°ê°€ ìˆëŠ” ê²Œ ì•„ë‹ˆë¯€ë¡œ, N/AëŠ” 0ìœ¼ë¡œ ì±„ì›Œì¤Œ # ê° ì±…ì˜ ì¤‘ìš”ë„ top 5 ì¸ë¬¼ ì„ íƒ set_of_char = set() for i in index: # 1~7ê¶Œ set_of_char |= set(list(degree_evol_df.T[i].sort_values(ascending=False)[0:5].index)) # |=ëŠ” ì™¼ìª½ê³¼ ì˜¤ë¥¸ìª½ì˜ í•©ì§‘í•©ì„ ì™¼ìª½ì— assigní•´ì£¼ëŠ” ì—­í•  list_of_char = list(set_of_char) # Evolution of Top Characters ì‹œê°í™” degree_evol_df[list_of_char].plot(figsize=(13, 7)); . ì¸ë¬¼ì˜ ì¤‘ìš”ë„ ë³€í™”: Betweenness Centrality . # Creating a list of betweenness centrality of all the books evol = [nx.betweenness_centrality(G, weight='weight') for G in G_books] # Making a DataFrame from the list index = ['Book1', 'Book2', 'Book3', 'Book4', 'Book5', 'Book6', 'Book7'] betweenness_evol_df = pd.DataFrame.from_records(evol, index=index).fillna(0) # ê° ì±…ì˜ ì¤‘ìš”ë„ top 5 ì¸ë¬¼ ì„ íƒ set_of_char = set() for i in index: # 1~7ê¶Œ set_of_char |= set(list(betweenness_evol_df.T[i].sort_values(ascending=False)[0:5].index)) list_of_char = list(set_of_char) # Evolution of Top Characters ì‹œê°í™” betweenness_evol_df[list_of_char].plot(figsize=(13, 7)); . ì¸ë¬¼ì˜ ì¤‘ìš”ë„ ë³€í™”: Eigenvector Centrality . # Creating a list of eigenvector centrality of all the books evol = [nx.eigenvector_centrality(G, weight='weight') for G in G_books] # Making a DataFrame from the list index = ['Book1', 'Book2', 'Book3', 'Book4', 'Book5', 'Book6', 'Book7'] eigenvector_evol_df = pd.DataFrame.from_records(evol, index=index).fillna(0) # ê° ì±…ì˜ ì¤‘ìš”ë„ top 5 ì¸ë¬¼ ì„ íƒ set_of_char = set() for i in index: # 1~7ê¶Œ set_of_char |= set(list(eigenvector_evol_df.T[i].sort_values(ascending=False)[0:5].index)) list_of_char = list(set_of_char) # Evolution of Top Characters ì‹œê°í™” eigenvector_evol_df[list_of_char].plot(figsize=(13, 7)); . ì¸ë¬¼ì˜ ì¤‘ìš”ë„ ë³€í™”: PageRank . | PageRankëŠ” Eigenvector Centralityë¥¼ ê¸°ë°˜ìœ¼ë¡œ nodeì˜ rankingì„ ë§¤ê¸°ëŠ” ì•Œê³ ë¦¬ì¦˜ | Googleì—ì„œ web page ê°„ì˜ link ê´€ê³„ë¥¼ ë°”íƒ•ìœ¼ë¡œ rankingì„ ë§¤ê¸°ëŠ” ë°©ë²•ìœ¼ë¡œ ê³ ì•ˆëœ ë°©ì‹ | ì›ë˜ëŠ” directed graphë¥¼ ìœ„í•´ ë””ìì¸ëœ ì•Œê³ ë¦¬ì¦˜ì´ì§€ë§Œ undirected graphì—ì„œë„ í™œìš© ê°€ëŠ¥ | . # Creating a list of pagerank of all the characters in all the books evol = [nx.pagerank(G) for G in G_books] # Making a DataFrame from the list index = ['Book1', 'Book2', 'Book3', 'Book4', 'Book5', 'Book6', 'Book7'] pagerank_evol_df = pd.DataFrame.from_records(evol, index=index).fillna(0) # ê° ì±…ì˜ ì¤‘ìš”ë„ top 5 ì¸ë¬¼ ì„ íƒ set_of_char = set() for i in index: # 1~7ê¶Œ set_of_char |= set(list(pagerank_evol_df.T[i].sort_values(ascending=False)[0:5].index)) list_of_char = list(set_of_char) # Evolution of Top Characters ì‹œê°í™” pagerank_evol_df[list_of_char].plot(figsize=(13, 7)); . | Eigenvector Centralityë¡œ ê³„ì‚°í–ˆì„ ë•Œì™€ ë§¤ìš° ìœ ì‚¬í•œ ê²°ê³¼ | . Correlation between different measures . 4ê°œì˜ Centrality ê³„ì‚° ë°©ì‹ë¼ë¦¬ ì–¼ë§ˆë‚˜ ìœ ì‚¬í•œ ê²°ê³¼ë¥¼ ë‚´ëŠ”ì§€ ë¹„êµ . | Correlation ê³„ì‚°: 1ê¶Œ ê¸°ì¤€ # List of 4 centrality scores of all the characters in book 1 measures = [nx.pagerank(G_books[0]), nx.eigenvector_centrality(G_books[0], weight='weight'), nx.betweenness_centrality(G_books[0], weight='weight'), nx.degree_centrality(G_books[0])] # DataFrame ë§Œë“¤ì–´ì„œ Correlation ê³„ì‚° cor = pd.DataFrame.from_records(measures, index=['PageRank', 'Eigenvector', 'Betweenness', 'Degree']) cor.T.corr() . | Â  | PageRank | Eigenvector | Betweenness | Degree | . | PageRank | 1 | 0.936511 | 0.932695 | 0.957365 | . | Eigenvector | 0.936511 | 1 | 0.880642 | 0.970408 | . | Betweenness | 0.932695 | 0.880642 | 1 | 0.926985 | . | Degree | 0.957365 | 0.970408 | 0.926985 | 1 | . | 1ê¶Œì—ì„œëŠ” 4ê°œì˜ ì¸ë¬¼ ì¤‘ìš”ë„ ê³„ì‚° ë°©ì‹ì´ ëª¨ë‘ ì„œë¡œ ìœ ì‚¬í•œ ê²°ê³¼ë¥¼ ëƒ„ | . Â  . | Correlation ê³„ì‚°: 3ê¶Œ ê¸°ì¤€ # Creating a list of all centrality of all the characters in book 3 measures = [nx.pagerank(G_books[2]), nx.eigenvector_centrality(G_books[2], weight='weight'), nx.betweenness_centrality(G_books[2], weight='weight'), nx.degree_centrality(G_books[2])] # DataFrame ë§Œë“¤ì–´ì„œ Correlation ê³„ì‚° cor = pd.DataFrame.from_records(measures, index=['PageRank', 'Eigenvector', 'Betweenness', 'Degree']) cor.T.corr() . | Â  | PageRank | Eigenvector | Betweenness | Degree | . | PageRank | 1 | 0.946001 | 0.508885 | 0.908312 | . | Eigenvector | 0.946001 | 1 | 0.561182 | 0.925465 | . | Betweenness | 0.508885 | 0.561182 | 1 | 0.705608 | . | Degree | 0.908312 | 0.925465 | 0.705608 | 1 | . | 3ê¶Œì—ì„œëŠ” ì¸ë¬¼ ì¤‘ìš”ë„ ê³„ì‚°ì—ì„œ ìœ ë… Betweenness Centralityê°€ ë‹¤ë¥¸ ë°©ë²•ë“¤ê³¼ ì°¨ì´ë¥¼ ë³´ì„ | PageRankì™€ Eigenvector Centrality ë°©ì‹ì´ íŠ¹íˆ ìœ ì‚¬í•œ ê²°ê³¼ë¥¼ ëƒ„ | . | . ",
    "url": "https://chaelist.github.io/docs/network_analysis/social_network/#%EC%9D%B8%EB%AC%BC%EC%9D%98-%EC%A4%91%EC%9A%94%EB%8F%84-%EB%B3%80%ED%99%94-%ED%99%95%EC%9D%B8-hp-17",
    "relUrl": "/docs/network_analysis/social_network/#ì¸ë¬¼ì˜-ì¤‘ìš”ë„-ë³€í™”-í™•ì¸-hp-17"
  },"232": {
    "doc": "SQL",
    "title": "SQL",
    "content": " ",
    "url": "https://chaelist.github.io/docs/sql",
    "relUrl": "/docs/sql"
  },"233": {
    "doc": "STEM Salaries 1",
    "title": "STEM Salaries 1",
    "content": ". | ë°ì´í„° íŒŒì•… ë° ì „ì²˜ë¦¬ . | ë¶ˆí•„ìš”í•œ ì¹¼ëŸ¼ ì‚­ì œ | ì¤‘ë³µê°’ ì œê±° | ê²°ì¸¡ì¹˜ ì±„ìš°ê¸° | ì´ìƒí•œ ê°’ì´ ìˆëŠ”ì§€ í™•ì¸ | company ì¹¼ëŸ¼ì˜ í‘œê¸° í†µì¼ | . | ê¸°ì—…ë³„, ì§ì—…ë³„ salary . | ê¸°ì—…ë³„ í‰ê·  salary | titleë³„ í‰ê·  salary | ê¸°ì—…ë³„ titleë³„ treemap | . | ì„±ë³„, ì¸ì¢…, êµìœ¡ìˆ˜ì¤€ë³„ salary . | genderë³„ í‰ê·  salary | raceë³„ í‰ê·  salary | educationë³„ í‰ê·  salary | . | locationë³„ salary . | ê°€ì¥ ë§ì€ íšŒì‚¬ê°€ ìœ„ì¹˜í•œ location | ê°€ì¥ í‰ê·  salaryê°€ ë†’ì€ location | . | . *ë¶„ì„ ëŒ€ìƒ ë°ì´í„°ì…‹: Data Science and STEM Salaries . | ë°ì´í„°ì…‹ ì¶œì²˜ | levels.fyiì—ì„œ ê°€ì ¸ì˜¨ 62,642ê°œì˜ salary data (Data Science &amp; STEM ì§ì¢…) | 2017.06.07 ~ 2021.08.17ì˜ ê¸°ê°„ì— ê¸°ë¡ëœ ì•½ 1060ê°œì˜ íšŒì‚¬, 15ê°œ job titleì— ëŒ€í•œ ë°ì´í„° | . ",
    "url": "https://chaelist.github.io/docs/kaggle/stem_salaries/",
    "relUrl": "/docs/kaggle/stem_salaries/"
  },"234": {
    "doc": "STEM Salaries 1",
    "title": "ë°ì´í„° íŒŒì•… ë° ì „ì²˜ë¦¬",
    "content": "# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ import import pandas as pd import numpy as np import plotly.express as px import plotly.io as pio pio.templates.default = \"plotly_white\" # default templateì„ ì§€ì • . salary_df = pd.read_csv('data/Levels_Fyi_Salary_Data.csv') salary_df.head(3) . | Â  | timestamp | company | level | title | totalyearlycompensation | location | yearsofexperience | yearsatcompany | tag | basesalary | stockgrantvalue | bonus | gender | otherdetails | cityid | dmaid | rowNumber | Masters_Degree | Bachelors_Degree | Doctorate_Degree | Highschool | Some_College | Race_Asian | Race_White | Race_Two_Or_More | Race_Black | Race_Hispanic | Race | Education | . | 0 | 6/7/2017 11:33:27 | Oracle | L3 | Product Manager | 127000 | Redwood City, CA | 1.5 | 1.5 | nan | 107000 | 20000 | 10000 | nan | nan | 7392 | 807 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | nan | nan | . | 1 | 6/10/2017 17:11:29 | eBay | SE 2 | Software Engineer | 100000 | San Francisco, CA | 5 | 3 | nan | 0 | 0 | 0 | nan | nan | 7419 | 807 | 2 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | nan | nan | . | 2 | 6/11/2017 14:53:57 | Amazon | L7 | Product Manager | 310000 | Seattle, WA | 8 | 0 | nan | 155000 | 0 | 0 | nan | nan | 11527 | 819 | 3 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | nan | nan | . â†’ ì¹¼ëŸ¼ ì •ë³´ íŒŒì•… . salary_df.info() . &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 62642 entries, 0 to 62641 Data columns (total 29 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 timestamp 62642 non-null object 1 company 62637 non-null object 2 level 62523 non-null object 3 title 62642 non-null object 4 totalyearlycompensation 62642 non-null int64 5 location 62642 non-null object 6 yearsofexperience 62642 non-null float64 7 yearsatcompany 62642 non-null float64 8 tag 61788 non-null object 9 basesalary 62642 non-null float64 10 stockgrantvalue 62642 non-null float64 11 bonus 62642 non-null float64 12 gender 43102 non-null object 13 otherdetails 40137 non-null object 14 cityid 62642 non-null int64 15 dmaid 62640 non-null float64 16 rowNumber 62642 non-null int64 17 Masters_Degree 62642 non-null int64 18 Bachelors_Degree 62642 non-null int64 19 Doctorate_Degree 62642 non-null int64 20 Highschool 62642 non-null int64 21 Some_College 62642 non-null int64 22 Race_Asian 62642 non-null int64 23 Race_White 62642 non-null int64 24 Race_Two_Or_More 62642 non-null int64 25 Race_Black 62642 non-null int64 26 Race_Hispanic 62642 non-null int64 27 Race 22427 non-null object 28 Education 30370 non-null object dtypes: float64(6), int64(13), object(10) memory usage: 13.9+ MB . ë¶ˆí•„ìš”í•œ ì¹¼ëŸ¼ ì‚­ì œ . | cityid, dmaid, rowNumber ì¹¼ëŸ¼ì€ ë¶ˆí•„ìš”í•˜ë‹¤ê³  íŒë‹¨ | Masters_Degree ~ Race_Hispanic ì¹¼ëŸ¼ì€ â€˜Raceâ€™ì™€ â€˜Educationâ€™ì˜ ì •ë³´ë¥¼ ë”ë¯¸ë³€ìˆ˜í™”í•´ë†“ì€ ì¹¼ëŸ¼ì´ë¯€ë¡œ ìš°ì„  ì‚­ì œ | . drop_columns = salary_df.loc[:, 'cityid':'Race_Hispanic'].columns salary_df.drop(drop_columns, axis='columns', inplace=True) salary_df.head(3) . | Â  | timestamp | company | level | title | totalyearlycompensation | location | yearsofexperience | yearsatcompany | tag | basesalary | stockgrantvalue | bonus | gender | otherdetails | Race | Education | . | 0 | 6/7/2017 11:33:27 | Oracle | L3 | Product Manager | 127000 | Redwood City, CA | 1.5 | 1.5 | nan | 107000 | 20000 | 10000 | nan | nan | nan | nan | . | 1 | 6/10/2017 17:11:29 | eBay | SE 2 | Software Engineer | 100000 | San Francisco, CA | 5 | 3 | nan | 0 | 0 | 0 | nan | nan | nan | nan | . | 2 | 6/11/2017 14:53:57 | Amazon | L7 | Product Manager | 310000 | Seattle, WA | 8 | 0 | nan | 155000 | 0 | 0 | nan | nan | nan | nan | . ì¤‘ë³µê°’ ì œê±° . salary_df.duplicated().sum() # ì¤‘ë³µê°’ì´ í¬í•¨ë˜ì–´ ìˆë‚˜ í™•ì¸ (ëª¨ë“  ì—´ì˜ ë°ì´í„°ê°€ ê°™ì€ ê²½ìš°) . 44 . â†’ ëª¨ë“  ì»¬ëŸ¼ì´ ì¤‘ë³µë˜ëŠ” ê²½ìš°ëŠ” ì‹¤ìˆ˜ë¡œ ì¤‘ë³µ ê¸°ì…ëœ ê²ƒì´ë¼ ê°„ì£¼, í•˜ë‚˜ì˜ rowë§Œ ë‚¨ê¸°ê³  ì‚­ì œ . print('ì¤‘ë³µ ì œê±° ì´ì „: ', len(salary_df)) salary_df.drop_duplicates(inplace=True, ignore_index=True) print('ì¤‘ë³µ ì œê±° ì´í›„: ', len(salary_df)) . ì¤‘ë³µ ì œê±° ì´ì „: 62642 ì¤‘ë³µ ì œê±° ì´í›„: 62598 . ê²°ì¸¡ì¹˜ ì±„ìš°ê¸° . salary_df.isna().sum() . timestamp 0 company 5 level 119 title 0 totalyearlycompensation 0 location 0 yearsofexperience 0 yearsatcompany 0 tag 854 basesalary 0 stockgrantvalue 0 bonus 0 gender 19529 otherdetails 22467 Race 40171 Education 32232 dtype: int64 . â†’ gender, Race, Educationì˜ nullê°’ì„ ëª¨ë‘ â€˜Unknownâ€™ìœ¼ë¡œ ì±„ì›Œì¤Œ (ë¶„ì„ì— ì‚¬ìš©í•˜ê¸° ìœ„í•¨) . salary_df[['gender', 'Race', 'Education']] = salary_df[['gender', 'Race', 'Education']].fillna('Unknown') salary_df.head(3) . | Â  | timestamp | company | level | title | totalyearlycompensation | location | yearsofexperience | yearsatcompany | tag | basesalary | stockgrantvalue | bonus | gender | otherdetails | Race | Education | . | 0 | 6/7/2017 11:33:27 | Oracle | L3 | Product Manager | 127000 | Redwood City, CA | 1.5 | 1.5 | nan | 107000 | 20000 | 10000 | Unknown | nan | Unknown | Unknown | . | 1 | 6/10/2017 17:11:29 | eBay | SE 2 | Software Engineer | 100000 | San Francisco, CA | 5 | 3 | nan | 0 | 0 | 0 | Unknown | nan | Unknown | Unknown | . | 2 | 6/11/2017 14:53:57 | Amazon | L7 | Product Manager | 310000 | Seattle, WA | 8 | 0 | nan | 155000 | 0 | 0 | Unknown | nan | Unknown | Unknown | . â†’ ì˜ ì±„ì›Œì¡ŒëŠ”ì§€ í™•ì¸ . salary_df.isna().sum() . timestamp 0 company 5 level 119 title 0 totalyearlycompensation 0 location 0 yearsofexperience 0 yearsatcompany 0 tag 854 basesalary 0 stockgrantvalue 0 bonus 0 gender 0 otherdetails 22467 Race 0 Education 0 dtype: int64 . ì´ìƒí•œ ê°’ì´ ìˆëŠ”ì§€ í™•ì¸ . | ìˆ«ìí˜• ì¹¼ëŸ¼ ì ê²€ . salary_df.describe() . | Â  | totalyearlycompensation | yearsofexperience | yearsatcompany | basesalary | stockgrantvalue | bonus | . | count | 62642.0 | 62642.0 | 62642.0 | 62642.0 | 62642.0 | 62642.0 | . | mean | 216300.37364707384 | 7.2041350850866825 | 2.7020929408384147 | 136687.28129689346 | 51486.08073259315 | 19334.746587752627 | . | std | 138033.7463773671 | 5.84037534823308 | 3.263655591673307 | 61369.27805673717 | 81874.56939076641 | 26781.292039968368 | . | min | 10000.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . | 25% | 135000.0 | 3.0 | 0.0 | 108000.0 | 0.0 | 1000.0 | . | 50% | 188000.0 | 6.0 | 2.0 | 140000.0 | 25000.0 | 14000.0 | . | 75% | 264000.0 | 10.0 | 4.0 | 170000.0 | 65000.0 | 26000.0 | . | max | 4980000.0 | 69.0 | 69.0 | 1659870.0 | 2800000.0 | 1000000.0 | . | ì „ë°˜ì ìœ¼ë¡œ íŠ¹ë³„íˆ ì´ìƒí•œ ê°’ì€ ì—†ë‹¤ê³  ìƒê°ë¨ | . fig = px.box(salary_df, x='yearsatcompany', height=300) fig.show() . salary_df.query('yearsatcompany == 69') . | Â  | timestamp | company | level | title | totalyearlycompensation | location | yearsofexperience | yearsatcompany | tag | basesalary | stockgrantvalue | bonus | gender | otherdetails | Race | Education | . | 46988 | 4/3/2021 11:04:46 | Disney | 5 | Product Designer | 102000 | Crapo, MD | 69 | 69 | Interaction Design | 100000 | 2000 | 0 | Unknown | Title: Joiooooo | Unknown | Unknown | . | yeartsatcompany == 69ë¼ëŠ” ê°’ì´ ë‹¤ë¥¸ ê°’ê³¼ ë„ˆë¬´ í¬ê¸°ê°€ ë‹¤ë¥´ê¸´ í•˜ì§€ë§Œ, ê·¸ëŸ´ ìˆ˜ ìˆë‹¤ê³  ìƒê°ë¨. | . | gender ì¹¼ëŸ¼ ì ê²€ . salary_df['gender'].unique() . array(['Unknown', 'Male', 'Female', 'Other', 'Title: Senior Software Engineer'], dtype=object) . | genderì— â€˜Title: Senior Software Engineerâ€™ë¼ëŠ” ê°’ì´ ë“¤ì–´ê°€ ìˆëŠ” ê²ƒì€ ì˜ëª» ê¸°ì…ëœ ê²ƒì´ë¼ ìƒê°ë¨ | . salary_df.query('gender == \"Title: Senior Software Engineer\"') . | Â  | timestamp | company | level | title | totalyearlycompensation | location | yearsofexperience | yearsatcompany | tag | basesalary | stockgrantvalue | bonus | gender | otherdetails | Race | Education | . | 11010 | 9/17/2019 6:23:02 | GitHub | E4 | Software Engineer | 205000 | Buda, TX | 15 | 4 | Distributed Systems (Back-End) | 177000 | 31000 | 1000 | Title: Senior Software Engineer | nan | Unknown | Unknown | . â†’ gender == â€œTitle: Senior Software Engineerâ€ë¼ê³  ê¸°ì…ë˜ì–´ ìˆë˜ index=10985 í–‰ì˜ gender ê°’ì„ â€˜Unknownâ€™ìœ¼ë¡œ ë°”ê¿”ì¤Œ . salary_df.loc[10985, 'gender'] = 'Unknown' . | Race, Education, title ì¹¼ëŸ¼ ì ê²€ . | íŠ¹ë³„íˆ ì´ìƒí•œ ê°’ì€ ì—†ìŒ | . salary_df['Race'].unique() . array(['Unknown', 'White', 'Asian', 'Black', 'Two Or More', 'Hispanic'], dtype=object) . salary_df['Education'].unique() . array(['Unknown', 'PhD', \"Master's Degree\", \"Bachelor's Degree\", 'Some College', 'Highschool'], dtype=object) . list(salary_df['title'].unique()) . ['Product Manager', 'Software Engineer', 'Software Engineering Manager', 'Data Scientist', 'Solution Architect', 'Technical Program Manager', 'Human Resources', 'Product Designer', 'Marketing', 'Business Analyst', 'Hardware Engineer', 'Sales', 'Recruiter', 'Mechanical Engineer', 'Management Consultant'] . | . company ì¹¼ëŸ¼ì˜ í‘œê¸° í†µì¼ . salary_df['company'].nunique() . 1631 . list(salary_df['company'].unique()) . ['Oracle', 'eBay', 'Amazon', 'Apple', 'Microsoft', 'Salesforce', 'Facebook', 'Uber', 'Oath', 'Google', 'Netflix', 'Pinterest', 'Linkedin', 'Adobe', 'LinkedIn', 'amazon', 'Symantec', 'Intel Corporation', 'Intel', (ìƒëµ)] . | â€˜Appleâ€™ = â€˜appleâ€™, â€˜Intelâ€™ = â€˜Intel Corporationâ€™ ë“± í†µì¼ë˜ì§€ ì•Šì€ ì´ë¦„ë“¤ì´ ê½¤ ìˆìŒ | . | ê³µë°±ì„ ì œê±°í•˜ê³  uppercaseë¡œ ë§ì¶°ì¤Œ salary_df['company'] = salary_df['company'].str.strip().str.upper() . | llc, org, ltd ë“± ë’¤ì— ë¶™ëŠ” ë‹¤ì–‘í•œ ë‹¨ì–´ë“¤ì„ ì§€ì›Œì¤Œ salary_df['company'] = salary_df['company'].str.replace(' LLC', '').str.replace('.ORG', '').str.replace(' LTD', '').str.replace(' CORPORATION', '').str.replace(' INC', '') salary_df['company'] = salary_df['company'].str.replace(' MEDIA', '').str.replace(' GROUP', '').str.replace(' TECHNOLOGY', '').str.replace(' TECHNOLOGIES', '').str.strip() . salary_df['company'].nunique() . 1081 . | ê°™ì€ íšŒì‚¬ì¸ë° ì´ë¦„ì´ ë‹¤ë¥¸ ê²½ìš°ê°€ ë” ìˆëŠ”ì§€ í™•ì¸ . | ì•ì—ì„œë¶€í„° 6ê¸€ìë¥¼ ë–¼ì–´ì„œ groupbyë¡œ ë¬¶ì–´ì„œ í™•ì¸ | . temp = salary_df[['company']] temp['6letters'] = temp['company'].str[:6] temp_groupby = temp.groupby('6letters')['company'].agg([pd.Series.nunique, set]) temp_groupby.query('nunique &gt; 1') . | 6letters | nunique | set | . | AMAZON | 3 | {â€˜AMAZON.COMâ€™, â€˜AMAZONâ€™, â€˜AMAZON WEB SERVICESâ€™} | . | AMERIC | 3 | {â€˜AMERICAN FAMILY INSURANCEâ€™, â€˜AMERICAN EXPRESSâ€™, â€˜AMERICAN AIRLINESâ€™} | . | ARISTA | 2 | {â€˜ARISTA NETWORKSâ€™, â€˜ARISTAâ€™} | . | BANK O | 2 | {â€˜BANK OF AMERICA MERRILL LYNCHâ€™, â€˜BANK OF AMERICAâ€™} | . | BETTER | 3 | {â€˜BETTER.COMâ€™, â€˜BETTERMENTâ€™, â€˜BETTER MORTGAGEâ€™} | . | (ìƒëµ) | â€¦ | â€¦ | . | ê°™ì€ íšŒì‚¬ì¸ë° í‘œê¸°ê°€ ë‹¤ë¥¸ ê²½ìš°ë¥¼ í•¨ìˆ˜ë¡œ ë§Œë“¤ì–´ì„œ ì •ë¦¬í•´ì¤Œ . | â€˜amazon.comâ€™ê³¼ â€˜amazon web servicesâ€™ì²˜ëŸ¼ ê³„ì—´ì‚¬ì§€ë§Œ ë‹¤ë¥¸ ê¸°ì—…ì´ë¼ê³  íŒë‹¨ë˜ëŠ” ê²½ìš°ëŠ” ê·¸ëƒ¥ ë‘  | . def clean_company_names(name): try: if name.startswith('AMAZON.COM'): final_name = 'AMAZON' elif name.startswith('ARISTA'): final_name = 'ARISTA' elif name.startswith('BLOOMBERG'): final_name = 'BLOOMBERG' elif name.startswith('BOOKING'): final_name = 'BOOKING.COM' elif name.startswith('DELOITTE CONSULTING'): final_name = 'DELOITTE CONSULTING' elif name.startswith('FORD'): final_name = 'FORD' elif name.startswith('CADENCE'): final_name = 'CADENCE' elif name.startswith('MCKINSEY'): final_name = 'MCKINSEY &amp; COMPANY elif name.startswith('MOTOROLA'): final_name = 'MOTOROLA' elif name.startswith('NUANCE'): final_name = 'NUANCE elif name.startswith('COSTCO'): final_name = 'COSTCO' elif name.startswith('TOYOTA'): final_name = 'TOYOTA' elif name.startswith('WALMART'): final_name = 'WALMART' elif name.startswith('MOODY'): final_name = \"MOODY'S\" elif name.startswith('MACY'): final_name = \"MACY'S\" elif name.startswith('ERNST'): final_name = \"ERNST &amp; YOUNG\" elif name.startswith('JANE STREET'): final_name = 'JANE STREET CAPITAL' elif name.startswith('LIBERTY MUTUAL'): final_name = 'LIBERTY MUTUAL' elif name.startswith('SAMSUNG'): final_name = 'SAMSUNG' else: final_name = name except: final_name = name return final_name salary_df['company'] = salary_df['company'].apply(lambda name: clean_company_names(name)) . | . â†’ ìµœì¢…ì ìœ¼ë¡œ ì •ë¦¬ ì™„ë£Œëœ ê¸°ì—… ìˆ˜: . salary_df['company'].nunique() . 1060 . ",
    "url": "https://chaelist.github.io/docs/kaggle/stem_salaries/#%EB%8D%B0%EC%9D%B4%ED%84%B0-%ED%8C%8C%EC%95%85-%EB%B0%8F-%EC%A0%84%EC%B2%98%EB%A6%AC",
    "relUrl": "/docs/kaggle/stem_salaries/#ë°ì´í„°-íŒŒì•…-ë°-ì „ì²˜ë¦¬"
  },"235": {
    "doc": "STEM Salaries 1",
    "title": "ê¸°ì—…ë³„, ì§ì—…ë³„ salary",
    "content": "ê¸°ì—…ë³„ í‰ê·  salary . | í‰ê·  salaryê°€ ë§ì€ ê¸°ì—… í™•ì¸ . | 100ëª… ì´ìƒì˜ ê¸°ë¡ì´ ìˆëŠ” ê¸°ì—…ì— í•œí•´ ë¶„ì„ (ìˆ˜ê°€ ì ìœ¼ë©´ 1ëª…ì˜ ê°’ì— ì˜í–¥ì„ ë„ˆë¬´ í¬ê²Œ ë°›ìœ¼ë¯€ë¡œ) | . sal_comp = salary_df.groupby(['company'])[['totalyearlycompensation']].agg(['mean', 'count']) sal_comp = sal_comp['totalyearlycompensation'].reset_index() sal_comp = sal_comp.query('count &gt;= 100') fig = px.bar(sal_comp.sort_values('mean', ascending=False).head(10), x='company', y='mean', color='mean', hover_name='company', hover_data=['count'], # hoverí–ˆì„ ë•Œì˜ ì œëª© &amp; ì •ë³´ ì¶”ê°€ labels={'mean':'average yearly compensation', 'count':'data count'}, color_continuous_scale = 'Brwnyl') fig.update(layout_coloraxis_showscale=False) # ì›ë˜ ì˜¤ë¥¸ìª½ì— ë‚˜ì˜¤ê²Œ ë˜ëŠ” colorbarë¥¼ ìˆ¨ê¹€ fig.show() . | 100ê°œ ì´ìƒì˜ ê¸°ë¡ì´ ìˆëŠ” ê¸°ì—… ì¤‘ ê°€ì¥ í‰ê·  yearly compensationì´ ë†’ì€ ê¸°ì—…ì€ â€˜Netflixâ€™ | . | í‰ê·  salaryê°€ ë§ì€ ê¸°ì—… top 10: genderë³„ë¡œ ë‚˜ëˆ ì„œ í™•ì¸ . sal_comp2 = salary_df.groupby(['company', 'gender'])[['totalyearlycompensation']].agg(['mean', 'count']) sal_comp2 = sal_comp2['totalyearlycompensation'].reset_index() top_10 = sal_comp.sort_values('mean', ascending=False).head(10)['company'] sal_comp2 = sal_comp2[sal_comp2['company'].isin(top_10)] fig = px.scatter(sal_comp2, x='company', y='mean', color='gender', category_orders={'company':top_10}, # ê°€ì¥ í‰ê·  salaryê°€ ë§ì€ ê¸°ì—…ë¶€í„° ìˆœì„œëŒ€ë¡œ ì‹œê°í™” hover_name='company', hover_data=['count'], # hoverí–ˆì„ ë•Œì˜ ì œëª© &amp; ì •ë³´ ì¶”ê°€ labels={'mean':'average yearly compensation', 'count':'data count'}, color_discrete_sequence=px.colors.qualitative.Antique) fig.update_traces(marker={'size': 10}) # markerì˜ sizeë¥¼ í‚¤ì›Œì¤Œ fig.show() . | 10ê°œ ê¸°ì—… ëª¨ë‘ Maleì´ Femaleë³´ë‹¤ í‰ê·  yearly compensationì´ ë†’ìŒ | . | í‰ê·  salaryê°€ ë§ì€ ê¸°ì—… top 10: job titleë³„ë¡œ ë‚˜ëˆ ì„œ í™•ì¸ . sal_comp3 = salary_df.groupby(['company', 'title'])[['totalyearlycompensation']].agg(['mean', 'count']) sal_comp3 = sal_comp3['totalyearlycompensation'].reset_index() top_10 = sal_comp.sort_values('mean', ascending=False).head(10)['company'] sal_comp3 = sal_comp3[sal_comp3['company'].isin(top_10)] fig = px.scatter(sal_comp3, x='company', y='mean', color='title', category_orders={'company':top_10}, # ê°€ì¥ í‰ê·  salaryê°€ ë§ì€ ê¸°ì—…ë¶€í„° ìˆœì„œëŒ€ë¡œ ì‹œê°í™” hover_name='title', hover_data=['count'], # hoverí–ˆì„ ë•Œì˜ ì œëª© &amp; ì •ë³´ ì¶”ê°€ labels={'mean':'average yearly compensation', 'count':'data count'}, color_discrete_sequence=px.colors.qualitative.Antique) fig.update_traces(marker={'size': 10}) # markerì˜ sizeë¥¼ í‚¤ì›Œì¤Œ fig.show() . | 10ê°œ ê¸°ì—… ëª¨ë‘ â€˜Software Engineering Managerâ€™ titleì´ í‰ê·  yearly compensationì´ ê°€ì¥ ë†’ìŒ | . | . titleë³„ í‰ê·  salary . | job titleë³„ í‰ê·  salary í™•ì¸ . sal_title = salary_df.groupby(['title'])[['totalyearlycompensation']].agg(['mean', 'count']) sal_title = sal_title['totalyearlycompensation'].reset_index() fig = px.bar(sal_title.sort_values('mean', ascending=False), x='title', y='mean', color='mean', hover_name='title', hover_data=['count'], # hoverí–ˆì„ ë•Œì˜ ì œëª© &amp; ì •ë³´ ì¶”ê°€ labels={'mean':'average yearly compensation', 'count':'data count'}, color_continuous_scale = 'Brwnyl') fig.update(layout_coloraxis_showscale=False) # ì›ë˜ ì˜¤ë¥¸ìª½ì— ë‚˜ì˜¤ê²Œ ë˜ëŠ” colorbarë¥¼ ìˆ¨ê¹€ fig.show() . | â€˜Software Engineering Managerâ€™ titleì´ ê°€ì¥ í‰ê·  yearly compensationì´ ë†’ìŒ | ê·¸ ë‹¤ìŒìœ¼ë¡œ í‰ê·  yearly compensationì´ ë†’ì€ ê±´ manager ì§ê¸‰ì¸ â€˜Product Managerâ€™, â€˜Technical Program Managerâ€™ title | . | job titleë³„ í‰ê·  salary: genderë³„ë¡œ ë‚˜ëˆ ì„œ í™•ì¸ . sal_title2 = salary_df.groupby(['title', 'gender'])[['totalyearlycompensation']].agg(['mean', 'count']) sal_title2 = sal_title2['totalyearlycompensation'].reset_index() top_order = sal_title.sort_values('mean', ascending=False)['title'] fig = px.scatter(sal_title2, x='title', y='mean', color='gender', category_orders={'title':top_order}, # ê°€ì¥ í‰ê·  salaryê°€ ë§ì€ titleë¶€í„° ìˆœì„œëŒ€ë¡œ ì‹œê°í™” hover_name='title', hover_data=['count'], # hoverí–ˆì„ ë•Œì˜ ì œëª© &amp; ì •ë³´ ì¶”ê°€ labels={'mean':'average yearly compensation', 'count':'data count'}, color_discrete_sequence=px.colors.qualitative.Antique) fig.update_traces(marker={'size': 10}) # markerì˜ sizeë¥¼ í‚¤ì›Œì¤Œ fig.show() . | ëŒ€ì²´ë¡œ ëª¨ë“  ì§êµ°ì—ì„œ Maleì´ Femaleë³´ë‹¤ í‰ê·  yearly compensationì´ ë†’ìŒ | Mechanical Engineer, Recruiter, Business Analyst ì§êµ°ì€ ë‚¨ë…€ ì°¨ì´ê°€ ê±°ì˜ ì—†ìŒ | . | job titleë³„ í‰ê·  salary: education levelë³„ë¡œ ë‚˜ëˆ ì„œ í™•ì¸ . sal_title3 = salary_df.groupby(['title', 'Education'])[['totalyearlycompensation']].agg(['mean', 'count']) sal_title3 = sal_title3['totalyearlycompensation'].reset_index() top_order = sal_title.sort_values('mean', ascending=False)['title'] fig = px.scatter(sal_title3, x='title', y='mean', color='Education', category_orders={'title':top_order}, # ê°€ì¥ í‰ê·  salaryê°€ ë§ì€ titleë¶€í„° ìˆœì„œëŒ€ë¡œ ì‹œê°í™” hover_name='title', hover_data=['count'], # hoverí–ˆì„ ë•Œì˜ ì œëª© &amp; ì •ë³´ ì¶”ê°€ labels={'mean':'average yearly compensation', 'count':'data count'}, color_discrete_sequence=px.colors.qualitative.Antique) fig.update_traces(marker={'size': 10}) # markerì˜ sizeë¥¼ í‚¤ì›Œì¤Œ fig.show() . | ëŒ€ë¶€ë¶„ì˜ ì§ì¢…ì—ì„œ PhDë¥¼ ì·¨ë“í•œ ì‚¬ëŒì´ í‰ê·  yearly compensationì´ ê°€ì¥ ë†’ìŒ | . | . ê¸°ì—…ë³„ titleë³„ treemap . | ê¸°ë¡ëœ ì§ì› ì •ë³´ê°€ ë§ì€ ê¸°ì—… top 20ì— ëŒ€í•´ì„œ treemapì„ ê·¸ë ¤ë´„ (íšŒì‚¬ë³„ ê·œëª¨ ì •ë³´ê°€ ë”°ë¡œ ìˆìœ¼ë©´ ì¢‹ê² ì§€ë§Œ, ì—†ìœ¼ë¯€ë¡œ ê¸°ë¡ëœ ì •ë³´ ìˆ˜ë¥¼ ê·œëª¨ë¼ê³  ê°„ì£¼í•˜ê³  ë¶„ì„) | . top_20 = salary_df.groupby('company')[['title']].count().sort_values(by='title', ascending=False).head(20).index fig = px.treemap(salary_df[(salary_df['company'].notnull()) &amp; (salary_df['company'].isin(top_20))], path=[px.Constant('top 20'), 'company', 'title'], color='totalyearlycompensation', color_continuous_scale='Brwnyl') fig.update_layout(margin = dict(t=50, l=25, r=25, b=25)) fig.show() . | ëŒ€ë¶€ë¶„ì˜ ê¸°ì—…ì—ì„œ â€˜Software Engineering Managerâ€™ titleì´ ê°€ì¥ í‰ê·  yearly compensationì´ ë†’ìŒ | . ",
    "url": "https://chaelist.github.io/docs/kaggle/stem_salaries/#%EA%B8%B0%EC%97%85%EB%B3%84-%EC%A7%81%EC%97%85%EB%B3%84-salary",
    "relUrl": "/docs/kaggle/stem_salaries/#ê¸°ì—…ë³„-ì§ì—…ë³„-salary"
  },"236": {
    "doc": "STEM Salaries 1",
    "title": "ì„±ë³„, ì¸ì¢…, êµìœ¡ìˆ˜ì¤€ë³„ salary",
    "content": "genderë³„ í‰ê·  salary . sal_gend = salary_df.groupby('gender')[['totalyearlycompensation']].agg(['mean', 'count']) sal_gend = sal_gend['totalyearlycompensation'].reset_index() fig = px.bar(sal_gend, x='gender', y='mean', color='gender', hover_name='gender', hover_data=['count'], # hoverí–ˆì„ ë•Œì˜ ì œëª© &amp; ì •ë³´ ì¶”ê°€ labels={'mean':'average yearly compensation', 'count':'data count'}, color_discrete_sequence=px.colors.qualitative.Antique) fig.update_traces(showlegend=False) # ì›ë˜ ì˜¤ë¥¸ìª½ì— ë‚˜íƒ€ë‚˜ê²Œ ë˜ëŠ” legendë¥¼ ìˆ¨ê¹€ fig.show() . | Maleì´ Femaleë³´ë‹¤ í‰ê·  yearly compensationì´ ë†’ìŒ | . raceë³„ í‰ê·  salary . | raceë³„ í‰ê·  yearly compensation . sal_race = salary_df.groupby('Race')[['totalyearlycompensation']].agg(['mean', 'count']) sal_race = sal_race['totalyearlycompensation'].reset_index() fig = px.bar(sal_race, x='Race', y='mean', color='Race', category_orders={'Race':['White', 'Asian', 'Black', 'Hispanic']}, hover_name='Race', hover_data=['count'], # hoverí–ˆì„ ë•Œì˜ ì œëª© &amp; ì •ë³´ ì¶”ê°€ labels={'mean':'average yearly compensation', 'count':'data count'}, color_discrete_sequence=px.colors.qualitative.Antique) fig.update_traces(showlegend=False) # ì›ë˜ ì˜¤ë¥¸ìª½ì— ë‚˜íƒ€ë‚˜ê²Œ ë˜ëŠ” legendë¥¼ ìˆ¨ê¹€ fig.show() . | Two Or More, Unknownì„ ì œì™¸í•˜ë©´ ë°±ì¸ì´ ê°€ì¥ í‰ê·  yearly compensationì´ ë§ê³ , í‘ì¸ì´ ê°€ì¥ ì ìŒ | . | gender * race êµì°¨í•´ì„œ ë¹„êµ . sal_race_gend = salary_df.groupby(['gender', 'Race'])[['totalyearlycompensation']].agg(['mean', 'count']) sal_race_gend = sal_race_gend['totalyearlycompensation'].reset_index() fig = px.scatter(sal_race_gend, x='Race', y='mean', color='gender', category_orders={'Race':['White', 'Asian', 'Black', 'Hispanic']}, hover_data=['count'], # hoverí–ˆì„ ë•Œì˜ ì œëª© &amp; ì •ë³´ ì¶”ê°€ labels={'mean':'average yearly compensation', 'count':'data count'}, color_discrete_sequence=px.colors.qualitative.Antique) fig.update_traces(marker={'size': 10}) # markerì˜ sizeë¥¼ í‚¤ì›Œì¤Œ fig.show() . | Two Or More, Unknownì„ ì œì™¸í•˜ë©´ ë°±ì¸ ë‚¨ì„±ì´ ê°€ì¥ í‰ê·  yearly compensationì´ ë§ìŒ | í˜¼í˜ˆ(Two Or More)ì€ íŠ¹ì´í•˜ê²Œë„ ì—¬ì„±ì´ ë‚¨ì„±ë³´ë‹¤ í‰ê·  yearly compensationì´ ë§ìŒ | . | raceë³„ genderë³„ ë¶„í¬ ë¹„êµ . fig = px.violin(salary_df.query('gender == \"Male\" or gender == \"Female\"'), x='Race', y='totalyearlycompensation', color='gender', category_orders={'Race':['White', 'Asian', 'Black', 'Hispanic'], 'gender':['Female', 'Male']}, hover_name='company', hover_data=['title'], labels={'totalyearlycompensation':'average yearly compensation'}, color_discrete_sequence=px.colors.qualitative.Antique) fig.show() . | ìµœìƒìœ„ê¶Œì—ëŠ” white / asian maleì´ ê°€ì¥ ë§ìŒ (Unknown ì œì™¸) | whiteëŠ” femaleë„ ìµœìƒìœ„ê¶Œì— ê½¤ ë¶„í¬ | Blackì€ 1M ì´ìƒì´ 1ëª…, Hispanicì€ ì—†ìŒ | . | . educationë³„ í‰ê·  salary . | educationë³„ í‰ê·  yearly compensation . sal_edu = salary_df.groupby('Education')[['totalyearlycompensation']].agg(['mean', 'count']) sal_edu = sal_edu['totalyearlycompensation'].reset_index() fig = px.bar(sal_edu, x='Education', y='mean', color='Education', category_orders={'Education':['PhD', \"Master's Degree\", \"Bachelor's Degree\", 'Some College', 'Highschool']}, hover_name='Education', hover_data=['count'], # hoverí–ˆì„ ë•Œì˜ ì œëª© &amp; ì •ë³´ ì¶”ê°€ labels={'mean':'average yearly compensation', 'count':'data count'}, color_discrete_sequence=px.colors.qualitative.Antique) fig.update_traces(showlegend=False) # ì›ë˜ ì˜¤ë¥¸ìª½ì— ë‚˜íƒ€ë‚˜ê²Œ ë˜ëŠ” legendë¥¼ ìˆ¨ê¹€ fig.show() . | PhD (ë°•ì‚¬ í•™ìœ„)ë¥¼ ì·¨ë“í•œ ì‚¬ëŒì´ í‰ê·  yearly compensationì´ ê°€ì¥ ë†’ìŒ | . | gender * education êµì°¨í•´ì„œ ë¹„êµ . sal_edu_gend = salary_df.groupby(['gender', 'Education'])[['totalyearlycompensation']].agg(['mean', 'count']) sal_edu_gend = sal_edu_gend['totalyearlycompensation'].reset_index() fig = px.scatter(sal_edu_gend, x='Education', y='mean', color='gender', category_orders={'Education':['PhD', \"Master's Degree\", \"Bachelor's Degree\", 'Some College', 'Highschool']}, hover_data=['count'], # hoverí–ˆì„ ë•Œì˜ ì œëª© &amp; ì •ë³´ ì¶”ê°€ labels={'mean':'average yearly compensation', 'count':'data count'}, color_discrete_sequence=px.colors.qualitative.Antique) fig.update_traces(marker={'size': 10}) # markerì˜ sizeë¥¼ í‚¤ì›Œì¤Œ fig.show() . | PhDê¹Œì§€ ì·¨ë“í•œ ë‚¨ì„±ì´ í‰ê·  yearly compensationì´ ê°€ì¥ ë†’ìŒ | Bachelorâ€™s Degree (í•™ì‚¬ í•™ìœ„)ë¥¼ ì·¨ë“í•œ ì—¬ì„±ì´ í‰ê·  yearly compensationì´ ê°€ì¥ ë‚®ìŒ (Highschoolì´ ìµœì¢…í•™ë ¥ì¸ ì—¬ì„±ì˜ ê²½ìš° ë°ì´í„° ì–‘ì´ ì ì–´ì„œ ìœ ì˜ë¯¸í•œ ë¹„êµë¼ê³  í•˜ê¸°ëŠ” ì• ë§¤) | . | educaitonë³„ genderë³„ ë¶„í¬ ë¹„êµ . orders_dict = {'Education':['PhD', \"Master's Degree\", \"Bachelor's Degree\", 'Some College', 'Highschool'], 'gender':['Female', 'Male']} fig = px.violin(salary_df.query('gender == \"Male\" or gender == \"Female\"'), x='Education', y='totalyearlycompensation', color='gender', category_orders = orders_dict, hover_name='company', hover_data=['title'], labels={'totalyearlycompensation':'average yearly compensation'}, color_discrete_sequence=px.colors.qualitative.Antique) fig.show() . | ì˜¤íˆë ¤ ìµœìƒìœ„ê¶Œ(1M ì´ìƒ)ì—ëŠ” PhDë³´ë‹¤ Mastersâ€™s &amp; Bacherâ€™s Degree ì†Œì§€ìê°€ ë” ë§ìŒ | . | . ",
    "url": "https://chaelist.github.io/docs/kaggle/stem_salaries/#%EC%84%B1%EB%B3%84-%EC%9D%B8%EC%A2%85-%EA%B5%90%EC%9C%A1%EC%88%98%EC%A4%80%EB%B3%84-salary",
    "relUrl": "/docs/kaggle/stem_salaries/#ì„±ë³„-ì¸ì¢…-êµìœ¡ìˆ˜ì¤€ë³„-salary"
  },"237": {
    "doc": "STEM Salaries 1",
    "title": "locationë³„ salary",
    "content": "ê°€ì¥ ë§ì€ íšŒì‚¬ê°€ ìœ„ì¹˜í•œ location . | ê°€ì¥ ë§ì€ íšŒì‚¬ê°€ ìœ„ì¹˜í•œ location top 10 . # ë°ì´í„°ì— ê¸°ë¡ëœ íšŒì‚¬ë§Œì„ ëŒ€ìƒìœ¼ë¡œ ê³„ì‚° comp_loc = salary_df.groupby(['location'])[['company']].nunique().reset_index() fig = px.bar(comp_loc.sort_values('company', ascending=False).head(10), x='location', y='company', color='company', hover_name='location', labels={'company':'# of companies'}, color_continuous_scale = 'Brwnyl') fig.update(layout_coloraxis_showscale=False) # ì›ë˜ ì˜¤ë¥¸ìª½ì— ë‚˜ì˜¤ê²Œ ë˜ëŠ” colorbarë¥¼ ìˆ¨ê¹€ fig.show() . | ì•½ 400ê°œì˜ íšŒì‚¬ê°€ ìœ„ì¹˜í•œ San Franciscoê°€ ê°€ì¥ ì¸ê¸° ìˆëŠ” location | . | ê°€ì¥ ë§ì€ íšŒì‚¬ê°€ ìœ„ì¹˜í•œ location top 10ì— ëŒ€í•´, ê·¸ ê³³ì— ìœ„ì¹˜í•œ íšŒì‚¬ &amp; í™”ì‚¬ë³„ salary ì •ë³´ë¥¼ ìª¼ê°œì–´ í™•ì¸ . sal_loc2 = salary_df.groupby(['location', 'company'])[['totalyearlycompensation']].agg(['mean', 'count']) sal_loc2 = sal_loc2['totalyearlycompensation'].reset_index() top_comp_locations = comp_loc.sort_values('company', ascending=False).head(10)['location'] fig = px.treemap(sal_loc2[sal_loc2['location'].isin(top_comp_locations)], path = [px.Constant('top locations'), 'location', 'company'], values='count', color = 'mean', hover_data=['count'], labels={'mean':'average yearly compensation', 'count':'data count'}, color_continuous_scale = 'Brwnyl') fig.update_layout(margin = dict(t=50, l=25, r=25, b=25)) fig.show() . | ê°€ì¥ ë§ì€ íšŒì‚¬ê°€ ìœ„ì¹˜í•œ San Franciscoì˜ ê²½ìš°, Google, Uber, Salesforce, Amazon, Facebook ë“±ì´ ëŒ€í‘œì ì¸ íšŒì‚¬ | San Franciscoì˜ í‰ê·  yearly compensationì´ Seattleì´ë‚˜ New Yorkë³´ë‹¤ ë‹¤ì†Œ ë†’ì€ í¸ | . | . ê°€ì¥ í‰ê·  salaryê°€ ë†’ì€ location . | í‰ê·  salaryê°€ ë†’ì€ location top 10 . | 100ëª… ì´ìƒì˜ ê¸°ë¡ì´ ìˆëŠ” ìœ„ì¹˜ì— í•œí•´ ë¶„ì„ (ìˆ˜ê°€ ì ìœ¼ë©´ 1ëª…ì˜ ê°’ì— ì˜í–¥ì„ ë„ˆë¬´ í¬ê²Œ ë°›ìœ¼ë¯€ë¡œ) | . sal_loc = salary_df.groupby(['location'])[['totalyearlycompensation']].agg(['mean', 'count']) sal_loc = sal_loc['totalyearlycompensation'].reset_index() sal_loc = sal_loc.query('count &gt;= 100') fig = px.bar(sal_loc.sort_values('mean', ascending=False).head(10), x='location', y='mean', color='mean', hover_name='location', hover_data=['count'], # hoverí–ˆì„ ë•Œì˜ ì œëª© &amp; ì •ë³´ ì¶”ê°€ labels={'mean':'average yearly compensation', 'count':'data count'}, color_continuous_scale = 'Brwnyl') fig.update(layout_coloraxis_showscale=False) # ì›ë˜ ì˜¤ë¥¸ìª½ì— ë‚˜ì˜¤ê²Œ ë˜ëŠ” colorbarë¥¼ ìˆ¨ê¹€ fig.show() . | 100ëª… ì´ìƒì˜ ê¸°ë¡ì´ ìˆëŠ” ìœ„ì¹˜ ì¤‘, í‰ê·  yearly compensationì´ ê°€ì¥ ë†’ì€ locationì€ Los Gatos | . | í‰ê·  salaryê°€ ê°€ì¥ ë§ì€ ìœ„ì¹˜ top 10ì— ëŒ€í•´, ê·¸ ê³³ì— ìœ„ì¹˜í•œ íšŒì‚¬ &amp; í™”ì‚¬ë³„ salary ì •ë³´ë¥¼ ìª¼ê°œì–´ í™•ì¸ . sal_loc2 = salary_df.groupby(['location', 'company'])[['totalyearlycompensation']].agg(['mean', 'count']) sal_loc2 = sal_loc2['totalyearlycompensation'].reset_index() top_locations = sal_loc.sort_values('mean', ascending=False).head(10)['location'] fig = px.treemap(sal_loc2[sal_loc2['location'].isin(top_locations)], path = [px.Constant('top locations'), 'location', 'company'], values='count', color = 'mean', hover_data=['count'], labels={'mean':'average yearly compensation', 'count':'data count'}, color_continuous_scale = 'Brwnyl') fig.update_layout(margin = dict(t=50, l=25, r=25, b=25)) fig.show() . | Los GatosëŠ” Netflixê°€ ìˆì–´ì„œ í‰ê·  salaryê°€ ë†’ê²Œ ë‚˜ì˜¨ ë“¯ | Los GatosëŠ” Netflix, Menlo Parkì€ Facebook, CupertinoëŠ” Appleì˜ í‰ê·  salaryê°€ í° ì˜í–¥ | . | . ",
    "url": "https://chaelist.github.io/docs/kaggle/stem_salaries/#location%EB%B3%84-salary",
    "relUrl": "/docs/kaggle/stem_salaries/#locationë³„-salary"
  },"238": {
    "doc": "STEM Salaries 2",
    "title": "STEM Salaries 2",
    "content": ". | ë°ì´í„° ì •ë¦¬ . | company í‘œê¸° í†µì¼ | . | ê¸°ê°„ë³„ ë¹„êµ . | datetime íƒ€ì… ê°€ê³µ | ë¶„ê¸°ë³„ ì¶”ì´ ë¹„êµ | . | basesalary, stock, bonus . | totalyearlycompensationê³¼ì˜ ê´€ê³„ | ê¸°ì—…ë³„ salary, bonus, stock grant value | . | Data Scientist ì •ë³´ íŒŒì•… . | ë¶„ê¸°ë³„ ì¶”ì´ | gender, race, education | Top ê¸°ì—… íŒŒì•… | . | . *ë¶„ì„ ëŒ€ìƒ ë°ì´í„°ì…‹: Data Science and STEM Salaries . | ë°ì´í„°ì…‹ ì¶œì²˜ | levels.fyiì—ì„œ ê°€ì ¸ì˜¨ 62,642ê°œì˜ salary data (Data Science &amp; STEM ì§ì¢…) | 2017.06.07 ~ 2021.08.17ì˜ ê¸°ê°„ì— ê¸°ë¡ëœ ì•½ 1060ê°œì˜ íšŒì‚¬, 15ê°œ job titleì— ëŒ€í•œ ë°ì´í„° | . ",
    "url": "https://chaelist.github.io/docs/kaggle/stem_salaries2/",
    "relUrl": "/docs/kaggle/stem_salaries2/"
  },"239": {
    "doc": "STEM Salaries 2",
    "title": "ë°ì´í„° ì •ë¦¬",
    "content": "# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ import import pandas as pd import numpy as np import plotly.express as px import plotly.io as pio pio.templates.default = \"plotly_white\" # default templateì„ ì§€ì • . salary_df = pd.read_csv('data/Levels_Fyi_Salary_Data.csv') salary_df.head(3) . | Â  | timestamp | company | level | title | totalyearlycompensation | location | yearsofexperience | yearsatcompany | tag | basesalary | stockgrantvalue | bonus | gender | otherdetails | cityid | dmaid | rowNumber | Masters_Degree | Bachelors_Degree | Doctorate_Degree | Highschool | Some_College | Race_Asian | Race_White | Race_Two_Or_More | Race_Black | Race_Hispanic | Race | Education | . | 0 | 6/7/2017 11:33:27 | Oracle | L3 | Product Manager | 127000 | Redwood City, CA | 1.5 | 1.5 | nan | 107000 | 20000 | 10000 | nan | nan | 7392 | 807 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | nan | nan | . | 1 | 6/10/2017 17:11:29 | eBay | SE 2 | Software Engineer | 100000 | San Francisco, CA | 5 | 3 | nan | 0 | 0 | 0 | nan | nan | 7419 | 807 | 2 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | nan | nan | . | 2 | 6/11/2017 14:53:57 | Amazon | L7 | Product Manager | 310000 | Seattle, WA | 8 | 0 | nan | 155000 | 0 | 0 | nan | nan | 11527 | 819 | 3 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | nan | nan | . | ë¶ˆí•„ìš”í•œ ì¹¼ëŸ¼ ì‚­ì œ . | cityid, dmaid, rowNumber ì¹¼ëŸ¼ì€ ë¶ˆí•„ìš”í•˜ë‹¤ê³  íŒë‹¨ | Masters_Degree ~ Race_Hispanic ì¹¼ëŸ¼ì€ â€˜Raceâ€™ì™€ â€˜Educationâ€™ì˜ ì •ë³´ë¥¼ ë”ë¯¸ë³€ìˆ˜í™”í•´ë†“ì€ ì¹¼ëŸ¼ì´ë¯€ë¡œ ìš°ì„  ì‚­ì œ | . drop_columns = salary_df.loc[:, 'cityid':'Race_Hispanic'].columns salary_df.drop(drop_columns, axis='columns', inplace=True) . | ì¤‘ë³µê°’ ì œê±° . | ëª¨ë“  ì»¬ëŸ¼ì´ ì¤‘ë³µë˜ëŠ” ê²½ìš°ëŠ” ì‹¤ìˆ˜ë¡œ ì¤‘ë³µ ê¸°ì…ëœ ê²ƒì´ë¼ ê°„ì£¼, í•˜ë‚˜ì˜ rowë§Œ ë‚¨ê¸°ê³  ì‚­ì œ | . print('ì¤‘ë³µ ì œê±° ì´ì „: ', len(salary_df)) salary_df.drop_duplicates(inplace=True, ignore_index=True) print('ì¤‘ë³µ ì œê±° ì´í›„: ', len(salary_df)) . ì¤‘ë³µ ì œê±° ì´ì „: 62642 ì¤‘ë³µ ì œê±° ì´í›„: 62598 . | ê²°ì¸¡ì¹˜ ì±„ìš°ê¸° . salary_df[['gender', 'Race', 'Education']] = salary_df[['gender', 'Race', 'Education']].fillna('Unknown') . | ì´ìƒí•œ ê°’ ì²˜ë¦¬ . # gender == \"Title: Senior Software Engineer\"ë¼ê³  ê¸°ì…ë˜ì–´ ìˆë˜ í–‰ì˜ gender ê°’ì„ 'Unknown'ìœ¼ë¡œ ë°”ê¿”ì¤Œ salary_df.loc[10985, 'gender'] = 'Unknown' . | . company í‘œê¸° í†µì¼ . | ê³µë°±ì„ ì œê±°í•˜ê³  uppercaseë¡œ ë§ì¶°ì¤Œ salary_df['company'] = salary_df['company'].str.strip().str.upper() . | llc, org, ltd ë“± ë’¤ì— ë¶™ëŠ” ë‹¤ì–‘í•œ ë‹¨ì–´ë“¤ì„ ì§€ì›Œì¤Œ salary_df['company'] = salary_df['company'].str.replace(' LLC', '').str.replace('.ORG', '').str.replace(' LTD', '').str.replace(' CORPORATION', '').str.replace(' INC', '') salary_df['company'] = salary_df['company'].str.replace(' MEDIA', '').str.replace(' GROUP', '').str.replace(' TECHNOLOGY', '').str.replace(' TECHNOLOGIES', '').str.strip() . salary_df['company'].nunique() . 1081 . | ê°™ì€ íšŒì‚¬ì¸ë° í‘œê¸°ê°€ ë‹¤ë¥¸ ê²½ìš°ë¥¼ í•¨ìˆ˜ë¡œ ë§Œë“¤ì–´ì„œ ì •ë¦¬ . | â€˜amazon.comâ€™ê³¼ â€˜amazon web servicesâ€™ì²˜ëŸ¼ ê³„ì—´ì‚¬ì§€ë§Œ ë‹¤ë¥¸ ê¸°ì—…ì´ë¼ê³  íŒë‹¨ë˜ëŠ” ê²½ìš°ëŠ” ê·¸ëƒ¥ ë‘  | . def clean_company_names(name): try: if name.startswith('AMAZON.COM'): final_name = 'AMAZON' elif name.startswith('ARISTA'): final_name = 'ARISTA' elif name.startswith('BLOOMBERG'): final_name = 'BLOOMBERG' elif name.startswith('BOOKING'): final_name = 'BOOKING.COM' elif name.startswith('DELOITTE CONSULTING'): final_name = 'DELOITTE CONSULTING' elif name.startswith('FORD'): final_name = 'FORD' elif name.startswith('CADENCE'): final_name = 'CADENCE' elif name.startswith('MCKINSEY'): final_name = 'MCKINSEY &amp; COMPANY elif name.startswith('MOTOROLA'): final_name = 'MOTOROLA' elif name.startswith('NUANCE'): final_name = 'NUANCE elif name.startswith('COSTCO'): final_name = 'COSTCO' elif name.startswith('TOYOTA'): final_name = 'TOYOTA' elif name.startswith('WALMART'): final_name = 'WALMART' elif name.startswith('MOODY'): final_name = \"MOODY'S\" elif name.startswith('MACY'): final_name = \"MACY'S\" elif name.startswith('ERNST'): final_name = \"ERNST &amp; YOUNG\" elif name.startswith('JANE STREET'): final_name = 'JANE STREET CAPITAL' elif name.startswith('LIBERTY MUTUAL'): final_name = 'LIBERTY MUTUAL' elif name.startswith('SAMSUNG'): final_name = 'SAMSUNG' else: final_name = name except: final_name = name return final_name salary_df['company'] = salary_df['company'].apply(lambda name: clean_company_names(name)) . â†’ ìµœì¢…ì ìœ¼ë¡œ ì •ë¦¬ ì™„ë£Œëœ ê¸°ì—… ìˆ˜: . salary_df['company'].nunique() . 1060 . | . ",
    "url": "https://chaelist.github.io/docs/kaggle/stem_salaries2/#%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%A0%95%EB%A6%AC",
    "relUrl": "/docs/kaggle/stem_salaries2/#ë°ì´í„°-ì •ë¦¬"
  },"240": {
    "doc": "STEM Salaries 2",
    "title": "ê¸°ê°„ë³„ ë¹„êµ",
    "content": "datetime íƒ€ì… ê°€ê³µ . | datetime íƒ€ì…ìœ¼ë¡œ ë³€í™˜ . salary_df['timestamp'] = pd.to_datetime(salary_df['timestamp']) . | timestamp ì¹¼ëŸ¼ì€ levels.fyiì— ë°ì´í„°ê°€ ê¸°ë¡ëœ ì‹œì ì„ ì˜ë¯¸ | . | ê¸°ê°„ì„ ì›”ë³„, ë¶„ê¸°ë³„ë¡œ ë¬¶ì–´ì¤Œ . salary_df['yearmonth'] = salary_df['timestamp'].dt.strftime('%Y%m') salary_df['yearquarter'] = salary_df['timestamp'].dt.to_period(\"Q\").astype('str') salary_df[['timestamp', 'yearmonth', 'yearquarter']].head() . | Â  | timestamp | yearmonth | yearquarter | . | 0 | 2017-06-07 11:33:27 | 201706 | 2017Q2 | . | 1 | 2017-06-10 17:11:29 | 201706 | 2017Q2 | . | 2 | 2017-06-11 14:53:57 | 201706 | 2017Q2 | . | 3 | 2017-06-17 00:23:14 | 201706 | 2017Q2 | . | 4 | 2017-06-20 10:58:51 | 201706 | 2017Q2 | . | . ë¶„ê¸°ë³„ ì¶”ì´ ë¹„êµ . | ë¶„ê¸°ë³„ í‰ê·  yearly compensation . quarterly_sal = salary_df.groupby(['yearquarter'])[['totalyearlycompensation']].agg(['mean', 'count']) quarterly_sal = quarterly_sal['totalyearlycompensation'].reset_index() fig = px.line(quarterly_sal, x='yearquarter', y='mean', hover_data=['count'], labels={'mean':'average yearly compensation', 'yearquarter':'posted quarter', 'count':'data count'}, color_discrete_sequence=px.colors.qualitative.Antique) fig.show() . | 2018Q1 ì´í›„ í‰ê·  yearly compensationì€ ì•½ê°„ í•˜ë½ì„¸ë¥¼ ë³´ì„. í•˜ì§€ë§Œ ì ì  ì‚¬ì´íŠ¸ì— ë“±ë¡ëœ data ìˆ˜ê°€ ì¦ê°€í•˜ë©´ì„œ yearly compensationì˜ í‰ê· ê°’ì´ í‰ê· ìœ¼ë¡œ íšŒê·€í•˜ëŠ” ê²ƒì¼ ìˆ˜ë„ ìˆìœ¼ë¯€ë¡œâ€¦ í‰ê·  salaryê°€ í•˜ë½ì„¸ë¼ê³  ë‹¨ì •ì ìœ¼ë¡œ ë§í•˜ê¸°ëŠ” ì–´ë µë‹¤. | . | ë¶„ê¸°ë³„ ë“±ë¡ëœ dataì˜ ìˆ˜ . fig = px.histogram(salary_df, x='yearquarter', labels={'yearquarter':'posted quarter'}, color_discrete_sequence=px.colors.qualitative.Antique) fig.show() . | . ",
    "url": "https://chaelist.github.io/docs/kaggle/stem_salaries2/#%EA%B8%B0%EA%B0%84%EB%B3%84-%EB%B9%84%EA%B5%90",
    "relUrl": "/docs/kaggle/stem_salaries2/#ê¸°ê°„ë³„-ë¹„êµ"
  },"241": {
    "doc": "STEM Salaries 2",
    "title": "basesalary, stock, bonus",
    "content": "totalyearlycompensationê³¼ì˜ ê´€ê³„ . | totalyearlycompensation (ì—°ê°„ ì´ì†Œë“)ê³¼ ë‹¤ë¥¸ ë³€ìˆ˜ë“¤ì´ ì–´ë–¤ ê´€ê³„ê°€ ìˆëŠ”ì§€ í™•ì¸ | . | ë³€ìˆ˜ë“¤ ê°„ì˜ ìƒê´€ê´€ê³„ë¥¼ íŒŒì•… . plt.figure(figsize=(8, 6)) sns.heatmap(salary_df.corr(), annot=True, cmap='pink_r') plt.xticks(rotation=45); . | basesalaryì™€ stockgrantvalueê°€ ê°€ì¥ totalyearlycompensationê³¼ ìƒê´€ê´€ê³„ê°€ ê°•í•œ í¸ | . | basesalaryì™€ totalyearlycompensation ì‚¬ì´ì˜ ê´€ê³„ . fig = px.scatter(salary_df, x='basesalary', y='totalyearlycompensation', trendline='ols', hover_name='company', hover_data = ['gender', 'Race', 'Education'], color_discrete_sequence=px.colors.qualitative.Antique, trendline_color_override='peachpuff') fig.show() . | stockgrantvalueì™€ totalyearlycompensation ì‚¬ì´ì˜ ê´€ê³„ . fig = px.scatter(salary_df, x='stockgrantvalue', y='totalyearlycompensation', trendline='ols', hover_name='company', hover_data = ['gender', 'Race', 'Education'], color_discrete_sequence=px.colors.qualitative.Antique, trendline_color_override='peachpuff') fig.show() . | . ê¸°ì—…ë³„ salary, bonus, stock grant value . | í‰ê·  yearly compensationì´ ë†’ì€ ê¸°ì—… top 10 . | 100ëª… ì´ìƒì˜ ê¸°ë¡ì´ ìˆëŠ” ê¸°ì—…ì— í•œí•´ ë¶„ì„ (ìˆ˜ê°€ ì ìœ¼ë©´ 1ëª…ì˜ ê°’ì— ì˜í–¥ì„ ë„ˆë¬´ í¬ê²Œ ë°›ìœ¼ë¯€ë¡œ) | . sal_comp = salary_df.groupby(['company'])[['totalyearlycompensation']].agg(['mean', 'count']) sal_comp = sal_comp['totalyearlycompensation'].reset_index() sal_comp = sal_comp.query('count &gt;= 100') fig = px.bar(sal_comp.sort_values('mean', ascending=False).head(10), x='company', y='mean', color='mean', hover_name='company', hover_data=['count'], labels={'mean':'average yearly compensation', 'count':'data count'}, color_continuous_scale = 'Brwnyl') fig.update(layout_coloraxis_showscale=False) fig.show() . | í‰ê·  base salaryê°€ ë†’ì€ ê¸°ì—… top 10 . | 100ëª… ì´ìƒì˜ ê¸°ë¡ì´ ìˆëŠ” ê¸°ì—…ì— í•œí•´ ë¶„ì„ | basesalary &gt; 0ì¸ ê²½ìš°ì— í•œí•´ì„œ ë¶„ì„ (base salaryê°€ ì—†ì„ ìˆ˜ëŠ” ì—†ìœ¼ë¯€ë¡œ, 0ì´ë¼ê³  ì íŒ ê²ƒì€ ê²°ì¸¡ì¹˜ë¼ê³  ê°„ì£¼) | . base_comp = salary_df.query('basesalary &gt; 0').groupby(['company'])[['basesalary']].agg(['mean', 'count']) base_comp = base_comp['basesalary'].reset_index() base_comp = base_comp.query('count &gt;= 100') fig = px.bar(base_comp.sort_values('mean', ascending=False).head(10), x='company', y='mean', color='mean', hover_name='company', hover_data=['count'], labels={'mean':'average base salary', 'count':'data count'}, color_continuous_scale = 'Brwnyl') fig.update(layout_coloraxis_showscale=False) fig.show() . | Netflixë¥¼ ì œì™¸í•˜ê³ ëŠ” total yearly compensationê³¼ ìˆœìœ„ê°€ ë§ì´ ë‹¬ë¼ì§ | Netflixì˜ ê²½ìš° total yearly compensationì˜ í‰ê· ê³¼ base salaryì˜ í‰ê· ì´ í¬ê²Œ ë‹¤ë¥´ì§€ ì•Šë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆìŒ | . | ë³€ìˆ˜ ê°„ ê´€ê³„ íŒŒì•…: Netflix . plt.figure(figsize=(8, 6)) sns.heatmap(salary_df.query('company == \"NETFLIX\"').corr(), annot=True, cmap='pink_r') plt.xticks(rotation=45); . | Netflixì˜ ê²½ìš°, total yearly compensationì€ base salaryì™€ ê°€ì¥ ê°•í•œ ê´€ê³„ë¥¼ ë³´ì´ê³ , stock grant valueë‚˜ bonusì™€ëŠ” ê´€ê³„ê°€ ì—†ë‹¤ëŠ” ê²ƒì´ í™•ì¸ë¨ | NetflixëŠ” stock grantë‚˜ bonusë¥¼ í†µí•´ ë³´ìƒì„ í¬ê²Œ ì§€ê¸‰í•˜ê¸°ë³´ë‹¤ëŠ” base salary ìì²´ë¥¼ ë†’ê²Œ ê°€ì ¸ê°€ëŠ” ì—°ë´‰ ì²´ê³„ë¥¼ ê°€ì§€ê³  ìˆëŠ” ê²ƒìœ¼ë¡œ ì¶”ì¸¡ë¨ | . | í‰ê·  bonusê°€ ë§ì€ ê¸°ì—… top 10 . | 100ëª… ì´ìƒì˜ ê¸°ë¡ì´ ìˆëŠ” ê¸°ì—…ì— í•œí•´ ë¶„ì„ | . bonus_comp = salary_df.groupby(['company'])[['bonus']].agg(['mean', 'count']) bonus_comp = bonus_comp['bonus'].reset_index() bonus_comp = bonus_comp.query('count &gt;= 100') fig = px.bar(bonus_comp.sort_values('mean', ascending=False).head(10), x='company', y='mean', color='mean', hover_name='company', hover_data=['count'], labels={'mean':'average bonus', 'count':'data count'}, color_continuous_scale = 'Brwnyl') fig.update(layout_coloraxis_showscale=False) fig.show() . | ë³€ìˆ˜ ê°„ ê´€ê³„ íŒŒì•…: Cruise . plt.figure(figsize=(8, 6)) sns.heatmap(salary_df.query('company == \"CRUISE\"').corr(), annot=True, cmap='pink_r') plt.xticks(rotation=45); . | Cruiseì˜ ê²½ìš°, base salaryë¿ ì•„ë‹ˆë¼ stock grant valueì™€ bonusë„ totaly yearly compensationê³¼ ê°•í•œ ê´€ê³„ë¥¼ ë³´ì„ | . | í‰ê·  stock grant valueê°€ ë§ì€ ê¸°ì—… top 10 . | 100ëª… ì´ìƒì˜ ê¸°ë¡ì´ ìˆëŠ” ê¸°ì—…ì— í•œí•´ ë¶„ì„ | . stock_comp = salary_df.groupby(['company'])[['stockgrantvalue']].agg(['mean', 'count']) stock_comp = stock_comp['stockgrantvalue'].reset_index() stock_comp = stock_comp.query('count &gt;= 100') fig = px.bar(stock_comp.sort_values('mean', ascending=False).head(10), x='company', y='mean', color='mean', hover_name='company', hover_data=['count'], # hoverí–ˆì„ ë•Œì˜ ì œëª© &amp; ì •ë³´ ì¶”ê°€ labels={'mean':'average stock grant value', 'count':'data count'}, color_continuous_scale = 'Brwnyl') fig.update(layout_coloraxis_showscale=False) # ì›ë˜ ì˜¤ë¥¸ìª½ì— ë‚˜ì˜¤ê²Œ ë˜ëŠ” colorbarë¥¼ ìˆ¨ê¹€ fig.show() . | ë³€ìˆ˜ ê°„ ê´€ê³„ íŒŒì•…: Snap . plt.figure(figsize=(8, 6)) sns.heatmap(salary_df.query('company == \"SNAP\"').corr(), annot=True, cmap='pink_r') plt.xticks(rotation=45); . | ë‹¤ì†Œ ì•½í•˜ê¸´ í•˜ì§€ë§Œ, stock grant valueê°€ total yearly compensationê³¼ ì–´ëŠ ì •ë„ì˜ ìƒê´€ê´€ê³„ë¥¼ ë³´ì„ | . | ë³€ìˆ˜ ê°„ ê´€ê³„ íŒŒì•…: Lyft . plt.figure(figsize=(8, 6)) sns.heatmap(salary_df.query('company == \"LYFT\"').corr(), annot=True, cmap='pink_r') plt.xticks(rotation=45); . | LyftëŠ” stock grant valueê°€ total yearly compensationê°€ ê°€ì¥ ê°•í•œ ê´€ê³„ë¥¼ ë³´ì„ | . | . &gt;&gt; ê²°ë¡ : ê¸°ì—…ë§ˆë‹¤ ê°ìì˜ ë³´ìƒ ì²´ê³„ì— ë”°ë¼, base salaryê°€ total yearly compensationì„ ê±°ì˜ ê²°ì •í•˜ê¸°ë„ í•˜ê³ , bonusë‚˜ stock grant valueê°€ total yearly compensationì˜ ë§ì€ ë¶€ë¶„ì„ ì°¨ì§€í•˜ê¸°ë„ í•œë‹¤ (total yearly compensationê³¼ ê° ë³€ìˆ˜ ê°„ì˜ ê´€ê³„ëŠ” ê¸°ì—…ì— ë”°ë¼ ë‹¬ë¼ì§„ë‹¤) . ",
    "url": "https://chaelist.github.io/docs/kaggle/stem_salaries2/#basesalary-stock-bonus",
    "relUrl": "/docs/kaggle/stem_salaries2/#basesalary-stock-bonus"
  },"242": {
    "doc": "STEM Salaries 2",
    "title": "Data Scientist ì •ë³´ íŒŒì•…",
    "content": ": ìµœê·¼ ìœ ë§í•œ ì§ì¢…ì¸ Data Scientistì— ëŒ€í•´ ì§‘ì¤‘ íƒêµ¬ . ë¶„ê¸°ë³„ ì¶”ì´ . | ë¶„ê¸°ë³„ data ìˆ˜ì˜ ë³€í™” . quarterly_ds_sal = ds_salary.groupby(['yearquarter'])[['totalyearlycompensation']].agg(['mean', 'count']) quarterly_ds_sal = quarterly_ds_sal['totalyearlycompensation'].reset_index() fig = px.bar(quarterly_ds_sal, x='yearquarter', y='count', labels={'yearquarter':'posted quarter', 'count':'data count'}, color_discrete_sequence=px.colors.qualitative.Antique) fig.show() . | 2021Q3ì€ 2021.08.17ê¹Œì§€ë°–ì— ì—†ì–´ì„œ ì ì€ ê²ƒ | ì ì  data scientistì— ëŒ€í•œ ê¸°ë¡ì´ ì¦ê°€í•˜ëŠ” ê²ƒìœ¼ë¡œ ë³´ì´ë‚˜, ì „ë°˜ì ìœ¼ë¡œ ì´ datasetì—ì„œëŠ” ì§ì—…ì— ê´€ê³„ ì—†ì´ ìµœê·¼ ë¶„ê¸°ì¼ìˆ˜ë¡ ë” ê¸°ë¡ì´ ë§ì•„ì§€ê¸° ë•Œë¬¸ì— data scientistì— ëŒ€í•œ ê´€ì‹¬ì´ ì¦ê°€í•œ ê²ƒì´ë¼ê³  í•´ì„í•˜ê¸°ëŠ” ì–´ë ¤ì›€ | . | ë¶„ê¸°ë³„ data scientist ê¸°ë¡ì˜ ì „ì²´ ëŒ€ë¹„ % . | data scientist ë¿ ì•„ë‹ˆë¼ ë‹¤ë¥¸ dataë„ ìˆ˜ê°€ ê°™ì´ ì¦ê°€í•˜ë¯€ë¡œ, ì „ì²´ ëŒ€ë¹„ %ë¥¼ ê³„ì‚°í•´ë´„ | . temp1 = salary_df.groupby(['yearquarter'])[['timestamp']].count() temp1.rename(columns={'timestamp':'total_count'}, inplace=True) temp2 = ds_salary.groupby(['yearquarter'])[['timestamp']].count() temp2.rename(columns={'timestamp':'ds_count'}, inplace=True) temp3 = temp1.join(temp2) temp3.fillna(0, inplace=True) temp3['ds_ratio'] = temp3['ds_count'] / temp3['total_count'] fig = px.bar(temp3.reset_index(), x='yearquarter', y='ds_ratio', labels={'yearquarter':'posted quarter'}, color_discrete_sequence=px.colors.qualitative.Antique) fig.layout.yaxis.tickformat = ',.2%' fig.show() . | 2018ë…„ Q4ë¶€í„°ëŠ” Data Scientistì— ëŒ€í•œ ê¸°ë¡ì´ ê¾¸ì¤€íˆ ì „ì²´ì˜ 4% ë‚´ì™¸ë¡œ ìœ ì§€ë¨ | ë¬¼ë¡  levels.fyiì— ê¸°ë¡ë˜ì§€ ì•Šì€ ë°ì´í„°ê°€ ë§ê² ì§€ë§Œ, ì£¼ì–´ì§„ ë°ì´í„°ë§Œ ë³´ë©´ Data Scientistì˜ ìˆ˜ê°€ ë‹¤ë¥¸ STEM ì§ì¢…ì— ë¹„í•´ ìœ ë… ì¦ê°€í•˜ê³  ìˆë‹¤ê³  ë³´ê¸°ëŠ” ì–´ë ¤ì›€ | . | ë¶„ê¸°ë³„ data scientistì˜ í‰ê·  yearly compensation . fig = px.bar(quarterly_ds_sal, x='yearquarter', y='mean', hover_data=['count'], labels={'mean':'average yearly compensation', 'yearquarter':'posted quarter', 'count':'data count'}, color_discrete_sequence=px.colors.qualitative.Antique) fig.show() . | 2018Q2ëŠ” data ìˆ˜ê°€ ë„ˆë¬´ ì ìœ¼ë¯€ë¡œ ì œì™¸í•˜ê³  ìƒê°í•˜ë©´, ëŒ€ì²´ë¡œ data scientistì˜ í‰ê·  yearly compensationì€ ë¹„ìŠ·í•œ ìˆ˜ì¤€ìœ¼ë¡œ ìœ ì§€ë¨ | . | . gender, race, education . | Genderë³„ ê¸°ë¡ ìˆ˜ . | levels.fyiì— ê¸°ë¡ëœ ë°ì´í„°ì—ë§Œ í•œì •ëœ ë¶„ì„ì´ì§€ë§Œ, Data Scientistë¼ëŠ” ì§ì—…ì— ì–´ëŠ ì„±ë³„ì´ ë” ë§ì€ì§€ ëŒ€ê°• íŒŒì•…í•  ìˆ˜ ìˆìŒ | . fig = px.pie(ds_salary.query('gender != \"Unknown\"'), names='gender', # Unkownì€ ì œì™¸í•˜ê³  ê³„ì‚° color_discrete_sequence=px.colors.qualitative.Antique, hole=0.4) fig.show() . | ë‚¨ì„±ì´ 3/4 ì´ìƒì„ ì°¨ì§€ | . | Raceë³„ ê¸°ë¡ ìˆ˜ . fig = px.pie(ds_salary.query('Race != \"Unknown\"'), names='Race', # Unkownì€ ì œì™¸í•˜ê³  ê³„ì‚° color_discrete_sequence=px.colors.qualitative.Antique, hole=0.4) fig.show() . | Asianì´ ê³¼ë°˜ìˆ˜ë¥¼ ì°¨ì§€, ê·¸ ë‹¤ìŒì€ White | . | Educationë³„ ê¸°ë¡ ìˆ˜ . fig = px.pie(ds_salary.query('Education != \"Unknown\"'), names='Education', color_discrete_sequence=px.colors.qualitative.Antique, hole=0.4) fig.show() . | Masterâ€™s Degree ì†Œì§€ìê°€ ê³¼ë°˜ìˆ˜, ê·¸ ë‹¤ìŒì€ PhD | ì„ì‚¬ í•™ìœ„ ì´ìƒì„ ì†Œì§€í•œ ì‚¬ëŒì´ ìœ ë… ë§ì€ ì§ì—…êµ°ìœ¼ë¡œ ë³´ì„ (cf. ì „ì²´ ë°ì´í„° ì¤‘ì—ì„œëŠ” í•™ì‚¬ í•™ìœ„ ì†Œì§€ìê°€ ì•½ 41.5%) | . | Genderë³„ í‰ê·  yearly compensation . ds_sal_gend = ds_salary.groupby('gender')[['totalyearlycompensation']].agg(['mean', 'count']) ds_sal_gend = ds_sal_gend['totalyearlycompensation'].reset_index() fig = px.bar(ds_sal_gend, x='gender', y='mean', color='gender', hover_name='gender', hover_data=['count'], labels={'mean':'average yearly compensation', 'count':'data count'}, color_discrete_sequence=px.colors.qualitative.Antique) fig.update_traces(showlegend=False) fig.show() . | Maleì´ Femaleë³´ë‹¤ ë‹¤ì†Œ ë†’ì§€ë§Œ, ì°¨ì´ê°€ ì•„ì£¼ ìœ ì˜ë¯¸í•˜ë‹¤ê³  ë³´ê¸°ëŠ” ì–´ë ¤ìš¸ ë“¯ (cf. í‰ê·  ì°¨ì´ë¥¼ t-testë¡œ ê²€ì •í•˜ë©´ pê°’ì€ 0.02 ì •ë„) | . | Raceë³„ í‰ê·  yearly compensation . ds_sal_race = ds_salary.groupby('Race')[['totalyearlycompensation']].agg(['mean', 'count']) ds_sal_race = ds_sal_race['totalyearlycompensation'].reset_index() fig = px.bar(ds_sal_race, x='Race', y='mean', color='Race', category_orders={'Race':['White', 'Asian', 'Black', 'Hispanic']}, hover_name='Race', hover_data=['count'], labels={'mean':'average yearly compensation', 'count':'data count'}, color_discrete_sequence=px.colors.qualitative.Antique) fig.update_traces(showlegend=False) fig.show() . | ë“±ë¡ëœ Data Scientistì˜ ìˆ˜ëŠ” Asianì´ ë” ë§ì§€ë§Œ, ì—°ê°„ ë³´ìƒì˜ í‰ê· ì€ Whiteê°€ Asianë³´ë‹¤ ë‹¤ì†Œ ë†’ìŒ | . | Educationë³„ í‰ê·  yearly compensation . ds_sal_edu = ds_salary.groupby('Education')[['totalyearlycompensation']].agg(['mean', 'count']) ds_sal_edu = ds_sal_edu['totalyearlycompensation'].reset_index() fig = px.bar(ds_sal_edu, x='Education', y='mean', color='Education', category_orders={'Education':['PhD', \"Master's Degree\", \"Bachelor's Degree\", 'Some College', 'Highschool']}, hover_name='Education', hover_data=['count'], labels={'mean':'average yearly compensation', 'count':'data count'}, color_discrete_sequence=px.colors.qualitative.Antique) fig.update_traces(showlegend=False) fig.show() . | ë“±ë¡ëœ Data Scientistì˜ ìˆ˜ëŠ” Masterâ€™s Degree ì†Œì§€ìê°€ ê°€ì¥ ë§ì§€ë§Œ, ì—°ê°„ ë³´ìƒì˜ í‰ê· ì€ PhD ì†Œì§€ìê°€ ê°€ì¥ ë†’ìŒ | . | . Top ê¸°ì—… íŒŒì•… . | ë“±ë¡ëœ data scientist ìˆ˜ê°€ ë§ì€ ê¸°ì—… í™•ì¸ . ds_count = ds_salary.groupby(['company'])[['timestamp']].count().reset_index() fig = px.bar(ds_count.sort_values('timestamp', ascending=False).head(10), x='company', y='timestamp', color='timestamp', hover_name='company', labels={'timestamp':'data count'}, color_continuous_scale = 'Brwnyl') fig.update(layout_coloraxis_showscale=False) fig.show() . | ê°€ì¥ Data Scientistë¥¼ ë§ì´ ê³ ìš©í•˜ëŠ” íšŒì‚¬ëŠ” Amazon, Microsoft, Facebookì´ë¼ê³  ì¶”ì • (íšŒì‚¬ì˜ ê·œëª¨ì™€ ë¹„ë¡€í•˜ëŠ” ë“¯) | . | Salary Top ê¸°ì—… . | í‰ê·  yearly compensationì´ ë†’ì€ top 10 ê¸°ì—… íŒŒì•… | 20ëª… ì´ìƒì˜ ê¸°ë¡ì´ ìˆëŠ” ê¸°ì—…ì— í•œí•´ ë¶„ì„ (ìˆ˜ê°€ ì ìœ¼ë©´ í‰ê· ì´ 1ëª…ì˜ ì˜í–¥ì„ í¬ê²Œ ë°›ìœ¼ë¯€ë¡œ) | . ds_sal_comp = ds_salary.groupby(['company'])[['totalyearlycompensation']].agg(['mean', 'count']) ds_sal_comp = ds_sal_comp['totalyearlycompensation'].reset_index() ds_sal_comp = ds_sal_comp.query('count &gt;= 20') fig = px.bar(ds_sal_comp.sort_values('mean', ascending=False).head(10), x='company', y='mean', color='mean', hover_name='company', hover_data=['count'], # hoverí–ˆì„ ë•Œì˜ ì œëª© &amp; ì •ë³´ ì¶”ê°€ labels={'mean':'average yearly compensation', 'count':'data count'}, color_continuous_scale = 'Brwnyl') fig.update(layout_coloraxis_showscale=False) # ì›ë˜ ì˜¤ë¥¸ìª½ì— ë‚˜ì˜¤ê²Œ ë˜ëŠ” colorbarë¥¼ ìˆ¨ê¹€ fig.show() . | ê°€ì¥ í‰ê·  yearly compensationì´ ë†’ì€ ê¸°ì—…ì€ Netflix | . | . ",
    "url": "https://chaelist.github.io/docs/kaggle/stem_salaries2/#data-scientist-%EC%A0%95%EB%B3%B4-%ED%8C%8C%EC%95%85",
    "relUrl": "/docs/kaggle/stem_salaries2/#data-scientist-ì •ë³´-íŒŒì•…"
  },"243": {
    "doc": "Text ë¶„ì„",
    "title": "Text ë¶„ì„",
    "content": " ",
    "url": "https://chaelist.github.io/docs/text_analysis",
    "relUrl": "/docs/text_analysis"
  },"244": {
    "doc": "ì‹œê³„ì—´ ë°ì´í„° ì˜ˆì¸¡",
    "title": "ì‹œê³„ì—´ ë°ì´í„° ì˜ˆì¸¡",
    "content": ". | Prophet ê¸°ì´ˆ . | ì´ë¡  | ì‹œê³„ì—´ ë°ì´í„° ì¤€ë¹„ | Prophetìœ¼ë¡œ ì‹œê³„ì—´ ì˜ˆì¸¡ | . | íŒŒë¼ë¯¸í„° ì¡°ì •í•˜ê¸° . | Trend ì¡°ì ˆ (changepoint ì¡°ì ˆ) | Seasonality ë°˜ì˜ | Holiday ë°˜ì˜ | Outlier ì œì™¸í•˜ê¸° | . | . *Facebookì˜ ì˜¤í”ˆì†ŒìŠ¤ ë¼ì´ë¸ŒëŸ¬ë¦¬ì¸ â€˜Prophetâ€˜ì„ í™œìš© . ",
    "url": "https://chaelist.github.io/docs/ml_application/time_series/",
    "relUrl": "/docs/ml_application/time_series/"
  },"245": {
    "doc": "ì‹œê³„ì—´ ë°ì´í„° ì˜ˆì¸¡",
    "title": "Prophet ê¸°ì´ˆ",
    "content": "ì´ë¡  . | ì„¸ ê°œì˜ ì£¼ìš” ìš”ì†Œë¥¼ í™œìš©í•´ ì˜ˆì¸¡: Trend, Seasonality, Holiday | $ y(t) = g(t) + s(t) + h(t) + \\epsilon_i $ . | $g(t)$: the trend function. models non-periodic changes in the value of the time series. ì£¼ê¸°ì ì´ì§€ ì•Šì€ ë³€í™”ë¥¼ ë°˜ì˜ (íŠ¸ë Œë“œë¥¼ ë°˜ì˜) | $s(t)$: represents periodic changes (ex. weekly / yearly seasonality) ì£¼ê¸°ì ì¸ ë³€í™”ë¥¼ ë°˜ì˜ (ì£¼, ì¼, ì—° ë“±ì˜ ê¸°ê°„ì— ë”°ë¼ ì£¼ê¸°ì ìœ¼ë¡œ ë‚˜íƒ€ë‚˜ëŠ” íë¦„) | $h(t)$: represents the effects of holidays ë¶ˆê·œì¹™í•œ eventì¸ íœ´ì¼ì˜ ì˜í–¥ì„ ë°˜ì˜ | $\\epsilon_i$: the error term. represents any unusual changes not accommodated by the model (ì •ê·œë¶„í¬ë¥¼ ë”°ë¥¸ë‹¤ê³  ê°€ì •) | . | . ì‹œê³„ì—´ ë°ì´í„° ì¤€ë¹„ . # í•„ìš”í•œ libraryë¥¼ import import pandas as pd import matplotlib.pyplot as plt %matplotlib inline from datetime import datetime from pandas_datareader import data . â£1. Alphabet (Google ëª¨ê¸°ì—…) ì£¼ì‹ ì •ë³´ ê°€ì ¸ì˜¤ê¸° . start_date = datetime(2020, 1, 1) end_date = datetime(2021, 5, 1) ## cf) datetime.now()ë¼ê³  í•˜ë©´ ì˜¤ëŠ˜ ë‚ ì§œë¡œ ê°€ì ¸ì˜´ Google = data.DataReader('GOOGL','yahoo', start_date, end_date) # yahoo ì£¼ì‹ ë°ì´í„°ì—ì„œ Alphabet ì£¼ì‹ ë°ì´í„° ê°€ì ¸ì˜´ Google.head() . | Date | High | Low | Open | Close | Volume | Adj Close | . | 2020-01-02 | 1368.68 | 1346.49 | 1348.41 | 1368.68 | 1363900 | 1368.68 | . | 2020-01-03 | 1373.75 | 1347.32 | 1348 | 1361.52 | 1170400 | 1361.52 | . | 2020-01-06 | 1398.32 | 1351 | 1351.63 | 1397.81 | 2338400 | 1397.81 | . | 2020-01-07 | 1403.5 | 1391.56 | 1400.46 | 1395.11 | 1716500 | 1395.11 | . | 2020-01-08 | 1411.85 | 1392.63 | 1394.82 | 1405.04 | 1765700 | 1405.04 | . Â  . # ì¢…ê°€(Close) ê¸°ì¤€ìœ¼ë¡œ ê·¸ë˜í”„ ê·¸ë ¤ë³´ê¸° Google['Close'].plot(figsize=(11,5), grid=True); . Â  . â£2. 2021.01 ì „ê¹Œì§€ë¡œ ë°ì´í„°ë¥¼ ìë¥´ê¸° . # 2020-12-31ê¹Œì§€ì˜ ë°ì´í„°ë§Œì„ ì‚¬ìš©: 2021-05-01ê¹Œì§€ì˜ ë°ì´í„°ë¥¼ ì˜ˆì¸¡í•´ë³¼ ì˜ˆì • Google_2020 = Google[:'2020-12-31'] Google_2020.tail() . | Date | High | Low | Open | Close | Volume | Adj Close | . | 2020-12-24 | 1742.41 | 1724.35 | 1729 | 1734.16 | 465600 | 1734.16 | . | 2020-12-28 | 1787 | 1741.82 | 1744.91 | 1773.96 | 1382500 | 1773.96 | . | 2020-12-29 | 1788.47 | 1755.11 | 1787.23 | 1757.76 | 986300 | 1757.76 | . | 2020-12-30 | 1767.76 | 1728 | 1765 | 1736.25 | 1051300 | 1736.25 | . | 2020-12-31 | 1757.5 | 1736.09 | 1737.27 | 1752.64 | 1053500 | 1752.64 | . â£3. Prophetì— í•™ìŠµì‹œí‚¤ê¸° ìœ„í•œ í¬ë§·ìœ¼ë¡œ ë§ì¶”ê¸° . | pandas.DataFrame with â€˜yâ€™ and â€˜dsâ€™ columns | . # Prophetì— í•™ìŠµì‹œí‚¤ë ¤ë©´ ì•„ë˜ì™€ ê°™ì€ í¬ë§·ì´ì—¬ì•¼ í•¨ df = pd.DataFrame({'ds':Google_2020.index, 'y':Google_2020['Close']}) df.reset_index(inplace=True, drop=True) df.head() . | Â  | ds | y | . | 0 | 2020-01-02 | 1368.68 | . | 1 | 2020-01-03 | 1361.52 | . | 2 | 2020-01-06 | 1397.81 | . | 3 | 2020-01-07 | 1395.11 | . | 4 | 2020-01-08 | 1405.04 | . Prophetìœ¼ë¡œ ì‹œê³„ì—´ ì˜ˆì¸¡ . | ë¨¼ì € pip install pystan, pip install prophetìœ¼ë¡œ ì„¤ì¹˜í•´ì¤˜ì•¼ ì‚¬ìš© ê°€ëŠ¥ (ì„¤ì¹˜ ë°©ë²•) | Google Colaboratoryì—ì„œëŠ” ë³„ë„ì˜ ì„¤ì¹˜ ì—†ì´ importí•´ì„œ ì‚¬ìš© ê°€ëŠ¥ | . from fbprophet import Prophet m = Prophet() m.fit(df) . | yearly seasonalityì™€ daily seasonalityëŠ” ë”°ë¡œ ì¨ì£¼ì§€ ì•Šìœ¼ë©´ ë°˜ì˜ë˜ì§€ ì•ŠìŒ | . Â  . â£1. 2021.05.01ê¹Œì§€ì˜ ì£¼ê°€ë¥¼ ì˜ˆì¸¡í•´ ë´„ . future = m.make_future_dataframe(periods=121) # 2021.05.01ê¹Œì§€ì˜ ë°ì´í„°ë¥¼ ì˜ˆì¸¡í•  ì˜ˆì • (=121ì¼) forecast = m.predict(future) forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail() . | Â  | ds | yhat | yhat_lower | yhat_upper | . | 369 | 2021-04-27 | 2044.79 | 1870.67 | 2224.93 | . | 370 | 2021-04-28 | 2050.44 | 1880.35 | 2231.62 | . | 371 | 2021-04-29 | 2046.42 | 1869.55 | 2222.84 | . | 372 | 2021-04-30 | 2050.11 | 1872.39 | 2221.89 | . | 373 | 2021-05-01 | 2032.17 | 1848.47 | 2218.29 | . â£2. forecast ì‹œê°í™”í•´ì„œ í™•ì¸ . m.plot(forecast); . â£3. ìš”ì†Œë³„ë¡œ í™•ì¸í•´ë³´ê¸° . m.plot_components(forecast); . â£4. 2021ë…„ì˜ ì‹¤ì œ ë°ì´í„°ì™€ ì˜ˆì¸¡ê°’ì„ ë¹„êµí•´ì„œ ê·¸ë ¤ë³´ê¸° . plt.figure(figsize=(12,6)) plt.plot(Google.index, Google['Close'], label='real') plt.plot(forecast['ds'], forecast['yhat'], label='forecast') plt.grid() plt.legend() plt.show() . ",
    "url": "https://chaelist.github.io/docs/ml_application/time_series/#prophet-%EA%B8%B0%EC%B4%88",
    "relUrl": "/docs/ml_application/time_series/#prophet-ê¸°ì´ˆ"
  },"246": {
    "doc": "ì‹œê³„ì—´ ë°ì´í„° ì˜ˆì¸¡",
    "title": "íŒŒë¼ë¯¸í„° ì¡°ì •í•˜ê¸°",
    "content": "Trend ì¡°ì ˆ (changepoint ì¡°ì ˆ) . | changepoint_range: changepoint ì„¤ì • ê°€ëŠ¥ ë²”ìœ„ë¥¼ ì¡°ì ˆ (default: 80%) | changepoint_prior_scale: changepointì˜ ìœ ì—°ì„± ì¡°ì ˆ (default: 0.5) | changepoints: íŠ¸ë Œë“œ ë³€í™” ì‹œì ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ì§ì ‘ assign | n_changepoints: changepointì˜ ê°œìˆ˜ë¥¼ ì¡°ì ˆ | . Â  . ## ê¸°ë³¸ìœ¼ë¡œ ì¡íˆëŠ” changepoint ì‹œê°í™”í•´ë³´ê¸° from fbprophet.plot import add_changepoints_to_plot # í•„ìš”í•œ í•¨ìˆ˜ m = Prophet(yearly_seasonality=False, daily_seasonality=True) m.fit(df) future = m.make_future_dataframe(periods=121) forecast = m.predict(future) fig = m.plot(forecast) add_changepoints_to_plot(fig.gca(), m, forecast); . 1. changepoint_range ì¡°ì ˆ . | ê¸°ë³¸ì ìœ¼ë¡œ Prophetì€ ì‹œê³„ì—´ ë°ì´í„°ì˜ 80% í¬ê¸°ì—ì„œ ChangePointë¥¼ ì§€ì • (overfittingì„ í”¼í•˜ê¸° ìœ„í•´ ì „ì²´ ë°ì´í„°ê°€ ì•„ë‹Œ í•™ìŠµ ë°ì´í„°ì˜ ì•ë¶€ë¶„ 80%ì˜ ë°ì´í„°ë§Œì„ ì‚¬ìš©í•´ ë³€ë™ì ì„ ì°¾ëŠ” ê²ƒ) | . # changepoint_rangeë¥¼ 0.5ë¡œ ë³€ê²½ # ì‹œê³„ì—´ ë°ì´í„°ì˜ ì• 50%ì—ì„œë§Œ changepointë¥¼ ì§€ì •í•˜ë¼ëŠ” ëœ» m = Prophet(yearly_seasonality=False, daily_seasonality=True, changepoint_range=0.5) m.fit(df) future = m.make_future_dataframe(periods=121) forecast = m.predict(future) fig = m.plot(forecast) add_changepoints_to_plot(fig.gca(), m, forecast); . 2. changepoint_prior_scale ì¡°ì ˆ . | ChangePointì˜ ìœ ì—°ì„±ì„ ì¡°ì •í•˜ëŠ” ë°©ë²• | ê¸°ë³¸ ê°’ì€ 0.05ì´ë©°, ê°’ì„ ëŠ˜ë¦¬ë©´ ë” ìœ ì—°í•´ì§€ê³ (=underfitting í•´ê²°), ê°’ì„ ì¤„ì´ë©´ ìœ ì—°ì„± ê°ì†Œ(=overfitting í•´ê²°) | . # changepoint_prior_scaleë¥¼ 0.1ë¡œ ëŠ˜ë ¤ë´„. # 0.05ì¼ ë•Œ(=default)ë³´ë‹¤ ìœ ë™ì ìœ¼ë¡œ changepointë¥¼ ì°¾ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤. (â†’ overfitting ê°€ëŠ¥ì„±) m = Prophet(yearly_seasonality=False, daily_seasonality=True, changepoint_prior_scale=0.1) m.fit(df) future = m.make_future_dataframe(periods=121) forecast = m.predict(future) fig = m.plot(forecast) a = add_changepoints_to_plot(fig.gca(), m, forecast) . 3. changepointsë¡œ ì§ì ‘ ë‚ ì§œ assign . | changepointì¼ ìˆ˜ ìˆëŠ” ë‚ ì§œë“¤ì„ ì§ì ‘ ì§€ì •í•´ì£¼ëŠ” ê²ƒ. | changepointsì— ê°’ì„ ëª…ì‹œí•´ì£¼ì§€ ì•Šìœ¼ë©´ ìë™ìœ¼ë¡œ changepointë“¤ì´ ì„ íƒëœë‹¤ | . # changepointë¥¼ ë‚´ê°€ ì§ì ‘ ì§€ì •í•´ì¤Œ m = Prophet(yearly_seasonality=False, daily_seasonality=True, changepoints=['2020-03-20', '2020-09-25']) m.fit(df) future = m.make_future_dataframe(periods=121) forecast = m.predict(future) fig = m.plot(forecast) a = add_changepoints_to_plot(fig.gca(), m, forecast) . Seasonality ë°˜ì˜ . | yearly_seasonality: ì—° ì£¼ê¸°ì˜ ê³„ì ˆì„±ì„ ë°˜ì˜. defaultëŠ” 10. | weekly_seasonality: ì£¼ê°„ ì£¼ê¸°ì˜ ê³„ì ˆì„±ì„ ë°˜ì˜. defaultëŠ” 3 | daily_seasonality: ì¼ ì£¼ê¸°ì˜ ê³„ì ˆì„±ì„ ë°˜ì˜. defaultëŠ” 4 | seasonality_mode: â€˜additiveâ€™ í˜¹ì€ â€˜multiplicativeâ€™ (â€˜additiveâ€™ê°€ default) | seasonality_prior_scale: ê³„ì ˆì„± ë°˜ì˜ ê°•ë„ | . Â  . 1. yearly_seasonality . # yearly_seasonality=Trueë¼ê³  í•´ì£¼ë©´ defaultê°’ì¸ 10ìœ¼ë¡œ ì„¤ì •ë¨ m = Prophet(yearly_seasonality=True) m.fit(df) future = m.make_future_dataframe(periods=121) forecast = m.predict(future) m.plot_components(forecast); . # ì—° ë‹¨ìœ„ì˜ seasonalityê°€ ìˆë‹¤ê³  ê°„ì£¼â†’ 2020ë…„ê³¼ ë™ì¼í•˜ê²Œ 3ì›”ì— ê¸‰ê°í•˜ëŠ” ëª¨ì–‘ìœ¼ë¡œ ì˜ˆì¸¡ m.plot(forecast); . +) yearly_seasonalityì˜ ê°•ë„ ë†’ì—¬ë³´ê¸° . # yearly_seasonalityë¥¼ 20ìœ¼ë¡œ ì„¤ì •í•´ì£¼ë©´, defaultê°’ì¸ 10ì— ë¹„í•´ ë” ì—° ê³„ì ˆì„±ì„ ê°•í•˜ê²Œ ë°˜ì˜ m = Prophet(yearly_seasonality=20) m.fit(df) future = m.make_future_dataframe(periods=121) forecast = m.predict(future) m.plot_components(forecast); . m.plot(forecast); # yearly_seasonlityê°€ ë” ê°•í•˜ê²Œ ë°˜ì˜ë¨ . 2. weekly_seasonality . # weekly_seasonalityëŠ” ë”°ë¡œ ì¨ì£¼ì§€ ì•Šì•„ë„ defaultë¡œ 3ì´ ë“¤ì–´ê°. 20ìœ¼ë¡œ ë„£ìœ¼ë©´ ë” ê°•í•˜ê²Œ ë°˜ì˜. m = Prophet(weekly_seasonality=20) m.fit(df) future = m.make_future_dataframe(periods=121) forecast = m.predict(future) m.plot_components(forecast); . m.plot(forecast); # weekly_seasonlityê°€ ê°•í•˜ê²Œ ë°˜ì˜ë¨ . 3. daily_seasonality . # daily_seasonality=Trueë¼ê³  í•´ì£¼ë©´ defaultê°’ì¸ 4ìœ¼ë¡œ ì„¤ì •ë¨ m = Prophet(daily_seasonality=True) m.fit(df) future = m.make_future_dataframe(periods=121) forecast = m.predict(future) m.plot_components(forecast); . m.plot(forecast); # daily_seasonlityê°€ ë°˜ì˜ë¨ . +) daily_seasonalityì˜ ê°•ë„ ë†’ì—¬ë³´ê¸° . m = Prophet(daily_seasonality=20) # default: 4 m.fit(df) future = m.make_future_dataframe(periods=121) forecast = m.predict(future) m.plot_components(forecast); . m.plot(forecast); # daily_seasonlityê°€ ê°•í•˜ê²Œ ë°˜ì˜ë¨ . 4. custom seasonality ì¶”ê°€ . | m.add_seasonality()ë¡œ ì§ì ‘ seasonalityë¥¼ ë§Œë“¤ì–´ ì¶”ê°€í•  ìˆ˜ ìˆë‹¤ | . # monthly seasonality ë§Œë“¤ì–´ì„œ ì¶”ê°€í•´ë³´ê¸° m = Prophet() m.add_seasonality(name='monthly', period=30.5, fourier_order=5) m.fit(df) future = m.make_future_dataframe(periods=121) forecast = m.predict(future) m.plot_components(forecast); . m.plot(forecast); . 5. multiplicative seasonality . | seasonality_mode: additiveê°€ default | additiveëŠ” seasonalityê°€ ì¼ì •í•¨ì„ ì˜ë¯¸í•˜ê³ , multiplicativeëŠ” seasonalityê°€ íŠ¸ë Œë“œì™€ í•¨ê»˜ ì ì  ì¦ê°€í•¨ì„ ì˜ë¯¸ | multiplicative seasonalityê°€ í•„ìš”í•œ ê²½ìš°: number of air passengers ì˜ˆì‹œ | . # seasonality_mode='multiplicative'ë¼ê³  ì„¤ì •í•´ì£¼ë©´ ì ì  seasonalityê°€ ì¦ê°€í•˜ëŠ” ê²ƒìœ¼ë¡œ ë°˜ì˜ë¨ m = Prophet(yearly_seasonality=False, daily_seasonality=True, seasonality_mode='multiplicative') m.fit(df) future = m.make_future_dataframe(periods=121) forecast = m.predict(future) m.plot(forecast); # ì´ ë°ì´í„°ì—ì„œëŠ” ì‚¬ì‹¤ multiplicative seasonalityê°€ í•„ìš”í•˜ì§€ ì•ŠìŒ . Holiday ë°˜ì˜ . | holidays: íœ´ì¼ / ì´ë²¤íŠ¸ ê¸°ê°„ì„ ëª…ì‹œí•œ ë°ì´í„°í”„ë ˆì„ . | â€˜lower_windowâ€™, â€˜upper_windowâ€™ë¥¼ ì„¤ì •í•´ ì „í›„ ê¸°ê°„ì— ë¯¸ì¹˜ëŠ” íœ´ì¼ì˜ ì˜í–¥ì„ ë°˜ì˜ ê°€ëŠ¥ | . | holiday_prior_scale: holiday ë°˜ì˜ ê°•ë„ (defaultëŠ” 10) . | holidayì˜ ì˜í–¥ì„ í‚¤ìš°ê³  ì‹¶ìœ¼ë©´ ìˆ«ìë¥¼ í‚¤ìš°ê³ , ì˜í–¥ì„ ì¤„ì´ê³  ì‹¶ìœ¼ë©´ ìˆ«ìë¥¼ ì¤„ì—¬ì„œ ì¨ì£¼ë©´ ë¨ | . | m.add_country_holidays(country_name=â€™USâ€™) ì´ë ‡ê²Œ í•˜ë©´ íŠ¹ì • êµ­ê°€ì˜ ê³µíœ´ì¼ ì •ë³´ë¥¼ ê°€ì ¸ë‹¤ ì“¸ ìˆ˜ ìˆìŒ. | . # ì§ì ‘ holiday ì •ë³´ë¥¼ ë‹´ì€ dataframeì„ ìƒì„± # lower_window=0, upper_window=1: íœ´ì¼ì´ ì „ë‚ ì—ëŠ” ì˜í–¥ì„ ì£¼ì§€ ì•Šê³ , ë‹¤ìŒë‚  í•˜ë£¨ ì •ë„ëŠ” ì˜í–¥ì„ ë” ì¤€ë‹¤ëŠ” ê°€ì • holiday = pd.DataFrame({ 'holiday': 'holiday', 'ds': pd.to_datetime(['2020-01-24', '2020-01-25', '2020-01-26', '2021-02-11', '2021-02-12', '2021-02-13']), 'lower_window': 0, 'upper_window': 1 }) m = Prophet(holidays=holiday, daily_seasonality=True) m.add_country_holidays(country_name='US') # ë‚´ì¥ë˜ì–´ ìˆëŠ” íœ´ì¼ ì •ë³´ë¥¼ ë¶ˆëŸ¬ì™€ì„œ ì¶”ê°€ m.fit(df) future = m.make_future_dataframe(periods=121) forecast = m.predict(future) m.plot(forecast); . m.plot_components(forecast); . +) ì–´ë–¤ holidyë“¤ì´ trainingì— ë°˜ì˜ë˜ì—ˆëŠ”ì§€ í™•ì¸: . m.train_holiday_names . 0 holiday 1 New Year's Day 2 Martin Luther King Jr. Day 3 Washington's Birthday 4 Memorial Day 5 Independence Day 6 Independence Day (Observed) 7 Labor Day 8 Columbus Day 9 Veterans Day 10 Thanksgiving 11 Christmas Day dtype: object . Outlier ì œì™¸í•˜ê¸° . | íŠ¹ì • ê¸°ê°„ì˜ ë°ì´í„°ê°€ ìœ ë… íŠ¸ë Œë“œë¥¼ ë²—ì–´ë‚˜ê²Œ ì´ìƒí•œ ìˆ˜ì¹˜ë¥¼ ë³´ì¼ ê²½ìš°, ì œì™¸í•˜ê³  í•™ìŠµì‹œí‚¬ ìˆ˜ë„ ìˆë‹¤ | . ## ì½”ë¡œë‚˜ pandemicìœ¼ë¡œ ì£¼ê°€ê°€ ê¸‰ë½-ê¸‰ì¦í•œ 2020ë…„ ìƒë°˜ê¸° ì¼ë¶€ë¥¼ ì œì™¸í•˜ê³  í•™ìŠµ df.loc[(df['ds'] &gt; '2020-02-01') &amp; (df['ds'] &lt; '2020-05-01'), 'y'] = None m = Prophet(yearly_seasonality=False, daily_seasonality=True) m.fit(df) future = m.make_future_dataframe(periods=121) m.plot(m.predict(future)); . ",
    "url": "https://chaelist.github.io/docs/ml_application/time_series/#%ED%8C%8C%EB%9D%BC%EB%AF%B8%ED%84%B0-%EC%A1%B0%EC%A0%95%ED%95%98%EA%B8%B0",
    "relUrl": "/docs/ml_application/time_series/#íŒŒë¼ë¯¸í„°-ì¡°ì •í•˜ê¸°"
  },"247": {
    "doc": "Topic Modeling (LDA)",
    "title": "Topic Modeling (LDA)",
    "content": ". | Topic Modelingì´ë€? . | ì ì¬ ë””ë¦¬í´ë ˆ í• ë‹¹(Latent Dirichlet Allocation, LDA) | LDAì˜ ìˆ˜í–‰ ê³¼ì • | . | LDA: scikit-learn . | ë°ì´í„° ì¤€ë¹„: ì•± ë¦¬ë·° | Tokenization (í•œê¸€ í˜•íƒœì†Œ ë‹¨ìœ„ë¡œ ìª¼ê°œê¸°) | Vectorization &amp; LDA | LDA ì‹œê°í™”: pyLDAvis | ë¬¸ì„œë³„ í† í”½ í• ë‹¹ | í† í”½ë³„ ì •ë¦¬ | . | LDA: gensim . | Tokenization | Vectorization &amp; LDA | LDA ì‹œê°í™”: pyLDAvis | ë¬¸ì„œë³„ í† í”½ í• ë‹¹ | . | . ",
    "url": "https://chaelist.github.io/docs/ml_application/topic_modeling/",
    "relUrl": "/docs/ml_application/topic_modeling/"
  },"248": {
    "doc": "Topic Modeling (LDA)",
    "title": "Topic Modelingì´ë€?",
    "content": ": ë¬¸ì„œì—ì„œ ì£¼ì œ(topic)ì„ ì¶”ì¶œí•˜ëŠ” ê¸°ë²• . | ê´€ë ¨ì´ ë†’ì€ ë‹¨ì–´ë“¤ë¼ë¦¬ ë¬¶ì–´ í† í”½ì„ êµ¬ì„± â†’ ë‹¨ì–´ì˜ ì¡°í•©ìœ¼ë¡œ í† í”½ì˜ í•µì‹¬ì„ ì •ì˜ ê°€ëŠ¥ | ê° ë¬¸ì„œê°€ ì–´ë–¤ ë‹¨ì–´ë“¤ë¡œ êµ¬ì„±ë˜ëŠ”ì§€ì— ë”°ë¼ ê°€ì¥ ìœ ì‚¬í•œ í† í”½ìœ¼ë¡œ ë¬¸ì„œë¥¼ í• ë‹¹ | . (ì¶œì²˜: medium.com/@connectwithghosh) . ì ì¬ ë””ë¦¬í´ë ˆ í• ë‹¹(Latent Dirichlet Allocation, LDA) . : ëŒ€í‘œì ì¸ í† í”½ ëª¨ë¸ë§ ê¸°ë²•. ë‹¤ìˆ˜ì˜ ë¬¸ì„œì—ì„œ ì ì¬ì ìœ¼ë¡œ ì˜ë¯¸ ìˆëŠ” í† í”½ì„ ë°œê²¬í•˜ëŠ” ì ˆì°¨ì  í™•ë¥  ë¶„í¬ ëª¨ë¸ . | ë‹¨ì–´ë“¤ì˜ ì§‘í•©ì´ ì–´ë–¤ í† í”½ë“¤ë¡œ ë¬¶ì¸ë‹¤ê³  ê°€ì •í•˜ê³ , ì´ ë‹¨ì–´ë“¤ì´ ê°ê°ì˜ í† í”½ì— êµ¬ì„±ë  í™•ë¥ ì„ ê³„ì‚°í•˜ì—¬ ê²°ê³¼ ê°’ì„ í† í”½ì— í•´ë‹¹í•  ê°€ëŠ¥ì„±ì´ ë†’ì€ ë‹¨ì–´ë“¤ì˜ ì§‘í•©ìœ¼ë¡œ ì¶”ì¶œí•˜ëŠ” ë°©ì‹ | ë¬¸ì„œì˜ ë‹¤ì–‘ì„±ì— ë¹„í•´ í† í”½ì˜ ìˆ˜ë¥¼ ë„ˆë¬´ ì ê²Œ ì§€ì •í•˜ê±°ë‚˜, í•˜ë‚˜ì˜ ë¬¸ì„œì— ë‹¤ì–‘í•œ ì£¼ì œê°€ í˜¼ìš©ë˜ì–´ ìˆëŠ” ê²½ìš°ì—ëŠ” í† í”½ë¼ë¦¬ ê²¹ì¹˜ëŠ” ê²°ê³¼ê°€ ë‚˜ì˜¬ ìˆ˜ ìˆë‹¤. í† í”½ ìˆ˜ë¥¼ ì •ë°€í•˜ê²Œ ì§€ì •í•˜ëŠ” ê²ƒì´ ì¤‘ìš”! | . (ì¶œì²˜: bookdown.org/Maxine/tidy-text-mining/) . LDAì˜ ìˆ˜í–‰ ê³¼ì • . | í† í”½ì˜ ê°œìˆ˜ kë¥¼ ì •í•œë‹¤ | ëª¨ë“  ë¬¸ì„œì˜ ëª¨ë“  ë‹¨ì–´ë¥¼ kê°œì˜ í† í”½ ì¤‘ í•˜ë‚˜ì— ëœë¤ìœ¼ë¡œ í• ë‹¹í•œë‹¤ . | ì´ ì‘ì—…ì´ ëë‚˜ë©´ ê° ë¬¸ì„œëŠ” í† í”½ì˜ ë¶„í¬ë¥¼ ê°€ì§€ë©°, í† í”½ì€ ë‹¨ì–´ì˜ ë¶„í¬ë¥¼ ê°–ê²Œ ëœë‹¤ | . | ëª¨ë“  ë¬¸ì„œì˜ ëª¨ë“  ë‹¨ì–´ì— ëŒ€í•´ ì•„ë˜ ê³¼ì •ì„ ë°˜ë³µ: . | ë‹¨ì–´ w ì™¸ì— ë‹¤ë¥¸ ë‹¨ì–´ë“¤ì€ ëª¨ë‘ ì˜¬ë°”ë¥¸ í† í”½ì— í• ë‹¹ë˜ì–´ ìˆë‹¤ê³  ê°€ì •í•˜ê³ , 1) ë‹¨ì–´ wê°€ ì†í•œ ë¬¸ì„œ doc1ì˜ ë‹¨ì–´ë“¤ì´ ì–´ë–¤ í† í”½ì— í•´ë‹¹í•˜ëŠ”ì§€, 2) ì „ì²´ ë¬¸ì„œì—ì„œ ë‹¨ì–´ wê°€ ë³´í†µ ì–´ë–¤ í† í”½ì— ì†í•´ ìˆëŠ”ì§€ ë‘ ê°€ì§€ ê¸°ì¤€ì„ ì°¸ê³ í•˜ì—¬ wì— í† í”½ì„ ì¬í• ë‹¹í•œë‹¤ | . | . (ì°¸ê³ ìë£Œ: ë”¥ëŸ¬ë‹ì„ ì´ìš©í•œ ìì—°ì–´ ì²˜ë¦¬ ì…ë¬¸) . (ì¶œì²˜: donghwa-kim.github.io) . ",
    "url": "https://chaelist.github.io/docs/ml_application/topic_modeling/#topic-modeling%EC%9D%B4%EB%9E%80",
    "relUrl": "/docs/ml_application/topic_modeling/#topic-modelingì´ë€"
  },"249": {
    "doc": "Topic Modeling (LDA)",
    "title": "LDA: scikit-learn",
    "content": ". | sklearn.decomposition.LatentDirichletAllocationì„ ì‚¬ìš© | . ë°ì´í„° ì¤€ë¹„: ì•± ë¦¬ë·° . | google play storeì—ì„œ â€˜Netflixâ€™ì˜ 2020.1.1~2021.5.28 ì‚¬ì´ì˜ 1ì  ë¦¬ë·°ë¥¼ ìˆ˜ì§‘ (ì´ 4242ê°œ) | . ## ì§ì ‘ ìˆ˜ì§‘í•´ ì˜¨ ë°ì´í„°ë¥¼ dataframeìœ¼ë¡œ ì •ë¦¬í•´ ë‘  review_df.head() . | Â  | Score | Content | Date | . | 0 | 1 | ì‚¬ê¸° | 2021-05-28 13:04:04 | . | 1 | 1 | ê°¤ëŸ­ì‹œs6ì…ë‹ˆë‹¤. ì˜ ë˜ë‹¤ê°€ í•œë²ˆì”© ë¬´í•œë¡œë”© ê±¸ë¦½ë‹ˆë‹¤. ê·¼ë° ë¬´í•œë¡œë”© í•œë²ˆ ê±¸ë¦¬ë©´â€¦ | 2021-05-28 11:30:37 | . | 2 | 1 | ë¡œë”©ì´ ë„ˆë¬´ ëŠë ¤ì„œ ìê¾¸ ë©ˆì¶°. ì‚¬ìš´ë“œëŠ” ë‚˜ì˜¤ëŠ”ë° ì˜ìƒì´ ë©ˆì¶”ëŠ” ê²½ìš°ë„ ìˆê³ , ê·¸ëŸ¬ë‹¤â€¦ | 2021-05-28 08:15:41 | . | 3 | 1 | ì–¸ì œê¹Œì§€ ë¡œë“œ ì¤‘ë§Œ ëœ°ì§€ ê¶ê¸ˆí•¨ ë¹ˆì„¼ì¡° 7í™” ë³´ë‹¤ê°€ ë‹¤ì‹œ ë³´ë ¤ë‹ˆê¹Œ ë¬´í•œë¡œë”©,, ë‚´ê°€â€¦ | 2021-05-28 01:12:33 | . | 4 | 1 | ì•„ê¹Œê¹Œì§„ ì˜ ë˜ë‹¤ ê°‘ìê¸° íŠ•ê¸°ë”ë‹ˆ ê·¸ ë‹¤ìŒë¶€í„°ëŠ” í°ì„ ë„ê³  ë‹¤ì‹œ ì¼œì„œ ë“¤ì–´ê°€ë„ ë””ë°”ì´ìŠ¤â€¦ | 2021-05-27 19:37:03 | . Tokenization (í•œê¸€ í˜•íƒœì†Œ ë‹¨ìœ„ë¡œ ìª¼ê°œê¸°) . import konlpy import re # tokenization í•¨ìˆ˜ë¥¼ ë§Œë“¤ì–´ë‘  def tokenize_korean_text(text): text = re.sub(r'[^,.?!\\w\\s]','', text) ## ,.?!ì™€ ë¬¸ì+ìˆ«ì+_(\\w)ì™€ ê³µë°±(\\s)ë§Œ ë‚¨ê¹€ # ì•ì— rì„ ë¶™ì—¬ì£¼ë©´ deprecation warningì´ ì•ˆëœ¸ (raw stringìœ¼ë¡œ declare) okt = konlpy.tag.Okt() Okt_morphs = okt.pos(text) # stem=Trueë¡œ ì„¤ì •í•˜ë©´ ë™ì‚¬ì›í˜•ìœ¼ë¡œ ë°”ê¿”ì„œ return words = [] for word, pos in Okt_morphs: if pos == 'Adjective' or pos == 'Verb' or pos == 'Noun': # ì´ ê²½ìš°ì—ëŠ” í˜•ìš©ì‚¬, ë™ì‚¬, ëª…ì‚¬ë§Œ ë‚¨ê¹€ words.append(word) words_str = ' '.join(words) return words_str # review_df['Content']ë¥¼ í•˜ë‚˜ì”© tokenizeí•´ì„œ listë¡œ ì €ì¥ tokenized_list = [] for text in review_df['Content']: tokenized_list.append(tokenize_korean_text(text)) print(len(tokenized_list)) print(tokenized_list[0]) . 4242 ì‚¬ê¸° . +) ë‹¨ì–´ê°€ 1-2ê°œë§Œ í¬í•¨ëœ corpusëŠ” ì‚­ì œ . drop_corpus = [] for index in range(len(tokenized_list)): corpus = tokenized_list[index] if len(set(corpus.split())) &lt; 3: # ê°™ì€ ë‹¨ì–´ 1-2ê°œë§Œ ë°˜ë³µë˜ëŠ” corpusë„ ì§€ìš°ê¸° ìœ„í•´ set()ì„ ì‚¬ìš© review_df.drop(index, axis='index', inplace=True) drop_corpus.append(corpus) for corpus in drop_corpus: tokenized_list.remove(corpus) review_df.reset_index(drop=True, inplace=True) print(len(tokenized_list)) print(len(review_df)) . 3936 3936 . Vectorization &amp; LDA . from sklearn.feature_extraction.text import CountVectorizer from sklearn.decomposition import LatentDirichletAllocation . â£1. vectorí™” . #LDA ëŠ” Countê¸°ë°˜ì˜ Vectorizerë§Œ ì ìš© count_vectorizer = CountVectorizer(max_df=0.1, max_features=1000, min_df=2, ngram_range=(1,2)) # 2ê°œì˜ ë¬¸ì„œ ë¯¸ë§Œìœ¼ë¡œ ë“±ì¥í•˜ëŠ” ë‹¨ì–´ëŠ” ì œì™¸, ì „ì²´ì˜ 10% ì´ìƒìœ¼ë¡œ ìì£¼ ë“±ì¥í•˜ëŠ” ë‹¨ì–´ëŠ” ì œì™¸ # bigramë„ í¬í•¨ feat_vect = count_vectorizer.fit_transform(tokenized_list) print('CountVectorizer Shape:', feat_vect.shape) . CountVectorizer Shape: (3936, 1000) . â£2. í† í”½ëª¨ë¸ë§: LDA . lda = LatentDirichletAllocation(n_components=6) # í† í”½ ìˆ˜ëŠ” 6ê°œë¡œ ì„¤ì • lda.fit(feat_vect) . LatentDirichletAllocation(batch_size=128, doc_topic_prior=None, evaluate_every=-1, learning_decay=0.7, learning_method='batch', learning_offset=10.0, max_doc_update_iter=100, max_iter=10, mean_change_tol=0.001, n_components=6, n_jobs=None, n_topics=None, perp_tol=0.1, random_state=None, topic_word_prior=None, total_samples=1000000.0, verbose=0) . â£3. í† í”½ë³„ ì—°ê´€ì–´ ì¶œë ¥ . def display_topics(model, feature_names, num_top_words): for topic_index, topic in enumerate(model.components_): print('Topic #', topic_index) # components_ arrayì—ì„œ ê°€ì¥ ê°’ì´ í° ìˆœìœ¼ë¡œ ì •ë ¬í–ˆì„ ë•Œ, ê·¸ ê°’ì˜ array indexë¥¼ ë°˜í™˜. topic_word_indexes = topic.argsort()[::-1] top_indexes=topic_word_indexes[:num_top_words] # top_indexesëŒ€ìƒì¸ indexë³„ë¡œ feature_namesì— í•´ë‹¹í•˜ëŠ” word feature ì¶”ì¶œ í›„ joinìœ¼ë¡œ concat feature_concat = ' '.join([feature_names[i] for i in top_indexes]) print(feature_concat) # CountVectorizerê°ì²´ë‚´ì˜ ì „ì²´ wordë“¤ì˜ ëª…ì¹­ì„ get_features_names( )ë¥¼ í†µí•´ ì¶”ì¶œ feature_names = count_vectorizer.get_feature_names() # Topicë³„ ê°€ì¥ ì—°ê´€ë„ê°€ ë†’ì€ wordë¥¼ 10ê°œë§Œ ì¶”ì¶œ display_topics(lda, feature_names, 10) . Topic # 0 ë¬´ë£Œ ê°€ì… í–ˆëŠ”ë° í•´ì§€ ì•„ë‹ˆ ì´ìš© í™˜ë¶ˆ ì‚¬ìš© ê³„ì • í•´ì„œ Topic # 1 ìë§‰ ì˜í™” ì¬ìƒ ì˜ìƒ ë„·í”Œë¦­ìŠ¤ ì†Œë¦¬ ë¬¸ì œ ë‚˜ì˜¤ê³  ê¸°ëŠ¥ ë“œë¼ë§ˆ Topic # 2 ë¡œë”© ìê¾¸ í”Œë¦½ ì§„ì§œ í™”ì§ˆ ë‚´ê³  ë¬´í•œ ë¬´í•œ ë¡œë”© ë³´ëŠ”ë° ì…ë‹ˆë‹¤ Topic # 3 ì•ˆë¨ í•´ì£¼ì„¸ìš” ë…¸íŠ¸ ì‹¤í–‰ ê¹”ì•„ë„ ì‚­ì œ ê°‘ìê¸° ì–´í”Œ ì‹œì²­ ë‹¤ìš´ Topic # 4 ì˜ìƒ ì¬ìƒ í•´ê²° ì†Œë¦¬ í•´ì£¼ì„¸ìš” ì•ˆë˜ìš” ë©ˆì¶”ê³  ì œë°œ ê°œì„  ì—ëŸ¬ Topic # 5 ë¡œê·¸ì¸ ì‹¤í–‰ ì„¤ì¹˜ ì–´í”Œ ì‚­ì œ í–ˆëŠ”ë° ì•ˆë˜ê³  ì•ˆë˜ë„¤ìš” í•´ë„ ê³ ê° . LDA ì‹œê°í™”: pyLDAvis . | pip install pyLDAvisë¡œ ì„¤ì¹˜í•´ì„œ ì‚¬ìš© | . import pyLDAvis.sklearn # sklearnì˜ ldamodelì— ìµœì í™”ëœ ë¼ì´ë¸ŒëŸ¬ë¦¬ pyLDAvis.enable_notebook() vis = pyLDAvis.sklearn.prepare(lda, feat_vect, count_vectorizer) pyLDAvis.display(vis) . (ì´ë¯¸ì§€ë¥¼ í´ë¦­í•˜ë©´ htmlë¡œ êµ¬í˜„ëœ ë²„ì „ìœ¼ë¡œ í™•ì¸ ê°€ëŠ¥) . ë¬¸ì„œë³„ í† í”½ í• ë‹¹ . â£1. ê° ë¬¸ì„œë³„ë¡œ ê°€ì¥ ê°€ê¹Œìš´ topicìœ¼ë¡œ í• ë‹¹ . # ë¬¸ì„œë³„ë¡œ, ê°€ì¥ í™•ë¥ ì´ ë†’ì€ topicìœ¼ë¡œ í• ë‹¹í•´ì¤Œ doc_topic = lda.transform(feat_vect) doc_per_topic_list = [] for n in range(doc_topic.shape[0]): topic_most_pr = doc_topic[n].argmax() topic_pr = doc_topic[n].max() doc_per_topic_list.append([n, topic_most_pr, topic_pr]) doc_topic_df = pd.DataFrame(doc_per_topic_list, columns=['Doc_Num', 'Topic', 'Percentage']) doc_topic_df.head() . | Â  | Doc_Num | Topic | Percentage | . | 0 | 0 | 2 | 0.735108 | . | 1 | 1 | 1 | 0.455361 | . | 2 | 2 | 3 | 0.544513 | . | 3 | 3 | 0 | 0.241815 | . | 4 | 4 | 2 | 0.880451 | . â†’ ì‹¤ì œ review ë‚´ìš©ê³¼ join . doc_topic_df = doc_topic_df.join(review_df) doc_topic_df.head() . | Â  | Doc_Num | Topic | Percentage | Score | Content | Date | . | 0 | 0 | 2 | 0.735108 | 1 | ê°¤ëŸ­ì‹œs6ì…ë‹ˆë‹¤. ì˜ ë˜ë‹¤ê°€ í•œë²ˆì”© ë¬´í•œë¡œë”© ê±¸ë¦½ë‹ˆë‹¤. ê·¼ë° ë¬´í•œë¡œë”© í•œë²ˆ ê±¸ë¦¬ë©´â€¦ | 2021-05-28 11:30:37 | . | 1 | 1 | 1 | 0.455361 | 1 | ë¡œë”©ì´ ë„ˆë¬´ ëŠë ¤ì„œ ìê¾¸ ë©ˆì¶°. ì‚¬ìš´ë“œëŠ” ë‚˜ì˜¤ëŠ”ë° ì˜ìƒì´ ë©ˆì¶”ëŠ” ê²½ìš°ë„ ìˆê³ , ê·¸ëŸ¬ë‹¤â€¦ | 2021-05-28 08:15:41 | . | 2 | 2 | 3 | 0.544513 | 1 | ì–¸ì œê¹Œì§€ ë¡œë“œ ì¤‘ë§Œ ëœ°ì§€ ê¶ê¸ˆí•¨ ë¹ˆì„¼ì¡° 7í™” ë³´ë‹¤ê°€ ë‹¤ì‹œ ë³´ë ¤ë‹ˆê¹Œ ë¬´í•œë¡œë”©,, ë‚´ê°€â€¦ | 2021-05-28 01:12:33 | . | 3 | 3 | 0 | 0.241815 | 1 | ì•„ê¹Œê¹Œì§„ ì˜ ë˜ë‹¤ ê°‘ìê¸° íŠ•ê¸°ë”ë‹ˆ ê·¸ ë‹¤ìŒë¶€í„°ëŠ” í°ì„ ë„ê³  ë‹¤ì‹œ ì¼œì„œ ë“¤ì–´ê°€ë„ ë””ë°”ì´ìŠ¤â€¦ | 2021-05-27 19:37:03 | . | 4 | 4 | 2 | 0.880451 | 1 | ë¬´í•œë¡œë”©ì¤‘â€¦â€¦ë¹ ë¥¸ ë¬¸ì œí•´ê²° ë¶€íƒë“œë¦½ë‹ˆë‹¤. | 2021-05-27 17:22:05 | . â£2. í† í”½ë³„ ë¬¸ì„œ ìˆ˜ ê³„ì‚° . doc_topic_df.groupby('Topic')[['Doc_Num']].count() . | Topic | Doc_Num | . | 0 | 654 | . | 1 | 818 | . | 2 | 600 | . | 3 | 531 | . | 4 | 702 | . | 5 | 631 | . â£3. í† í”½ë³„ë¡œ, ê°€ì¥ ë†’ì€ í™•ë¥ ë¡œ í• ë‹¹ëœ ë¬¸ì„œ top 3 í™•ì¸ . for topic in range(len(doc_topic_df['Topic'].unique())): print('Topic #', topic, '-----------------------------') top_pr_topics = doc_topic_df[doc_topic_df['Topic'] == topic].sort_values(by='Percentage', ascending=False) print(top_pr_topics['Content'].iloc[0]) print(top_pr_topics['Content'].iloc[1]) print(top_pr_topics['Content'].iloc[2], '\\n') . Topic # 0 ----------------------------- í˜¼ì ì‚¬ìš©í•´ì„œ ë² ì´ì§ ìš”ê¸ˆì œë¥¼ ì„ íƒí–ˆëŠ”ë° ì²«ë‹¬ ë¬´ë£Œë¡œ ì‚¬ìš©í•˜ë©´ì„œ ìŠ¤íƒ ë‹¤ë“œë¡œ ì“¸ìˆ˜ ìˆê²Œ í•´ì¤€ë‹¤ í•´ì„œ ìŠ¤íƒ ë‹¤ë“œë¡œ í•œë‹¬ ì‚¬ìš©í•˜ê³  ê·¸ ë‹¤ìŒë‹¬ ë² ì´ì§ìœ¼ë¡œ ë°”ê¿¨ëŠ”ë° ìŠ¤íƒ ë‹¤ë“œ ìš”ê¸ˆ ë‚˜ì˜´. ë¬¸ì˜ì „í™” í•˜ë‹ˆê¹Œ ë°ì´í„° ìƒ ë‚´ê°€ ë² ì´ì§ìœ¼ë¡œ ë°”ê¾¼í›„ ë°”ë¡œ ìŠ¤íƒ ë‹¤ë“œë¡œ ë‹¤ì‹œ ë°”ê¿¨ë‹¤í•¨ ê·¸ëŸ°ì  ì—†ë‹¤ í•˜ë‹ˆ ë°ì´í„°ìƒ ê·¸ë ‡ê¸° ë•Œë¬¸ì— ì–´ì©”ìˆ˜ ì—†ë‹¤ í•¨ ë² ì´ì§ìœ¼ë¡œ ë°”ë€ì‹œê°„ê³¼ ìŠ¤íƒ ë‹¤ë“œë¡œ ë°”ë€ì‹œê°„ì´ ë™ì¼í•˜ë©° ì´ˆê¹Œì§€ëŠ” ì•ˆë‚˜ì™€ì„œ ëª¨ë¥´ê² ìœ¼ë‚˜ ì‹œê°„ê³¼ ë¶„ì´ ì¼ì¹˜í•˜ë¯€ë¡œ ëª‡ì´ˆì´ë‚´ì— ë°”ê¿¨ì„ê±°ë¼ í•¨. ë‚œ ë² ì´ì§ìœ¼ë¡œ ë°”ê¾¸ê³  ìŠ¤íƒ ë‹¤ë“œë¡œ ë°”ë¡œ ë°”ê¾¼ì  ì—†ë‹¤ í•˜ë‹ˆê¹Œ ë‚´ê°€ ë² ì´ì§ìœ¼ë¡œ ë°”ê¾¼í›„ ë² ì´ì§ìœ¼ë¡œ ë°”ë€ê±¸ ê³„ì • ë“¤ì–´ê°€ì„œ í•œë²ˆë” í™•ì¸ ì•ˆí–ˆìœ¼ë‹ˆ ë‚´ ì˜ëª»ì´ë¼ í•¨. ë‚˜ë„ ëª¨ë¥´ê²Œ ë©¤ë²„ì‹­ ë°”ë€”ìˆ˜ ìˆìœ¼ë‹ˆ ê²°ì œë  ë•Œë§ˆë‹¤ ë² ì´ì§ ìŠ¤íƒ ë‹¤ë“œ í™•ì¸í•˜ë€ ì–˜ê¸´ê°€ìš”? ì¹´ë“œê²°ì¬ ì£¼ì˜í•˜ì‹œê³  í•´ì§€ë„ ì¡°ì‹¬í•˜ì„¸ìš”. ê³„ì •ì´ ë‘ê°œë¼ í•œê°œë§Œ ê²°ì¬í–ˆëŠ”ë° í•œë‹¬í›„ë³´ë‹ˆ ë‘ê°œë‹¤ ê²°ì¬ë¼ê³  ê²°ì¬ 5ì¼ì „ì— í•´ì§€í–ˆëŠ”ë° ë˜ ê²°ì¬ë¼ê³  ì „í™”í–ˆë”ë‹ˆ ë‘ê³„ì •ê²°ì¬ ê±´ì€ ë‚´ê°€ ì•„ì§ëª¨ë¥´ëŠ” ìƒíƒœë¼ê³  ìƒë‹´ì‚¬ê°€ ë§ë„ ì•ˆí•´ì£¼ê³  í•œë²ˆë§Œ í™˜ë¶ˆ ê°€ëŠ¥í•˜ë‹¤ê³  ìš”ë²ˆ í•´ì§€ ê²°ì¬ê±´ë§Œ í™˜ë¶ˆ ì²˜ë¦¬ í•´ì¤ë””ë‹¤. í•œê°œì •ë§Œ ì¹´ë“œê²°ì¬í•˜ê³  í•œë‹¬ ì‹œì²­í•´ëŠ”ë° ë‹¤ë¥¸í•œê°œì •ì€ ì •ì§€ìƒíƒœì¸ë° ì™œ ê²°ì¬ë¼ëƒê³ ìš”. ì˜¤ëŠ˜ ë„·í”Œë¦­ìŠ¤ ì²˜ìŒ ì´ìš© í•´ë´¤ëŠ”ë° íšŒì›ê°€ì…í•˜ê³  ì²« ë‹¬ ë¬´ë£Œë¼ë©´ì„œ ì¹´ë“œ ë“±ë¡í•˜ë‹ˆê¹Œ ë°”ë¡œ ëˆ ë¹ ì ¸ë‚˜ê°€ëŠ” ê±° ë­¡ë‹ˆê¹Œ? ë‹¹í™©í•´ì„œ ë‹¤ê¸‰íˆ ë©¤ë²„ì‹­ í•´ì§€.í–ˆëŠ”ë°ë„ ëˆ ë„ ì•ˆë“¤ì–´ì˜¤ê³ ... ì‚¬ê¸° ë‹¹í•´ì„œ ê¸°ë¶„ ë‚˜ì˜ë„¤ìš”.... ì²« ë‹¬ ë¬´ë£Œë¼ë‹ˆ ë­ë¼ë‹ˆ ì´ëŸ° ê±°ì§“ë§ì€.í•˜ì§€ ë§ˆì„¸ìš”... ë‹¹í•œ ì…ì¥ìœ¼ë¡œì„œ ì—„ì²­ ë³„ë¡œì…ë‹ˆë‹¤... Topic # 1 ----------------------------- ë„·í”Œë¦­ìŠ¤ì— ì˜¬ë¼ì˜¤ëŠ” ì‘í’ˆë“¤ì€ ë§Œì¡±ìŠ¤ëŸ½ê³  ì‘í’ˆì„±ì´ ë›°ì–´ë‚œë° í•œê¸€ ë²ˆì—­ì€ ì™œ ê·¸ë”°ìœ„ì¸ê°€ìš”? ë¶„ëª… ë³¸ë¬¸ì€ ê·¸ëŸ° ë‚´ìš©ì´ ì•„ë‹Œë° í˜ì˜¤ì  í‘œí˜„ì„ ì“´ë‹¤ë˜ê°€.. ì•„ë‹ˆ ëŒ€ì²´ ì˜ì–´ë¥¼ í•œê¸€ë¡œ ë²ˆì—­í•˜ë©´ì„œ ë¶€ì¸ì€ ë‚¨í¸í•œí…Œ ì¡´ëŒ€ì“°ê³  ë‚¨í¸ì€ ë¶€ì¸í•œí…Œ ë°˜ë§ ì“°ëŠ” 80ë…„ëŒ€ì  ë°œìƒì€ ì–´ë””ì„œ ë‚˜ì˜¨ ê±°ì£ ?? ìºë¦­í„° ì„±ê²©ìƒ ê¼¬ë°•ê¼¬ë°• ì¡´ëŒ€ ì“°ê³  ë°˜ë§ ì“°ëŠ”ê²Œ ì–´ìƒ‰í•œë°ë„ ì£½ì–´ë„ ë†“ì§ˆ ëª»í•˜ì‹œë„¤ìš”,, ì´ì œëŠ” ì˜ì–´ ì˜í™”ëŠ” ê·¸ëƒ¥ ë³´ê³  ìŠ¤í˜ì¸ì´ë‚˜ í”„ë‘ìŠ¤ ì˜í™”ê°™ì€ê±´ ì˜ì–´ë¡œ ë²ˆì—­í•´ì„œ ë³´ê³  ìˆëŠ”ë°.. ì œë°œ ê·¸ ê¸€ëŸ¬ë¨¹ì€ ì‚¬ìƒì¢€ ì–´ë”” ë²„ë¦¬ê³  ì™€ì£¼ì„¸ìš” ë²ˆì—­ê°€ë‹˜. ë²ˆì—­ ì‹ ê³  ê¸°ëŠ¥ì´ ìˆì–´ì„œ ê³„ì† ì‹ ê³ í• ë˜ë„ ë³¸ì§ˆì´ ê¸€ëŸ¬ë¨¹ì€ ê²ƒ ê°™ì•„ì„œ í‰ì  ê¸°ëŠ¥ì— ë‚¨ê²¨ë´…ë‹ˆë‹¤ ìì²´ ì œì‘í•˜ëŠ” ì½˜í…ì¸ , ë“¤ì—¬ì˜¤ëŠ” ì˜í™”ë‚˜ ë“œë¼ë§ˆ ëª¨ë‘ ì¢‹ì•„ìš” ì½˜í…ì¸  ë©´ì—ì„  ëª¨ë‘ ë§Œì¡±í•©ë‹ˆë‹¤ ê·¸ëŸ¬ë‚˜ ìë§‰ì´ë‚˜ ë²ˆì—­ì— ì‹¬ê°í•œ ì˜¤ì˜ì—­ì´ ì¡´ì¬í•˜ëŠ” ê²½ìš°ë¥¼ ë§ì´ ë³´ì•˜ì–´ìš”. ì˜ˆë¥¼ ë“¤ì–´ ë“œë¼ë§ˆ ê¸€ë¦¬ ìë§‰ì˜ ê²½ìš° ì£¼ì¸ê³µì´ ëˆˆì— ëŒì„ ë§ì•„ ê±°ì˜ ì‹¤ëª…í•  ë»” í•˜ëŠ” ì‚¬ê±´ì´ ë‚˜ì˜¤ëŠ”ë° ì´ë¥¼ ëˆˆì— ì†Œê¸ˆì„ ë§ì•˜ë‹¤ê³  ì˜ëª» ë²ˆì—­ì´ ë˜ì–´ ìˆìŠµë‹ˆë‹¤.. ì´ëŸ´ ê²½ìš° ì˜í™”ë‚˜ ë“œë¼ë§ˆì— ì‹¬ê°í•œ ì˜í–¥ì„ ë¼ì¹  ê²ƒ ê°™ì•„ìš” ë˜ ì´ ì •ë„ì˜ ì˜¤ì—­ì´ ì•„ë‹ˆì–´ë„ ì‹¬ì‹¬í•œ ì˜¤ë¥˜ë¥¼ ë§ì´ ë³´ì•˜ìŠµë‹ˆë‹¤ ìë§‰ì´ë‚˜ ë²ˆì—­ì— ì¡°ê¸ˆ ë” í˜ì¨ ì£¼ì…¨ìœ¼ë©´ í•©ë‹ˆë‹¤. ì •ë§ ì˜ ì‚¬ìš©í•˜ê³  ìˆë‹¤ê°€ ìµœê·¼ì— ì—…ë°ì´íŠ¸ ì´í›„ v50s ë“€ì–¼ìŠ¤í¬ë¦° ì¬ìƒì´ ë˜ì§€ë¥¼ ì•ŠìŠµë‹ˆë‹¤. ì˜ìƒ ì¬ìƒ ì¤‘ì— ì¹´ì¹´ì˜¤í†¡ì´ë‚˜ ì¸í„°ë„· ê²€ìƒ‰ ë“± ë©€í‹° ê¸°ëŠ¥ì„ í™œìš©í•  ìˆ˜ ìˆì—ˆëŠ”ë° ì§€ê¸ˆì€ í™˜ê²½ì„¤ì • ì°½ë§Œ ë‚´ë ¤ë„ ì˜ìƒì´ ì •ì§€ê°€ ë˜ë„¤ìš”. í•˜ë£¨ë¹¨ë¦¬ ë‹¤ì‹œ ì˜ˆì „ì²˜ëŸ¼ ì˜ìƒì´ ì§€ì†ì ìœ¼ë¡œ ì¬ìƒë  ìˆ˜ ìˆë„ë¡ ì¡°ì¹˜í•´ ì£¼ì‹œë©´ ëŒ€ë‹¨íˆ ê°ì‚¬ë“œë¦¬ê² ìŠµë‹ˆë‹¤. Topic # 2 ----------------------------- ì €ë„ì œíŠ¸í”Œë¦½ì‚¬ìš©ìì…ë‹ˆë‹¤ ì œíŠ¸í”Œë¦½ ì‚¬ìš©ìëŠ” ì „ë¶€ ê·¸ëŸ¬ë‚˜ë´ìš” ì €ë„ ë¡œë”©ë§Œëœ¨ê³  ë„˜ì–´ê°€ì§ˆì•ŠëŠ”ë° ë¬¸ì œí•´ê²°ê³¼ ì•ˆë‚´ê³µì§€ê°€ ìˆì–´ì•¼í•˜ì§€ì•Šì„ê¹Œì—¬? ì™  ëˆë°›ê³  ì €ì§ˆì„œë¹„ìŠ¤ì„? ì œíŠ¸í”Œë¦½ ì‚¬ìš©ìì¸ë° ì—…ë°ì´íŠ¸í•˜ê³  ê°‘ìê¸° ë¬´í•œë¡œë”©... ëª»ë³´ê³ ìˆëŠ”ë° ë‹¤ë¥¸ ë¦¬ë·°ë“¤ë„ ì œíŠ¸í”Œë¦½ ì•ˆëœë‹¤ê³  í•˜ë„¤ìš”... ëˆì€ ëˆëŒ€ë¡œ ë‚˜ê°€ëŠ”ë° ë³´ì§€ëŠ” ëª»í•˜ê³  ì´ê±° ë¬´ìŠ¨ ìƒí™©ì…ë‹ˆê¹Œ? ì €ë„ ì•„ë˜ ë¦¬ë·°ì²˜ëŸ¼ ì œíŠ¸í”Œë¦½ ì‚¬ìš©ìì¸ë° ë¬´í•œë¡œë“œ ë°˜ë³µí•˜ê³  ê·¸ ì–´ëŠ ì»¨í…ì¸  í•˜ë‚˜ ì¬ìƒì´ ì•ˆë©ë‹ˆë‹¤. ì´ê±° ë­ ë©°ì¹ ë™ì•ˆ ëˆë‚ ë ¤ë¨¹ìœ¼ë€ê²ƒë„ ì•„ë‹ˆê³  ë³´ìƒë„ ì•ˆë°”ë¼ë‹ˆê¹Œ í•´ê²°ì´ë‚˜ ë¹¨ë¦¬í•´ì¤˜ìš”. Topic # 3 ----------------------------- í•¸ë“œí°ì„ ì‹œì²­í›„ ê»ì„ë•Œ ë‹¤ë¥¸ê¸°ê¸°ë¡œ ì¬ì‹œì²­ì„í•˜ë©´ ê¸°ì¡´ì— ì‹œì²­í–ˆë˜ í”„ë¡œê·¸ë¨ ì‹œê°„ëŒ€ë¡œ ë‹¤ì‹œ ì¬ì‹œì²­ì´ ê°€ëŠ¥í•œë° ì–´ëŠ” ìˆœê°„ë¶€í„°ëŠ” ê¸°ê¸°ë§ˆë‹¤ ì‹œì²­í–ˆë˜ ì‹œê°„ëŒ€ë¡œë§Œ ì¬ì‹œì²­ì´ ê°€ëŠ¥í•´ì§€ë„¤ìš” ë‹¤ë¥¸ê¸°ê¸°ë¡œ ì¬ì‹œì²­ì‹œ ê¸°ì¡´ì— ì‹œì²­í–ˆë˜ ì‹œê°„ëŒ€ë¡œ ì‹œì²­í• ìˆ˜ìˆê²Œë” ì—…ë°ì´íŠ¸ì¢€ ë¶€íƒë“œë ¤ìš” ì•„ë‹ˆë©´ ì„¤ì • ë°©ë²•ì¢€ ì•Œë ¤ì£¼ì„¸ìš” ë‚´ê°€ ì°œí•œ ì½˜í…ì¸  ëˆŒëŸ¬ë„ í™•ì¸í•´ë³´ë©´ ì¶”ê°€ ì•ˆë˜ì–´ìˆê³  ì˜ˆì „ì—ëŠ” ì°œí•œ ì½˜í…ì¸ ì‚­ì œí•˜ëŠ”ê²ƒë„ ëì—ˆëŠ”ë° ì§€ê¸ˆì€ ì™œ ì•ˆë ê¹Œìš”? ë‹¤ ë³¸ê±´ ì§€ìš°ê³  ìƒˆë¡œ ë³¼ ê²ƒë§Œ ì¶”ê°€í•´ë‘ê³  ì‹¶ì€ë° ì‚­ì œí–ˆë‹¤ê°€ ë‹¤ì‹œ ê¹”ì•„ë„ ì•ˆë˜ê³  ì œê°€ ë­˜ ì–´ë–»ê²Œ í•´ìš”. ê°¤ëŸ­ì‹œ ë…¸íŠ¸10+ ìœ ì €ì…ë‹ˆë‹¤. 501-109??ì˜¤ë¥˜ë©”ì‹œì§€ ëœ¨ë©´ì„œ ì•±ì‹¤í–‰ì´ 2ì£¼ì§¸ ì•ˆë˜ê³ ìˆê³  ë§¤ë‹¬ ê²°ì¬ëŠ” ë˜ëŠ”ìƒí™©ì…ë‹ˆë‹¤. ì—…ëƒí•˜ë¼í•´ì„œ ë‹¤í–ˆê³ .. ë­¡ë‹ˆê¹Œ ì´ê²Œ.. í™˜ë¶ˆì´ë¼ë„ í•´ì£¼ë˜ê°€ ì•± ì œëŒ€ë¡œ ê´€ë¦¬í•˜ë˜ê°€! í™˜ë¶ˆí•´ì¤˜ìš” !!! Topic # 4 ----------------------------- ì•„ë‹ˆ ì˜¤ëŠ˜ ì—…ë°ì´íŠ¸í•˜ë¼í•´ì„œ í–ˆëŠ”ë° 30ì´ˆë§ˆë‹¤ ì˜ìƒì´ ì¤‘ì§€ë˜ë„¤ìš”ã…¡ã…¡ ì–´ì œê¹Œì§€ë§Œ í•´ë„ ì•„ë‹ˆ ì—…ëƒì „ê¹Œì§€ë§Œ í•´ë„ ë©€ì©¡í–ˆëŠ”ë° ì™œ ì´ëŸ¬ëŠ”ê±°ì£ ? ì˜ìƒì´ ì¤‘ì§€ë˜ì„œ ë‹¤ì‹œ ë³´ë ¤ê³  í•˜ë©´ ì¬ìƒ ë²„íŠ¼ì€ ëˆŒë¦¬ì§€ë„ ì•Šì•„ìš”. ê·¸ë˜ì„œ ë‹¤ì‹œ ì°½ ë„ê³  ì˜ìƒ ëˆ„ë¥´ê³  30ì´ˆí›„ì— ì¤‘ì§€. ë˜ ë„ê³  ì˜ìƒ ëˆ„ë¥´ê³ . ê°œì„ ì¢€í•´ì£¼ì„¸ìš” ì‰¬ëŠ”ë‚  ë„·í”Œ ë³´ë ¤ëŠ”ë° ì˜¤ë¥˜ë•Œë¬¸ì— ì‹œê°„ ë‹¤ë‚ ë ¸ì–´ìš” ì˜ìƒì´ ìê¾¸ ëŠê²¨ìš” ì›ë˜ë„ ì ê¹ì”© ëŠê¸´ ì ì´ ìˆê¸´ í–ˆì§€ë§Œ ì˜¤ëŠ˜ì€ ì¢€ ì‹¬í•˜ë„¤ìš”... ì•„ì˜ˆ í•œ ì¥ë©´ì—ì„œ ê³„ì† ë©ˆì¶”ê³  10ì´ˆ ì „ì´ë‚˜ 10ì´ˆ í›„ë¡œ ëŒë ¤ë„ ì˜ìƒ ì‹œê°„ë§Œ ê°€ê³  ì•±ì„ ì•„ì˜ˆ ë‹«ì•˜ë‹¤ê°€ ë‹¤ì‹œ ë“¤ì–´ê°€ë„ ë˜‘ê°™ì•„ìš” ì˜ìƒì€ ë©ˆì¶”ê³  ì†Œë¦¬ë§Œ ë“¤ë¦´ ë•Œë„ ë§ê³  ë„ˆë¬´ í™”ë‚˜ë„¤ìš” ã…  ê³ ì³ ì£¼ì„¸ìš” ì œë°œ ì•„ë‹ˆ ì €ë§Œ ì´ëŸ°ê°€ìš”? í•œ ì¼ì£¼ì¼ ì „ë¶€í„° ì˜ìƒ ì¬ìƒí•˜ë©´ í•œ 10ì´ˆ?ì€ ì¬ìƒë˜ë‹¤ê°€ ê°‘ìê¸° í™”ë©´ ë©ˆì¶”ê³  ì†Œë¦¬ë§Œ ë‚˜ì˜¤ë„¤ìš”ã…¡ã…¡ í•´ê²°ì±…ì¢€ìš” ì œë°œã…¡ã…¡ Topic # 5 ----------------------------- ã…¡ã…¡ì €ê¸°ìš” ê²°ì œì •ë³´ë³€ê²½í•´ì„œ ê²°ì œí–ˆë”ë‹ˆã…¡ã…¡ê³„ì •ì •ë³´ë¥¼ë¶ˆëŸ¬ì˜¬ìˆ˜ì—†ë‹¤ê¸¸ë˜ ë¡œê·¸ì•„ì›ƒí–ˆë‹¤ë‹¤ì‹œë¡œê·¸ì¸í•˜ë ¤í•˜ë‹ˆ ë¡œê·¸ì¸ë„ì•ˆë˜ê³ ã…¡ã…¡ì‚­ì œí–ˆë‹¤ ë‹¤ì‹œê¹”ì•„ë„ì•ˆë˜ê³  ê³ ê°ì„¼í„°ì—°ê²°ë„ì•ˆë˜ê³  í™ˆí˜ì´ì§€ë„ì•ˆë“¤ì–´ê°€ì§€ê³  ë­¡ë‹ˆê¹Œ? ë„·í”Œë¦­ìŠ¤ì— ì ‘ì†í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤? ë°©ê¸ˆ ì „ê¹Œì§€ë§Œ í•´ë„ ëì—ˆëŠ”ë°.... ê°‘ìê¸° ì ‘ì†ì´ ì•ˆë˜ê¸¸ë˜ íœ´ëŒ€í° ê»ì¼°, ì–´í”Œ ì‚­ì œ í›„ ì¬ì„¤ì¹˜, ë°ì´í„° ì‚­ì œ ë“± í•  ìˆ˜ ìˆëŠ”ê±´ ë‹¤ í–ˆëŠ”ë° ì—¬ì „íˆ ë¨¹í†µì´ë„¤ìš”... ì¥ë‚œí•˜ëŠ”ê²ƒë„ ì•„ë‹ˆê³  ê²°ì œí•˜ë‹ˆê¹Œ ì•ˆë“¤ì–´ê°€ì§‘ë‹ˆë‹¤. ì¬ì„¤ì¹˜ í•˜ë ¤ê³  ì§€ì› ë‹¤ ì„¤ì¹˜ì¤‘ì¸ë° ì„¤ì¹˜ë„ ê³„ì† ì•ˆë˜ê³  ìˆë„¤ìš”. ë­í•˜ìëŠ”ê±´ì§€ ê°œì„ ì´ ì•ˆë˜ê³  ìˆëŠ”ë° ì–´ì©ŒìëŠ” ê²ë‹ˆê¹Œ? . í† í”½ë³„ ì •ë¦¬ . | keywords, ë¬¸ì„œ ì˜ˆì‹œ ë“±ì„ í†µí•´ í† í”½ì„ ì •ì˜/ì„¤ëª…í•˜ë©´ í† í”½ ëª¨ë¸ë§ì˜ ê²°ê³¼ì—ì„œ ì¸ì‚¬ì´íŠ¸ë¥¼ ëŒì–´ë‚´ëŠ” ë° ë„ì›€ì´ ëœë‹¤. í•˜ì§€ë§Œ ëª¨ë“  í† í”½ì´ ì •í™•íˆ ì„¤ëª… ê°€ëŠ¥í•˜ê²Œ ë¶„ë¥˜ë˜ì§€ëŠ” ì•Šì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì£¼ì˜. | . *Topic #0: â€œê²°ì œ ê´€ë ¨ ë¶ˆë§Œ ì‚¬í•­ - ë¬´ë£Œ ê°€ì… í›„ ìë™ ê²°ì¬, í•´ì§€/í™˜ë¶ˆ ì ˆì°¨ì— ëŒ€í•œ ë¶ˆë§Œâ€ . | keywords: ë¬´ë£Œ ê°€ì… í–ˆëŠ”ë° í•´ì§€ ì•„ë‹ˆ ì´ìš© í™˜ë¶ˆ ì‚¬ìš© ê³„ì • í•´ì„œ | ë¬¸ì„œ ì˜ˆì‹œ: â€œì¹´ë“œê²°ì¬ ì£¼ì˜í•˜ì‹œê³  í•´ì§€ë„ ì¡°ì‹¬í•˜ì„¸ìš”. ê³„ì •ì´ ë‘ê°œë¼ í•œê°œë§Œ ê²°ì¬í–ˆëŠ”ë° í•œë‹¬í›„ë³´ë‹ˆ ë‘ê°œë‹¤ ê²°ì¬ë¼ê³  ê²°ì¬ 5ì¼ì „ì— í•´ì§€í–ˆëŠ”ë° ë˜ ê²°ì¬ë¼ê³ â€, â€œì˜¤ëŠ˜ ë„·í”Œë¦­ìŠ¤ ì²˜ìŒ ì´ìš© í•´ë´¤ëŠ”ë° íšŒì›ê°€ì…í•˜ê³  ì²« ë‹¬ ë¬´ë£Œë¼ë©´ì„œ ì¹´ë“œ ë“±ë¡í•˜ë‹ˆê¹Œ ë°”ë¡œ ëˆ ë¹ ì ¸ë‚˜ê°€ëŠ” ê±° ë­¡ë‹ˆê¹Œ?â€ | . *Topic #1: â€œìë§‰ ë° ì˜ìƒ ê¸°ëŠ¥ ê´€ë ¨ ë¶ˆë§Œ ì‚¬í•­ - ìë§‰ ì˜¤ë¥˜, ì˜¤ì—­, ê¸°ëŠ¥ìƒì˜ ê²°í•¨ì— ëŒ€í•œ ë¶ˆë§Œâ€ . | keywords: ìë§‰ ì˜í™” ì¬ìƒ ì˜ìƒ ë„·í”Œë¦­ìŠ¤ ì†Œë¦¬ ë¬¸ì œ ë‚˜ì˜¤ê³  ê¸°ëŠ¥ ë“œë¼ë§ˆ | ë¬¸ì„œ ì˜ˆì‹œ: â€œë„·í”Œë¦­ìŠ¤ì— ì˜¬ë¼ì˜¤ëŠ” ì‘í’ˆë“¤ì€ ë§Œì¡±ìŠ¤ëŸ½ê³  ì‘í’ˆì„±ì´ ë›°ì–´ë‚œë° í•œê¸€ ë²ˆì—­ì€ ì™œ ê·¸ë”°ìœ„ì¸ê°€ìš”?â€, â€œìë§‰ì´ë‚˜ ë²ˆì—­ì— ì‹¬ê°í•œ ì˜¤ì˜ì—­ì´ ì¡´ì¬í•˜ëŠ” ê²½ìš°ë¥¼ ë§ì´ ë³´ì•˜ì–´ìš”.â€, â€œì—…ë°ì´íŠ¸ ì´í›„ v50s ë“€ì–¼ìŠ¤í¬ë¦° ì¬ìƒì´ ë˜ì§€ë¥¼ ì•ŠìŠµë‹ˆë‹¤.â€ | . *Topic #2: â€œë¬´í•œ ë¡œë”© ë¬¸ì œ - íŠ¹ì • ê¸°ì¢…(ì œíŠ¸í”Œë¦½) ê´€ë ¨ ë¬¸ì œê°€ ë‹¤ìˆ˜â€ . | keywords: ë¡œë”© ìê¾¸ í”Œë¦½ ì§„ì§œ í™”ì§ˆ ë‚´ê³  ë¬´í•œ ë¬´í•œ ë¡œë”© ë³´ëŠ”ë° ì…ë‹ˆë‹¤ | ë¬¸ì„œ ì˜ˆì‹œ: â€œì œíŠ¸í”Œë¦½ ì‚¬ìš©ìëŠ” ì „ë¶€ ê·¸ëŸ¬ë‚˜ë´ìš” ì €ë„ ë¡œë”©ë§Œëœ¨ê³  ë„˜ì–´ê°€ì§ˆì•ŠëŠ”ë°â€, â€œì œíŠ¸í”Œë¦½ ì‚¬ìš©ìì¸ë° ì—…ë°ì´íŠ¸í•˜ê³  ê°‘ìê¸° ë¬´í•œë¡œë”©â€¦â€ | . *Topic #3: â€œë‹¤ì–‘í•œ ì£¼ì œì˜ ë¶ˆë§Œ ì‚¬í•­ ë° ìš”ì²­ ì‚¬í•­â€ . | keywords: ì•ˆë¨ í•´ì£¼ì„¸ìš” ë…¸íŠ¸ ì‹¤í–‰ ê¹”ì•„ë„ ì‚­ì œ ê°‘ìê¸° ì–´í”Œ ì‹œì²­ ë‹¤ìš´ | ë¬¸ì„œ ì˜ˆì‹œ: â€œë‹¤ë¥¸ê¸°ê¸°ë¡œ ì¬ì‹œì²­ì‹œ ê¸°ì¡´ì— ì‹œì²­í–ˆë˜ ì‹œê°„ëŒ€ë¡œ ì‹œì²­í• ìˆ˜ìˆê²Œë” ì—…ë°ì´íŠ¸ì¢€ ë¶€íƒë“œë ¤ìš”â€, â€œë‚´ê°€ ì°œí•œ ì½˜í…ì¸  ëˆŒëŸ¬ë„ í™•ì¸í•´ë³´ë©´ ì¶”ê°€ ì•ˆë˜ì–´ìˆê³  ì˜ˆì „ì—ëŠ” ì°œí•œ ì½˜í…ì¸ ì‚­ì œí•˜ëŠ”ê²ƒë„ ëì—ˆëŠ”ë° ì§€ê¸ˆì€ ì™œ ì•ˆë ê¹Œìš”?â€ | â€» ì°œí•˜ê¸°, ì•Œë¦¼, ì´ì–´ë³´ê¸° ë“± íŠ¹ì • ê¸°ëŠ¥ ê´€ë ¨, ì•± ì‹¤í–‰ ì˜¤ë¥˜ ê´€ë ¨â€¦ ë‹¤ì–‘í•œ ì£¼ì œê°€ mixëœ í† í”½ - í•˜ë‚˜ë¡œ ì •ì˜í•˜ê¸° ì–´ë µë‹¤ | . *Topic #4: â€œì˜ìƒ ì¬ìƒ ê´€ë ¨ ë¶ˆë§Œ ì‚¬í•­ - ì¬ìƒ ê²½í—˜ì—ì„œì˜ ë¶ˆì¾Œê°â€ . | keywords: ì˜ìƒ ì¬ìƒ í•´ê²° ì†Œë¦¬ í•´ì£¼ì„¸ìš” ì•ˆë˜ìš” ë©ˆì¶”ê³  ì œë°œ ê°œì„  ì—ëŸ¬ | ë¬¸ì„œ ì˜ˆì‹œ: â€œì•„ë‹ˆ ì˜¤ëŠ˜ ì—…ë°ì´íŠ¸í•˜ë¼í•´ì„œ í–ˆëŠ”ë° 30ì´ˆë§ˆë‹¤ ì˜ìƒì´ ì¤‘ì§€ë˜ë„¤ìš”â€, â€œì˜ìƒì´ ìê¾¸ ëŠê²¨ìš” ì›ë˜ë„ ì ê¹ì”© ëŠê¸´ ì ì´ ìˆê¸´ í–ˆì§€ë§Œ ì˜¤ëŠ˜ì€ ì¢€ ì‹¬í•˜ë„¤ìš”â€ | Topic #2ì™€ ìœ ì‚¬ (pyLDAvisë¡œ ì‹œê°í™”í•œ Intertopic Distance Mapì—ì„œë„ ìœ ì‚¬í•˜ê²Œ ë‚˜íƒ€ë‚¨) | . *Topic #5: â€œì ‘ì† ì¥ì•  ê´€ë ¨ ë¶ˆë§Œ ì‚¬í•­ - ì ‘ì† ë¶ˆê°€, ë¡œê·¸ì¸ ë¶ˆê°€ ë“±ì˜ ë¬¸ì œâ€ . | keywords: ë¡œê·¸ì¸ ì‹¤í–‰ ì„¤ì¹˜ ì–´í”Œ ì‚­ì œ í–ˆëŠ”ë° ì•ˆë˜ê³  ì•ˆë˜ë„¤ìš” í•´ë„ ê³ ê° | ë¬¸ì„œ ì˜ˆì‹œ: â€œë„·í”Œë¦­ìŠ¤ì— ì ‘ì†í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤? ë°©ê¸ˆ ì „ê¹Œì§€ë§Œ í•´ë„ ëì—ˆëŠ”ë°â€, â€œì¥ë‚œí•˜ëŠ”ê²ƒë„ ì•„ë‹ˆê³  ê²°ì œí•˜ë‹ˆê¹Œ ì•ˆë“¤ì–´ê°€ì§‘ë‹ˆë‹¤. ì¬ì„¤ì¹˜ í•˜ë ¤ê³  ì§€ì› ë‹¤ ì„¤ì¹˜ì¤‘ì¸ë° ì„¤ì¹˜ë„ ê³„ì† ì•ˆë˜ê³  ìˆë„¤ìš”â€ | . ",
    "url": "https://chaelist.github.io/docs/ml_application/topic_modeling/#lda-scikit-learn",
    "relUrl": "/docs/ml_application/topic_modeling/#lda-scikit-learn"
  },"250": {
    "doc": "Topic Modeling (LDA)",
    "title": "LDA: gensim",
    "content": ". | gensim.models.ldamodel.LdaModelì„ ì‚¬ìš© | gensim: ìì—°ì–´ ì²˜ë¦¬ì— íŠ¹í™”ëœ ë¼ì´ë¸ŒëŸ¬ë¦¬ | . ## sklearnì„ í™œìš©í•œ LDAì—ì„œì™€ ê°™ì€ ë°ì´í„°ë¥¼ ì‚¬ìš© review_df.head() . | Â  | Score | Content | Date | . | 0 | 1 | ì‚¬ê¸° | 2021-05-28 13:04:04 | . | 1 | 1 | ê°¤ëŸ­ì‹œs6ì…ë‹ˆë‹¤. ì˜ ë˜ë‹¤ê°€ í•œë²ˆì”© ë¬´í•œë¡œë”© ê±¸ë¦½ë‹ˆë‹¤. ê·¼ë° ë¬´í•œë¡œë”© í•œë²ˆ ê±¸ë¦¬ë©´â€¦ | 2021-05-28 11:30:37 | . | 2 | 1 | ë¡œë”©ì´ ë„ˆë¬´ ëŠë ¤ì„œ ìê¾¸ ë©ˆì¶°. ì‚¬ìš´ë“œëŠ” ë‚˜ì˜¤ëŠ”ë° ì˜ìƒì´ ë©ˆì¶”ëŠ” ê²½ìš°ë„ ìˆê³ , ê·¸ëŸ¬ë‹¤â€¦ | 2021-05-28 08:15:41 | . | 3 | 1 | ì–¸ì œê¹Œì§€ ë¡œë“œ ì¤‘ë§Œ ëœ°ì§€ ê¶ê¸ˆí•¨ ë¹ˆì„¼ì¡° 7í™” ë³´ë‹¤ê°€ ë‹¤ì‹œ ë³´ë ¤ë‹ˆê¹Œ ë¬´í•œë¡œë”©,, ë‚´ê°€â€¦ | 2021-05-28 01:12:33 | . | 4 | 1 | ì•„ê¹Œê¹Œì§„ ì˜ ë˜ë‹¤ ê°‘ìê¸° íŠ•ê¸°ë”ë‹ˆ ê·¸ ë‹¤ìŒë¶€í„°ëŠ” í°ì„ ë„ê³  ë‹¤ì‹œ ì¼œì„œ ë“¤ì–´ê°€ë„ ë””ë°”ì´ìŠ¤â€¦ | 2021-05-27 19:37:03 | . Tokenization . import konlpy import re def tokenize_korean_text(text): text = re.sub(r'[^,.?!\\w\\s]','', text) okt = konlpy.tag.Okt() Okt_morphs = okt.pos(text) words = [] for word, pos in Okt_morphs: if pos == 'Adjective' or pos == 'Verb' or pos == 'Noun': words.append(word) ## wordë¥¼ ì´ì–´ë¶™ì¸ string í˜•íƒœê°€ ì•„ë‹Œ wordì˜ listë¥¼ returní•´ì£¼ëŠ” ê²Œ sklearn lda ì¤€ë¹„ ê³¼ì •ê³¼ì˜ ì°¨ì´ return words tokenized_list = [] for text in review_df['Content']: tokenized_list.append(tokenize_korean_text(text)) print(len(tokenized_list)) print(tokenized_list[1]) . 4242 ['ì‚¬ê¸°'] . +) ë‹¨ì–´ê°€ 1-2ê°œë§Œ í¬í•¨ëœ corpusëŠ” ì‚­ì œ . drop_corpus = [] for index in range(len(tokenized_list)): corpus = tokenized_list[index] if len(set(corpus)) &lt; 3: # corpus ìì²´ê°€ list í˜•íƒœì¸ê²Œ sklearn lda ì¤€ë¹„ ê³¼ì •ê³¼ì˜ ì°¨ì´ review_df.drop(index, axis='index', inplace=True) drop_corpus.append(corpus) for corpus in drop_corpus: tokenized_list.remove(corpus) review_df.reset_index(drop=True, inplace=True) . Vectorization &amp; LDA . # bigram ìƒì„±ì— í•„ìš”í•œ library from gensim.models import Phrases from gensim.models.phrases import Phraser # vectorize &amp; ldaì— í•„ìš”í•œ library from gensim import corpora from gensim.models.ldamodel import LdaModel . â£1. bigramì„ í¬í•¨í•˜ê¸° ìœ„í•œ ì¶”ê°€ ì„¸íŒ… . # Build the bigram models bigram = Phrases(tokenized_list, min_count=4, threshold=10) bigram_mod = Phraser(bigram) # See example print(bigram_mod[tokenized_list[0]]) . ['ê°¤ëŸ­ì‹œ', 'ì…ë‹ˆë‹¤', 'ì˜_ë˜ë‹¤ê°€', 'í•œë²ˆ', 'ë¬´í•œ_ë¡œë”©', 'ê±¸ë¦½ë‹ˆë‹¤', 'ë¬´í•œ_ë¡œë”©', 'í•œë²ˆ', 'ê±¸ë¦¬ë©´', 'í•œ', 'ì£¼ê°„', 'ë„·í”Œë¦­ìŠ¤', 'ì•±', 'ì•„ì˜ˆ', 'ì¼œì§€ì§€ê°€', 'ì•ŠìŠµë‹ˆë‹¤', 'ì›ì¸', 'ëª¨ë¥´ê² ê³ ', 'ì´ëŸ´', 'ë•Œ', 'í”¼ì”¨', 'ë´', 'ì•¼í•´ì„œ', 'ë²ˆê±°ë¡­ìŠµë‹ˆë‹¤'] . | gensim.models.Phrases: ìë™ìœ¼ë¡œ ngram colloationì„ detectí•˜ëŠ” ëª¨ë¸. Phrasesë¥¼ ë°˜ë³µ ì‚¬ìš©í•˜ë©´ bigramë¿ ì•„ë‹ˆë¼ trigramê³¼ ê·¸ ì´ìƒë„ ìƒì„± ê°€ëŠ¥ . | min_count: ìµœì†Œí•œ min_countë³´ë‹¤ ë§ì´ ë“±ì¥í•œ tokenì´ ëŒ€ìƒ | threshold: defaultëŠ” 10, ê°’ì´ ì‘ì„ìˆ˜ë¡ ëœ ë¹ˆë²ˆí•œ ë‹¨ì–´ ì¡°í•©ì´ë”ë¼ë„ bigramìœ¼ë¡œ ìƒì„±í•´ì¤€ë‹¤. ì•„ì£¼ ë¹ˆë²ˆí•˜ê²Œ ì‚¬ìš©ë˜ëŠ” ìµœì†Œí•œì˜ bigramë§Œ ë§Œë“¤ê³ ì í•˜ë©´ thresholdë¥¼ ë†’ì—¬ì£¼ë©´ ëœë‹¤ | . | . # tokenized_listì˜ ëª¨ë“  ë¬¸ì„œì— ëŒ€í•´ bigramì„ ìƒì„±í•´ì¤Œ words_bigram = [bigram_mod[doc] for doc in tokenized_list] . â£2. vectorí™” . dictionary = corpora.Dictionary(words_bigram) # ê° ë‹¨ì–´ì— ë²ˆí˜¸ë¥¼ í• ë‹¹í•´ì¤Œ # bigram í¬í•¨í•˜ëŠ” ê³¼ì •ì„ ìƒëµí•˜ê³  ì‹¶ìœ¼ë©´, ê·¸ëƒ¥ ë°”ë¡œ ì—¬ê¸°ì— tokenized_listë¥¼ ë„£ì–´ì£¼ë©´ ë¨ dictionary.filter_extremes(no_below=2, no_above=0.05) # 2ê°œì˜ ë¬¸ì„œ ë¯¸ë§Œìœ¼ë¡œ ë“±ì¥í•˜ëŠ” ë‹¨ì–´ëŠ” ì œì™¸, ì „ì²´ì˜ 5% ì´ìƒìœ¼ë¡œ ìì£¼ ë“±ì¥í•˜ëŠ” ë‹¨ì–´ëŠ” ì œì™¸ corpus = [dictionary.doc2bow(text) for text in words_bigram] print(corpus[0]) # ì²«ë²ˆì§¸ corpusë¥¼ í…ŒìŠ¤íŠ¸ë¡œ ì¶œë ¥: ëª‡ ë²ˆì§¸ ë‹¨ì–´ê°€ ëª‡ ë²ˆ ë‚˜ì™”ëŠ”ì§€ ì €ì¥ë˜ì–´ ìˆìŒ . [(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 2), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 2)] . â£3. LDA ëª¨ë¸ í•™ìŠµ &amp; ì—°ê´€ì–´ í™•ì¸ . ldamodel = LdaModel(corpus, num_topics=6, id2word=dictionary, passes=20, iterations=500) # í† í”½ ìˆ˜: 6 ldamodel.print_topics(num_words=7) # num_words=10ì´ default . [(0, '0.011*\"ë¡œê·¸ì¸\" + 0.006*\"ì•ˆ\" + 0.006*\"ê³ ê°_ì„¼í„°\" + 0.006*\"í™˜ë¶ˆ\" + 0.006*\"í•´ë„\" + 0.005*\"í•˜ë©´\" + 0.005*\"ì˜\"'), (1, '0.009*\"ì•±_ì‹¤í–‰\" + 0.009*\"ì•ˆë˜ë„¤ìš”\" + 0.008*\"ì´\" + 0.008*\"ê²°ì¬\" + 0.008*\"ë‹¤ìš´\" + 0.007*\"ì•ˆë˜ìš”\" + 0.007*\"ë””ë°”ì´ìŠ¤_ì˜¤ë¥˜\"'), (2, '0.014*\"ë­\" + 0.010*\"ê°€ì…\" + 0.010*\"ì˜í™”\" + 0.009*\"í–ˆëŠ”ë°\" + 0.008*\"ë¬´ë£Œ\" + 0.007*\"ì–´ë–»ê²Œ\" + 0.006*\"í•´ì§€\"'), (3, '0.022*\"í™”ì§ˆ\" + 0.017*\"ìë§‰\" + 0.008*\"ì˜í™”\" + 0.007*\"ì œë°œ\" + 0.006*\"ì—†ê³ \" + 0.006*\"ë³¼\" + 0.005*\"ìƒê°\"'), (4, '0.009*\"ë¬´í•œ_ë¡œë”©\" + 0.008*\"ì‚­ì œ\" + 0.007*\"ì•ˆë©ë‹ˆë‹¤\" + 0.007*\"ì…ë‹ˆë‹¤\" + 0.007*\"í•´ê²°\" + 0.007*\"ë¡œë”©\" + 0.007*\"ë¬¸ì œ\"'), (5, '0.011*\"ëˆ_ë‚´ê³ \" + 0.009*\"ì•ˆ\" + 0.009*\"ì œë°œ\" + 0.008*\"ë³´ëŠ”ë°\" + 0.006*\"ìë§‰\" + 0.006*\"ë¬¸ì œ\" + 0.006*\"ì•„ë‹ˆê³ \"')] . | LdaModelì˜ íŒŒë¼ë¯¸í„°: . | passes: Number of passes through the corpus during training. (default: 1) | iterations: Maximum number of iterations through the corpus when inferring the topic distribution of a corpus. (default: 50) | corpus ìˆ˜ê°€ ì ìœ¼ë©´ passesë¥¼ ë†’ì—¬ì£¼ëŠ” ê²ƒì´ ìœ ìš©í•  ìˆ˜ ìˆê³ , ì‹œê°„ë§Œ ì¶©ë¶„í•˜ë‹¤ë©´ iterationsë¥¼ ë†’ì—¬ì£¼ë©´ ë” í•™ìŠµì´ ì˜ ëœë‹¤ (iterationsê°€ ë‚®ìœ¼ë©´ ì œëŒ€ë¡œ ìˆ˜ë ´í•˜ê¸° ì „ì— í•™ìŠµì´ ì¢…ë£Œë  ìˆ˜ ìˆìŒ) | . | . LDA ì‹œê°í™”: pyLDAvis . import pyLDAvis.gensim # gensimì˜ ldamodelì— ìµœì í™”ëœ ë¼ì´ë¸ŒëŸ¬ë¦¬ vis = pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary) pyLDAvis.display(vis) . (ì´ë¯¸ì§€ë¥¼ í´ë¦­í•˜ë©´ htmlë¡œ êµ¬í˜„ëœ ë²„ì „ìœ¼ë¡œ í™•ì¸ ê°€ëŠ¥) . ë¬¸ì„œë³„ í† í”½ í• ë‹¹ . doc_per_topic_list = [] for n in range(len(corpus)): doc_topic = ldamodel[corpus[n]] doc_topic = sorted(doc_topic, key=lambda x: (x[1]), reverse=True) topic_most_pr = doc_topic[0][0] topic_pr = doc_topic[0][1] doc_per_topic_list.append([n, topic_most_pr, topic_pr]) doc_topic_df = pd.DataFrame(doc_per_topic_list, columns=['Doc_Num', 'Topic', 'Percentage']) # ì‹¤ì œ review ë‚´ìš©ê³¼ join doc_topic_df = doc_topic_df.join(review_df) doc_topic_df.head() . | Â  | Doc_Num | Topic | Percentage | Score | Content | Date | . | 0 | 0 | 4 | 0.959966 | 1 | ê°¤ëŸ­ì‹œs6ì…ë‹ˆë‹¤. ì˜ ë˜ë‹¤ê°€ í•œë²ˆì”© ë¬´í•œë¡œë”© ê±¸ë¦½ë‹ˆë‹¤. ê·¼ë° ë¬´í•œë¡œë”© í•œë²ˆ ê±¸ë¦¬ë©´â€¦ | 2021-05-28 11:30:37 | . | 1 | 1 | 2 | 0.955639 | 1 | ë¡œë”©ì´ ë„ˆë¬´ ëŠë ¤ì„œ ìê¾¸ ë©ˆì¶°. ì‚¬ìš´ë“œëŠ” ë‚˜ì˜¤ëŠ”ë° ì˜ìƒì´ ë©ˆì¶”ëŠ” ê²½ìš°ë„ ìˆê³ , ê·¸ëŸ¬ë‹¤â€¦ | 2021-05-28 08:15:41 | . | 2 | 2 | 0 | 0.836514 | 1 | ì–¸ì œê¹Œì§€ ë¡œë“œ ì¤‘ë§Œ ëœ°ì§€ ê¶ê¸ˆí•¨ ë¹ˆì„¼ì¡° 7í™” ë³´ë‹¤ê°€ ë‹¤ì‹œ ë³´ë ¤ë‹ˆê¹Œ ë¬´í•œë¡œë”©,, ë‚´ê°€â€¦ | 2021-05-28 01:12:33 | . | 3 | 3 | 1 | 0.976007 | 1 | ì•„ê¹Œê¹Œì§„ ì˜ ë˜ë‹¤ ê°‘ìê¸° íŠ•ê¸°ë”ë‹ˆ ê·¸ ë‹¤ìŒë¶€í„°ëŠ” í°ì„ ë„ê³  ë‹¤ì‹œ ì¼œì„œ ë“¤ì–´ê°€ë„ ë””ë°”ì´ìŠ¤â€¦ | 2021-05-27 19:37:03 | . | 4 | 4 | 4 | 0.83286 | 1 | ë¬´í•œë¡œë”©ì¤‘â€¦â€¦ë¹ ë¥¸ ë¬¸ì œí•´ê²° ë¶€íƒë“œë¦½ë‹ˆë‹¤. | 2021-05-27 17:22:05 | . ",
    "url": "https://chaelist.github.io/docs/ml_application/topic_modeling/#lda-gensim",
    "relUrl": "/docs/ml_application/topic_modeling/#lda-gensim"
  },"251": {
    "doc": "Twitter ë°ì´í„° ìˆ˜ì§‘",
    "title": "Twitter ë°ì´í„° ìˆ˜ì§‘",
    "content": ". | Tweepyë¡œ ìˆ˜ì§‘ (Twitter API v1.1) . | ì¤€ë¹„ ì‘ì—… | íŠ¹ì • userì˜ timeline ìˆ˜ì§‘ | Cursorë¥¼ í™œìš©í•´ ìˆ˜ì§‘ | íŠ¹ì • userì˜ follower ìˆ˜ì§‘ | íŠ¹ì • queryë¥¼ í¬í•¨í•œ tweet ìˆ˜ì§‘ | . | . ",
    "url": "https://chaelist.github.io/docs/webscraping/twitter_api/",
    "relUrl": "/docs/webscraping/twitter_api/"
  },"252": {
    "doc": "Twitter ë°ì´í„° ìˆ˜ì§‘",
    "title": "Tweepyë¡œ ìˆ˜ì§‘ (Twitter API v1.1)",
    "content": ". | Tweepy: Twitter APIë¥¼ pythonìœ¼ë¡œ ì‰½ê²Œ í™œìš©í•  ìˆ˜ ìˆê²Œ í•´ì£¼ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ . | â€»TwitterëŠ” ìì‚¬ ì„œë¹„ìŠ¤ì—ì„œ ë°œìƒí•˜ëŠ” ë‹¤ì–‘í•œ ë°ì´í„°ë¥¼ ì‰½ê²Œ í™œìš©í•  ìˆ˜ ìˆë„ë¡ APIë¥¼ ì œê³µí•œë‹¤ | . | pip install tweepyë¡œ ì„¤ì¹˜í•´ì„œ ì‚¬ìš© | . ì¤€ë¹„ ì‘ì—… . â£1. Twitter API ê¶Œí•œ ì–»ê¸° . | Tweepyë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´ì„ , ìš°ì„  Twitter APIë¥¼ í™œìš©í•  ìˆ˜ ìˆëŠ” ê¶Œí•œì´ í•„ìš”í•˜ë‹¤ | https://developer.twitter.com/en/apply-for-accessì—ì„œ ê³„ì •ì„ ë§Œë“¤ê³ , applicationì„ ìƒì„±í•˜ë©´ APIì— ì ‘ì† ê°€ëŠ¥í•œ token ë“±ì„ ë°›ì„ ìˆ˜ ìˆë‹¤ | . â£2. ê¶Œí•œ ì¸ì¦ &amp; API instance ìƒì„± . import tweepy # íŠ¸ìœ„í„° APIì— ì ‘ê·¼í•˜ê¸° ìœ„í•œ ê°œì¸ í‚¤ë¥¼ ì…ë ¥ consumer_key = \"\" consumer_secret = \"\" access_token = \"\" access_token_secret = \"\" # OAuth í•¸ë“¤ëŸ¬ ìƒì„± &amp; ê°œì¸ì •ë³´ ì¸ì¦ ìš”ì²­ auth = tweepy.OAuthHandler(consumer_key, consumer_secret) # ì•¡ì„¸ìŠ¤ ìš”ì²­ auth.set_access_token(access_token, access_token_secret) # api instace ìƒì„± api = tweepy.API(auth) . íŠ¹ì • userì˜ timeline ìˆ˜ì§‘ . | References: tweepy, developer.twitter | . API.user_timeline(user_id, screen_name, since_id, count, max_id, trim_user, exclude_replies, include_rts) . | user_idë‚˜ screen_name ì¤‘ í•˜ë‚˜ì˜ parameterë¡œ timelineì„ ê°€ì ¸ì˜¬ userë¥¼ ëª…ì‹œ | cout: defaultëŠ” 20. maximum 200ê¹Œì§€ ê°€ëŠ¥ | . | íŠ¸ìœ— ë‚´ìš© ê°€ì ¸ì˜¤ê¸°: status.text . | ì˜ˆì‹œë¡œ, ì‚¼ì„±ì „ì ë‰´ìŠ¤ë£¸(@SamsungNewsroom)ì˜ íƒ€ì„ë¼ì¸ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜´ | . status = api.user_timeline(screen_name = 'SamsungNewsroom', count=1)[0] status.text . 'ì‚¼ì„±ì „ì, ë³´í˜¸ì¢…ë£Œ ì²­ì†Œë…„ ìë¦½ ë•ëŠ” â€˜ì‚¼ì„± í¬ë§ë””ë”¤ëŒâ€™ ê´‘ì£¼ì„¼í„° ê°œì†Œ\\n\\nhttps://t.co/TUZGjUwlUn . | íŠ¸ìœ— ì‘ì„± ë‚ ì§œ ê°€ì ¸ì˜¤ê¸°: status.created_at print(status.created_at) # datetime.datetime í˜•ì‹ # ì•„ë˜ì™€ ê°™ì´ ì›í•˜ëŠ” í˜•ì‹ìœ¼ë¡œ ë°”ê¿”ì¤˜ë„ ì¢‹ë‹¤ import datetime print(datetime.datetime.strftime(status.created_at, '%Y.%m.%d')) . 2021-06-02 03:34:26 2021.06.02 . | retweetëœ íšŸìˆ˜ ê°€ì ¸ì˜¤ê¸°: status.retweet_count status.retweet_count . 3 . | ì¢‹ì•„ìš”(í•˜íŠ¸) ìˆ˜ ê°€ì ¸ì˜¤ê¸°: status.favorite_count status.favorite_count . 12 . | tweet ì‘ì„±ì ì •ë³´: status.author # status.authorì— ì ‘ê·¼ í›„, ì•„ë˜ì™€ ê°™ì´ í•œ ë‹¨ê³„ì”© ë” ì ‘ê·¼í•´ì„œ ê° ì •ë³´ë¥¼ ì¶”ì¶œ print(status.author.name) print(status.author.screen_name) print(status.author.location) print(status.author.description) . ì‚¼ì„±ì „ì ë‰´ìŠ¤ë£¸ SamsungNewsroom Seoul, Korea ì‚¼ì„±ì „ì ê³µì‹ íŠ¸ìœ„í„°ì…ë‹ˆë‹¤. (Global Newsroom now uses @Samsung) . | retweetëœ ê¸€ì¸ì§€ ì—¬ë¶€ í™•ì¸ # status.retweeted_status.textê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•˜ë©´ ë¨ try: print(status.retweeted_status.text) except: print('not retweeted') . not retweeted . | . Cursorë¥¼ í™œìš©í•´ ìˆ˜ì§‘ . | Reference: tweepy | Cursorë¥¼ í™œìš©í•˜ë©´ limitì—†ì´ ì›í•˜ëŠ” ë§Œí¼ì˜ tweetì„ ìˆ˜ì§‘í•  ìˆ˜ ìˆë‹¤ | tweepy.Cursor()ì•ˆì— api.user_timeline ë“± ì›í•˜ëŠ” ìˆ˜ì§‘ ì˜µì…˜ì„ ë„£ì–´ì£¼ë©´ ëœë‹¤ | .items()ë¡œ limit(ìˆ˜ì§‘ì„ ì›í•˜ëŠ” ì–‘)ì„ ë„˜ê²¨ì¤€ë‹¤ | . import pandas as pd tweet_list = [] for status in tweepy.Cursor(api.user_timeline, id='SamsungNewsroom').items(400): temp_list = [status.text, status.created_at,status.retweet_count, status.favorite_count] tweet_list.append(temp_list) df = pd.DataFrame(tweet_list, columns=['Tweets', 'Created_Date', '#_of_Retweets', '#_of_Likes']) print(len(df)) df.head() . 400 . | Â  | Tweets | Created_Date | #_of_Retweets | #_of_Likes | . | 0 | ì‚¼ì„±ì „ì, ë³´í˜¸ì¢…ë£Œ ì²­ì†Œë…„ ìë¦½ ë•ëŠ” â€˜ì‚¼ì„± í¬ë§ë””ë”¤ëŒâ€™ ê´‘ì£¼ì„¼í„° ê°œì†Œ\\n\\nhttps://t.co/TUZGjUwlUn | 2021-06-02 03:34:26+00:00 | 3 | 12 | . | 1 | ì‚¼ì„±ì „ì, ì°¨ì„¸ëŒ€ ê¸°ì—… ì„œë²„ìš© â€˜ZNS SSDâ€™ ì¶œì‹œ\\n\\nhttps://t.co/fZuY9s5J7P | 2021-06-02 02:24:53+00:00 | 3 | 17 | . | 2 | ì‚¼ì„±ì „ì, ì¸ê³µì§€ëŠ¥ìœ¼ë¡œ ì™„ì „íˆ ìƒˆë¡œì›Œì§„ ì‹œìŠ¤í…œì—ì–´ì»¨ â€˜DVM S2â€™ ì¶œì‹œ\\n\\nhttps://t.co/CXdr48M34c | 2021-06-01 02:02:53+00:00 | 3 | 14 | . | 3 | ì‚¼ì„±ì „ì, ì‹œì²­ê° ì¥ì• ì¸ìš© TV ë³´ê¸‰ì‚¬ì—… ê³µê¸‰ìë¡œ ì„ ì •\\n\\nhttps://t.co/TmByXt3uLA | 2021-05-30 01:09:24+00:00 | 2 | 18 | . | 4 | ì‚¼ì„±ì „ì, ì„¸ê³„ ìµœê³  ê´‘íš¨ìœ¨ì— ìƒ‰í’ˆì§ˆ í˜ì‹ ì„ ë”í•œ LM301B EVO íŒ¨í‚¤ì§€ ì¶œì‹œ\\n\\nhttps://t.co/gOurN6OYZL | 2021-05-27 02:05:31+00:00 | 2 | 10 | . +) â€˜TooManyRequestsâ€™ error . | twitter apië¥¼ ì‚¬ìš©í•´ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¬ ë•Œì—ëŠ” ì¼ì • ê¸°ê°„ ë™ì•ˆ ê°€ì ¸ì˜¬ ìˆ˜ ìˆëŠ” íšŸìˆ˜ê°€ ì œí•œë˜ì–´ ìˆë‹¤. | íŠ¹íˆ, Cursorë¡œ ë¬´ì œí•œ ìˆ˜ì§‘í•˜ë‹¤ë³´ë©´ rate limitì´ ì´ˆê³¼ë˜ì–´ errorê°€ ë‚  ìˆ˜ ìˆê¸°ì—, ì•„ë˜ì™€ ê°™ì€ ì½”ë“œë¥¼ ì¶”ê°€í•´ì£¼ë©´ rate limitì´ ì´ˆê³¼ë  ë•Œ ì¼ì • ì‹œê°„ì„ ê¸°ë‹¤ë ¸ë‹¤ ì‘ì—…í•´ì£¼ê²Œ ëœë‹¤ | . def limit_handled(cursor): while True: try: yield next(cursor) except tweepy.TooManyRequests: time.sleep(15 * 60) # 15ë¶„ ê¸°ë‹¤ë ¤ì¤Œ. for status in limit_handled(tweepy.Cursor(api.user_timeline, screen_name = '').items()): ## -- ì½”ë“œ -- ## . | tweepy exceptionì˜ ì¢…ë¥˜: https://docs.tweepy.org/en/latest/exceptions.html | . íŠ¹ì • userì˜ follower ìˆ˜ì§‘ . | References: tweepy, developer.twitter | . API.followers(user_id, screen_name, cursor, count, skip_status, include_user_entities) . | ì´ë¦„, screen_name ë“± ì •ë³´ ê°€ì ¸ì˜¤ê¸° . | ì˜ˆì‹œë¡œ, ë°©íƒ„ì†Œë…„ë‹¨(@BTS_twt)ì˜ follower ì •ë³´ë¥¼ ìˆ˜ì§‘ | . followers = api.followers(screen_name = 'BTS_twt') follower = followers[0] print(follower.name) # ì´ë¦„ print(follower.screen_name) # screen name (@ë’¤ì— ë¶™ì€ ì´ë¦„) print(follower.created_at) # ê³„ì •ì´ ìƒì„±ëœ ë‚  print(follower.description) # description print(follower.followers_count) # íŒ”ë¡œì›Œ ìˆ˜ print(follower.friends_count) # íŒ”ë¡œì‰í•˜ëŠ” ì‚¬ëŒ ìˆ˜ . Suci Suci90898274 2021-06-03 01:28:22+00:00 ğŸ’œğŸ’œ 0 13 . | Cursorë¥¼ ì´ìš©í•´ ë‹¤ëŸ‰ì„ í•œë²ˆì— ìˆ˜ì§‘ follower_list = [] for follower in tweepy.Cursor(api.followers, screen_name = 'BTS_twt').items(400): temp_list = [follower.name, follower.screen_name, follower.created_at, follower.description, follower.followers_count, follower.friends_count] follower_list.append(temp_list) df = pd.DataFrame(follower_list, columns=['Name', 'ID', 'Created_Date', 'Description', 'Followers', 'Following']) df.head() . | Â  | Name | ID | Created_Date | Description | Followers | Following | . | 0 | felix | felix22210574 | 2021-06-03 01:54:20+00:00 | Â  | 0 | 2 | . | 1 | aily | 2OOrb | 2021-06-02 23:46:19+00:00 | she / they ; ğŸ” | 0 | 11 | . | 2 | delin | delin73271563 | 2021-06-03 01:54:00+00:00 | Â  | 0 | 1 | . | 3 | Muniri Iri | IriMuniri | 2021-06-03 01:49:15+00:00 | nikmatilah hidup selagi masih bernafasğŸ™‚ | 0 | 10 | . | 4 | Rhealyn Mamaril | MamarilRhealyn | 2021-06-03 01:52:35+00:00 | yolo | 0 | 5 | . | . íŠ¹ì • queryë¥¼ í¬í•¨í•œ tweet ìˆ˜ì§‘ . | References: tweepy, developer.twitter | . API.search_tweets(q, geocode, lang, locale, result_type, count, until, since_id, max_id, include_entities) . | q: ì•Œê³  ì‹¶ì€ queryë¥¼ ì…ë ¥. í•´ë‹¹ queryê°€ í¬í•¨ëœ tweetë§Œ ìˆ˜ì§‘ë˜ì–´ ì˜¨ë‹¤ | result_type: defaultëŠ” â€˜mixedâ€™ . | mixed: include both popular &amp; real time results | recent: return only the most recent results | popular: return only the most popular results | . | until: íŠ¹ì • ì¼ì ì´ì „ì˜ tweetë§Œ ê²€ìƒ‰. ë‚ ì§œëŠ” â€˜YYYY-MM-DDâ€™í˜•íƒœë¡œ ì…ë ¥. | â€» search indexëŠ” 7-day limitì´ ìˆê¸°ì—, ì¼ì£¼ì¼ ì´ì „ì˜ tweetì€ ê²€ìƒ‰ë˜ì§€ ì•ŠëŠ”ë‹¤ | . | . | tweet ì •ë³´ ê°€ì ¸ì˜¤ê¸° # ì´ëŸ°ì‹ìœ¼ë¡œ íŠ¹ì • geocodeì—ì„œì˜ tweetë§Œ ê²€ìƒ‰í•´ì˜¬ ìˆ˜ë„ ìˆë‹¤ korea_geo = \"%s,%s,%s\" % (\"35.95\", \"128.25\", \"1000km\") statuses = api.search_tweets(q='ë°©íƒ„', geocode=korea_geo, count=1) status = statuses[0] print(status.text) # tweet ë‚´ìš© print(status.created_at) # ê²Œì‹œ ì¼ì print(status.retweet_count) # retweetëœ íšŸìˆ˜ print(status.favorite_count) # ì¢‹ì•„ìš” ë°›ì€ íšŸìˆ˜ . RT @corwin1129: ë§¥ë„ë‚ ë“œê°€ ë¹„ê²(?)í•˜ê²Œ ë°©íƒ„ì„ ëŒê³  ì˜¤ëŠ” ë°”ëŒì—.... 2021-06-03 02:03:36+00:00 2797 0 . | tweet ì‘ì„±ìì˜ ì •ë³´ ê°€ì ¸ì˜¤ê¸° print(status.user.id) # ID print(status.user.name) # ì´ë¦„ print(status.user.screen_name) # screen name (@ë’¤ì— ë¶™ì€ ì´ë¦„) print(status.user.description) # description print(status.user.followers_count) # íŒ”ë¡œì›Œ ìˆ˜ print(status.user.friends_count) # íŒ”ë¡œì‰í•˜ëŠ” ì‚¬ëŒ ìˆ˜ . 3018274394 ğŸBABARAMANğŸ‰ BKB_shot ê²œë• 551042289 ì²´ì¸ë¸”ë½ì”ë‹ˆë‹¤ 71 98 . | retweetëœ ê¸€ì˜ ê²½ìš°, ì›ë³¸ í™•ì¸ . | retweeted_statusê°€ ì¡´ì¬í•˜ë©´ retweetëœ ê¸€ì´ë‹¤ | . # ì›ë³¸ ê¸€ í™•ì¸ print(status.retweeted_status.text) # ì›ë³¸ ê¸€ ì‘ì„±ì ì •ë³´ í™•ì¸ print(status.retweeted_status.user.id) print(status.retweeted_status.user.name) print(status.retweeted_status.user.screen_name) . ë§¥ë„ë‚ ë“œê°€ ë¹„ê²(?)í•˜ê²Œ ë°©íƒ„ì„ ëŒê³  ì˜¤ëŠ” ë°”ëŒì—.... https://t.co/cA3C1FCC1H 42567931 ì¼ê¸°í†µê´€ corwin1129 . | Cursorë¥¼ ì´ìš©í•´ ë‹¤ëŸ‰ì„ í•œë²ˆì— ìˆ˜ì§‘ . tweet_list = [] korea_geo = \"%s,%s,%s\" % (\"35.95\", \"128.25\", \"1000km\") for status in tweepy.Cursor(api.search_tweets, q='ë°©íƒ„', geocode=korea_geo, until='2021-06-01').items(400): temp_list = [status.text, status.created_at, status.retweet_count, status.favorite_count] try: temp_list.append(status.retweeted_status.text) except: temp_list.append('Not Retweeted') temp_list.extend([status.user.name, status.user.followers_count]) tweet_list.append(temp_list) df = pd.DataFrame(tweet_list, columns=['Text', 'Created_Date', '#_of_Retweets', '#_of_Likes', 'Original_Text', 'User', 'User_#_of_Followers']) df.head() . | Â  | Text | Created_Date | #_of_Retweets | #_of_Likes | Original_Text | User | User_#_of_Followers | Â  | . | 0 | RT @borahe7p: ìŠ¤ë°ì¸ì¦í•˜ë©´ ì•„ì´ìŠ¤í¬ë¦¼ì— ì„¸ì°¨ê¶Œì— ëª©ê±¸ì´ë¼â€¦â€¦â€¦â€¦â€¦â€¦. ë” ë°©íƒ„ ìŠ¤ë°ì´ë‘ ë®¤ìŠ¤ ì£½ì–´ë¼ê³  ë‹¬ë¦¬ê² ìŠµë‹ˆë‹¤. https://t.co/1H6y54MHPZ | 2021-05-31 23:57:28+00:00 | 605 | 0 | ìŠ¤ë°ì¸ì¦í•˜ë©´ ì•„ì´ìŠ¤í¬ë¦¼ì— ì„¸ì°¨ê¶Œì— ëª©ê±¸ì´ë¼â€¦â€¦â€¦â€¦â€¦â€¦. ë” ë°©íƒ„ ìŠ¤ë°ì´ë‘ ë®¤ìŠ¤ ì£½ì–´ë¼ê³  ë‹¬ë¦¬ê² ìŠµë‹ˆë‹¤. https://t.co/1H6y54MHPZ | á´®á´±purpleU4ever | 77 | Â  | . | 1 | @guitar_aeong ì‘ë…„ë³´ë‹¤ ë” ë¹¡ì„¸ìš” ã…‹ã…‹ã…‹ ìš°ë¦¬ ë°©íƒ„ í˜ ì‹¤ì–´ ì£¼ê¸°â€¦(ìƒëµ) | 2021-05-31 23:57:20+00:00 | 0 | 1 | Not Retweeted | ìµœì• ëŠ”7ëª…ì°¨ì• ëŠ”ì•„ë¯¸ | 211 | Â  | . | 2 | RT @soulmate91_bts: ë‹¤ë§ˆê°€ 11ì–µë·°ê°€ ë˜ì—ˆì–´ìš”!!ğŸ‰ğŸ‰ğŸ‰ğŸ‰â€¦(ìƒëµ) | 2021-05-31 23:54:24+00:00 | 15 | 0 | ë‹¤ë§ˆê°€ 11ì–µë·°ê°€ ë˜ì—ˆì–´ìš”!!ğŸ‰ğŸ‰ğŸ‰ğŸ‰â€¦(ìƒëµ) | ë‹¤ì •ë¯¸ë‹ˆğŸ’•ğŸ¤ ğ”…ğ”²ğ”±ğ”±ğ”¢ğ”¯ğŸ§ˆ | 2434 | Â  | . | 3 | RT @BABO_bts0613: ë²„í„°ë®¤ë¹„í•©ì‚°..ì¶œì²˜ê°€ ì–´ë”˜ì§€ëŠ” ëª¨ë¥´ì§€ë§Œâ€¦í•©ì‚°ëœì ì´ ì—†ì—ˆê¸° ë•Œë¬¸ì— ìœ„í—˜í•œê²ë‹ˆë‹¤â€¦(ìƒëµ) | 2021-05-31 23:53:31+00:00 | 301 | 0 | ë²„í„°ë®¤ë¹„í•©ì‚°..ì¶œì²˜ê°€ ì–´ë”˜ì§€ëŠ” ëª¨ë¥´ì§€ë§Œâ€¦í•©ì‚°ëœì ì´ ì—†ì—ˆê¸° ë•Œë¬¸ì— ìœ„í—˜í•œê²ë‹ˆë‹¤â€¦(ìƒëµ) | âŸ­âŸ¬ íœ˜ë¹„ âŸ¬âŸ­ | 422 | Â  | . | 4 | RT @borahe7p: ìŠ¤ë°ì¸ì¦í•˜ë©´ ì•„ì´ìŠ¤í¬ë¦¼ì— ì„¸ì°¨ê¶Œì— ëª©ê±¸ì´ë¼â€¦â€¦â€¦â€¦â€¦â€¦. ë” ë°©íƒ„ ìŠ¤ë°ì´ë‘ ë®¤ìŠ¤ ì£½ì–´ë¼ê³  ë‹¬ë¦¬ê² ìŠµë‹ˆë‹¤. https://t.co/1H6y54MHPZ | 2021-05-31 23:53:02+00:00 | 605 | 0 | ìŠ¤ë°ì¸ì¦í•˜ë©´ ì•„ì´ìŠ¤í¬ë¦¼ì— ì„¸ì°¨ê¶Œì— ëª©ê±¸ì´ë¼â€¦â€¦â€¦â€¦â€¦â€¦. ë” ë°©íƒ„ ìŠ¤ë°ì´ë‘ ë®¤ìŠ¤ ì£½ì–´ë¼ê³  ë‹¬ë¦¬ê² ìŠµë‹ˆë‹¤. https://t.co/1H6y54MHPZ | ë™ê²½ | 510 | Â  | . | . ",
    "url": "https://chaelist.github.io/docs/webscraping/twitter_api/#tweepy%EB%A1%9C-%EC%88%98%EC%A7%91-twitter-api-v11",
    "relUrl": "/docs/webscraping/twitter_api/#tweepyë¡œ-ìˆ˜ì§‘-twitter-api-v11"
  },"253": {
    "doc": "UK Ecommerce Data 1",
    "title": "UK Ecommerce Data",
    "content": ". | ë°ì´í„° íŒŒì•… ë° ì „ì²˜ë¦¬ . | ë°ì´í„° íƒ€ì… ì •ë¦¬ | ì¤‘ë³µê°’ í™•ì¸ | ê²°ì¸¡ì¹˜ í™•ì¸ | ì´ìƒì¹˜ / ë¶ˆí•„ìš”í•œ ê°’ ì •ë¦¬ | ê²°ì¸¡ì¹˜ ì¬í™•ì¸ | ìƒˆë¡œìš´ ì¹¼ëŸ¼ ìƒì„± | . | ì›”ë³„ ë§¤ì¶œì•¡, êµ¬ë§¤ììˆ˜ | ì½”í˜¸íŠ¸ ë¶„ì„ . | ì½”í˜¸íŠ¸ë³„ ì¬ë°©ë¬¸ë¥  | ì½”í˜¸íŠ¸ë³„ ì´ë§¤ì¶œì•¡ | ì½”í˜¸íŠ¸ë³„ í‰ê·  ì§€ì¶œì•¡ | . | ê³ ê°ë³„ ì´íƒˆ ê°€ëŠ¥ì„± íŒŒì•… | ì›”ë³„ ì´íƒˆë¥  . | ì›”ë³„ ì´íƒˆë¥  ì¶”ì´ | . | ì›”ë³„ ì‹ ê·œ ìœ ì…ë¥  . | ì›”ë³„ ì‹ ê·œìœ ì…ë¥  ì¶”ì´ | . | . *ë¶„ì„ ëŒ€ìƒ ë°ì´í„°ì…‹: UK E-Commerce Data . | ë°ì´í„°ì…‹ ì¶œì²˜ | UK-based retailerì˜ 2020-12-01ì—ì„œ 2011-12-09 ì‚¬ì´ì˜ ëª¨ë“  ê±°ë˜ë¥¼ ê¸°ë¡í•œ ë°ì´í„° | giftë¥¼ ì£¼ë¡œ íŒë§¤í•˜ë©°, ëŒ€ë‹¤ìˆ˜ì˜ ê³ ê°ì€ wholesalerì„ | . ",
    "url": "https://chaelist.github.io/docs/kaggle/uk_ecommerce/#uk-ecommerce-data",
    "relUrl": "/docs/kaggle/uk_ecommerce/#uk-ecommerce-data"
  },"254": {
    "doc": "UK Ecommerce Data 1",
    "title": "ë°ì´í„° íŒŒì•… ë° ì „ì²˜ë¦¬",
    "content": "# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ import import pandas as pd import numpy as np import scipy.stats as stats from matplotlib import pyplot as plt import seaborn as sns import plotly.express as px import plotly.io as pio pio.templates.default = \"plotly_white\" # default templateì„ ì§€ì • . ecom_df = pd.read_csv('data/uk_ecommerce_data.csv') ecom_df.head() . | Â  | InvoiceNo | StockCode | Description | Quantity | InvoiceDate | UnitPrice | CustomerID | Country | . | 0 | 536365 | 85123A | WHITE HANGING HEART T-LIGHT HOLDER | 6 | 12/1/2010 8:26 | 2.55 | 17850 | United Kingdom | . | 1 | 536365 | 71053 | WHITE METAL LANTERN | 6 | 12/1/2010 8:26 | 3.39 | 17850 | United Kingdom | . | 2 | 536365 | 84406B | CREAM CUPID HEARTS COAT HANGER | 8 | 12/1/2010 8:26 | 2.75 | 17850 | United Kingdom | . | 3 | 536365 | 84029G | KNITTED UNION FLAG HOT WATER BOTTLE | 6 | 12/1/2010 8:26 | 3.39 | 17850 | United Kingdom | . | 4 | 536365 | 84029E | RED WOOLLY HOTTIE WHITE HEART. | 6 | 12/1/2010 8:26 | 3.39 | 17850 | United Kingdom | . â†’ ì¹¼ëŸ¼ ì •ë³´ íŒŒì•… . ecom_df.info() . &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 541909 entries, 0 to 541908 Data columns (total 8 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 InvoiceNo 541909 non-null object 1 StockCode 541909 non-null object 2 Description 540455 non-null object 3 Quantity 541909 non-null int64 4 InvoiceDate 541909 non-null object 5 UnitPrice 541909 non-null float64 6 CustomerID 406829 non-null float64 7 Country 541909 non-null object dtypes: float64(2), int64(1), object(5) memory usage: 33.1+ MB . ë°ì´í„° íƒ€ì… ì •ë¦¬ . | Customer ID: string í˜•íƒœë¡œ ë³€ê²½ . | í˜„ì¬ëŠ” Nullê°’ì´ ì„ì´ ì¹¼ëŸ¼ì´ë¼ float í˜•íƒœë¡œ ë˜ì–´ ìˆìŒ | ì†Œìˆ˜ì ì„ ì—†ì• ê¸° ìœ„í•´ ì •ìˆ˜ë¡œ ë¨¼ì € ë³€ê²½í•œ í›„ strë¡œ ë°”ê¿”ì¤Œ | . ecom_df['CustomerID'].fillna(0, inplace=True) # ì¼ë‹¨ N/Aë¥¼ 0ìœ¼ë¡œ ë°”ê¿”ì¤Œ ecom_df['CustomerID'] = ecom_df['CustomerID'].astype('int').astype('str') # ì •ìˆ˜ë¡œ ë³€ê²½í•œ í›„, strë¡œ ë°”ê¿”ì¤Œ ecom_df.replace({'CustomerID': '0'}, 'N/A', inplace=True) # 0ì€ 'N/A'ë¡œ ë°”ê¿”ì¤Œ print(ecom_df['CustomerID'].dtypes) . object . | InvoiceDate: datetime í˜•íƒœë¡œ ë³€ê²½ . ecom_df['InvoiceDate'] = pd.to_datetime(ecom_df['InvoiceDate']) print(ecom_df['InvoiceDate'].dtypes) ecom_df.head(3) . datetime64[ns] . | Â  | InvoiceNo | StockCode | Description | Quantity | InvoiceDate | UnitPrice | CustomerID | Country | . | 0 | 536365 | 85123A | WHITE HANGING HEART T-LIGHT HOLDER | 6 | 2010-12-01 08:26:00 | 2.55 | 17850 | United Kingdom | . | 1 | 536365 | 71053 | WHITE METAL LANTERN | 6 | 2010-12-01 08:26:00 | 3.39 | 17850 | United Kingdom | . | 2 | 536365 | 84406B | CREAM CUPID HEARTS COAT HANGER | 8 | 2010-12-01 08:26:00 | 2.75 | 17850 | United Kingdom | . +) ê¸°ê°„ ë²”ìœ„ í™•ì¸: . print(min(ecom_df['InvoiceDate'])) print(max(ecom_df['InvoiceDate'])) . 2010-12-01 08:26:00 2011-12-09 12:50:00 . | . ì¤‘ë³µê°’ í™•ì¸ . # ì¤‘ë³µê°’ì˜ ìˆ˜ë¥¼ í™•ì¸ ecom_df.duplicated().sum() . 5268 . â†’ ì „ì²´ ê°’ì´ ë‹¤ ì¤‘ë³µëœ ê²½ìš°, ê¸°ë¡ ê³¼ì •ì—ì„œ ì¤‘ë³µ ê¸°ì…ëœ ê²ƒì´ë¼ê³  íŒë‹¨. â†’ ê°€ì¥ ìœ„ì— ìˆëŠ” í–‰ë§Œ ë‚¨ê¸°ê³  drop . print(len(ecom_df)) ecom_df.drop_duplicates(inplace=True) ecom_df.reset_index(drop=True, inplace=True) print(len(ecom_df)) . 541909 536641 . ê²°ì¸¡ì¹˜ í™•ì¸ . | Description ì¹¼ëŸ¼ . ecom_df['Description'].isna().sum() . 1454 . â†’ StockCodeì™€ Descriptionì´ 1:1 ëŒ€ì‘ë˜ëŠ” ê°œë…ì¸ì§€ í™•ì¸ (ê°™ì€ StockCodeì˜ Descriptionìœ¼ë¡œ nullê°’ì„ ì±„ì›Œë„£ì„ ìˆ˜ ìˆì„ì§€ ì ê²€) . temp1 = ecom_df.groupby('StockCode')[['Description']].nunique() print(len(temp1.query('Description &gt; 1'))) print(len(temp1.query('Description == 1'))) print(len(temp1.query('Description &lt; 1'))) . 650 3308 112 . | StockCode 1ê°€ì§€ì—ë„ Descriptionì´ 2ê°œ ì´ìƒ ë¶™ê¸°ë„ í•´ì„œ, ë‹¨ìˆœí•˜ê²Œ StockCodeë¥¼ ë°”íƒ•ìœ¼ë¡œ Descriptionì„ ì±„ì›Œë„£ì„ ìˆ˜ëŠ” ì—†ì„ ë“¯. | ìš°ì„  Descriptionì˜ nullê°’ì„ ê·¸ëŒ€ë¡œ ë‘ . | . | CustemerID ì¹¼ëŸ¼ . | . sum(ecom_df['CustomerID'] == 'N/A') . 135037 . # ê°™ì€ InvoiceNoë¼ë©´ í•œ ëª…ì´ í•œë²ˆì— êµ¬ë§¤í•œ ê²ƒì´ë¯€ë¡œ, InvoiceNoì˜ unique ê°’ìœ¼ë¡œë„ í™•ì¸í•´ë´„ ecom_df.query('CustomerID == \"N/A\"')['InvoiceNo'].nunique() . 3710 . | ê°œë³„ transactionìœ¼ë¡œ ë”°ì§€ë©´, 3710ê°œì˜ êµ¬ë§¤ê°€ êµ¬ë§¤ìê°€ ë‚¨ê²¨ì ¸ ìˆì§€ ì•ŠìŒ | ìš°ì„ ì€ ì§€ìš°ì§€ ì•Šê³  ë‘ . | . ì´ìƒì¹˜ / ë¶ˆí•„ìš”í•œ ê°’ ì •ë¦¬ . ecom_df.describe() . | Â  | Quantity | UnitPrice | . | count | 536641 | 536641 | . | mean | 9.62003 | 4.63266 | . | std | 219.13 | 97.2331 | . | min | -80995 | -11062.1 | . | 25% | 1 | 1.25 | . | 50% | 3 | 2.08 | . | 75% | 10 | 4.13 | . | max | 80995 | 38970 | . | Quanityë‚˜ UnitPriceì— -ê°’ì´ ì„ì–´ ìˆìŒ | . | UnitPriceê°€ 0ì¸ ê²½ìš° ì²´í¬ . sum(ecom_df['UnitPrice'] == 0) . 2510 . | UnitPrice = 0ì¸ ë°ì´í„°ëŠ” ë§¤ì¶œê³¼ ê´€ê³„ê°€ ì—†ìœ¼ë¯€ë¡œ ì‚­ì œí•´ì¤€ë‹¤ | ì•„ë§ˆ ì¬ê³  ê´€ë¦¬ë¥¼ ìœ„í•´ Price=0ì¸ ì¹¼ëŸ¼ì„ ë§ë¶™ì—¬ì„œ Quantityë¥¼ ì¡°ì •í•´ì¤€ ë“¯.. | . print(len(ecom_df)) drop_index = ecom_df[ecom_df['UnitPrice'] == 0].index ecom_df.drop(drop_index, axis='index', inplace=True) print(len(ecom_df)) # reset index ecom_df.reset_index(drop=True, inplace=True) . 536641 534131 . | UnitPriceê°€ -ì¸ ê²½ìš° ì²´í¬ . ecom_df.query('UnitPrice &lt; 0') ## Stock Codeê°€ 'B'ì„ . | Â  | InvoiceNo | StockCode | Description | Quantity | InvoiceDate | UnitPrice | CustomerID | Country | . | 297646 | A563186 | B | Adjust bad debt | 1 | 2011-08-12 14:51:00 | -11062.1 | N/A | United Kingdom | . | 297647 | A563187 | B | Adjust bad debt | 1 | 2011-08-12 14:52:00 | -11062.1 | N/A | United Kingdom | . | StockCode ì ê²€ . # Stock Codeì— ìƒí’ˆì½”ë“œì™€ ì¡°ê¸ˆ ë‹¤ë¥¸ ê°’ë“¤ì´ ìˆìŒ print(sorted(ecom_df['StockCode'].unique())[-30:]) . ['90214T', '90214U', '90214V', '90214W', '90214Y', '90214Z', 'AMAZONFEE', 'B', 'BANK CHARGES', 'C2', 'CRUK', 'D', 'DCGS0003', 'DCGS0004', 'DCGS0069', 'DCGS0070', 'DCGS0076', 'DCGSSBOY', 'DCGSSGIRL', 'DOT', 'M', 'PADS', 'POST', 'S', 'gift_0001_10', 'gift_0001_20', 'gift_0001_30', 'gift_0001_40', 'gift_0001_50', 'm'] . | â€˜gift_ìˆ«ìâ€™ í˜•íƒœëŠ” gift voucher ìƒí’ˆ | . â€» StockCode ì¤‘ ì œí’ˆì´ ì•„ë‹Œ ê²ƒ ì •ë¦¬: . | StockCode | Desciption | ì²˜ë¦¬ ë°©í–¥ì„± | . | S | SAMPLES | ë¬´ë£Œë¡œ ìƒ˜í”Œì„ ì£¼ëŠ” ë°ì— ì“´ ëˆì´ë¼ê³  ì¶”ì • â†’ ë§¤ì¶œê³¼ ê´€ë ¨ì´ ì ìœ¼ë¯€ë¡œ ì¼ë‹¨ drop | . | POST | POSTAGE | ìš°í¸ ìš”ê¸ˆìœ¼ë¡œ ì¶”ì •â€¦ Countryê°€ ì£¼ë¡œ í•´ì™¸ì¸ ê²ƒì„ ë³´ë©´ ì¼ì¢…ì˜ ìƒí’ˆì´ë¼ê³  ìƒê°ë˜ê¸°ë„ í•¨ â†’ keep | . | PADS | PADS TO MATCH ALL CUSHIONS | Price=0.001 â†’ í•¨ê»˜ íŒë§¤í•˜ëŠ” ë¶€ì†í’ˆìœ¼ë¡œ ì¶”ì • â†’ keep | . | M | Manual | ì œí’ˆëª…ì´ ì •í™•í•˜ê²Œ ì°íˆì§€ ì•Šì€ êµ¬ë§¤/í™˜ë¶ˆê±´ì— ëŒ€í•´ Manualì´ë¼ê³  ì ì–´ë†“ì€ ê²ƒìœ¼ë¡œ ì¶”ì • â†’ keep | . | DOT | DOTCOM POSTAGE | POSTì™€ ë§ˆì°¬ê°€ì§€ë¡œ, ì¼ì¢…ì˜ ìƒí’ˆì´ë¼ê³  ê°„ì£¼ â†’ keep | . | D | Discount | êµ¬ë§¤í•  ë•Œ Discountí•´ì¤€ ê¸ˆì•¡ â†’ keep | . | CRUK | CRUK Commission | íŠ¹ì • ë‹¨ì²´(CRUK)ì— ë‚´ëŠ” Commisionì´ë¼ê³  ì¶”ì • â†’ ë§¤ì¶œê³¼ ê´€ë ¨ì´ ì ìœ¼ë¯€ë¡œ ì¼ë‹¨ drop | . | C2 | CARRIAGE | ìš´ì†¡ë¹„ â†’ keep | . | BANK CHARGES | Bank Charges | ë§¤ì¶œê³¼ ê´€ë ¨ì´ ì ìœ¼ë¯€ë¡œ ì¼ë‹¨ drop | . | B | Adjust bad debt | ë§¤ì¶œê³¼ ê´€ë ¨ì´ ì ìœ¼ë¯€ë¡œ ì¼ë‹¨ drop | . | AMAZONFEE | AMAZON FEE | ì•„ë§ˆì¡´ì— ë‚´ëŠ” ìˆ˜ìˆ˜ë£Œ â†’ ë§¤ì¶œê³¼ ê´€ë ¨ì´ ì ìœ¼ë¯€ë¡œ ì¼ë‹¨ drop | . â†’ ì œí’ˆ íŒë§¤ë¡œ ì¸í•œ ë§¤ì¶œê³¼ ê´€ê³„ ì—†ëŠ” Stock CodeëŠ” drop . # CRUK, BANK CHARGES, B, AMAZONFEE, S ì‚­ì œ print(len(ecom_df)) drop_index = ecom_df.query(\"StockCode in ['CRUK', 'BANK CHARGES', 'B', 'S', 'AMAZONFEE']\").index ecom_df.drop(drop_index, axis='index', inplace=True) print(len(ecom_df)) # reset index ecom_df.reset_index(drop=True, inplace=True) . 534131 533979 . | Quantityê°€ -ì¸ ê²½ìš° ì²´í¬ . ecom_df.query('Quantity &lt; 0').head(3) . | Â  | InvoiceNo | StockCode | Description | Quantity | InvoiceDate | UnitPrice | CustomerID | Country | . | 141 | C536379 | D | Discount | -1 | 2010-12-01 09:41:00 | 27.5 | 14527 | United Kingdom | . | 154 | C536383 | 35004C | SET OF 3 COLOURED FLYING DUCKS | -1 | 2010-12-01 09:49:00 | 4.65 | 15311 | United Kingdom | . | 235 | C536391 | 22556 | PLASTERS IN TIN CIRCUS PARADE | -12 | 2010-12-01 10:24:00 | 1.65 | 17548 | United Kingdom | . | Quantityê°€ -ì¸ ê²½ìš°ëŠ” refund, discount, í˜¹ì€ ê¸°íƒ€ ì§€ì¶œì„ ì˜ë¯¸í•˜ëŠ” ë“¯ | refundë‚˜ discount ê¸ˆì•¡ì€ ë§¤ì¶œê³¼ ê´€ë ¨ì´ ìˆìœ¼ë¯€ë¡œ Quantityê°€ -ì¸ í–‰ì€ ë‹¤ ë‚¨ê²¨ë‘ . | cf) Quantityê°€ 0ì¸ ê²½ìš°ëŠ” ì—†ìŒ | . | . ê²°ì¸¡ì¹˜ ì¬í™•ì¸ . | Description ì¹¼ëŸ¼ . ecom_df['Description'].isna().sum() . 0 . | ì •ë¦¬í•˜ê³  ë‚˜ë‹ˆ Descriptionì´ nanì¸ í–‰ì€ ëª¨ë‘ ì‚¬ë¼ì§ | . | CustemerID ì¹¼ëŸ¼ . # Invoice ê¸°ì¤€ìœ¼ë¡œ ë¬¶ì–´ì£¼ë©´ 1531ëª…ì˜ Customerê°€ null ecom_df.query('CustomerID == \"N/A\"')['InvoiceNo'].nunique() . 1531 . | nanì¸ ê°’ì€ â€˜ë“±ë¡ë˜ì§€ ì•Šì€ êµ¬ë§¤ìâ€™ ê°œë…ì¸ ë“¯. (ë¹„íšŒì› êµ¬ë§¤ ê°œë…) | ì‹ ê·œ ì´ìš©ì / ì´íƒˆì ë“±ì„ ë¶„ì„í•  ë•ŒëŠ” ì œì™¸í•˜ëŠ” ê²ƒì´ ë§ì„ ê²ƒ ê°™ì§€ë§Œ, ì¸ê¸° ìƒí’ˆ / ê¸°ê°„ë³„ ë§¤ì¶œ ë“±ì„ íŒŒì•…í•  ë•Œì—ëŠ” ë¯¸ë“±ë¡ ì´ìš©ìì˜ êµ¬ë§¤ë„ ì¤‘ìš”í•˜ë¯€ë¡œ ì¼ë‹¨ ë‚¨ê²¨ë‘ . | . | . ìƒˆë¡œìš´ ì¹¼ëŸ¼ ìƒì„± . | InvoiceMonth, InvoiceWeekday ì¹¼ëŸ¼ ìƒì„± . ecom_df['InvoiceMonth'] = ecom_df['InvoiceDate'].dt.strftime('%Y%m') # YYYYmm í˜•ì‹ìœ¼ë¡œ ì •ë¦¬ ecom_df['InvoiceWeekday'] = ecom_df['InvoiceDate'].dt.weekday # ì›”: 0 ~ ì¼: 6 ecom_df[['InvoiceDate', 'InvoiceMonth', 'InvoiceWeekday']].head() . | Â  | InvoiceDate | InvoiceMonth | InvoiceWeekday | . | 0 | 2010-12-01 08:26:00 | 201012 | 2 | . | 1 | 2010-12-01 08:26:00 | 201012 | 2 | . | 2 | 2010-12-01 08:26:00 | 201012 | 2 | . | 3 | 2010-12-01 08:26:00 | 201012 | 2 | . | 4 | 2010-12-01 08:26:00 | 201012 | 2 | . | TotalSpending ì¹¼ëŸ¼ ìƒì„±: Quantity * UnitPrice . ecom_df['TotalSpending'] = ecom_df['Quantity'] * ecom_df['UnitPrice'] ecom_df[['Quantity', 'UnitPrice', 'TotalSpending']].head() . | Â  | Quantity | UnitPrice | TotalSpending | . | 0 | 6 | 2.55 | 15.3 | . | 1 | 6 | 3.39 | 20.34 | . | 2 | 8 | 2.75 | 22 | . | 3 | 6 | 3.39 | 20.34 | . | 4 | 6 | 3.39 | 20.34 | . | . ",
    "url": "https://chaelist.github.io/docs/kaggle/uk_ecommerce/#%EB%8D%B0%EC%9D%B4%ED%84%B0-%ED%8C%8C%EC%95%85-%EB%B0%8F-%EC%A0%84%EC%B2%98%EB%A6%AC",
    "relUrl": "/docs/kaggle/uk_ecommerce/#ë°ì´í„°-íŒŒì•…-ë°-ì „ì²˜ë¦¬"
  },"255": {
    "doc": "UK Ecommerce Data 1",
    "title": "ì›”ë³„ ë§¤ì¶œì•¡, êµ¬ë§¤ììˆ˜",
    "content": ". | 201112ëŠ” 9ì¼ë°–ì— ì—†ìœ¼ë¯€ë¡œ ì œì™¸í•˜ê³  ê·¸ë ¤ì¤Œ | . | ì›”ë³„ ì´ ë§¤ì¶œì•¡ fig = px.bar(ecom_df.query('InvoiceMonth != \"201112\"').groupby('InvoiceMonth')[['TotalSpending']].sum().reset_index(), x='InvoiceMonth', y='TotalSpending', color='TotalSpending', color_continuous_scale = 'Teal') fig.update(layout_coloraxis_showscale=False) fig.show() . | ì›”ë³„ ìˆœêµ¬ë§¤ììˆ˜ ecom_no_na = ecom_df.query('CustomerID != \"N/A\"') # CustomerIDê°€ N/Aì¸ ê±¸ ì œì™¸í•œ dfë¥¼ ë”°ë¡œ ì €ì¥í•´ë‘  fig = px.bar(ecom_no_na.query('InvoiceMonth != \"201112\"').groupby('InvoiceMonth')[['CustomerID']].nunique().reset_index(), x='InvoiceMonth', y='CustomerID', color='CustomerID', labels = {'CustomerID':'# of Customers'}, color_continuous_scale = 'Teal') fig.update(layout_coloraxis_showscale=False) fig.show() . | ì›”ë³„ ë§¤ì¶œì•¡, êµ¬ë§¤ììˆ˜ ëª¨ë‘ 2011.09 ~ 2011.11 ê¸°ê°„ì— ê¸‰ì¦í•˜ëŠ” ì¶”ì„¸ | í˜„ì¬ë¡œì„œëŠ” ì „ë§ì´ ê¸ì •ì ìœ¼ë¡œ íŒë‹¨ë˜ì§€ë§Œ, ê²¨ìš¸ seasonalityì˜ ì˜í–¥ì¸ì§€, ì‹¤ì ì´ ì „ë°˜ì ìœ¼ë¡œ ì¢‹ì•„ì§€ëŠ” ì¶”ì„¸ì¸ ê²ƒì¸ì§€ êµ¬ë¶„í•˜ë ¤ë©´ ì¡°ê¸ˆ ë” ì¥ê¸°ê°„ì˜ ë°ì´í„°ê°€ í•„ìš”í•  ë“¯ | . | ì›”ë³„ ê³ ê° 1ì¸ë‹¹ ì§€ì¶œ . # CustomerID = N/Aì¸ ê°’ì€ ì œì™¸í•˜ê³  ê³„ì‚° cus_spend = ecom_no_na.query('InvoiceMonth != \"201112\"').groupby('InvoiceMonth') cus_spend = cus_spend.agg({'CustomerID':pd.Series.nunique, 'TotalSpending':'sum'}) cus_spend['SpendingPerCustomer'] = cus_spend['TotalSpending'] / cus_spend['CustomerID'] fig = px.bar(cus_spend.reset_index(), x='InvoiceMonth', y='SpendingPerCustomer', color='SpendingPerCustomer', labels = {'SpendingPerCustomer':'Spending per Customer'}, color_continuous_scale = 'Teal') fig.update(layout_coloraxis_showscale=False) fig.show() . | 1ì¸ë‹¹ ì§€ì¶œ ê¸ˆì•¡ì´ í¬ê²Œ ì¦ê°€í•˜ëŠ” ì¶”ì„¸ëŠ” ì•„ë‹˜. | 2011.09 ~ 2011.11 ê¸°ê°„ì— ë¹„êµì  1ì¸ë‹¹ ì§€ì¶œì´ ë†’ê²Œ ë‚˜íƒ€ë‚˜ì§€ë§Œ, ê²¨ìš¸ seasonalityì˜ ì˜í–¥ì¸ì§€, ì‹¤ì œë¡œ 1ì¸ë‹¹ ì§€ì¶œì´ ì¦ê°€í•˜ëŠ” ì¶”ì„¸ì¸ ê²ƒì¸ì§€ êµ¬ë¶„í•˜ë ¤ë©´ ì¡°ê¸ˆ ë” ì¥ê¸°ê°„ì˜ ë°ì´í„°ê°€ í•„ìš”í•  ë“¯ | . | . ",
    "url": "https://chaelist.github.io/docs/kaggle/uk_ecommerce/#%EC%9B%94%EB%B3%84-%EB%A7%A4%EC%B6%9C%EC%95%A1-%EA%B5%AC%EB%A7%A4%EC%9E%90%EC%88%98",
    "relUrl": "/docs/kaggle/uk_ecommerce/#ì›”ë³„-ë§¤ì¶œì•¡-êµ¬ë§¤ììˆ˜"
  },"256": {
    "doc": "UK Ecommerce Data 1",
    "title": "ì½”í˜¸íŠ¸ ë¶„ì„",
    "content": ". | 201112ëŠ” 9ì¼ë°–ì— ì—†ìœ¼ë¯€ë¡œ ì œì™¸í•˜ê³  ê³„ì‚° | ì²« êµ¬ë§¤ì›”ì„ ê¸°ì¤€ìœ¼ë¡œ cohort ë¶„ë¥˜ | . ecom_cohort_df = ecom_no_na.query('InvoiceMonth != \"201112\"') min_month =ecom_cohort_df.groupby('CustomerID')[['InvoiceMonth']].min() min_month.rename(columns={'InvoiceMonth': 'CohortGroup'}, inplace=True) ecom_cohort_df = pd.merge(ecom_cohort_df, min_month, on='CustomerID', how='left') ecom_cohort_df.head(3) . | Â  | InvoiceNo | StockCode | Description | Quantity | InvoiceDate | UnitPrice | CustomerID | Country | InvoiceMonth | InvoiceWeekday | TotalSpending | CohortGroup | . | 0 | 536365 | 85123A | WHITE HANGING HEART T-LIGHT HOLDER | 6 | 2010-12-01 08:26:00 | 2.55 | 17850 | United Kingdom | 201012 | 2 | 15.3 | 201012 | . | 1 | 536365 | 71053 | WHITE METAL LANTERN | 6 | 2010-12-01 08:26:00 | 3.39 | 17850 | United Kingdom | 201012 | 2 | 20.34 | 201012 | . | 2 | 536365 | 84406B | CREAM CUPID HEARTS COAT HANGER | 8 | 2010-12-01 08:26:00 | 2.75 | 17850 | United Kingdom | 201012 | 2 | 22 | 201012 | . â†’ CohortGroupì„ ê¸°ì¤€ìœ¼ë¡œ ì •ë¦¬ . ecom_cohort_df = ecom_cohort_df.groupby(['CohortGroup', 'InvoiceMonth']) ecom_cohort_df = ecom_cohort_df.agg({'CustomerID':pd.Series.nunique, 'TotalSpending':['sum', 'mean']}) ecom_cohort_df.columns = ['Customers', 'TotalSpending', 'AverageSpending'] # ì¹¼ëŸ¼ëª… ì •ë¦¬ ecom_cohort_df.head() . | CohortGroup | InvoiceMonth | Customers | TotalSpending | AverageSpending | . | 201012 | 201012 | 947 | 552358 | 20.9623 | . | ^^ | 201101 | 361 | 271937 | 25.2237 | . | ^^ | 201102 | 317 | 230416 | 25.3427 | . | ^^ | 201103 | 367 | 301779 | 25.2345 | . | ^^ | 201104 | 341 | 200541 | 20.0741 | . â†’ ì²«êµ¬ë§¤ì›”ì„ ê¸°ì¤€ìœ¼ë¡œ ì–¼ë§ˆë‚˜ ì§€ë‚¬ëŠ”ì§€ í‘œê¸° . def cohort_period(df): df['CohortPeriod'] = np.arange(len(df)) + 1 return df ecom_cohort_df = ecom_cohort_df.groupby(level=0).apply(cohort_period).reset_index() ecom_cohort_df.head() . | Â  | CohortGroup | InvoiceMonth | Customers | TotalSpending | AverageSpending | CohortPeriod | . | 0 | 201012 | 201012 | 947 | 552358 | 20.9623 | 1 | . | 1 | 201012 | 201101 | 361 | 271937 | 25.2237 | 2 | . | 2 | 201012 | 201102 | 317 | 230416 | 25.3427 | 3 | . | 3 | 201012 | 201103 | 367 | 301779 | 25.2345 | 4 | . | 4 | 201012 | 201104 | 341 | 200541 | 20.0741 | 5 | . ì½”í˜¸íŠ¸ë³„ ì¬ë°©ë¬¸ë¥  . cohorts1 = pd.pivot_table(ecom_cohort_df, index='CohortGroup', columns='CohortPeriod', values='Customers', aggfunc='sum') cohort_retention = cohorts1.divide(cohorts1[1], axis=0) cohort_retention.loc['MEAN'] = cohort_retention.mean(axis=0) cohort_retention = cohort_retention.reindex(['MEAN', '201012', '201101', '201102', '201103', '201104', '201105', '201106', '201107', '201108', '201109', '201110', '201111']) # heatmapìœ¼ë¡œ ì‹œê°í™” plt.figure(figsize=(15,7)) sns.heatmap(cohort_retention, annot=True, cmap='GnBu', fmt='.0%') plt.yticks(rotation=0); . | 2010.12ì— ì²«êµ¬ë§¤í•œ cohortê°€ ê°€ì¥ ì´í›„ ì¬ë°©ë¬¸ë¥ ì´ ë†“ì€ í¸. (2010.12 ì´ì „ë¶€í„° ì§€ì†ì ìœ¼ë¡œ êµ¬ë§¤í•´ì˜¤ë˜ ì´ìš©ì ì—­ì‹œ ì´ ê·¸ë£¹ìœ¼ë¡œ ê³„ì‚°ë˜ì—ˆê¸° ë•Œë¬¸ì¼ ìˆ˜ë„ ìˆìŒ) | 2010.12ì— ì²«êµ¬ë§¤í•œ cohortì˜ 50%ê°€ 2011.11ì— ë‹¤ì‹œ ë°©ë¬¸ â†’ ê²¨ìš¸ seasonalityê°€ ìˆëŠ” ìƒí’ˆì— ëŒ€í•œ ìˆ˜ìš”ê°€ ìˆëŠ” í¸ì´ë¼ê³  ì¶”ì • | 2011.09, 2011.10ì— ì²«êµ¬ë§¤í•œ cohortì˜ ê²½ìš° ì´ì „ cohortì— ë¹„í•´ ë‹¤ìŒë‹¬ ì¬ë°©ë¬¸ë¥ ì´ ë‹¤ì†Œ ë†’ì€ í¸ì´ì§€ë§Œ, â€˜ìµœê·¼ì— ìœ ì…ëœ ê³ ê°ì¼ìˆ˜ë¡ ì¬ë°©ë¬¸ë¥ ì´ ë†’ë‹¤â€™ê³  ë§í•˜ê¸°ëŠ” ì–´ë ¤ì›€ | . ì½”í˜¸íŠ¸ë³„ ì´ë§¤ì¶œì•¡ . cohort_spending = pd.pivot_table(ecom_cohort_df, index='CohortGroup', columns='CohortPeriod', values='TotalSpending', aggfunc='sum') cohort_spending.loc['MEAN'] = cohort_spending.mean(axis=0) cohort_spending = cohort_spending.reindex(['MEAN', '201012', '201101', '201102', '201103', '201104', '201105', '201106', '201107', '201108', '201109', '201110', '201111']) # heatmapìœ¼ë¡œ ì‹œê°í™” plt.figure(figsize=(15,7)) sns.heatmap(cohort_spending, annot=True, cmap='GnBu', fmt='.0f') plt.yticks(rotation=0); . | 2010.12ì— ì²«êµ¬ë§¤í•œ cohortê°€ ì´ë§¤ì¶œì— ì§€ì†ì ìœ¼ë¡œ ê°€ì¥ í° ê¸°ì—¬ë¥¼ í•¨ | íŠ¹íˆ, 2010.12ì— ì²«êµ¬ë§¤í•œ cohortì˜ ê²½ìš°, 2011.09 ~ 2010.11 ê¸°ê°„ì—ë„ ê±°ì˜ 2010.12ì— ë²„ê¸ˆê°€ëŠ” í° ê¸ˆì•¡ì„ ì§€ì¶œ | . ì½”í˜¸íŠ¸ë³„ í‰ê·  ì§€ì¶œì•¡ . cohort_avgspend = pd.pivot_table(ecom_cohort_df, index='CohortGroup', columns='CohortPeriod', values='AverageSpending', aggfunc='sum') cohort_avgspend.loc['MEAN'] = cohort_avgspend.mean(axis=0) cohort_avgspend = cohort_avgspend.reindex(['MEAN', '201012', '201101', '201102', '201103', '201104', '201105', '201106', '201107', '201108', '201109', '201110', '201111']) # heatmapìœ¼ë¡œ ì‹œê°í™” plt.figure(figsize=(15,7)) sns.heatmap(cohort_avgspend, annot=True, cmap='GnBu', fmt='.2f') plt.yticks(rotation=0); . | 2010.12ì— ì²«êµ¬ë§¤í•œ cohortê°€ ì§€ì†ì ìœ¼ë¡œ í‰ê·  ì§€ì¶œì•¡ì´ ë†’ì€ í¸. ì²«êµ¬ë§¤ì›” ì´í›„ ì˜¤íˆë ¤ í‰ê·  ì§€ì¶œì•¡ì´ ì¦ê°€í•˜ëŠ” ëª¨ìŠµì„ ë³´ì„ | ì˜¤íˆë ¤ ìµœê·¼ì— ìœ ì…ëœ cohortë“¤ì˜ ê²½ìš°, 2ë²ˆì§¸ ë‹¬ ì§€ì¶œì•¡ì´ ì´ì „ cohortëŒ€ë¹„ ë‚®ì€ í¸ìœ¼ë¡œ ë‚˜íƒ€ë‚¨ â†’ ê¸ì •ì ì´ì§€ ì•Šì€ ì‹ í˜¸ë¼ê³  ìƒê°ë¨ (ì„œë¹„ìŠ¤ê°€ ê°œì„ ë˜ê³  ìˆë‹¤ë©´, ì ì  ì²«ë‹¬ ì´í›„ ì§€ì¶œì•¡ ê°ì†Œë¶„ì´ ì¤„ì–´ë“œëŠ” ê²ƒì´ ì´ìƒì ) | . ",
    "url": "https://chaelist.github.io/docs/kaggle/uk_ecommerce/#%EC%BD%94%ED%98%B8%ED%8A%B8-%EB%B6%84%EC%84%9D",
    "relUrl": "/docs/kaggle/uk_ecommerce/#ì½”í˜¸íŠ¸-ë¶„ì„"
  },"257": {
    "doc": "UK Ecommerce Data 1",
    "title": "ê³ ê°ë³„ ì´íƒˆ ê°€ëŠ¥ì„± íŒŒì•…",
    "content": ". | ê³ ê°ë³„ êµ¬ë§¤ ë‚´ì—­ì„ ì›”ë³„ë¡œ ì •ë¦¬ . ecom_purchase = ecom_no_na.query('TotalSpending &gt;= 0') # TotalSpendingì´ ì–‘ìˆ˜ì¸ ê°’ë§Œ ë°˜ì˜. (-ëŠ” ë°˜í’ˆí•œ ë‚´ì—­ìœ¼ë¡œ ì¶”ì •) temp = ecom_purchase.query('InvoiceMonth != \"201112\"') # 201112ëŠ” ì œì™¸í•˜ê³  ê³„ì‚° customer_usage = pd.pivot_table(temp, index='CustomerID', columns='InvoiceMonth', values='InvoiceNo', aggfunc='count') customer_usage = customer_usage.applymap(lambda x: 'O' if x &gt; 0 else 'X') customer_usage.head() . | CustomerID | 201012 | 201101 | 201102 | 201103 | 201104 | 201105 | 201106 | 201107 | 201108 | 201109 | 201110 | 201111 | . | 12346 | X | O | X | X | X | X | X | X | X | X | X | X | . | 12347 | O | O | X | X | O | X | O | X | O | X | O | X | . | 12348 | O | O | X | X | O | X | X | X | X | O | X | X | . | 12349 | X | X | X | X | X | X | X | X | X | X | X | O | . | 12350 | X | X | O | X | X | X | X | X | X | X | X | X | . | ê³ ê°ë³„ë¡œ, êµ¬ë§¤ì™€ êµ¬ë§¤ ì‚¬ì´ì— ê±¸ë¦¬ëŠ” í‰ê·  ê¸°ê°„ì„ ê³„ì‚° . | 2011.10 ~ 2011.11 ê¸°ê°„ì— í•œ ë²ˆë§Œ êµ¬ë§¤í•œ ê³ ê°ì€ â€˜New Customerâ€™ë¼ê³  ê¸°ì¬ (ì´íƒˆìë¼ê³  ë³´ê¸°ì—ëŠ” ìµœê·¼ì— ìƒˆë¡œ êµ¬ì…í•œ ê³ ê°ì´ë¯€ë¡œ) | 2010.12 ~ 2011.09 ê¸°ê°„ì— í•œ ë²ˆ êµ¬ë§¤í•œ ì´í›„ ë‹¤ì‹œ ëŒì•„ì˜¤ì§€ ì•Šì€ ê³ ê°ì€ â€˜Never Returnedâ€™ë¼ê³  ê¸°ì¬ | . for index in customer_usage.index: templist = customer_usage.loc[index].to_list() month_btw = [] for i in range(12): if templist[i] == 'O': for j in range(1, 12 - i): if templist[i + j] == 'O': month_btw.append(j) break if month_btw: month_btw_avg = sum(month_btw) / len(month_btw) else: if (customer_usage.loc[index, '201111'] == 'O') or (customer_usage.loc[index, '201110'] == 'O'): month_btw_avg = 'New Customer' else: month_btw_avg = 'Never Returned' customer_usage.loc[index, 'avg_month_btw_purchases'] = month_btw_avg customer_usage.head() . | CustomerID | 201012 | 201101 | 201102 | 201103 | 201104 | 201105 | 201106 | 201107 | 201108 | 201109 | 201110 | 201111 | avg_month_btw_purchases | . | 12346 | X | O | X | X | X | X | X | X | X | X | X | X | Never Returned | . | 12347 | O | O | X | X | O | X | O | X | O | X | O | X | 2.0 | . | 12348 | O | O | X | X | O | X | X | X | X | O | X | X | 3.0 | . | 12349 | X | X | X | X | X | X | X | X | X | X | X | O | New Customer | . | 12350 | X | X | O | X | X | X | X | X | X | X | X | X | Never Returned | . â†’ â€˜Never Returnedâ€™ì¸ ê³ ê°ì˜ ìˆ˜ íŒŒì•…: . len(customer_usage.query('avg_month_btw_purchases == \"Never Returned\"')) . 1097 . | 2010.12 ~ 2011.09 ê¸°ê°„ì— í•œ ë²ˆ êµ¬ë§¤í•œ ì´í›„ ë‹¤ì‹œ ëŒì•„ì˜¤ì§€ ì•Šì€ ê³ ê°ì´ 1097ëª… | . +) ì§€ì†ì ìœ¼ë¡œ êµ¬ë§¤í•´ ì˜¨ ê³ ê°ì˜ í‰ê·  êµ¬ë§¤ ê°„ê²© íŒŒì•…: . temp = customer_usage.query('avg_month_btw_purchases not in [\"Never Returned\", \"New Customer\"]') temp['avg_month_btw_purchases'].mean() . 2.870106683984906 . | í‰ê· ì ìœ¼ë¡œ 2.9ê°œì›” ê°„ê²©ìœ¼ë¡œ êµ¬ë§¤ | . | ë§ˆì§€ë§‰ êµ¬ë§¤ ì´í›„ ê²½ê³¼í•œ ê¸°ê°„ ê³„ì‚° . # ê³ ê°ë³„ ë§ˆì§€ë§‰ êµ¬ë§¤ì›” íŒŒì•… last_month = ecom_purchase.query('InvoiceMonth != \"201112\"').groupby('CustomerID')[['InvoiceMonth']].max() last_month.rename(columns={'InvoiceMonth':'LastMonth'}, inplace=True) customer_usage = customer_usage.join(last_month) # ë§ˆì§€ë§‰ êµ¬ë§¤ ì´í›„ ê²½ê³¼í•œ ê¸°ê°„ ê³„ì‚°: í˜„ì¬ì‹œì (2010.12) - ë§ˆì§€ë§‰ êµ¬ë§¤ì›” for index in customer_usage.index: lastmonth = customer_usage.loc[index, 'LastMonth'] if lastmonth == '201012': lastmonth = 201100 customer_usage.loc[index, 'months_after_latest_purchase'] = 201112 - int(lastmonth) customer_usage.head() . | CustomerID | 201012 | 201101 | 201102 | 201103 | 201104 | 201105 | 201106 | 201107 | 201108 | 201109 | 201110 | 201111 | avg_month_btw_purchases | LastMonth | months_after_latest_purchase | . | 12346 | X | O | X | X | X | X | X | X | X | X | X | X | Never Returned | 201101 | 11 | . | 12347 | O | O | X | X | O | X | O | X | O | X | O | X | 2.0 | 201110 | 2 | . | 12348 | O | O | X | X | O | X | X | X | X | O | X | X | 3.0 | 201109 | 3 | . | 12349 | X | X | X | X | X | X | X | X | X | X | X | O | New Customer | 201111 | 1 | . | 12350 | X | X | O | X | X | X | X | X | X | X | X | X | Never Returned | 201102 | 10 | . +) â€˜Never Returnedâ€™ ê³ ê°ì˜ í‰ê·  êµ¬ë§¤ í›„ ê²½ê³¼ ê¸°ê°„: . customer_usage.query('avg_month_btw_purchases == \"Never Returned\"')['months_after_latest_purchase'].mean() . 7.353691886964448 . | í‰ê· ì ìœ¼ë¡œ ë§ˆì§€ë§‰ êµ¬ë§¤ ì´í›„ 7ê°œì›” ì´ìƒì´ ì§€ë‚¨ | . | êµ¬ë§¤ ì§€ì† ê³ ê°ì˜ â€˜risk ratioâ€™ ê³„ì‚° . | risk ratio(ìœ„í—˜ ë¹„ìœ¨): ë§ˆì§€ë§‰ êµ¬ë§¤ ì´í›„ ê²½ê³¼í•œ ê¸°ê°„ / í‰ê·  êµ¬ë§¤ ê°„ê²© | ê³ ê°ë³„ë¡œ í‰ê·  êµ¬ë§¤ ê°„ê²©ì´ ë‹¤ë¥¸ ê²½ìš°ê°€ ë§ê¸° ë•Œë¬¸ì—, â€˜risk ratioâ€™ë¥¼ ê³„ì‚°í•´ë³´ë©´ ê°ê°ì˜ ì„±ê²©ì— ë§ê²Œ ì´íƒˆ ì—¬ë¶€ë¥¼ íŒë³„í•  ìˆ˜ ìˆë‹¤ | ex) í‰ê·  4ë‹¬ ê°„ê²©ìœ¼ë¡œ êµ¬ë§¤í•˜ë˜ ê³ ê°ì´ 6ë‹¬ ë™ì•ˆ êµ¬ë§¤í•˜ì§€ ì•Šì€ ê²½ìš°ë³´ë‹¤ í‰ê·  2ë‹¬ ê°„ê²©ìœ¼ë¡œ êµ¬ë§¤í•˜ë˜ ê³ ê°ì´ 6ë‹¬ ë™ì•ˆ êµ¬ë§¤í•˜ì§€ ì•Šì€ ê²½ìš°ê°€ ë” ê°•í•œ ì´íƒˆ ì‹ í˜¸ë¼ê³  íŒë‹¨ | . likely_customer = customer_usage.query('avg_month_btw_purchases not in [\"Never Returned\", \"New Customer\"]') likely_customer['risk_ratio'] = likely_customer['months_after_latest_purchase'] / likely_customer['avg_month_btw_purchases'] likely_customer.head() . | CustomerID | 201012 | 201101 | 201102 | 201103 | 201104 | 201105 | 201106 | 201107 | 201108 | 201109 | 201110 | 201111 | avg_month_btw_purchases | LastMonth | months_after_latest_purchase | risk_ratio | . | 12347 | O | O | X | X | O | X | O | X | O | X | O | X | 2 | 201110 | 2 | 1 | . | 12348 | O | O | X | X | O | X | X | X | X | O | X | X | 3 | 201109 | 3 | 1 | . | 12352 | X | X | O | O | X | X | X | X | X | O | X | O | 3 | 201111 | 1 | 0.333333 | . | 12356 | X | O | X | X | O | X | X | X | X | X | X | O | 5 | 201111 | 1 | 0.2 | . | 12359 | X | O | O | X | X | X | O | X | X | X | O | X | 3 | 201110 | 2 | 0.666667 | . â†’ risk ratioê°€ 1.5ë³´ë‹¤ ë†’ì€ ì´ìš©ììˆ˜: . print(len(likely_customer.query('risk_ratio &gt; 1.5'))) # í‰ê·  êµ¬ë§¤ ê°„ê²©ì˜ 1.5ë°°ê°€ ë„˜ëŠ” ê¸°ê°„ ë™ì•ˆ êµ¬ë§¤í•˜ì§€ ì•Šê³  ìˆëŠ” ê³ ê°ì˜ ìˆ˜ . 515 . | . &gt; ê²°ë¡ : 2010.12 ~ 2011.11 ë‚´ì˜ ìˆœêµ¬ë§¤ì 4371ëª… ì¤‘ 1097ëª…ì€ ì•„ë§ˆ ì´íƒˆí•œ ê²ƒì´ í™•ì •ì , 515ëª…ì€ ì´íƒˆ ìœ„ê¸° . | risk ratioê°€ ì¼ì • ìˆ˜ì¹˜ê°€ ë„˜ëŠ” ê³ ê°ì˜ ê²½ìš° íŠ¹ë³„í•œ ê´€ë¦¬ë¥¼ í†µí•´ ì´íƒˆì„ ë°©ì§€í•˜ëŠ” ê²ƒì´ í•„ìš”í•˜ë‹¤ê³  ìƒê°ë¨ | . ",
    "url": "https://chaelist.github.io/docs/kaggle/uk_ecommerce/#%EA%B3%A0%EA%B0%9D%EB%B3%84-%EC%9D%B4%ED%83%88-%EA%B0%80%EB%8A%A5%EC%84%B1-%ED%8C%8C%EC%95%85",
    "relUrl": "/docs/kaggle/uk_ecommerce/#ê³ ê°ë³„-ì´íƒˆ-ê°€ëŠ¥ì„±-íŒŒì•…"
  },"258": {
    "doc": "UK Ecommerce Data 1",
    "title": "ì›”ë³„ ì´íƒˆë¥ ",
    "content": ": êµ¬ë§¤ ì§€ì† ê³ ê°ì˜ ê²½ìš° í‰ê·  êµ¬ë§¤ ê°„ê²©ì´ 2.9ê°œì›” ì •ë„ì˜ ê¸°ê°„ë¼ê³  ê³„ì‚°ë˜ì—ˆìœ¼ë¯€ë¡œ, êµ¬ë§¤ ì´í›„ 3ê°œì›” ê°„ ì´ìš©ì´ ì—†ëŠ” ì´ìš©ìë¥¼ â€˜ì´íƒˆìâ€™ë¡œ ê°„ì£¼í•˜ê³  ì›”ë³„ ë³€í™”ë¥¼ ë¶„ì„ . | customer_usageì˜ ì›”ë³„ O, X ë¶€ë¶„ì„ ë³µì‚¬ . customer_churn = customer_usage.copy() customer_churn.drop(['avg_month_btw_purchases', 'LastMonth', 'months_after_latest_purchase'], axis='columns', inplace=True) # í•„ìš” ì—†ëŠ” ì¹¼ëŸ¼ì€ drop customer_churn.head() . | CustomerID | 201012 | 201101 | 201102 | 201103 | 201104 | 201105 | 201106 | 201107 | 201108 | 201109 | 201110 | 201111 | . | 12346 | X | O | X | X | X | X | X | X | X | X | X | X | . | 12347 | O | O | X | X | O | X | O | X | O | X | O | X | . | 12348 | O | O | X | X | O | X | X | X | X | O | X | X | . | 12349 | X | X | X | X | X | X | X | X | X | X | X | O | . | 12350 | X | X | O | X | X | X | X | X | X | X | X | X | . | ì›”ë³„ ì´ìš© ë‚´ì—­ì„ ë°”íƒ•ìœ¼ë¡œ, ì´íƒˆ ì—¬ë¶€ ì •ë¦¬ . | êµ¬ë§¤ê°€ ì¼ì–´ë‚œ ì›”ì„ ê¸°ì¤€ìœ¼ë¡œ, ê·¸ í›„ 3ê°œì›” ê°„ ë‹¤ì‹œ êµ¬ë§¤í•˜ì§€ ì•Šìœ¼ë©´ â€˜Churnâ€™ì´ë¼ê³  í‘œê¸° | 3ê°œì›” ì´ë‚´ì— ë‹¤ì‹œ êµ¬ë§¤í•œ ê²½ìš°, â€˜Returningâ€™ì´ë¼ê³  í‘œê¸° | êµ¬ë§¤ê°€ ì¼ì–´ë‚˜ì§€ ì•Šì€ ì›”ì€ â€˜N/Aâ€™ë¡œ í‘œê¸° | . columns = customer_churn.columns for index in customer_churn.index: usage_list = list(customer_churn.loc[index]) for i in range(len(columns) - 3): if usage_list[i] == 'X': customer_churn.loc[index, columns[i]] = 'N/A' else: if 'O' in usage_list[i+1 : i+4]: customer_churn.loc[index, columns[i]] = 'Returning' else: customer_churn.loc[index, columns[i]] = 'Churn' customer_churn.drop(['201109', '201110', '201111'], axis='columns', inplace=True) customer_churn.head() . | CustomerID | 201012 | 201101 | 201102 | 201103 | 201104 | 201105 | 201106 | 201107 | 201108 | . | 12346 | N/A | Churn | N/A | N/A | N/A | N/A | N/A | N/A | N/A | . | 12347 | Returning | Returning | N/A | N/A | Returning | N/A | Returning | N/A | Returning | . | 12348 | Returning | Returning | N/A | N/A | Churn | N/A | N/A | N/A | N/A | . | 12349 | N/A | N/A | N/A | N/A | N/A | N/A | N/A | N/A | N/A | . | 12350 | N/A | N/A | Churn | N/A | N/A | N/A | N/A | N/A | N/A | . | unstack &amp; rename columns . customer_churn = customer_churn.unstack().reset_index() customer_churn.rename(columns={'level_0':'InvoiceMonth', 0:'ChurnFlag'}, inplace=True) customer_churn.head() . | Â  | InvoiceMonth | CustomerID | ChurnFlag | . | 0 | 201012 | 12346 | N/A | . | 1 | 201012 | 12347 | Returning | . | 2 | 201012 | 12348 | Returning | . | 3 | 201012 | 12349 | N/A | . | 4 | 201012 | 12350 | N/A | . | ì›”ë³„ Churn / Returning ê³ ê° ìˆ˜ ë° ë¹„ìœ¨ì„ ê³„ì‚° . # InvoiceMonth, ChurnFlagë¥¼ ê¸°ì¤€ìœ¼ë¡œ groupby ('N/A'ëŠ” ë¬´ì‹œ) monthly_churn = customer_churn.query('ChurnFlag != \"N/A\"') monthly_churn = monthly_churn.groupby(['InvoiceMonth', 'ChurnFlag'])[['CustomerID']].count() monthly_churn.rename(columns={'CustomerID':'CustomerCount'}, inplace=True) # ì›”ë³„ë¡œ, Churn vs Returning ë¹„ìœ¨ì„ ê³„ì‚° monthly_churn['Percentage'] = monthly_churn.groupby(level=0)['CustomerCount'].transform(lambda x: x / x.sum()) monthly_churn.head(6) . | InvoiceMonth | ChurnFlag | CustomerCount | Percentage | . | 201012 | Churn | 357 | 0.403846 | . | ^^ | Returning | 527 | 0.596154 | . | 201101 | Churn | 263 | 0.354926 | . | ^^ | Returning | 478 | 0.645074 | . | 201102 | Churn | 254 | 0.335092 | . | ^^ | Returning | 504 | 0.664908 | . | . ì›”ë³„ ì´íƒˆë¥  ì¶”ì´ . | ì›”ë³„ ì¬êµ¬ë§¤ììˆ˜ &amp; ì´íƒˆììˆ˜ ì¶”ì´ . fig = px.bar(monthly_churn.reset_index(), x='InvoiceMonth', y='CustomerCount', color='ChurnFlag', barmode='group', category_orders = {'ChurnFlag':['Returning', 'Churn']}, hover_data={'Percentage': ':.0%'}, color_discrete_sequence = [px.colors.sequential.Teal[0], px.colors.sequential.Teal[2]]) fig.show() . | ì¬êµ¬ë§¤ì: 2010.12 ëŒ€ë¹„ 2011.08ì— ì•½ 33% ì¦ê°€ | ì´íƒˆì ìˆ˜ ìì²´ëŠ” í¬ê²Œ ê¸‰ê°í•˜ëŠ” ì¶”ì„¸ë¥¼ ë³´ì´ì§€ëŠ” ì•Šì§€ë§Œ, 2011.05 ì´í›„ì—ëŠ” ë¹„êµì  ê°ì†Œ ì¶”ì„¸ | í•˜ì§€ë§Œ êµ¬ë§¤ì ìˆ˜ê°€ ì¦ê°€í•˜ëŠ” ë™ì•ˆ ì´íƒˆì ìˆ˜ê°€ ì¦ê°€í•˜ì§€ ì•Šê³  ìœ ì§€ë˜ì—ˆë‹¤ëŠ” ê²ƒë§Œìœ¼ë¡œë„ ê¸ì •ì ì¸ ì„±ê³¼ë¼ê³  ìƒê°ë¨ | . | ì›”ë³„ ì¬êµ¬ë§¤ìœ¨ &amp; ì´íƒˆë¥  ì¶”ì´ . fig = px.bar(monthly_churn.reset_index(), x='InvoiceMonth', y='Percentage', color='ChurnFlag', category_orders = {'ChurnFlag':['Returning', 'Churn']}, color_discrete_sequence = [px.colors.sequential.Teal[0], px.colors.sequential.Teal[2]]) fig.update_yaxes(tickformat='.0%') fig.show() . | ì›”ë³„ ì „ì²´ êµ¬ë§¤ì ëŒ€ë¹„ ë¹„ìœ¨ë¡œ ë”°ì§€ë©´, â€˜ì´íƒˆë¥ â€™ì€ ê°ì†Œ ì¶”ì„¸ë¼ê³  í•  ìˆ˜ ìˆì„ ë“¯. | . | . ",
    "url": "https://chaelist.github.io/docs/kaggle/uk_ecommerce/#%EC%9B%94%EB%B3%84-%EC%9D%B4%ED%83%88%EB%A5%A0",
    "relUrl": "/docs/kaggle/uk_ecommerce/#ì›”ë³„-ì´íƒˆë¥ "
  },"259": {
    "doc": "UK Ecommerce Data 1",
    "title": "ì›”ë³„ ì‹ ê·œ ìœ ì…ë¥ ",
    "content": ": êµ¬ë§¤ ì§€ì† ê³ ê°ì˜ ê²½ìš° í‰ê·  êµ¬ë§¤ ê°„ê²©ì´ 2.9ê°œì›” ì •ë„ì˜ ê¸°ê°„ë¼ê³  ê³„ì‚°ë˜ì—ˆìœ¼ë¯€ë¡œ, êµ¬ë§¤ ì´ì „ì— 3ê°œì›” ê°„ ì´ìš©ì´ ì—†ì—ˆë˜ ì´ìš©ìë¥¼ â€˜ì‹ ê·œìœ ì…ìâ€™ë¡œ ê°„ì£¼í•˜ê³  ì›”ë³„ ë³€í™”ë¥¼ ë¶„ì„ . | ê³ ê°ë³„ ì›”ë³„ ì‹ ê·œ ìœ ì… ì—¬ë¶€ ì •ë¦¬ . | êµ¬ë§¤ê°€ ë°œìƒí•œ ì›”ì„ ê¸°ì¤€ìœ¼ë¡œ, ê·¸ ì „ 3ê°œì›” ê°„ ì´ìš©ì´ ì—†ì—ˆë‹¤ë©´ â€˜Newâ€™ë¼ê³  í‘œê¸° | 3ê°œì›” ì´ë‚´ì— êµ¬ë§¤í–ˆë˜ ì´ë ¥ì´ ìˆë‹¤ë©´ â€˜Returningâ€™ì´ë¼ê³  í‘œê¸° | êµ¬ë§¤í•˜ì§€ ì•Šì€ ì›”ì€ â€˜N/Aâ€™ë¡œ í‘œê¸° | . # customer_usageì˜ ì›”ë³„ O, X ë¶€ë¶„ì„ ë³µì‚¬ new_customer = customer_usage.copy() new_customer.drop(['avg_month_btw_purchases', 'LastMonth', 'months_after_latest_purchase'], axis='columns', inplace=True) # ì›”ë³„ ì´ìš© ë‚´ì—­ì„ ë°”íƒ•ìœ¼ë¡œ, ì‹ ê·œ ìœ ì… ì—¬ë¶€ ì •ë¦¬ columns = new_customer.columns for index in new_customer.index: usage_list = list(new_customer.loc[index]) for i in range(3, len(columns)): if usage_list[i] == 'X': new_customer.loc[index, columns[i]] = 'N/A' else: if 'O' in usage_list[i-3 : i]: new_customer.loc[index, columns[i]] = 'Returning' else: new_customer.loc[index, columns[i]] = 'New' new_customer.drop(['201012', '201101', '201102'], axis='columns', inplace=True) new_customer.head() . | CustomerID | 201103 | 201104 | 201105 | 201106 | 201107 | 201108 | 201109 | 201110 | 201111 | . | 12346 | N/A | N/A | N/A | N/A | N/A | N/A | N/A | N/A | N/A | . | 12347 | N/A | Returning | N/A | Returning | N/A | Returning | N/A | Returning | N/A | . | 12348 | N/A | Returning | N/A | N/A | N/A | N/A | New | N/A | N/A | . | 12349 | N/A | N/A | N/A | N/A | N/A | N/A | N/A | N/A | New | . | 12350 | N/A | N/A | N/A | N/A | N/A | N/A | N/A | N/A | N/A | . | ì›”ë³„ Churn / Returning ê³ ê° ìˆ˜ ë° ë¹„ìœ¨ì„ ê³„ì‚° . # unstack &amp; rename new_customer = new_customer.unstack().reset_index() new_customer.rename(columns={'level_0':'InvoiceMonth', 0:'NewFlag'}, inplace=True) # InvoiceMonth, NewFlagë¥¼ ê¸°ì¤€ìœ¼ë¡œ groupby ('N/A'ëŠ” ë¬´ì‹œ) monthly_new = new_customer.query('NewFlag != \"N/A\"') monthly_new = monthly_new.groupby(['InvoiceMonth', 'NewFlag'])[['CustomerID']].count() monthly_new.rename(columns={'CustomerID':'CustomerCount'}, inplace=True) # ì›”ë³„ë¡œ, New vs Returning ë¹„ìœ¨ì„ ê³„ì‚° monthly_new['Percentage'] = monthly_new.groupby(level=0)['CustomerCount'].transform(lambda x: x / x.sum()) monthly_new.head(6) . | InvoiceMonth | NewFlag | CustomerCount | Percentage | . | 201103 | New | 452 | 0.464066 | . | ^^ | Returning | 522 | 0.535934 | . | 201104 | New | 361 | 0.421729 | . | ^^ | Returning | 495 | 0.578271 | . | 201105 | New | 372 | 0.352607 | . | ^^ | Returning | 683 | 0.647393 | . | . ì›”ë³„ ì‹ ê·œìœ ì…ë¥  ì¶”ì´ . | ì›”ë³„ ì¬êµ¬ë§¤ììˆ˜ &amp; ì‹ ê·œìœ ì…ììˆ˜ ì¶”ì´ . fig = px.bar(monthly_new.reset_index(), x='InvoiceMonth', y='CustomerCount', color='NewFlag', barmode='group', category_orders = {'NewFlag':['Returning', 'New']}, hover_data={'Percentage': ':.0%'}, color_discrete_sequence = [px.colors.sequential.Teal[0], px.colors.sequential.Teal[2]]) fig.show() . | ì¬êµ¬ë§¤ì: 2011.03 ëŒ€ë¹„ 2011.11ì— ì•½ 92% ì¦ê°€ | 2011.09 ~ 2011.11 3ê°œì›” ê°„ ì‹ ê·œìœ ì…ìê°€ 500ëª… ìœ„ë¡œ ë†’ê²Œ ìœ ì§€ë¨ | . | ì›”ë³„ ì¬êµ¬ë§¤ìœ¨ &amp; ì‹ ê·œìœ ì…ë¥  ì¶”ì´ . fig = px.bar(monthly_new.reset_index(), x='InvoiceMonth', y='Percentage', color='NewFlag', category_orders = {'NewFlag':['Returning', 'New']}, color_discrete_sequence = [px.colors.sequential.Teal[0], px.colors.sequential.Teal[2]]) fig.update_yaxes(tickformat='.0%') fig.show() . | ì›”ë³„ ì „ì²´ êµ¬ë§¤ì ëŒ€ë¹„ ë¹„ìœ¨ë¡œ ë”°ì§€ë©´, ì‹ ê·œìœ ì…ë¥ ì€ ì—¬ë¦„(2011.06 ~ 2011.08)ì— ê°ì†Œ í›„ ê°€ì„ ì´í›„ë¡œ ë‹¤ì‹œ ì¦ê°€í•˜ëŠ” ê²½í–¥ì„ ë³´ì„ | 1ë…„ ì •ë„ì˜ ë°ì´í„°ë¥¼ ë” í™•ë³´í•˜ë©´ seasonalityë¥¼ ë” ëª…í™•íˆ íŒŒì•…í•˜ê³  ì „ëµì„ ì„¸ìš¸ ìˆ˜ ìˆì„ ê±°ë¼ ìƒê°ë¨ (ex. ì‹ ê·œ ìœ ì…ì´ ë§ì€ ê²¨ìš¸ ì‹œì¦Œì— ì§‘ì¤‘ í”„ë¡œëª¨ì…˜) | . | . ",
    "url": "https://chaelist.github.io/docs/kaggle/uk_ecommerce/#%EC%9B%94%EB%B3%84-%EC%8B%A0%EA%B7%9C-%EC%9C%A0%EC%9E%85%EB%A5%A0",
    "relUrl": "/docs/kaggle/uk_ecommerce/#ì›”ë³„-ì‹ ê·œ-ìœ ì…ë¥ "
  },"260": {
    "doc": "UK Ecommerce Data 1",
    "title": "UK Ecommerce Data 1",
    "content": " ",
    "url": "https://chaelist.github.io/docs/kaggle/uk_ecommerce/",
    "relUrl": "/docs/kaggle/uk_ecommerce/"
  },"261": {
    "doc": "UK Ecommerce Data 2",
    "title": "UK Ecommerce Data",
    "content": ". | ë°ì´í„° ì •ë¦¬ | 6ê°œì›” ì¬êµ¬ë§¤ìœ¨ | ê³ ê° íŠ¹ì„±ë³„ êµ¬ë§¤ë‹¹ ë°˜í’ˆ ë¹„ìœ¨ . | ì´íƒˆìì™€ ì¬êµ¬ë§¤ì ë¹„êµ | ì´íƒˆ ê³ ìœ„í—˜êµ°ê³¼ ì €ìœ„í—˜êµ° ë¹„êµ | . | ìƒí’ˆë³„ ë¶„ì„ . | ìƒí’ˆë³„ êµ¬ë§¤ ëŒ€ë¹„ ë°˜í’ˆ | ìƒí’ˆë³„ êµ¬ë§¤ ì¦ê°€í­ | . | êµ­ê°€ë³„ ë¶„ì„ . | êµ­ê°€ë³„ ë§¤ì¶œ | ìƒë°˜ê¸° ëŒ€ë¹„ í•˜ë°˜ê¸° ë¹„êµ | . | . *ë¶„ì„ ëŒ€ìƒ ë°ì´í„°ì…‹: UK E-Commerce Data . | ë°ì´í„°ì…‹ ì¶œì²˜ | UK-based retailerì˜ 2020-12-01ì—ì„œ 2011-12-09 ì‚¬ì´ì˜ ëª¨ë“  ê±°ë˜ë¥¼ ê¸°ë¡í•œ ë°ì´í„° | giftë¥¼ ì£¼ë¡œ íŒë§¤í•˜ë©°, ëŒ€ë‹¤ìˆ˜ì˜ ê³ ê°ì€ wholesalerì„ | . ",
    "url": "https://chaelist.github.io/docs/kaggle/uk_ecommerce2/#uk-ecommerce-data",
    "relUrl": "/docs/kaggle/uk_ecommerce2/#uk-ecommerce-data"
  },"262": {
    "doc": "UK Ecommerce Data 2",
    "title": "ë°ì´í„° ì •ë¦¬",
    "content": "# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ import import pandas as pd import numpy as np import scipy.stats as stats from matplotlib import pyplot as plt import seaborn as sns import plotly.express as px import plotly.io as pio pio.templates.default = \"plotly_white\" # default templateì„ ì§€ì • . ecom_df = pd.read_csv('data/uk_ecommerce_data.csv') ecom_df.head() . | Â  | InvoiceNo | StockCode | Description | Quantity | InvoiceDate | UnitPrice | CustomerID | Country | . | 0 | 536365 | 85123A | WHITE HANGING HEART T-LIGHT HOLDER | 6 | 12/1/2010 8:26 | 2.55 | 17850 | United Kingdom | . | 1 | 536365 | 71053 | WHITE METAL LANTERN | 6 | 12/1/2010 8:26 | 3.39 | 17850 | United Kingdom | . | 2 | 536365 | 84406B | CREAM CUPID HEARTS COAT HANGER | 8 | 12/1/2010 8:26 | 2.75 | 17850 | United Kingdom | . | 3 | 536365 | 84029G | KNITTED UNION FLAG HOT WATER BOTTLE | 6 | 12/1/2010 8:26 | 3.39 | 17850 | United Kingdom | . | 4 | 536365 | 84029E | RED WOOLLY HOTTIE WHITE HEART. | 6 | 12/1/2010 8:26 | 3.39 | 17850 | United Kingdom | . | ë°ì´í„° íƒ€ì… ì •ë¦¬ . # Customer IDë¥¼ string í˜•íƒœë¡œ ë°”ê¿”ì¤Œ ecom_df['CustomerID'].fillna(0, inplace=True) # ì¼ë‹¨ N/Aë¥¼ 0ìœ¼ë¡œ ë°”ê¿”ì¤Œ ecom_df['CustomerID'] = ecom_df['CustomerID'].astype('int').astype('str') # ì •ìˆ˜ë¡œ -&gt; strë¡œ ë°”ê¿”ì¤Œ ecom_df.replace({'CustomerID': '0'}, 'N/A', inplace=True) # 0ì€ 'N/A'ë¡œ ë°”ê¿”ì¤Œ # InvoiceDateë¥¼ datetime í˜•íƒœë¡œ ë°”ê¿”ì¤Œ ecom_df['InvoiceDate'] = pd.to_datetime(ecom_df['InvoiceDate']) . | ì¤‘ë³µê°’ ì œê±° . # ì „ì²´ ê°’ì´ ë‹¤ ì¤‘ë³µëœ ê²½ìš°, ê°€ì¥ ìœ„ì— ìˆëŠ” í–‰ë§Œ ë‚¨ê¸°ê³  drop ecom_df.drop_duplicates(inplace=True) ecom_df.reset_index(drop=True, inplace=True) . | ë°ì´í„° ì •ë¦¬ - í•„ìš” ì—†ëŠ” ë°ì´í„° ì‚­ì œ . # UnitPrice = 0ì¸ ë°ì´í„°ëŠ” ë§¤ì¶œê³¼ ê´€ê³„ê°€ ì—†ìœ¼ë¯€ë¡œ ì‚­ì œ drop_index = ecom_df[ecom_df['UnitPrice'] == 0].index ecom_df.drop(drop_index, axis='index', inplace=True) # CRUK, BANK CHARGES, B, AMAZONFEE, S ì‚­ì œ (ì œí’ˆ íŒë§¤ë¡œ ì¸í•œ ë§¤ì¶œê³¼ ê´€ê³„ ì—†ëŠ” Stock Code) drop_index = ecom_df.query(\"StockCode in ['CRUK', 'BANK CHARGES', 'B', 'S', 'AMAZONFEE']\").index ecom_df.drop(drop_index, axis='index', inplace=True) # reset index ecom_df.reset_index(drop=True, inplace=True) . | ë¶„ì„ì´ ìš©ì´í•˜ë„ë¡ ìƒˆë¡œìš´ ì¹¼ëŸ¼ ìƒì„± . # InvoiceMonth, InvoiceWeekday ì¹¼ëŸ¼ ìƒì„± ecom_df['InvoiceMonth'] = ecom_df['InvoiceDate'].dt.strftime('%Y%m') # YYYYmm í˜•ì‹ìœ¼ë¡œ ì •ë¦¬ ecom_df['InvoiceWeekday'] = ecom_df['InvoiceDate'].dt.weekday # ì›”: 0 ~ ì¼: 6 # TotalSpending ì¹¼ëŸ¼ ìƒì„±: Quantity * UnitPrice ecom_df['TotalSpending'] = ecom_df['Quantity'] * ecom_df['UnitPrice'] . | CustomerID == N/Aë¥¼ ì œì™¸í•œ dfë¥¼ ë”°ë¡œ ì €ì¥ . | nanì¸ ê°’ì€ â€˜ë“±ë¡ë˜ì§€ ì•Šì€ êµ¬ë§¤ìâ€™ ê°œë…ì¸ ë“¯. (ë¹„íšŒì› êµ¬ë§¤ ê°œë…) | ì‹ ê·œ ì´ìš©ì / ì´íƒˆì ë“±ì„ ë¶„ì„í•  ë•ŒëŠ” ì œì™¸í•˜ëŠ” ê²ƒì´ ë§ê² ì§€ë§Œ, ì¸ê¸° ìƒí’ˆ / ê¸°ê°„ë³„ ë§¤ì¶œ ë“±ì„ íŒŒì•…í•  ë•Œì—ëŠ” ìµëª… ì´ìš©ìì˜ êµ¬ë§¤ë„ ì¤‘ìš”í•˜ë¯€ë¡œ, ecom_dfì™€ ecom_no_na ë‘ ê°œì˜ dataframeìœ¼ë¡œ ë‚˜ëˆ ì„œ ì €ì¥í•´ë‘  | . ecom_no_na = ecom_df.query('CustomerID != \"N/A\"') . | . ",
    "url": "https://chaelist.github.io/docs/kaggle/uk_ecommerce2/#%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%A0%95%EB%A6%AC",
    "relUrl": "/docs/kaggle/uk_ecommerce2/#ë°ì´í„°-ì •ë¦¬"
  },"263": {
    "doc": "UK Ecommerce Data 2",
    "title": "6ê°œì›” ì¬êµ¬ë§¤ìœ¨",
    "content": ". | ê°™ì€ e-commerce ì‚¬ì—…ì´ë¼ë„ ì¬êµ¬ë§¤ìœ¨ê³¼ ì¬êµ¬ë§¤ ì£¼ê¸°ëŠ” ì·¨ê¸‰ í•­ëª©ì— ë”°ë¼ ë‹¤ë¦„ | ì¬êµ¬ë§¤ìœ¨ì— ë”°ë¼, ì¶©ì„± ê³ ê° ìœ ì§€ì— ì´ˆì ì„ ë§ì¶˜ ë§ˆì¼€íŒ…ì´ í•„ìš”í•  ìˆ˜ë„ ìˆê³ , ì‹ ê·œ ê³ ê° ìœ ì¹˜ì— ë” ë…¸ë ¥ì„ ê¸°ìš¸ì—¬ì•¼ í•  ìˆ˜ë„ ìˆë‹¤. (ex. ì‹ë£Œí’ˆ íŒë§¤: ê±°ì˜ ë§¤ì¼ ì ‘ì†í•˜ëŠ” ì´ìš©ìê°€ ë§ìŒ vs ëŒ€í˜• ê°€ì „ íŒë§¤: ëŒ€ë¶€ë¶„ í•œ ë²ˆ êµ¬ë§¤í•˜ë©´ ëª‡ ë…„ ë™ì•ˆ ì‚¬ìš©) | ì£¼ì–´ì§„ ë°ì´í„°ì˜ ê²½ìš° 1ë…„ ê°„ì˜ ì‚¬ì—… ë°ì´í„°ì´ë¯€ë¡œ, 6ê°œì›” ë‹¨ìœ„ì˜ ì¬êµ¬ë§¤ìœ¨ì„ í†µí•´ ì‚¬ì—… ë°©í–¥ì„±ì„ ì ê²€ | . # ìƒë°˜ê¸°: 2010-12-01 ~ 2011-05-31 first_half = ecom_no_na.query('InvoiceDate &lt; \"2011-06-01\"') first_half_customers = set(first_half['CustomerID'].unique()) # í•˜ë°˜ê¸°: 2011-06-01 ~ 2011-11-30 second_half = ecom_no_na.query('InvoiceDate &gt;= \"2011-06-01\" &amp; InvoiceDate &lt; \"2011-12-01\"') second_half_customers = set(second_half['CustomerID'].unique()) print('ìƒë°˜ê¸° êµ¬ë§¤ììˆ˜: ', len(first_half_customers)) print('ìƒë°˜ê¸°&amp;í•˜ë°˜ê¸° êµ¬ë§¤ììˆ˜: ', len(first_half_customers &amp; second_half_customers)) print('6ê°œì›” ì¬êµ¬ë§¤ìœ¨: ', len(first_half_customers &amp; second_half_customers) / len(first_half_customers)) . ìƒë°˜ê¸° êµ¬ë§¤ììˆ˜: 2767 ìƒë°˜ê¸°&amp;í•˜ë°˜ê¸° êµ¬ë§¤ììˆ˜: 1934 6ê°œì›” ì¬êµ¬ë§¤ìœ¨: 0.6989519335019877 . | 6ê°œì›” ì¬êµ¬ë§¤ìœ¨: ì•½ 69.9% | ì¬êµ¬ë§¤ìœ¨ì´ ë†’ì€ í¸ì´ë¯€ë¡œ, ì¶©ì„±ë„ ë†’ì€ ê³ ê°ì—ê²Œ ì´ˆì ì„ ë§ì¶˜ ë§ˆì¼€íŒ…ì— ì‹ ê²½ì„ ì“°ë©´ ì¢‹ì„ ë“¯ (ex. í¬ì¸íŠ¸ ì œë„ ë“±) | . ",
    "url": "https://chaelist.github.io/docs/kaggle/uk_ecommerce2/#6%EA%B0%9C%EC%9B%94-%EC%9E%AC%EA%B5%AC%EB%A7%A4%EC%9C%A8",
    "relUrl": "/docs/kaggle/uk_ecommerce2/#6ê°œì›”-ì¬êµ¬ë§¤ìœ¨"
  },"264": {
    "doc": "UK Ecommerce Data 2",
    "title": "ê³ ê° íŠ¹ì„±ë³„ êµ¬ë§¤ë‹¹ ë°˜í’ˆ ë¹„ìœ¨",
    "content": ": ê³ ê°ë³„ ì´íƒˆ ìœ„í—˜ë„ (risk ratio)ì™€ êµ¬ë§¤ë‹¹ ë°˜í’ˆ ë¹„ìœ¨ì˜ ê´€ê³„ë¥¼ í™•ì¸ . | ê³ ê°ë³„ êµ¬ë§¤ íŒ¨í„´ ì •ë¦¬ . ecom_purchase = ecom_no_na.query('TotalSpending &gt;= 0') # TotalSpendingì´ ì–‘ìˆ˜ì¸ ê°’ë§Œ ë°˜ì˜. (-ëŠ” ë°˜í’ˆí•œ ë‚´ì—­ìœ¼ë¡œ ì¶”ì •) temp = ecom_purchase.query('InvoiceMonth != \"201112\"') # 201112ëŠ” ì œì™¸í•˜ê³  ê³„ì‚° customer_usage = pd.pivot_table(temp, index='CustomerID', columns='InvoiceMonth', values='InvoiceNo', aggfunc='count') customer_usage = customer_usage.applymap(lambda x: 'O' if x &gt; 0 else 'X') for index in customer_usage.index: templist = customer_usage.loc[index].to_list() month_btw = [] for i in range(12): if templist[i] == 'O': for j in range(1, 12 - i): if templist[i + j] == 'O': month_btw.append(j) break if month_btw: month_btw_avg = sum(month_btw) / len(month_btw) else: if (customer_usage.loc[index, '201111'] == 'O') or (customer_usage.loc[index, '201110'] == 'O'): month_btw_avg = 'New Customer' else: month_btw_avg = 'Never Returned' customer_usage.loc[index, 'avg_month_btw_purchases'] = month_btw_avg last_month = ecom_purchase.query('InvoiceMonth != \"201112\"').groupby('CustomerID')[['InvoiceMonth']].max() last_month.rename(columns={'InvoiceMonth':'LastMonth'}, inplace=True) customer_usage = customer_usage.join(last_month) for index in customer_usage.index: lastmonth = customer_usage.loc[index, 'LastMonth'] if lastmonth == '201012': lastmonth = 201100 customer_usage.loc[index, 'months_after_latest_purchase'] = 201112 - int(lastmonth) customer_usage.head() . | CustomerID | 201012 | 201101 | 201102 | 201103 | 201104 | 201105 | 201106 | 201107 | 201108 | 201109 | 201110 | 201111 | avg_month_btw_purchases | LastMonth | months_after_latest_purchase | . | 12346 | X | O | X | X | X | X | X | X | X | X | X | X | Never Returned | 201101 | 11 | . | 12347 | O | O | X | X | O | X | O | X | O | X | O | X | 2.0 | 201110 | 2 | . | 12348 | O | O | X | X | O | X | X | X | X | O | X | X | 3.0 | 201109 | 3 | . | 12349 | X | X | X | X | X | X | X | X | X | X | X | O | New Customer | 201111 | 1 | . | 12350 | X | X | O | X | X | X | X | X | X | X | X | X | Never Returned | 201102 | 10 | . | êµ¬ë§¤ ì§€ì† ê³ ê°ì˜ risk ratio ê³„ì‚° . | risk ratio(ìœ„í—˜ ë¹„ìœ¨): ë§ˆì§€ë§‰ êµ¬ë§¤ ì´í›„ ê²½ê³¼í•œ ê¸°ê°„ / í‰ê·  êµ¬ë§¤ ê°„ê²© | . likely_customer = customer_usage.query('avg_month_btw_purchases not in [\"Never Returned\", \"New Customer\"]') likely_customer['risk_ratio'] = likely_customer['months_after_latest_purchase'] / likely_customer['avg_month_btw_purchases'] likely_customer.head() . | CustomerID | 201012 | 201101 | 201102 | 201103 | 201104 | 201105 | 201106 | 201107 | 201108 | 201109 | 201110 | 201111 | avg_month_btw_purchases | LastMonth | months_after_latest_purchase | risk_ratio | . | 12347 | O | O | X | X | O | X | O | X | O | X | O | X | 2 | 201110 | 2 | 1 | . | 12348 | O | O | X | X | O | X | X | X | X | O | X | X | 3 | 201109 | 3 | 1 | . | 12352 | X | X | O | O | X | X | X | X | X | O | X | O | 3 | 201111 | 1 | 0.333333 | . | 12356 | X | O | X | X | O | X | X | X | X | X | X | O | 5 | 201111 | 1 | 0.2 | . | 12359 | X | O | O | X | X | X | O | X | X | X | O | X | 3 | 201110 | 2 | 0.666667 | . | ê³ ê°ë³„ êµ¬ë§¤ ëŒ€ë¹„ ë°˜í’ˆ ë¹„ìœ¨ ê³„ì‚° . # ê³ ê°ë³„ Purchaseì™€ Refunds ê³„ì‚°í•´ì„œ join ecom_purchase = ecom_no_na.query('TotalSpending &gt;= 0') ecom_purchase = ecom_purchase.groupby('CustomerID')[['Quantity']].sum() ecom_purchase.rename(columns={'Quantity':'Purchases'}, inplace=True) ecom_refund = ecom_no_na.query('TotalSpending &lt; 0') ecom_refund = ecom_refund.groupby('CustomerID')[['Quantity']].sum() * -1 ecom_refund.rename(columns={'Quantity':'Refunds'}, inplace=True) purchase_refund = ecom_purchase.join(ecom_refund, how='left') purchase_refund['Refunds'].fillna(0, inplace=True) # Refunds per Purchases ìˆ˜ì¹˜ ê³„ì‚° purchase_refund['Re_per_Pur'] = purchase_refund['Refunds'] / purchase_refund['Purchases'] * 100 purchase_refund.head() . | CustomerID | Purchases | Refunds | Re_per_Pur | . | 12346 | 74215 | 74215 | 100 | . | 12347 | 2458 | 0 | 0 | . | 12348 | 2341 | 0 | 0 | . | 12349 | 631 | 0 | 0 | . | 12350 | 197 | 0 | 0 | . | êµ¬ë§¤ë‹¹ ë°˜í’ˆìˆ˜ &amp; ê³ ê° íŠ¹ì„± join . purchase_refund = purchase_refund.join(customer_usage['avg_month_btw_purchases'], how='left') purchase_refund = purchase_refund.join(customer_usage['months_after_latest_purchase'], how='left') purchase_refund = purchase_refund.join(likely_customer['risk_ratio'], how='left') purchase_refund.head() . | CustomerID | Purchases | Refunds | Re_per_Pur | avg_month_btw_purchases | months_after_latest_purchase | risk_ratio | . | 12346 | 74215 | 74215 | 100 | Never Returned | 11 | nan | . | 12347 | 2458 | 0 | 0 | 2.0 | 2 | 1 | . | 12348 | 2341 | 0 | 0 | 3.0 | 3 | 1 | . | 12349 | 631 | 0 | 0 | New Customer | 1 | nan | . | 12350 | 197 | 0 | 0 | Never Returned | 10 | nan | . | . ì´íƒˆìì™€ ì¬êµ¬ë§¤ì ë¹„êµ . | ì´íƒˆì vs 1ë…„ ì´ë‚´ì— 2ë²ˆ ì´ìƒ êµ¬ë§¤í•œ ê³ ê° | . # 'New Customer'ëŠ” ì œì™¸í•˜ê³  ë¶„ì„ purchase_refund1 = purchase_refund.query('avg_month_btw_purchases != \"New Customer\"') # ê³ ê°ì„ 'Never Returned'ì™€ 'Returned'ë¡œ êµ¬ë¶„í•´ì¤Œ purchase_refund1['flag1'] = purchase_refund1['avg_month_btw_purchases'].apply(lambda x: 'Never Returned' if x == 'Never Returned' else 'Returned') purchase_refund1.head() . | CustomerID | Purchases | Refunds | Re_per_Pur | avg_month_btw_purchases | months_after_latest_purchase | risk_ratio | flag1 | . | 12346 | 74215 | 74215 | 100 | Never Returned | 11 | nan | Never Returned | . | 12347 | 2458 | 0 | 0 | 2.0 | 2 | 1 | Returned | . | 12348 | 2341 | 0 | 0 | 3.0 | 3 | 1 | Returned | . | 12350 | 197 | 0 | 0 | Never Returned | 10 | nan | Never Returned | . | 12352 | 536 | 66 | 12.3134 | 3.0 | 1 | 0.333333 | Returned | . | ì´íƒˆìì™€ ì¬êµ¬ë§¤ìì˜ êµ¬ë§¤ë‹¹ ë°˜í’ˆìˆ˜ ë¹„êµ . purchase_refund1.groupby('flag1')[['Re_per_Pur']].agg(['mean', 'count']) . | Â  | Re_per_Pur | | Â  | . | flag1 | mean | count | . | Never Returned | 2.6610 | 1097 | . | Returned | 1.7426 | 2644 | . â†’ ì‹œê°í™”: . temp = purchase_refund1.groupby('flag1')[['Re_per_Pur']].agg(['mean', 'count']) temp = temp['Re_per_Pur'].reset_index() fig = px.bar(temp, x='flag1', y='mean', color='flag1', hover_data = ['count'], labels = {'mean':'Refunds per Purchase', 'flag1': 'Customer Flag', 'count':'Customer Count'}, category_orders = {'flag1':['Never Returned', 'Returned']}, color_discrete_sequence = [px.colors.sequential.Teal[0], px.colors.sequential.Teal[2]]) fig.update_traces(showlegend=False) fig.show() . | ì´íƒˆ ê³ ê°ì´ ì¬êµ¬ë§¤ ê³ ê° ëŒ€ë¹„ êµ¬ë§¤ë‹¹ ë°˜í’ˆ ë¹„ìœ¨ì´ ë†’ì€ í¸ | . | t-testë¡œ í‰ê·  ì°¨ì´ê°€ ìœ ì˜ë¯¸í•œì§€ ê²€ì • . never_returned = purchase_refund1.query('flag1 == \"Never Returned\"') returned = purchase_refund1.query('flag1 == \"Returned\"') # Leveneì˜ ë“±ë¶„ì‚° ê²€ì • lev_result = stats.levene(never_returned['Re_per_Pur'], returned['Re_per_Pur']) print('LeveneResult(F) : %.2f \\np-value : %.3f' % (lev_result)) . LeveneResult(F) : 5.76 p-value : 0.016 . â†’ p-valueê°€ 0.05 ë¯¸ë§Œì´ë¯€ë¡œ, ì´ë¶„ì‚°ì¸ ë…ë¦½í‘œë³¸ t-testë¡œ ì§„í–‰: . t_result = stats.ttest_ind(never_returned['Re_per_Pur'], returned['Re_per_Pur'], equal_var=False) print('t statistic : %.2f \\np-value : %.3f' % (t_result)) . t statistic : 1.80 p-value : 0.072 . | t-test ê²°ê³¼ p-valueëŠ” 0.05 ì´ìƒì´ì§€ë§Œ, ìˆ˜ì¹˜ ì°¨ì´ë¥¼ ë³¼ ë•Œ, ì´íƒˆí•œ ê³ ê°ê³¼ ì¬ë°©ë¬¸ ê³ ê° ê°„ì— êµ¬ë§¤ë‹¹ ë°˜í’ˆ ë¹„ìœ¨ ì°¨ì´ê°€ ì•½ê°„ì€ ì˜ë¯¸ê°€ ìˆë‹¤ê³  ìƒê°í•´ë„ ë  ë“¯. (ë°ì´í„° ìˆ˜ê°€ ë” ë§ìœ¼ë©´ pê°’ì´ ë” ë‚®ê²Œ ë‚˜ì˜¬ ìˆ˜ ìˆìŒ) | . | . ì´íƒˆ ê³ ìœ„í—˜êµ°ê³¼ ì €ìœ„í—˜êµ° ë¹„êµ . # 'risk ratio'ê°€ nullì¸ í–‰ì€ ì œì™¸í•˜ê³  ë¶„ì„ (Never Returnedì™€ New Customer ì œì™¸) purchase_refund2 = purchase_refund[purchase_refund['risk_ratio'].notna()] # risk ratioê°€ 1.5ë³´ë‹¤ ë†’ìœ¼ë©´ 'High Risk', ê·¸ ì™¸ì—ëŠ” 'Low Risk'ë¡œ ë¶„ë¥˜í•´ì¤Œ # risk ratioê°€ 1.5ë¼ëŠ” ê±´ í‰ì†Œ êµ¬ë§¤ ì£¼ê¸°ì˜ 1.5ë°°ì˜ ê¸°ê°„ ë™ì•ˆ ëŒì•„ì˜¤ì§€ ì•Šì•˜ë‹¤ëŠ” ì˜ë¯¸ purchase_refund2['flag2'] = purchase_refund2['risk_ratio'].apply(lambda x: 'High Risk' if x &gt; 1.5 else 'Low Risk') purchase_refund2.head() . | CustomerID | Purchases | Refunds | Re_per_Pur | avg_month_btw_purchases | months_after_latest_purchase | risk_ratio | flag2 | . | 12347 | 2458 | 0 | 0 | 2 | 2 | 1 | Low Risk | . | 12348 | 2341 | 0 | 0 | 3 | 3 | 1 | Low Risk | . | 12352 | 536 | 66 | 12.3134 | 3 | 1 | 0.333333 | Low Risk | . | 12356 | 1591 | 0 | 0 | 5 | 1 | 0.2 | Low Risk | . | 12359 | 1609 | 10 | 0.621504 | 3 | 2 | 0.666667 | Low Risk | . | ì´íƒˆ ê³ ìœ„í—˜êµ°ê³¼ ì €ìœ„í—˜êµ°ì˜ êµ¬ë§¤ë‹¹ ë°˜í’ˆìˆ˜ ë¹„êµ . purchase_refund2.groupby('flag2')[['Re_per_Pur']].agg(['mean', 'count']) . | Â  | Re_per_Pur | | Â  | . | flag2 | mean | count | . | High Risk | 2.45521 | 515 | . | Low Risk | 1.55297 | 2088 | . â†’ ì‹œê°í™”: . temp = purchase_refund2.groupby('flag2')[['Re_per_Pur']].agg(['mean', 'count']) temp = temp['Re_per_Pur'].reset_index() fig = px.bar(temp, x='flag2', y='mean', color='flag2', hover_data = ['count'], labels = {'mean':'Refunds per Purchase', 'flag2': 'Customer Flag', 'count':'Customer Count'}, color_discrete_sequence = [px.colors.sequential.Teal[0], px.colors.sequential.Teal[2]]) fig.update_traces(showlegend=False) fig.show() . | ì´íƒˆ ê³ ìœ„í—˜êµ°ì¸ ê³ ê°ë“¤ì´ êµ¬ë§¤ë‹¹ ë°˜í’ˆ ë¹„ìœ¨ì´ ë” ë†’ì€ í¸ìœ¼ë¡œ íŒŒì•…ë¨ | . | t-testë¡œ í‰ê·  ì°¨ì´ê°€ ìœ ì˜ë¯¸í•œì§€ ê²€ì • . high_risk = purchase_refund2.query('flag2 == \"High Risk\"') low_risk = purchase_refund2.query('flag2 == \"Low Risk\"') # Leveneì˜ ë“±ë¶„ì‚° ê²€ì • lev_result = stats.levene(high_risk['Re_per_Pur'], low_risk['Re_per_Pur']) print('LeveneResult(F) : %.2f \\np-value : %.3f' % (lev_result)) . LeveneResult(F) : 7.13 p-value : 0.008 . â†’ p-valueê°€ 0.05 ë¯¸ë§Œì´ë¯€ë¡œ, ì´ë¶„ì‚°ì¸ ë…ë¦½í‘œë³¸ t-testë¡œ ì§„í–‰: . t_result = stats.ttest_ind(high_risk['Re_per_Pur'], low_risk['Re_per_Pur'], equal_var=False) print('t statistic : %.2f \\np-value : %.3f' % (t_result)) . t statistic : 1.87 p-value : 0.062 . | pê°’ì´ 0.05 ì´ìƒì´ê¸´ í•˜ì§€ë§Œ, ìˆ˜ì¹˜ ì°¨ì´ë¥¼ ë³¼ ë•Œ, ì´íƒˆí•œ ê³ ê°ê³¼ ì¬ë°©ë¬¸ ê³ ê° ê°„ì— êµ¬ë§¤ë‹¹ ë°˜í’ˆ ë¹„ìœ¨ ì°¨ì´ê°€ ì•½ê°„ì€ ì˜ë¯¸ê°€ ìˆë‹¤ê³  ìƒê°í•´ë„ ë  ë“¯. (ë°ì´í„° ìˆ˜ê°€ ë” ë§ìœ¼ë©´ pê°’ì´ ë” ë‚®ê²Œ ë‚˜ì˜¬ ìˆ˜ ìˆìŒ) | ë°˜í’ˆì„ ë°©ì§€í•˜ëŠ” ê²ƒì´ ì´íƒˆ ê³ ìœ„í—˜êµ° ê³ ê°ì˜ ë§Œì¡±ë„ë¥¼ ë†’ì—¬ ì´íƒˆì„ ë°©ì§€í•˜ëŠ” ë°ì— ë„ì›€ì´ ë  ê±°ë¼ê³  ê°€ì„¤ì„ ì„¸ìš¸ ìˆ˜ ìˆì„ ë“¯ | . | . ",
    "url": "https://chaelist.github.io/docs/kaggle/uk_ecommerce2/#%EA%B3%A0%EA%B0%9D-%ED%8A%B9%EC%84%B1%EB%B3%84-%EA%B5%AC%EB%A7%A4%EB%8B%B9-%EB%B0%98%ED%92%88-%EB%B9%84%EC%9C%A8",
    "relUrl": "/docs/kaggle/uk_ecommerce2/#ê³ ê°-íŠ¹ì„±ë³„-êµ¬ë§¤ë‹¹-ë°˜í’ˆ-ë¹„ìœ¨"
  },"265": {
    "doc": "UK Ecommerce Data 2",
    "title": "ìƒí’ˆë³„ ë¶„ì„",
    "content": "# Descriptionì„ ê¸°ì¤€ìœ¼ë¡œ ì •ë¦¬í•˜ê¸° ì „ì—, í˜¹ì‹œ ëª¨ë¥¼ ì‚¬ì†Œí•œ ì°¨ì´ë¥¼ ëŒ€ë¹„í•´ ëª¨ë‘ upperë¡œ ë§ì¶°ì£¼ê³ , ì–‘ ì˜† ê³µë°±ì„ ì—†ì• ì¤Œ print(ecom_df['Description'].nunique()) # ì •ë¦¬ ì´ì „ì˜ Description ìˆ˜ ecom_df['Description'] = ecom_df['Description'].str.upper().str.strip() print(ecom_df['Description'].nunique()) # ì •ë¦¬ ì´í›„ì˜ Description ìˆ˜ . 4037 4026 . ìƒí’ˆë³„ êµ¬ë§¤ ëŒ€ë¹„ ë°˜í’ˆ . | ë°˜í’ˆì„ ìµœì†Œí™”í•˜ê¸° ìœ„í•´, êµ¬ë§¤ ëŒ€ë¹„ ë°˜í’ˆì´ ë§ì€ ì œí’ˆì„ ì‚´í´ë³´ê¸°ë¡œ í•¨ | . # ìƒí’ˆë³„ë¡œ êµ¬ë§¤ìˆ˜ì™€ ë°˜í’ˆìˆ˜ë¥¼ ì§‘ê³„í•œ ë‹¤ìŒ join product_purchase = ecom_df.query('TotalSpending &gt;= 0') product_purchase = product_purchase.groupby('Description')[['Quantity']].sum() product_purchase.rename(columns={'Quantity':'Purchases'}, inplace=True) product_refund = ecom_df.query('TotalSpending &lt; 0') product_refund = product_refund.groupby('Description')[['Quantity']].sum() * -1 product_refund.rename(columns={'Quantity':'Refunds'}, inplace=True) products = product_purchase.join(product_refund, how='left') products['Refunds'].fillna(0, inplace=True) # Refunds per Purchases ìˆ˜ì¹˜ ê³„ì‚° products['Re_per_Pur'] = products['Refunds'] / products['Purchases'] * 100 products.head() . | Description | Purchases | Refunds | Re_per_Pur | . | *BOOMBOX IPOD CLASSIC | 1 | 0 | 0 | . | *USB OFFICE MIRROR BALL | 2 | 0 | 0 | . | 10 COLOUR SPACEBOY PEN | 6564 | 172 | 2.62035 | . | 12 COLOURED PARTY BALLOONS | 2134 | 20 | 0.93721 | . | 12 DAISY PEGS IN WOOD BOX | 344 | 0 | 0 | . â†’ êµ¬ë§¤ ëŒ€ë¹„ ë°˜í’ˆë¹„ìœ¨ì´ ê°€ì¥ ë†’ì€ ì œí’ˆ Top 10ì„ ì‹œê°í™”: . # êµ¬ë§¤ê°€ ë„ˆë¬´ ì ì„ ê²½ìš° í•œë‘ëª…ì˜ ë³€ì‹¬ì— ë”°ë¼ ë°˜í’ˆë¹„ìœ¨ì´ ë„ˆë¬´ ë†’ê²Œ ë‚˜íƒ€ë‚  ìˆ˜ ìˆìœ¼ë¯€ë¡œ, êµ¬ë§¤ê°€ 10ë²ˆ ì´ìƒì¸ ì œí’ˆë§Œ ëŒ€ìƒìœ¼ë¡œ ë¹„êµ temp = products.reset_index().query('Purchases &gt;= 10') temp = temp.sort_values(by='Re_per_Pur', ascending=False).head(10) fig = px.bar(temp, x='Re_per_Pur', y='Description', color='Re_per_Pur', hover_data=['Purchases', 'Refunds'], labels = {'Re_per_Pur':'Refunds per Purchase'}, category_orders = {'Description': temp['Description'].to_list()}, color_continuous_scale = 'Teal') fig.update(layout_coloraxis_showscale=False) fig.show() . | ë°˜í’ˆ ë¹„ìœ¨ì´ 100% ì´ìƒì¸ ì œí’ˆë“¤: 2010.12 ì´ì „ ë°ì´í„°ëŠ” ê¸°ë¡ë˜ì–´ ìˆì§€ ì•Šê¸° ë•Œë¬¸ì—, ê·¸ ì´ì „ì— êµ¬ë§¤í•œ í›„ ê¸°ë¡ ê¸°ê°„ ë‚´ì— ë°˜í’ˆëœ ê±´ì´ ë§ì•„ì„œ ê·¸ëŸ° ë“¯. | êµ¬ë§¤ë‹¹ ë°˜í’ˆ ë¹„ìœ¨ì´ 50% ì´ìƒì¸ ì œí’ˆë“¤ì€ ì§‘ì¤‘ í’ˆì§ˆ ê´€ë¦¬ê°€ í•„ìš”í•˜ë‹¤ê³  ìƒê°ë¨. | . Â  . +) ë°˜í’ˆ ë¹„ìœ¨ì´ ê°€ì¥ ë†’ì€ ì œí’ˆì˜ êµ¬ë§¤ì›” &amp; ë°˜í’ˆì›” í™•ì¸: . # WOODEN BOX ADVENT CALENDAR fig = px.bar(ecom_df.query('Description == \"WOODEN BOX ADVENT CALENDAR\"'), x='InvoiceMonth', y='Quantity', color='Quantity', color_continuous_scale = 'Teal') fig.update(layout_coloraxis_showscale=False) fig.show() . | WOODEN BOX ADVENT CALENDAR ì œí’ˆì˜ ê²½ìš°, ì£¼ë¡œ 2010.12~2011.01 ê¸°ê°„ì— ë°˜í’ˆì´ ëª°ë ¤ ìˆê³ , ê·¸ ì´í›„ì—ëŠ” ë°˜í’ˆì´ ê±°ì˜ ì—†ìŒ. â€“ 2011ë…„ ì´ˆì— ì œí’ˆ í’ˆì§ˆ ê°œì„ ì´ ì´ë¯¸ ì´ë£¨ì–´ì¡ŒëŠ”ì§€ í™•ì¸í•´ë³´ë©´ ì¢‹ì„ ë“¯ | . ìƒí’ˆë³„ êµ¬ë§¤ ì¦ê°€í­ . | ìƒë°˜ê¸° ëŒ€ë¹„ í•˜ë°˜ê¸° êµ¬ë§¤ëŸ‰ì´ ê°€ì¥ ë§ì´ ì¦ê°€í•œ ì œí’ˆ . first_half = ecom_df.query('InvoiceDate &lt; \"2011-06-01\"') temp1 = first_half.groupby('Description')[['Quantity']].sum() temp1.rename(columns={'Quantity':'first_half'}, inplace=True) second_half = ecom_no_na.query('InvoiceDate &gt;= \"2011-06-01\" &amp; InvoiceDate &lt; \"2011-12-01\"') temp2 = second_half.groupby('Description')[['Quantity']].sum() temp2.rename(columns={'Quantity':'second_half'}, inplace=True) prod_purchase_change = temp1.join(temp2, how='outer') prod_purchase_change.fillna(0, inplace=True) prod_purchase_change['change'] = prod_purchase_change['second_half'] - prod_purchase_change['first_half'] prod_purchase_change.sort_values(by='change', ascending=False).head(10) . | Description | first_half | second_half | change | . | POPCORN HOLDER | 0 | 25149 | 25149 | . | RABBIT NIGHT LIGHT | 1130 | 22278 | 21148 | . | MINI PAINT SET VINTAGE | 0 | 14033 | 14033 | . | RED HARMONICA IN BOX | 0 | 12787 | 12787 | . | ROTATING SILVER ANGELS T-LIGHT HLDR | -6887 | 5284 | 12171 | . | PAPER CHAIN KIT 50â€™S CHRISTMAS | 0 | 11993 | 11993 | . | BROCADE RING PURSE | 0 | 11842 | 11842 | . | PACK OF 12 LONDON TISSUES | 0 | 11763 | 11763 | . | BUBBLEGUM RING ASSORTED | 0 | 11419 | 11419 | . | 60 CAKE CASES VINTAGE CHRISTMAS | 1336 | 12642 | 11306 | . | Popcorn Holderê°€ ê°€ì¥ êµ¬ë§¤ëŸ‰ì´ ë§ì´ ì¦ê°€í•¨. | ì£¼ë¡œ ìƒë°˜ê¸°ì—ëŠ” ì—†ì—ˆë˜ ì‹ ìƒí’ˆë“¤ì´ êµ¬ë§¤ ì¦ê°€ëŸ‰ì´ ê°€ì¥ ë§ì€ ê²ƒì„ ì•Œ ìˆ˜ ìˆìŒ | . cf) ì „ì²´ ê¸°ê°„ êµ¬ë§¤ëŸ‰ Top 10: . ecom_df.groupby('Description')[['Quantity']].sum().sort_values(by='Quantity', ascending=False).head(10) . | Description | Quantity | . | WORLD WAR 2 GLIDERS ASSTD DESIGNS | 53751 | . | JUMBO BAG RED RETROSPOT | 47256 | . | POPCORN HOLDER | 36322 | . | ASSORTED COLOUR BIRD ORNAMENT | 36282 | . | PACK OF 72 RETROSPOT CAKE CASES | 36016 | . | WHITE HANGING HEART T-LIGHT HOLDER | 35294 | . | RABBIT NIGHT LIGHT | 30631 | . | MINI PAINT SET VINTAGE | 26437 | . | PACK OF 12 LONDON TISSUES | 26095 | . | PACK OF 60 PINK PAISLEY CAKE CASES | 24719 | . | ìƒë°˜ê¸° ëŒ€ë¹„ í•˜ë°˜ê¸° êµ¬ë§¤ ê¸ˆì•¡ì´ ê°€ì¥ ë§ì´ ì¦ê°€í•œ ì œí’ˆ . first_half = ecom_df.query('InvoiceDate &lt; \"2011-06-01\"') temp1 = first_half.groupby('Description')[['TotalSpending']].sum() temp1.rename(columns={'TotalSpending':'first_half'}, inplace=True) second_half = ecom_no_na.query('InvoiceDate &gt;= \"2011-06-01\" &amp; InvoiceDate &lt; \"2011-12-01\"') temp2 = second_half.groupby('Description')[['TotalSpending']].sum() temp2.rename(columns={'TotalSpending':'second_half'}, inplace=True) prod_purchase_change2 = temp1.join(temp2, how='outer') prod_purchase_change2.fillna(0, inplace=True) prod_purchase_change2['change'] = prod_purchase_change2['second_half'] - prod_purchase_change2['first_half'] # 'MANUAL'ì€ í•˜ë‚˜ì˜ ì œí’ˆì€ ì•„ë‹ˆë¯€ë¡œ ì œì™¸ prod_purchase_change2.query('Description != \"MANUAL\"').sort_values(by='change', ascending=False).head(10) . | Description | first_half | second_half | change | . | RABBIT NIGHT LIGHT | 2277.5 | 42033.0 | 39755.5 | . | PICNIC BASKET WICKER 60 PIECES | 0 | 39619.5 | 39619.5 | . | PAPER CHAIN KIT 50â€™S CHRISTMAS | 0 | 32806.9 | 32806.9 | . | SET OF 3 REGENCY CAKE TINS | 0 | 25219.8 | 25219.8 | . | HOT WATER BOTTLE KEEP CALM | 0 | 21902.0 | 21902.0 | . | SET OF TEA COFFEE SUGAR TINS PANTRY | 0 | 21305.5 | 21305.5 | . | DOORMAT KEEP CALM AND COME IN | 6175.5 | 27088.7 | 20913.2 | . | SPOTTY BUNTING | 7513.9 | 27418.4 | 19904.5 | . | SET OF 3 CAKE TINS PANTRY DESIGN | 0 | 19277.6 | 19277.6 | . | POPCORN HOLDER | 0 | 19109.8 | 19109.8 | . | Rabbit Night Lightê°€ êµ¬ë§¤ ê¸ˆì•¡ì´ ê°€ì¥ ë§ì´ ì¦ê°€í•¨. | ì£¼ë¡œ ìƒë°˜ê¸°ì—ëŠ” ì—†ì—ˆë˜ ì‹ ìƒí’ˆë“¤ì´ êµ¬ë§¤ ê¸ˆì•¡ ì¦ê°€í­ì´ ê°€ì¥ í¬ë‹¤ | . cf) ì „ì²´ ê¸°ê°„ êµ¬ë§¤ ê¸ˆì•¡ Top 10: . # 'DOTCOM POSTAGE', 'POSTAGE'ëŠ” íŒë§¤ ì œí’ˆì´ë¼ê³  ë³´ê¸°ëŠ” ì• ë§¤í•˜ë¯€ë¡œ ì œì™¸ products_spending = ecom_df.groupby('Description')[['TotalSpending']].sum().sort_values(by='TotalSpending', ascending=False) products_spending.query('Description not in [\"DOTCOM POSTAGE\", \"POSTAGE\"]').head(10) . | Description | TotalSpending | . | REGENCY CAKESTAND 3 TIER | 164459.5 | . | WHITE HANGING HEART T-LIGHT HOLDER | 99612.4 | . | PARTY BUNTING | 98243.9 | . | JUMBO BAG RED RETROSPOT | 92175.8 | . | RABBIT NIGHT LIGHT | 66661.6 | . | PAPER CHAIN KIT 50â€™S CHRISTMAS | 63715.2 | . | ASSORTED COLOUR BIRD ORNAMENT | 58792.4 | . | CHILLI LIGHTS | 53746.7 | . | SPOTTY BUNTING | 42030.7 | . | JUMBO BAG PINK POLKADOT | 41584.4 | . | . ",
    "url": "https://chaelist.github.io/docs/kaggle/uk_ecommerce2/#%EC%83%81%ED%92%88%EB%B3%84-%EB%B6%84%EC%84%9D",
    "relUrl": "/docs/kaggle/uk_ecommerce2/#ìƒí’ˆë³„-ë¶„ì„"
  },"266": {
    "doc": "UK Ecommerce Data 2",
    "title": "êµ­ê°€ë³„ ë¶„ì„",
    "content": "êµ­ê°€ë³„ ë§¤ì¶œ . | êµ­ê°€ë³„ ì´ ë§¤ì¶œ . # ì „ì²´ ê¸°ê°„ ë§¤ì¶œ Top 10 êµ­ê°€ ì‹œê°í™” country_spending = ecom_df.groupby('Country')[['TotalSpending']].sum().sort_values(by='TotalSpending', ascending=False) fig = px.bar(country_spending.head(10).reset_index(), x='Country', y='TotalSpending', color='TotalSpending', color_continuous_scale = 'Teal') fig.update(layout_coloraxis_showscale=False) fig.show() . | UK ê¸°ë°˜ ì—…ì²´ì´ë¯€ë¡œ, UKê°€ ì••ë„ì ìœ¼ë¡œ ë§ê³ , ì£¼ë¡œ ì¸ì ‘í•œ ìœ ëŸ½ êµ­ê°€ë“¤ì—ì„œ êµ¬ë§¤ì•¡ì´ ë†’ìŒ | . | êµ­ê°€ë³„ ê³ ê° 1ì¸ë‹¹ ë§¤ì¶œ . # êµ­ê°€ë³„ë¡œ, ê³ ê° 1ì¸ë‹¹ ë§¤ì¶œ ê³„ì‚° spc_country = ecom_df.groupby('Country').agg({'TotalSpending':'sum', 'CustomerID':pd.Series.nunique}) spc_country['SpendingPerCustomer'] = spc_country['TotalSpending'] / spc_country['CustomerID'] # ê³ ê° 1ì¸ë‹¹ ë§¤ì¶œ Top 10 êµ­ê°€ ì‹œê°í™” # ê³ ê° ìˆ˜ê°€ 5ëª… ì´ìƒì¸ êµ­ê°€ë¡œ í•œì • spc_country_over3 = spc_country.query('CustomerID &gt;= 5').sort_values(by='SpendingPerCustomer', ascending=False) fig = px.bar(spc_country_over3.head(10).reset_index(), x='Country', y='SpendingPerCustomer', color='SpendingPerCustomer', hover_data = ['CustomerID'], labels = {'CustomerID': 'Customer Count'}, color_continuous_scale = 'Teal') fig.update(layout_coloraxis_showscale=False) fig.show() . | Netherlandsê°€ ê°€ì¥ ê³ ê° ë‹¹ ë§¤ì¶œì´ ë†’ì€ ê²ƒìœ¼ë¡œ í™•ì¸ë¨. | UKì˜ ê²½ìš°, ê³ ê°ìˆ˜ê°€ ì••ë„ì ìœ¼ë¡œ ë§ì•„ì„œ ê³ ê° ë‹¹ ë§¤ì¶œì•¡ì´ ë³´ë‹¤ í‰ê· ìœ¼ë¡œ íšŒê·€í•œ ê²°ê³¼ê°€ ë‚˜ì˜¨ ê²ƒì¼ ìˆ˜ë„ ìˆê³ , ë°°ì†¡ë¹„ê°€ ì ê²Œ ë¶™ì–´ì„œ ê·¸ëŸ° ì˜í–¥ë„ ì¡°ê¸ˆ ìˆì„ ìˆ˜ ìˆì„ ë“¯. | . | . ìƒë°˜ê¸° ëŒ€ë¹„ í•˜ë°˜ê¸° ë¹„êµ . | êµ­ê°€ë³„ ìƒë°˜ê¸° ëŒ€ë¹„ í•˜ë°˜ê¸° êµ¬ë§¤ê¸ˆì•¡ . first_half = ecom_no_na.query('InvoiceDate &lt; \"2011-06-01\"') temp1 = first_half.groupby('Country')[['TotalSpending']].sum() temp1.rename(columns={'TotalSpending':'first_half'}, inplace=True) second_half = ecom_no_na.query('InvoiceDate &gt;= \"2011-06-01\" &amp; InvoiceDate &lt; \"2011-12-01\"') temp2 = second_half.groupby('Country')[['TotalSpending']].sum() temp2.rename(columns={'TotalSpending':'second_half'}, inplace=True) spending_change = temp1.join(temp2, how='outer') spending_change.fillna(0, inplace=True) spending_change['change'] = spending_change['second_half'] - spending_change['first_half'] spending_change.sort_values(by='change', ascending=False).head(10) . | Country | first_half | second_half | change | . | United Kingdom | 2535957.8 | 3920927.9 | 1384970.1 | . | EIRE | 78764.5 | 164260.9 | 85496.3 | . | Netherlands | 112906.7 | 160026.8 | 47120.2 | . | France | 71743.5 | 117833.7 | 46090.2 | . | Germany | 91606.4 | 122098.4 | 30492.1 | . | Australia | 55600.0 | 81409.8 | 25809.8 | . | Switzerland | 15438.4 | 40301.0 | 24862.5 | . | Norway | 4722.7 | 27655.1 | 22932.4 | . | Belgium | 13140.0 | 26361.5 | 13221.5 | . | Spain | 21485.9 | 32998.7 | 11512.9 | . â†’ êµ¬ë§¤ì•¡ ì¦ê°€í­ì´ ë†’ì€ êµ­ê°€ Top 10 ì‹œê°í™”: . country_spending_change = spending_change.sort_values(by='change', ascending=False).head(10) fig = px.bar(country_spending_change[['first_half', 'second_half']].unstack().reset_index(), x='Country', y=0, color='level_0', barmode='group', labels={'0':'Total Spending', 'level_0':'Time Period'}, category_orders = {'Country': country_spending_change.index}, # êµ¬ë§¤ì•¡ ì¦ê°€í­ì´ ë†’ì€ ìˆœì„œëŒ€ë¡œ ì •ë ¬ color_discrete_sequence = [px.colors.sequential.Teal[0], px.colors.sequential.Teal[2]]) fig.update(layout_coloraxis_showscale=False) fig.show() . | êµ¬ë§¤ì•¡ ì¦ê°€í­ ì—­ì‹œ UKê°€ ê°€ì¥ í¬ê³ , ê·¸ ë‹¤ìŒì´ ì•„ì¼ëœë“œì™€ ë„¤ëœë€ë“œ. | . | êµ­ê°€ë³„ ìƒë°˜ê¸° ëŒ€ë¹„ í•˜ë°˜ê¸° ê³ ê°ìˆ˜ . temp1 = first_half.groupby('Country')[['CustomerID']].nunique() temp1.rename(columns={'CustomerID':'first_half'}, inplace=True) temp2 = second_half.groupby('Country')[['CustomerID']].nunique() temp2.rename(columns={'CustomerID':'second_half'}, inplace=True) customer_change = temp1.join(temp2, how='outer') customer_change.fillna(0, inplace=True) customer_change['change'] = customer_change['second_half'] - customer_change['first_half'] customer_change.sort_values(by='change', ascending=False).head(10) . | Country | first_half | second_half | change | . | United Kingdom | 2508 | 3165 | 657 | . | Germany | 57 | 80 | 23 | . | France | 56 | 70 | 14 | . | Spain | 19 | 25 | 6 | . | Norway | 3 | 9 | 6 | . | Switzerland | 11 | 17 | 6 | . | Finland | 4 | 10 | 6 | . | Denmark | 3 | 8 | 5 | . | Portugal | 11 | 14 | 3 | . | Poland | 3 | 6 | 3 | . â†’ êµ¬ë§¤ ê³ ê°ìˆ˜ ì¦ê°€í­ì´ ë†’ì€ êµ­ê°€ Top 10 ì‹œê°í™”: . country_customer_change = customer_change.sort_values(by='change', ascending=False).head(10) fig = px.bar(country_customer_change[['first_half', 'second_half']].unstack().reset_index(), x='Country', y=0, color='level_0', barmode='group', labels={'0':'Customer Count', 'level_0':'Time Period'}, category_orders = {'Country': country_customer_change.index}, # ê³ ê°ìˆ˜ ì¦ê°€í­ì´ ë†’ì€ ìˆœì„œëŒ€ë¡œ ì •ë ¬ color_discrete_sequence = [px.colors.sequential.Teal[0], px.colors.sequential.Teal[2]]) fig.update(layout_coloraxis_showscale=False) fig.show() . | êµ¬ë§¤ê³ ê° ì¦ê°€í­ ì—­ì‹œ UKê°€ ê°€ì¥ í¬ê³ , ê·¸ ë‹¤ìŒì€ ë…ì¼ê³¼ í”„ë‘ìŠ¤. | . | . ",
    "url": "https://chaelist.github.io/docs/kaggle/uk_ecommerce2/#%EA%B5%AD%EA%B0%80%EB%B3%84-%EB%B6%84%EC%84%9D",
    "relUrl": "/docs/kaggle/uk_ecommerce2/#êµ­ê°€ë³„-ë¶„ì„"
  },"267": {
    "doc": "UK Ecommerce Data 2",
    "title": "UK Ecommerce Data 2",
    "content": " ",
    "url": "https://chaelist.github.io/docs/kaggle/uk_ecommerce2/",
    "relUrl": "/docs/kaggle/uk_ecommerce2/"
  },"268": {
    "doc": "ìœ ìš©í•œ Python ë‚´ì¥í•¨ìˆ˜",
    "title": "ìœ ìš©í•œ Python ë‚´ì¥í•¨ìˆ˜",
    "content": ". | map, zip | Itertools . | ì¡°í•©í˜• iterator | ë¬´í•œ iterator | ì¡°ê±´í˜• iterator | . | Collections . | Counter | . | . ",
    "url": "https://chaelist.github.io/docs/data_handling/useful_functions/",
    "relUrl": "/docs/data_handling/useful_functions/"
  },"269": {
    "doc": "ìœ ìš©í•œ Python ë‚´ì¥í•¨ìˆ˜",
    "title": "map, zip",
    "content": ". | map(function, iterable): inputìœ¼ë¡œ ë°›ì€ í•¨ìˆ˜ë¥¼ inputìœ¼ë¡œ ë°›ì€ iterable ë‚´ elementì— ëª¨ë‘ ì ìš©í•´ì£¼ëŠ” í•¨ìˆ˜ . print(list(map(int, [1.4, 2.3, 3.5, 4.0]))) # ê° elementë¥¼ êº¼ë‚´ì„œ int(x)ì— ë„£ì–´ì¤€ í›„ ë‹¤ì‹œ listì— ë„£ëŠ” ëŠë‚Œ . [1, 2, 3, 4] . +) function ë¶€ë¶„ì— ê¸°ì¡´ í•¨ìˆ˜ê°€ ì•„ë‹Œ lambda ì‹ì„ ë„£ëŠ” ê²ƒë„ ê°€ëŠ¥ . print(list(map(lambda x: x**2, [1, 2, 3, 4, 5]))) . [1, 4, 9, 16, 25] . | zip(iterables): ê° iterableì˜ elementë“¤ì„ ì—°ê²°í•´ì£¼ëŠ” í•¨ìˆ˜ . # ë³´í†µì€ ì´ëŸ° ì‹ìœ¼ë¡œ ë°˜ë³µë¬¸ì—ì„œ indexë¥¼ ë¶™ì—¬ì¤„ ë•Œ ë§ì´ í™œìš© list1 = ['A', 'B', 'C', 'D', 'E'] for i, element in zip(range(len(list1)), list1): print(i, ':', element) . 0 : A 1 : B 2 : C 3 : D 4 : E . +) zip(*zipped_element)ë¥¼ í•´ì£¼ë©´ unzipí•  ìˆ˜ ìˆìŒ . print(list(zip(*zip(range(len(list1)), list1)))) . [(0, 1, 2, 3, 4), ('A', 'B', 'C', 'D', 'E')] . | . ",
    "url": "https://chaelist.github.io/docs/data_handling/useful_functions/#map-zip",
    "relUrl": "/docs/data_handling/useful_functions/#map-zip"
  },"270": {
    "doc": "ìœ ìš©í•œ Python ë‚´ì¥í•¨ìˆ˜",
    "title": "Itertools",
    "content": ": íš¨ìœ¨ì ì¸ ë£¨í•‘ì„ ìœ„í•œ ì´í„°ë ˆì´í„°ë¥¼ ë§Œë“œëŠ” í•¨ìˆ˜ë“¤ì„ ì œê³µí•˜ëŠ” ëª¨ë“ˆ . ì¡°í•©í˜• iterator . | product(iterables, repeat=1): inputìœ¼ë¡œ ë°›ì€ iterableë“¤ì˜ ë°ì¹´ë¥´íŠ¸ê³±ì„ ë°˜í™˜ . | iterable: ìš”ì†Œë¥¼ í•˜ë‚˜ì”© ë°˜í™˜í•  ìˆ˜ ìˆëŠ” ê°ì²´. sequence typeì¸ list, str, tuple ë“±. | ë°ì¹´ë¥´íŠ¸ê³± (= ê³±ì§‘í•©): ê° ì§‘í•©ì˜ ì›ì†Œë¥¼ ê° ì„±ë¶„ìœ¼ë¡œ í•˜ëŠ” tupleì˜ ì§‘í•© | . from itertools import product print(list(product('ABC', 'xyz'))) . [('A', 'x'), ('A', 'y'), ('A', 'z'), ('B', 'x'), ('B', 'y'), ('B', 'z'), ('C', 'x'), ('C', 'y'), ('C', 'z')] . +) iterableì´ listë¡œ ë¬¶ì—¬ ìˆëŠ” ê²½ìš°, *ë¡œ listë¥¼ í•´ì œí•˜ê³  ë„£ì–´ì£¼ë©´ ëœë‹¤ . list1 = ['ABC', 'xyz'] print(list(product(*list1))) . [('A', 'x'), ('A', 'y'), ('A', 'z'), ('B', 'x'), ('B', 'y'), ('B', 'z'), ('C', 'x'), ('C', 'y'), ('C', 'z')] . +) repeatì„ ì„¤ì •í•˜ë©´ iterableì´ në²ˆ ë°˜ë³µí•´ì„œ ì¡´ì¬í•˜ëŠ” ê²ƒìœ¼ë¡œ ê°„ì£¼ . print(list(product('01', repeat=2))) # product('01', '01)ê³¼ ë™ì¼í•œ ê°œë… . [('0', '0'), ('0', '1'), ('1', '0'), ('1', '1')] . print(list(product('A', repeat=4))) . [('A', 'A', 'A', 'A')] . print(list(product('01', 'ab', repeat=2))) . [('0', 'a', '0', 'a'), ('0', 'a', '0', 'b'), ('0', 'a', '1', 'a'), ('0', 'a', '1', 'b'), ('0', 'b', '0', 'a'), ('0', 'b', '0', 'b'), ('0', 'b', '1', 'a'), ('0', 'b', '1', 'b'), ('1', 'a', '0', 'a'), ('1', 'a', '0', 'b'), ('1', 'a', '1', 'a'), ('1', 'a', '1', 'b'), ('1', 'b', '0', 'a'), ('1', 'b', '0', 'b'), ('1', 'b', '1', 'a'), ('1', 'b', '1', 'b')] . | combinations(iterable, r): iterableì—ì„œ ì›ì†Œ ê°œìˆ˜ê°€ rê°œì¸ â€˜ì¡°í•©â€™ ë½‘ê¸° . | ì¡°í•©: ìˆœì„œë¥¼ ìƒê°í•˜ì§€ ì•Šê³  ë½‘ëŠ” ê²ƒ | iterableì´ nê°œì˜ ì›ì†Œë¥¼ ê°–ëŠ”ë‹¤ë©´, nCrê°œì˜ ì¡°í•©ì´ ê°€ëŠ¥ | . from itertools import combinations print(list(combinations([1, 2, 3, 4], 2))) # 4C2 = 6ê°€ì§€ ê²½ìš°ì˜ ìˆ˜ . [(1, 2), (1, 3), (1, 4), (2, 3), (2, 4), (3, 4)] . | combinations_with_replacement(iterable, r): iterableì—ì„œ ì›ì†Œ ê°œìˆ˜ê°€ rê°œì¸ ì¤‘ë³µ ì¡°í•© ë½‘ê¸° . | iterableì´ nê°œì˜ ì›ì†Œë¥¼ ê°–ëŠ”ë‹¤ë©´, nHrê°œì˜ ì¡°í•©ì´ ê°€ëŠ¥ | nHr = n+r-1Cr | . from itertools import combinations_with_replacement print(list(combinations_with_replacement([1, 2, 3, 4], 2))) # 4H2 = 5C2 = 10ê°€ì§€ ê²½ìš°ì˜ ìˆ˜ . [(1, 1), (1, 2), (1, 3), (1, 4), (2, 2), (2, 3), (2, 4), (3, 3), (3, 4), (4, 4)] . | permutations(iterable, r=None): iterableì—ì„œ ì›ì†Œ ê°œìˆ˜ê°€ rê°œì¸ â€˜ìˆœì—´â€™ ë½‘ê¸° . | ìˆœì—´: ìˆœì„œë¥¼ ê³ ë ¤í•˜ê³  ë½‘ëŠ” ê²ƒ | iterableì´ nê°œì˜ ì›ì†Œë¥¼ ê°–ëŠ”ë‹¤ë©´, nPrê°œì˜ ìˆœì—´ì´ ê°€ëŠ¥ | rê°’ì„ ì§€ì •í•˜ì§€ ì•Šê±°ë‚˜ r=Noneì´ë¼ê³  ì…ë ¥í•˜ë©´, rì˜ ê¸°ë³¸ê°’ì€ iterableì˜ ê¸¸ì´ì´ë©° ê°€ëŠ¥í•œ ëª¨ë“  ìµœëŒ€ ê¸¸ì´ ìˆœì—´ì´ ìƒì„±ë¨ | . from itertools import permutations print(list(permutations([1, 2, 3, 4], 2))) # 4P2 = 12ê°€ì§€ ê²½ìš°ì˜ ìˆ˜ # (1, 2)ì™€ (2, 1)ì€ ë‹¤ë¥¸ ê²ƒìœ¼ë¡œ ê°„ì£¼ë¨ . [(1, 2), (1, 3), (1, 4), (2, 1), (2, 3), (2, 4), (3, 1), (3, 2), (3, 4), (4, 1), (4, 2), (4, 3)] . +) rê°’ì„ ì…ë ¥í•˜ì§€ ì•ŠëŠ” ê²½ìš°: . # rì„ ì…ë ¥í•˜ì§€ ì•Šìœ¼ë©´ defaultë¡œ iterableì˜ ê¸¸ì´ì¸ 4ê°€ ë¨ â†’ 4P4 = 24ê°€ì§€ ê²½ìš°ì˜ ìˆ˜ print(list(permutations([1, 2, 3, 4]))) . [(1, 2, 3, 4), (1, 2, 4, 3), (1, 3, 2, 4), (1, 3, 4, 2), (1, 4, 2, 3), (1, 4, 3, 2), (2, 1, 3, 4), (2, 1, 4, 3), (2, 3, 1, 4), (2, 3, 4, 1), (2, 4, 1, 3), (2, 4, 3, 1), (3, 1, 2, 4), (3, 1, 4, 2), (3, 2, 1, 4), (3, 2, 4, 1), (3, 4, 1, 2), (3, 4, 2, 1), (4, 1, 2, 3), (4, 1, 3, 2), (4, 2, 1, 3), (4, 2, 3, 1), (4, 3, 1, 2), (4, 3, 2, 1)] . | . ë¬´í•œ iterator . | count(start=0, step=1): startë¶€í„° ì‹œì‘í•´ stepë§Œí¼ì”© ë¬´í•œ ì¦ê°€í•˜ëŠ” ìˆ«ìë¥¼ ë§Œë“ ë‹¤ . | ê·¸ëƒ¥ ì‚¬ìš©í•˜ë©´ ìˆ«ìê°€ ë¬´í•œí•˜ê²Œ ê³„ì† ìƒì„±ë˜ë¯€ë¡œ, ë³´í†µì€ zipê³¼ ê°™ì€ í•¨ìˆ˜ ì•ˆì— ë„£ì–´ì„œ ì‚¬ìš© | . from itertools import count print(list(zip(count(10, 2), ['A', 'B', 'C', 'D']))) . [(10, 'A'), (12, 'B'), (14, 'C'), (16, 'D')] . *ë°˜ë³µë¬¸ì—ì„œ indexë¥¼ ë¶™ì—¬ì¤„ ë•Œ í™œìš©í•˜ê¸° ì¢‹ìŒ . list1 = ['A', 'B', 'C', 'D', 'E'] for i, element in zip(count(0, 1), list1): print(i, ':', element) # zip(range(len(list1)), list1)ê³¼ ê²°ê³¼ëŠ” ê°™ì§€ë§Œ, len(list1)ì„ ê³„ì‚°í•˜ì§€ ì•Šì•„ë„ ëœë‹¤ëŠ” ì¥ì ì´ ìˆë‹¤ . 0 : A 1 : B 2 : C 3 : D 4 : E . | cycle(iterable): iterableì˜ ìš”ì†Œë¥¼ ë¬´í•œ ë°˜ë³µí•´ì¤€ë‹¤ . | countì™€ ë§ˆì°¬ê°€ì§€ë¡œ, ê·¸ëƒ¥ ì‚¬ìš©í•˜ë©´ ë¬´í•œí•˜ê²Œ ìˆ«ìê°€ ìƒì„±ë˜ë¯€ë¡œ ì£¼ì˜í•´ì„œ ì‚¬ìš© | . from itertools import cycle print(list(zip(cycle('ABCD'), [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]))) . [('A', 1), ('B', 2), ('C', 3), ('D', 4), ('A', 5), ('B', 6), ('C', 7), ('D', 8), ('A', 9), ('B', 10)] . | repeat(object, [times]): objectë¥¼ timesë§Œí¼ ë°˜ë³µí•œë‹¤ (timesë¥¼ ì§€ì •í•˜ì§€ ì•Šìœ¼ë©´ ë¬´í•œ ë°˜ë³µë¨) . from itertools import repeat print(list(repeat('A', 5))) # 'A'ë¥¼ 5ë²ˆ ë°˜ë³µ . ['A', 'A', 'A', 'A', 'A'] . +) ë¬´í•œ ë°˜ë³µëœë‹¤ëŠ” ì ì„ í™œìš©í•´ ì´ëŸ° ì‹ìœ¼ë¡œ ì‚¬ìš© ê°€ëŠ¥: . list(map(pow, range(10), repeat(2))) # pow(x, y): xì˜ yì œê³±ê°’ì„ return # pow: ê¸°ë³¸ ë‚´ì¥í•¨ìˆ˜ë„ ìˆê³ , math.pow()ë„ ìˆìŒ (ë‘˜ ë‹¤ ê°€ëŠ¥) . [0, 1, 4, 9, 16, 25, 36, 49, 64, 81] . | . ì¡°ê±´í˜• iterator . | filterfalse(predicate, iterable): predicate(ìˆ ì–´, ì¡°ê±´ì ˆ)ì´ Falseì¸ ìš”ì†Œë§Œ ë°˜í™˜ . from itertools import filterfalse print(list(filterfalse(lambda x: x &gt; 5, range(10)))) . [0, 1, 2, 3, 4, 5] . +) returnê°’ì´ Booleanì´ ì•„ë‹Œ ìˆ ì–´ë¶€ì˜ ê²½ìš°, ê°’ì´ 0ì¸ ê²½ìš°ë¥¼ Falseë¡œ ê°„ì£¼ . print(list(filterfalse(lambda x: x % 2, range(10)))) . [0, 2, 4, 6, 8] . | takewhile(predicate, iterable): ì¡°ê±´ì´ Trueì¸ ë™ì•ˆë§Œ ìš”ì†Œë¥¼ ë°˜í™˜ (iterableì„ ì²˜ìŒë¶€í„° íƒìƒ‰í•˜ë‹¤ê°€, ì¡°ê±´ì´ Falseê°€ ë˜ëŠ” ìˆœê°„ stop) . from itertools import takewhile print(list(takewhile(lambda x: x &gt; 5, [6, 10, 7, 2, 9, 3, 1, 11, 12]))) . [6, 10, 7] . | dropwhile(predicate, iterable): ì¡°ê±´ì´ Falseê°€ ë˜ëŠ” ìˆœê°„ë¶€í„° ìš”ì†Œë¥¼ ë°˜í™˜ (takewhileê³¼ ë°˜ëŒ€) . from itertools import dropwhile print(list(dropwhile(lambda x: x &gt; 5, [6, 10, 7, 2, 9, 3, 1, 11, 12]))) . [2, 9, 3, 1, 11, 12] . | groupby(iterable, key=None): ì—°ì†ì ì¸ í‚¤ì™€ ê·¸ë£¹ì„ ë°˜í™˜í•˜ëŠ” iterator ìƒì„± (iterableì„ ì²˜ìŒë¶€í„° íƒìƒ‰í•˜ë©°, ê°™ì€ elementê°€ ì—°ì†ì ìœ¼ë¡œ ë‚˜ì˜¬ ê²½ìš° ì´ë¥¼ ë¬¶ì–´ì¤€ë‹¤) . from itertools import groupby dict1 = {} iterator = groupby('AAABBCCCCDDEE') for i, group in iterator: dict1[i] = list(group) print(dict1) . {'A': ['A', 'A', 'A'], 'B': ['B', 'B'], 'C': ['C', 'C', 'C', 'C'], 'D': ['D', 'D'], 'E': ['E', 'E']} . +) ê°™ì€ elementì—¬ë„, ì—°ì†ì ìœ¼ë¡œ ë‚˜ì˜¤ì§€ ì•Šê³  ì‚¬ì´ì— ë‹¤ë¥¸ elementê°€ ìˆìœ¼ë©´ í•¨ê»˜ ë¬¶ì´ì§€ ì•ŠëŠ”ë‹¤ . iterator = groupby([1, 1, 1, 1, 2, 3, 1, 1, 4, 4]) # ì•ì˜ 1 ë„¤ ê°œì™€ ë’¤ì˜ 1 ë‘ ê°œëŠ” ë”°ë¡œ ë¬¶ì„ for i, group in iterator: print(i, ':', list(group)) . 1 : [1, 1, 1, 1] 2 : [2] 3 : [3] 1 : [1, 1] 4 : [4, 4] . | . ",
    "url": "https://chaelist.github.io/docs/data_handling/useful_functions/#itertools",
    "relUrl": "/docs/data_handling/useful_functions/#itertools"
  },"271": {
    "doc": "ìœ ìš©í•œ Python ë‚´ì¥í•¨ìˆ˜",
    "title": "Collections",
    "content": ": íŠ¹ìˆ˜ ì»¨í…Œì´ë„ˆ ë°ì´í„°í˜•ì„ êµ¬í˜„í•´ì£¼ëŠ” ëª¨ë“ˆ . | íŒŒì´ì¬ì˜ ë²”ìš© ë‚´ì¥ ì»¨í…Œì´ë„ˆ dict, list, set ë° tupleì— ëŒ€í•œ ëŒ€ì•ˆì„ ì œê³µí•´ì¤€ë‹¤. (namedtuple, deque ë“±) | . Counter . | Counter([iterable-or-mappine]): í•´ì‹œ ê°€ëŠ¥í•œ ê°ì²´ë¥¼ ì„¸ëŠ” ë° ì‚¬ìš©í•˜ëŠ” ë”•ì…”ë„ˆë¦¬ ì„œë¸Œ í´ë˜ìŠ¤. ìš”ì†Œê°€ ë”•ì…”ë„ˆë¦¬ í‚¤ë¡œ ì €ì¥ë˜ê³  ê°œìˆ˜ê°€ ë”•ì…”ë„ˆë¦¬ê°’ìœ¼ë¡œ ì €ì¥ë¨ . | iterableë¡œ counter ìƒì„±í•˜ëŠ” ë²• ì˜ˆì‹œ: c = Counter('gallahad') | mappingìœ¼ë¡œ counter ìƒì„±í•˜ëŠ” ë²• ì˜ˆì‹œ: c = Counter({'red': 4, 'blue': 2}) | . | . | ëŒ€ì²´ë¡œ ê° elementì˜ ë¹ˆë„ë¥¼ ì„¸ì–´ì£¼ëŠ” ë°ì— í™œìš© . from collections import Counter c = Counter('AABCDADCBADFEGEOADGOAEFBBDA') print(c) . Counter({'A': 7, 'D': 5, 'B': 4, 'E': 3, 'C': 2, 'F': 2, 'G': 2, 'O': 2}) . | c.most_common(n): ê°€ì¥ ë¹ˆë„ê°€ ë†’ì€ nê°œì˜ elementë¥¼ ë³´ì—¬ì¤Œ . c.most_common(5) . [('A', 7), ('D', 5), ('B', 4), ('E', 3), ('C', 2)] . | dictionary typeì— ì ìš©ë˜ëŠ” ê¸°ëŠ¥ë“¤ì€ ëŒ€ë¶€ë¶„ ë™ì¼í•˜ê²Œ í™œìš© ê°€ëŠ¥ . print(c.items()) . dict_items([('A', 7), ('B', 4), ('C', 2), ('D', 5), ('F', 2), ('E', 3), ('G', 2), ('O', 2)]) . | . ",
    "url": "https://chaelist.github.io/docs/data_handling/useful_functions/#collections",
    "relUrl": "/docs/data_handling/useful_functions/#collections"
  },"272": {
    "doc": "ë°ì´í„° ì‹œê°í™”",
    "title": "ë°ì´í„° ì‹œê°í™”",
    "content": " ",
    "url": "https://chaelist.github.io/docs/visualization",
    "relUrl": "/docs/visualization"
  },"273": {
    "doc": "Web Scraping",
    "title": "Web Scraping",
    "content": " ",
    "url": "https://chaelist.github.io/docs/webscraping",
    "relUrl": "/docs/webscraping"
  },"274": {
    "doc": "YouTube Trending Videos",
    "title": "YouTube Trending Videos",
    "content": ". | ë°ì´í„° íŒŒì•… ë° ì „ì²˜ë¦¬ . | ê²°ì¸¡ì¹˜, ì¤‘ë³µê°’ íŒŒì•… | category_idì— categoryëª… ì—°ê²° | datetime íƒ€ì… ë³€ê²½ | . | trending ë™ì˜ìƒ íŠ¹ì§• íŒŒì•… . | ë™ì˜ìƒë³„ trending íšŸìˆ˜ | ì›”ë³„ ì¹´í…Œê³ ë¦¬ë³„ trending ë™ì˜ìƒ ì¶”ì´ | ë“±ë¡ ìš”ì¼ë³„ / ì‹œê°„ëŒ€ë³„ ì°¨ì´ | . | trendingí•˜ê¸°ê¹Œì§€ì˜ ê¸°ê°„ ë¹„êµ . | ë°ì´í„° ê°€ê³µ | ë°ì´í„° ë¶„í¬ í™•ì¸ | ì¹´í…Œê³ ë¦¬ë³„ ë¶„í¬ í™•ì¸ | . | viewsì™€ ë‹¤ë¥¸ ë³€ìˆ˜ ê°„ì˜ ê´€ê³„ . | viewsì™€ likes | viewsì™€ dislikes | . | ê¸°ëŠ¥ ì‚¬ìš© ìœ ë¬´ì— ë”°ë¥¸ ë¹„êµ . | comments/ratings_disabled ê¸°ëŠ¥ ì‚¬ìš© ë¹„ìœ¨ | comments/ratings_disabled ê¸°ëŠ¥ì˜ viewsì—ì˜ ì˜í–¥ | views ê¸°ì¤€ ì‚¬ë¶„ìœ„ ë¶„ë¥˜ â†’ ë¶„ìœ„ë³„ ë¹„ìœ¨ í™•ì¸ | . | . *ë¶„ì„ ëŒ€ìƒ ë°ì´í„°ì…‹: YouTube Trending Videos (Korea) . | ë°ì´í„°ì…‹ ì¶œì²˜ | 2017-11-14 ~ 2018-06-14 ì‚¬ì´ì— trendingí•œ ë™ì˜ìƒ 34,567ê°œì˜ ë°ì´í„° | ì¼ë³„ trending ë™ì˜ìƒì´ ìµœì†Œ 52ê°œì—ì„œ ìµœëŒ€ 192ê°œê¹Œì§€ ì €ì¥ë˜ì–´ ìˆìœ¼ë©°, ì´í‹€ ì´ìƒ trendingí•œ ë™ì˜ìƒì˜ ê²½ìš° ì—¬ëŸ¬ ë²ˆ ì¤‘ë³µí•´ì„œ í¬í•¨ë˜ì–´ ìˆìŒ | Columns (16ê°œ): â€˜video_idâ€™, â€˜trending_dateâ€™, â€˜titleâ€™, â€˜channel_titleâ€™, â€˜category_idâ€™, â€˜publish_timeâ€™, â€˜tagsâ€™, â€˜viewsâ€™, â€˜likesâ€™, â€˜dislikesâ€™, â€˜comment_countâ€™, â€˜thumbnail_linkâ€™, â€˜comments_disabledâ€™, â€˜ratings_disabledâ€™, â€˜video_error_or_removedâ€™, â€˜descriptionâ€™ | . ",
    "url": "https://chaelist.github.io/docs/kaggle/youtube_trending/",
    "relUrl": "/docs/kaggle/youtube_trending/"
  },"275": {
    "doc": "YouTube Trending Videos",
    "title": "ë°ì´í„° íŒŒì•… ë° ì „ì²˜ë¦¬",
    "content": "# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ import import pandas as pd import numpy as np from matplotlib import pyplot as plt import seaborn as sns . ê²°ì¸¡ì¹˜, ì¤‘ë³µê°’ íŒŒì•… . videos_df = pd.read_csv('data/KRvideos.csv') videos_df.head(3) . | Â  | video_id | trending_date | title | channel_title | category_id | publish_time | tags | views | likes | dislikes | comment_count | thumbnail_link | comments_disabled | ratings_disabled | video_error_or_removed | description | . | 0 | RxGQe4EeEpA | 17.14.11 | ì¢‹ì•„ by ë¯¼ì„œ_ìœ¤ì¢…ì‹ _ì¢‹ë‹ˆ ë‹µê°€ | ë¼í‘¸ë§ˆì½”ë¦¬ì•„ | 22 | 2017-11-13T07:07:36.000Z | ë¼í‘¸ë§ˆ|\"ìœ¤ì¢…ì‹ \"|\"ì¢‹ë‹ˆ\"|\"ì¢‹ì•„\"|\"ìƒ¬ë ˆ\"|\"ë¯¼ì„œ\" | 156130 | 1422 | 40 | 272 | https://i.ytimg.com/vi/RxGQe4EeEpA/default.jpg | False | False | False | ìœ¤ì¢…ì‹  â€˜ì¢‹ë‹ˆâ€™ì˜ ë‹µê°€ â€˜ì¢‹ì•„â€™ ìµœì´ˆ ê³µê°œ!\\nê·¸ ì—¬ìì˜ ì´ì•¼ê¸°ë¥¼ ì§€ê¸ˆ ë§Œë‚˜ë³´ì„¸ìš”â€¦ (ìƒëµ) | . | 1 | hH7wVE8OlQ0 | 17.14.11 | JSA ê·€ìˆœ ë¶í•œêµ° ì´ê²© ë¶€ìƒ | Edward | 25 | 2017-11-13T10:59:16.000Z | JSA|\"ê·€ìˆœ\"|\"ë¶í•œêµ°\"|\"ì´ê²©\"|\"ë¶€ìƒ\"|\"JSA ê·€ìˆœ ë¶í•œêµ° ì´ê²© ë¶€ìƒ\" | 76533 | 211 | 28 | 113 | https://i.ytimg.com/vi/hH7wVE8OlQ0/default.jpg | False | False | False | [ì±„ë„Aë‹¨ë…]åŒ— ë³‘ì‚¬ í˜„ì¬ â€˜ì˜ì‹ë¶ˆëª…â€™â€¦ í˜ˆì•• ë–¨ì–´ì§€ëŠ” ì¤‘ \\n[ì±„ë„Aë‹¨ë…]ìš°ë¦¬ì¸¡ ì´ˆì†Œ 50m ì•ì„œ ì˜ì‹ ìƒê³  ì“°ëŸ¬ì ¸â€¦ (ìƒëµ) | . | 2 | 9V8bnWUmE9U | 17.14.11 | ë‚˜ëª°ë¼íŒ¨ë°€ë¦¬ ìš´ë™í™” ì˜ìƒ 2íƒ„ (ë¹¼ë¹¼ë¡œë°ì´ë²„ì ¼) | ë‚˜ëª°ë¼íŒ¨ë°€ë¦¬ í•«ì‡¼ | 22 | 2017-11-11T07:16:08.000Z | ì•„ë””ë‹¤ìŠ¤|\"ë¹¼ë¹¼ë¡œ\"|\"í•«ì‡¼\"|\"ë‚˜ëª°ë¼íŒ¨ë°€ë¦¬\"|\"ëŒ€í•™ë¡œ\"|\"ê³µì—°\" | 421409 | 5112 | 166 | 459 | https://i.ytimg.com/vi/9V8bnWUmE9U/default.jpg | False | False | False | í¼ê°€ì‹¤ë•Œ ê¼­ ì¶œì²˜ ë¶€íƒë“œë ¤ìš” | . | dataframe ì •ë³´ í™•ì¸: data type, nullê°’ ì—¬ë¶€ . videos_df.info() . &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 34567 entries, 0 to 34566 Data columns (total 16 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 video_id 34567 non-null object 1 trending_date 34567 non-null object 2 title 34567 non-null object 3 channel_title 34567 non-null object 4 category_id 34567 non-null int64 5 publish_time 34567 non-null object 6 tags 34567 non-null object 7 views 34567 non-null int64 8 likes 34567 non-null int64 9 dislikes 34567 non-null int64 10 comment_count 34567 non-null int64 11 thumbnail_link 34567 non-null object 12 comments_disabled 34567 non-null bool 13 ratings_disabled 34567 non-null bool 14 video_error_or_removed 34567 non-null bool 15 description 31404 non-null object dtypes: bool(3), int64(5), object(8) memory usage: 3.5+ MB . | description ì¹¼ëŸ¼ì—ë§Œ nullê°’ì´ ì¡°ê¸ˆ ìˆê³  ë‚˜ë¨¸ì§€ëŠ” ì—†ìŒ | description ì¹¼ëŸ¼ì˜ nullê°’ì€ ë¶„ì„ì— í° ì˜í–¥ì„ ì£¼ì§€ ì•ŠëŠ”ë‹¤ê³  ìƒê°ë˜ì–´ ë³„ë‹¤ë¥¸ ì²˜ë¦¬ë¥¼ í•˜ì§€ëŠ” ì•ŠìŒ. | . | ì¤‘ë³µê°’ í™•ì¸ . # ì¤‘ë³µê°’ì´ í¬í•¨ë˜ì–´ ìˆë‚˜ í™•ì¸ (ëª¨ë“  ì—´ì˜ ë°ì´í„°ê°€ ê°™ì€ ê²½ìš°) videos_df.duplicated().sum() . 2316 . â†’ ëª¨ë“  ì—´ì˜ ê°’ì´ ë‹¤ ì¤‘ë³µë˜ëŠ” ë°ì´í„°ëŠ” í•˜ë‚˜ì˜ rowë§Œ ë‚¨ê¸°ê³  ì‚­ì œ . print('ì¤‘ë³µ ì œê±° ì´ì „: ', len(videos_df)) videos_df.drop_duplicates(inplace=True, ignore_index=True) print('ì¤‘ë³µ ì œê±° ì´í›„: ', len(videos_df)) . | . category_idì— categoryëª… ì—°ê²° . | KR_category_id.json íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸° . import json with open(\"data/KR_category_id.json\", \"r\", encoding=\"utf-8\") as f: json_data = json.load(f) json_data . {'kind': 'youtube#videoCategoryListResponse', 'etag': '\"XI7nbFXulYBIpL0ayR_gDh3eu1k/1v2mrzYSYG6onNLt2qTj13hkQZk\"', 'items': [{'kind': 'youtube#videoCategory', 'etag': '\"XI7nbFXulYBIpL0ayR_gDh3eu1k/Xy1mB4_yLrHy_BmKmPBggty2mZQ\"', 'id': '1', 'snippet': {'channelId': 'UCBR8-60-B28hp2BmDPdntcQ', 'title': 'Film &amp; Animation', 'assignable': True}}, {'kind': 'youtube#videoCategory', 'etag': '\"XI7nbFXulYBIpL0ayR_gDh3eu1k/UZ1oLIIz2dxIhO45ZTFR3a3NyTA\"', 'id': '2', 'snippet': {'channelId': 'UCBR8-60-B28hp2BmDPdntcQ', 'title': 'Autos &amp; Vehicles', 'assignable': True}}, (ìƒëµ)} . | category_id_dfì— category_idì™€ category_name ì¹¼ëŸ¼ ì •ë¦¬ . temp_list = [] for item in json_data['items']: temp_list.append([int(item['id']), item['snippet']['title']]) category_id_df = pd.DataFrame(temp_list, columns=['category_id', 'category_name']) print(len(category_id_df)) category_id_df.head() . | Â  | category_id | category_name | . | 0 | 1 | Film &amp; Animation | . | 1 | 2 | Autos &amp; Vehicles | . | 2 | 10 | Music | . | 3 | 15 | Pets &amp; Animals | . | 4 | 17 | Sports | . | videos_dfì— category_id_df ê²°í•© . videos_df = pd.merge(videos_df, category_id_df, on='category_id', how='left') videos_df[['title', 'category_id', 'category_name']].head() . | Â  | title | category_id | category_name | . | 0 | ì¢‹ì•„ by ë¯¼ì„œ_ìœ¤ì¢…ì‹ _ì¢‹ë‹ˆ ë‹µê°€ | 22 | People &amp; Blogs | . | 1 | JSA ê·€ìˆœ ë¶í•œêµ° ì´ê²© ë¶€ìƒ | 25 | News &amp; Politics | . | 2 | ë‚˜ëª°ë¼íŒ¨ë°€ë¦¬ ìš´ë™í™” ì˜ìƒ 2íƒ„ (ë¹¼ë¹¼ë¡œë°ì´ë²„ì ¼) | 22 | People &amp; Blogs | . | 3 | á„‹á…µá„†á…§á†¼á„‡á…¡á†¨ ì¶œêµ­ í˜„ì¥, ë†“ì¹˜ë©´ ì•ˆë˜ëŠ” ì¥ë©´ | 25 | News &amp; Politics | . | 4 | ê¹€ì¥ê²¸ì€ ë¬¼ëŸ¬ê°”ë‹¤ MBC ë…¸ì¡° í™˜í˜¸ì™€ ëˆˆë¬¼ | 25 | News &amp; Politics | . â†’ nullê°’ì´ ëª‡ ê°œ ìƒê²¼ë‚˜ í™•ì¸ . videos_df['category_name'].isnull().sum() . 269 . â†’ nullê°’ì˜ ì›ì¸ íŒŒì•…: json íŒŒì¼ì— 29ë²ˆì´ ì—†ì–´ì„œ ì°¨ì´ê°€ ìƒê¸´ ê²ƒ . set(videos_df['category_id']) - set(category_id_df['category_id']) . {29} . â†’ category_name ì¹¼ëŸ¼ì˜ nullê°’ì— â€˜N/Aâ€™ë¼ëŠ” stringê°’ì„ ë„£ì–´ì¤Œ . videos_df['category_name'].fillna('N/A', inplace=True) videos_df['category_name'].isnull().sum() # nullê°’ì´ ì˜ ëŒ€ì²´ë˜ì—ˆë‚˜ í™•ì¸ . 0 . | . datetime íƒ€ì… ë³€ê²½ . | trending_dateì™€ publish_timeì„ datetime íƒ€ì…ìœ¼ë¡œ ë°”ê¿”ì¤Œ | . # datetime íƒ€ì…ìœ¼ë¡œ ë°”ê¿”ì¤Œ videos_df['trending_date'] = pd.to_datetime(videos_df['trending_date'], format='%y.%d.%m') videos_df['publish_time'] = pd.to_datetime(videos_df['publish_time']) # typeì´ ì˜ ë°”ë€Œì—ˆëŠ”ì§€ í™•ì¸ videos_df[['trending_date', 'publish_time']].dtypes . trending_date datetime64[ns] publish_time datetime64[ns, UTC] dtype: object . ",
    "url": "https://chaelist.github.io/docs/kaggle/youtube_trending/#%EB%8D%B0%EC%9D%B4%ED%84%B0-%ED%8C%8C%EC%95%85-%EB%B0%8F-%EC%A0%84%EC%B2%98%EB%A6%AC",
    "relUrl": "/docs/kaggle/youtube_trending/#ë°ì´í„°-íŒŒì•…-ë°-ì „ì²˜ë¦¬"
  },"276": {
    "doc": "YouTube Trending Videos",
    "title": "trending ë™ì˜ìƒ íŠ¹ì§• íŒŒì•…",
    "content": "ë™ì˜ìƒë³„ trending íšŸìˆ˜ . trending_count = videos_df.groupby('title')[['trending_date']].count().reset_index() trending_count.sort_values(by='trending_date', ascending=False, inplace=True) trending_count.head() . | Â  | title | trending_date | . | 11072 | ë¸Œë²  ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° | 19 | . | 13718 | ì´ì¬ìš© ì—¬ìê´€ê³„ì™€ ì—°ì˜ˆì¸, ë°°ìš° ì´ë‚˜ì˜, ìœ¤ì€í˜œê°€, ì¡°ìˆ˜ë¹ˆ | 9 | . | 2074 | Bruno Mars,Charlie Puth,Ed Sheeran Best Christmas Songs,Greatest Hits Pop Playlist Christmas 2018 | 9 | . | 2751 | Marvel Studiosâ€™ Avengers: Infinity War Official Trailer | 8 | . | 8552 | ë‚˜ì–¼ (Naul) - ê¸°ì–µì˜ ë¹ˆìë¦¬ (Emptiness in Memory) MV | 8 | . *í†µê³„ëŸ‰ í™•ì¸ . print(trending_count.describe()) . trending_date count 16353.000000 mean 1.972176 std 1.068082 min 1.000000 25% 1.000000 50% 2.000000 75% 3.000000 max 19.000000 . *ì‹œê°í™”í•´ì„œ í™•ì¸ . fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12, 4)) sns.kdeplot(trending_count['trending_date'], bw_method=0.5, color='skyblue', ax=ax1) sns.countplot(data=trending_count, x=\"trending_date\", color='skyblue', ax=ax2) plt.close(2) plt.close(3) plt.tight_layout() . | 1-2ë²ˆ trendingí•œ ë™ì˜ìƒì´ ê°€ì¥ ë§ìœ¼ë©°, ì „ì²´ì˜ 75% ì´ìƒì´ 3ë²ˆ ì´í•˜ trending | . ì›”ë³„ ì¹´í…Œê³ ë¦¬ë³„ trending ë™ì˜ìƒ ì¶”ì´ . | month ë‹¨ìœ„ë¡œ ì •ë¦¬ . ## ì—°-ì›”ë¡œë§Œ trending_dateë¥¼ ë‹¤ì‹œ ì •ë¦¬ videos_df['trending_month'] = videos_df['trending_date'].dt.strftime('%Y%m') videos_df[['trending_date', 'trending_month']].head() . | Â  | trending_date | trending_month | . | 0 | 2017-11-14 | 201711 | . | 1 | 2017-11-14 | 201711 | . | 2 | 2017-11-14 | 201711 | . | 3 | 2017-11-14 | 201711 | . | 4 | 2017-11-14 | 201711 | . | ì›”ë³„ ì¹´í…Œê³ ë¦¬ë³„ trending video ìˆ˜ ì§‘ê³„ . # nunique()ë¡œ, ì¤‘ë³µì„ ì œê±°í•´ ì§‘ê³„ (ê°™ì€ monthì— ê°™ì€ titleì˜ ì˜ìƒì´ 2ë²ˆ ì´ìƒ trendingí•œ ê²½ìš°, 1ë²ˆìœ¼ë¡œ count) groupby_df1 = copied_df1.groupby(['trending_month', 'category_name'])[['title']].nunique().reset_index() groupby_df1.rename(columns={'title': 'count'}, inplace=True) groupby_df1.head() . | Â  | trending_month | category_name | count | . | 0 | 201711 | Autos &amp; Vehicles | 12 | . | 1 | 201711 | Comedy | 62 | . | 2 | 201711 | Education | 19 | . | 3 | 201711 | Entertainment | 354 | . | 4 | 201711 | Film &amp; Animation | 111 | . | ì‹œê°í™”í•´ì„œ ì›”ë³„ ì¶”ì´ë¥¼ í™•ì¸ . plt.figure(figsize=(12, 5)) sns.lineplot(data=groupby_df1, x='trending_month', y='count', hue='category_name', palette='Set3') # legendë¥¼ box ë°–ìœ¼ë¡œ ë¹¼ ì¤Œ plt.legend(bbox_to_anchor=(1.01, 1), borderaxespad=0); . | ëª¨ë“  ì›”ì— Entertainment, News &amp; Politics, People &amp; Blogs ì¹´í…Œê³ ë¦¬ê°€ ê°€ì¥ ì¸ê¸°ê°€ ìˆìŒì„ ì•Œ ìˆ˜ ìˆë‹¤ | 2018.03ì›”ì„ ê¸°ì ìœ¼ë¡œ News &amp; Politics ì¹´í…Œê³ ë¦¬ê°€ People &amp; Blogs ì¹´í…Œê³ ë¦¬ì˜ ì¸ê¸°ë¥¼ ì¶”ì›” | 2017-11-14 ~ 2018-06-14 ì‚¬ì´ì˜ datasetì´ê¸° ë•Œë¬¸ì— 201711ê³¼ 201806ì€ ëŒ€ë¶€ë¶„ì˜ ì¹´í…Œê³ ë¦¬ì—ì„œ trending ë™ì˜ìƒ ìˆ˜ê°€ ë‚®ê²Œ ë‚˜ì˜´ | . | . ë“±ë¡ ìš”ì¼ë³„ / ì‹œê°„ëŒ€ë³„ ì°¨ì´ . # ë“±ë¡ëœ 'ìš”ì¼'ì„ ë³„ë„ ì¹¼ëŸ¼ìœ¼ë¡œ ì €ì¥ (0: ì›” ~ 6: ì¼) videos_df['publish_weekday'] = videos_df['publish_time'].dt.weekday # ë“±ë¡ëœ 'ì‹œê°„ëŒ€'ë¥¼ ë³„ë„ ì¹¼ëŸ¼ìœ¼ë¡œ ì €ì¥ (0 ~ 24) videos_df['publish_hour'] = videos_df['publish_time'].dt.hour videos_df[['publish_time', 'publish_weekday', 'publish_hour']].head() . | Â  | publish_time | publish_weekday | publish_hour | . | 0 | 2017-11-13 07:07:36+00:00 | 0 | 7 | . | 1 | 2017-11-13 10:59:16+00:00 | 0 | 10 | . | 2 | 2017-11-11 07:16:08+00:00 | 5 | 7 | . | 3 | 2017-11-12 11:19:52+00:00 | 6 | 11 | . | 4 | 2017-11-13 11:08:59+00:00 | 0 | 11 | . | ë“±ë¡ ìš”ì¼ë³„ ë¹„êµ sns.countplot(data=videos_df, x='publish_weekday', color='skyblue'); . | ê¸ˆìš”ì¼ì— publishëœ ë™ì˜ìƒì´ ë¹„êµì  trendingí•œ ìˆ˜ê°€ ë§ì€ í¸ | í•œ ë™ì˜ìƒì´ ì—¬ëŸ¬ ë²ˆ trendingí•œ ê²½ìš°ë„ í¬í•¨í•´ì„œ ê³„ì‚° | . | ë“±ë¡ ì‹œê°„ëŒ€ë³„ ë¹„êµ . groupby_df2 = videos_df.groupby(['publish_hour'])[['video_id']].count().reset_index() plt.figure(figsize=(10, 6)) sns.lineplot(data=groupby_df2, x='publish_hour', y='video_id', color='pink'); . | ì˜¤ì „ 9ì‹œ ì „í›„ì— publishëœ ë™ì˜ìƒì´ trendingí•œ ìˆ˜ê°€ ë§ì€ í¸ | . Â  . +) ì‹œê°„ëŒ€ë³„, ìš”ì¼ë³„ ì°¨ì´ ë¹„êµ . groupby_df3 = videos_df.groupby(['publish_hour', 'publish_weekday'])[['video_id']].count().reset_index() plt.figure(figsize=(10, 6)) sns.lineplot(data=groupby_df3, x='publish_hour', y='video_id', hue='publish_weekday'); . | íŠ¹íˆ ê¸ˆìš”ì¼ ì˜¤ì „ 9ì‹œê²½ì— publishëœ ë™ì˜ìƒì´ ë¹„êµì  trendingí•œ ìˆ˜ê°€ ë§ì€ í¸ | . | . ",
    "url": "https://chaelist.github.io/docs/kaggle/youtube_trending/#trending-%EB%8F%99%EC%98%81%EC%83%81-%ED%8A%B9%EC%A7%95-%ED%8C%8C%EC%95%85",
    "relUrl": "/docs/kaggle/youtube_trending/#trending-ë™ì˜ìƒ-íŠ¹ì§•-íŒŒì•…"
  },"277": {
    "doc": "YouTube Trending Videos",
    "title": "trendingí•˜ê¸°ê¹Œì§€ì˜ ê¸°ê°„ ë¹„êµ",
    "content": ". | ë™ì˜ìƒë“¤ì´ ëŒ€ì²´ë¡œ publish í›„ trendingí•˜ê¸°ê¹Œì§€ ì–´ëŠ ì •ë„ì˜ ê¸°ê°„ì´ ê±¸ë ¸ëŠ”ì§€ í™•ì¸ | . ë°ì´í„° ê°€ê³µ . # publish ~ trending ì‚¬ì´ì˜ ê¸°ê°„ì„ ê³„ì‚°: integerë¡œ ì €ì¥ (ë‹¨ìœ„: days) ## dt.dateë¡œ ë‘ ì¹¼ëŸ¼ì˜ typeì„ í†µì¼í•´ì¤˜ì•¼ ë¹¼ì„œ ê¸°ê°„ì„ ê³„ì‚°í•˜ëŠ” ê²Œ ê°€ëŠ¥ videos_df['publish_to_trending'] = (videos_df['trending_date'].dt.date - videos_df['publish_time'].dt.date).dt.days videos_df[['trending_date', 'publish_time', 'publish_to_trending']].head() . | Â  | trending_date | publish_time | publish_to_trending | . | 0 | 2017-11-14 | 2017-11-13 07:07:36+00:00 | 1 | . | 1 | 2017-11-14 | 2017-11-13 10:59:16+00:00 | 1 | . | 2 | 2017-11-14 | 2017-11-11 07:16:08+00:00 | 3 | . | 3 | 2017-11-14 | 2017-11-12 11:19:52+00:00 | 2 | . | 4 | 2017-11-14 | 2017-11-13 11:08:59+00:00 | 1 | . *ê°™ì€ video_id &amp; title, ë‹¤ë¥¸ trending_dateë¥¼ ê°€ì§„ ë°ì´í„° ì‚­ì œ . | â€» â€˜ì˜ìƒ ë“±ë¡ í›„ trendingë˜ê¸°ê¹Œì§€ì˜ ê¸°ê°„ì´ ë³´í†µ ì–´ëŠ ì •ë„ ê±¸ë¦¬ëŠ”ì§€â€™ë¥¼ í™•ì¸í•˜ê³ ì í•˜ëŠ” ê²ƒì´ê¸°ì—, í•˜ë‚˜ì˜ ë™ì˜ìƒì´ ì²˜ìŒìœ¼ë¡œ trendingí•œ ì‹œì ì˜ ë°ì´í„°ë§Œ ê°€ì§€ê³  ë¶„í¬ë¥¼ í™•ì¸í•˜ê¸°ë¡œ. â†’ ì²˜ìŒìœ¼ë¡œ trendingí•œ ì‹œì ì˜ ë°ì´í„°ë§Œ ë‚¨ê¸°ê³  ì¤‘ë³µê°’ ì‚­ì œ (= ì¤‘ë³µëœ í–‰ë“¤ ì¤‘ ê°€ì¥ ìœ„ì— ìˆëŠ” í–‰ë§Œ ë‚¨ê¸°ê³  ë‹¤ ì‚­ì œ) | . # trending_date ê¸°ì¤€ìœ¼ë¡œ ì˜¤ë¦„ì°¨ìˆœ ì •ë ¬ videos_df_sorted = videos_df.sort_values(by='trending_date', ignore_index=True) # ì˜ìƒë³„ë¡œ ê°€ì¥ ë¨¼ì € trendingí•œ ë‚ ì˜ ë°ì´í„°ë§Œ ë‚¨ê¸´ ë°ì´í„°ì…‹ì„ unique_videos_dfì— ìƒˆë¡œ ì €ì¥ unique_videos_df = videos_df_sorted.drop_duplicates(subset=['video_id', 'title'], ignore_index=True) print('videos_df: ', len(videos_df)) print('unique_videos_df: ', len(unique_videos_df)) . videos_df: 32251 unique_videos_df: 16393 . ë°ì´í„° ë¶„í¬ í™•ì¸ . plt.figure(figsize=(20,8)) sns.countplot(data=unique_videos_df, x=\"publish_to_trending\", color='skyblue'); . | ëŒ€ì²´ë¡œ ì˜ìƒ ë“±ë¡ í›„ trendingë˜ê¸°ê¹Œì§€ì˜ ê¸°ê°„ì´ 3ì¼ ì´ë‚´ì´ë©°, íŠ¹íˆ ë“±ë¡ í›„ í•˜ë£¨ ë§Œì— trendingí•˜ëŠ” ê²½ìš°ê°€ ê°€ì¥ ë§ìŒ | . sns.catplot(data=unique_videos_df, x='publish_to_trending', height=5, aspect=3, color='skyblue'); . | ì•½ 20ì¼ ì •ë„ë¥¼ ë„˜ì–´ì„œë©´, ê·¸ ì´í›„ì—ëŠ” ìƒë‹¹íˆ outlier ê°„ì˜ ê°„ê²©ì´ í¬ê²Œ ë¶„í¬ | . ì¹´í…Œê³ ë¦¬ë³„ ë¶„í¬ í™•ì¸ . | ì–´ëŠ ì¹´í…Œê³ ë¦¬ì— outlierê°€ ë§ì´ í¬ì§„ë˜ì–´ ìˆëŠ”ì§€, publish í›„ trendingí•˜ê¸°ê¹Œì§€ì˜ í‰ê·  ê¸°ê°„ì´ ì–´ë–»ê²Œ ë‹¤ë¥¸ì§€ í™•ì¸ | . # ì–´ëŠ ì¹´í…Œê³ ë¦¬ì— outlierê°€ ë§ì´ í¬ì§„ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸ sns.catplot(data=unique_videos_df, x='category_name', y='publish_to_trending', palette='Blues_d', height=5, aspect=3) plt.xticks(rotation=45); . | íŠ¹íˆ Music, People &amp; Blogs, Pets &amp; Animals, Film &amp; Animation ì¹´í…Œê³ ë¦¬ì—ì„œ ì •ìƒ ë²”ì£¼ë¥¼ í¬ê²Œ ë²—ì–´ë‚˜ëŠ” ë°ì´í„°ê°€ ë°œê²¬ë¨ | í¬ê²Œ ìœ í–‰ì„ íƒ€ì§€ ì•Šê³  ì¸ê¸°ë¥¼ ëŒ ìˆ˜ ìˆëŠ” ì¹´í…Œê³ ë¦¬ì´ê¸° ë•Œë¬¸ì— trendingë˜ê¸°ê¹Œì§€ì˜ ê¸°ê°„ì´ ê¸´ ê²½ìš°ê°€ ë°œê²¬ë˜ëŠ” í¸ì´ë¼ê³  ìƒê°ë¨ | . # ì¹´í…Œê³ ë¦¬ë³„ í‰ê·  publish~trending ê¸°ê°„ ë¹„êµ (ì„ ì€ 95% ì‹ ë¢°êµ¬ê°„) plt.figure(figsize=(20, 6)) sns.barplot(data=unique_videos_df, x='category_name', y='publish_to_trending', palette='Blues_d'); plt.xticks(rotation=45); . | Music, Pets &amp; Animals ë“±ì€ í¸ì°¨ê°€ í° ë°˜ë©´, Entertainment, News &amp; Politics, Sports, Gaming ë“± ì‹œì˜ì„±ì´ ì¤‘ìš”í•  ìˆ˜ ìˆëŠ” ì˜ìƒë“¤ì€ ëŒ€ì²´ë¡œ ë¹ ë¥´ì‹  ì‹œì¼ ë‚´ì— trendingí•œ ê²½ìš°ê°€ ë§ìŒ. | . # Music ì¹´í…Œê³ ë¦¬ì˜ outlier top3 í™•ì¸ videos_music_df = unique_videos_df[unique_videos_df['category_name'] == 'Music'] videos_music_df.sort_values(by='publish_to_trending', ascending=False)[['title', 'trending_date', 'publish_time', 'publish_to_trending']].head(3) . | Â  | title | trending_date | publish_time | publish_to_trending | . | 7864 | ì†ì„±ì œ - Goodbye | 2018-02-19 00:00:00 | 2011-09-29 13:08:39+00:00 | 2335 | . | 1695 | Taylor swift - See You Again | 2017-12-02 00:00:00 | 2017-02-20 14:25:18+00:00 | 285 | . | 12731 | ê°ˆìˆ˜ë¡ ì–´ë ¤ì›Œì§€ëŠ” ì “ê°€ë½ í–‰ì§„ê³¡ì„ ì´ë ‡ê²Œ ì¹œë‹¤ê³ ???? | 2018-04-28 00:00:00 | 2018-04-09 16:36:09+00:00 | 19 | . | Music ì¹´í…Œê³ ë¦¬ëŠ” íŠ¹ì • ê³„ê¸°ë¡œ ì¬ë°œê²¬ë˜ì–´ ì¸ê¸°ë¥¼ ì–»ê²Œ ë˜ëŠ” ìŒì•… ì˜ìƒë“¤ì´ ì¡´ì¬í•´ ìœ ë… ë¶„í¬ì˜ ì •ìƒ ë²”ìœ„ë¥¼ í¬ê²Œ ë²—ì–´ë‚˜ëŠ” outlierê°€ ë°œê²¬ë˜ëŠ” í¸ì´ë¼ê³  ìƒê°ë¨ | ì˜ˆë¥¼ ë“¤ì–´, trendingê¹Œì§€ì˜ ê¸°ê°„ì´ ê°€ì¥ ê¸¸ê²Œ ë‚˜íƒ€ë‚œ Music ì¹´í…Œê³ ë¦¬ì˜ â€˜ì†ì„±ì œ-Goodbyeâ€™ ì˜ìƒì˜ ê²½ìš°, 2018.02 â€˜íš¨ë¦¬ë„¤ ë¯¼ë°•â€™ ë°©ì†¡ì„ í†µí•´ ì¬ë°œê²¬ë˜ë©´ì„œ trending ì˜ìƒì— í¬í•¨ëœ ê²ƒìœ¼ë¡œ ì¶”ì • | . ",
    "url": "https://chaelist.github.io/docs/kaggle/youtube_trending/#trending%ED%95%98%EA%B8%B0%EA%B9%8C%EC%A7%80%EC%9D%98-%EA%B8%B0%EA%B0%84-%EB%B9%84%EA%B5%90",
    "relUrl": "/docs/kaggle/youtube_trending/#trendingí•˜ê¸°ê¹Œì§€ì˜-ê¸°ê°„-ë¹„êµ"
  },"278": {
    "doc": "YouTube Trending Videos",
    "title": "viewsì™€ ë‹¤ë¥¸ ë³€ìˆ˜ ê°„ì˜ ê´€ê³„",
    "content": "# ìƒê´€ê³„ìˆ˜ í™•ì¸ (heatmapìœ¼ë¡œ ì‹œê°í™”í•´ì„œ í™•ì¸) sns.heatmap(videos_df[['views', 'likes', 'dislikes', 'comment_count']].corr(), annot=True, cmap='Blues') plt.yticks(rotation=0); . | likesì™€ comment_count, viewsê°€ ê°•í•œ ìƒê´€ê´€ê³„ë¥¼ ë³´ì´ë©°, likesëŠ” comment_countì™€ ë§¤ìš° ê°•í•œ ìƒê´€ê´€ê³„ë¥¼ ë³´ì¸ë‹¤ | . viewsì™€ likes . : viewsê°€ ë§ì€ ì˜ìƒì¼ìˆ˜ë¡ likesë„ ë§ì„ê¹Œ? . # ratings_disabled ê¸°ëŠ¥ì´ ì¼œì ¸ ìˆìœ¼ë©´ likes / dislikesë¥¼ ëˆ„ë¥´ëŠ” ê²ƒì´ ë¶ˆê°€ëŠ¥ print(videos_df[videos_df['ratings_disabled'] == True][['likes', 'dislikes']].describe()) . likes dislikes count 1308.00 1308.00 mean 0.00 0.00 std 0.00 0.00 min 0.00 0.00 25% 0.00 0.00 50% 0.00 0.00 75% 0.00 0.00 max 0.00 0.00 . â†’ ratings_diabled ê¸°ëŠ¥ì´ ì‚¬ìš©ëœ ì˜ìƒë“¤ì„ ì œì™¸í•˜ê³  ì§„í–‰ . # ratings_disabled ê¸°ëŠ¥ì„ ì‚¬ìš©í•œ ì˜ìƒë“¤ì„ ì œì™¸í•œ ë‚˜ë¨¸ì§€ ë°ì´í„°ë¥¼ rated_videos_dfì— ì €ì¥ rated_videos_df = videos_df[~videos_df['ratings_disabled']] rated_videos_df.reset_index(drop=True, inplace=True) # reset index print('videos_df: ', len(videos_df)) print('rated_videos_df: ', len(rated_videos_df)) . videos_df: 32251 rated_videos_df: 30943 . | ìƒê´€ê³„ìˆ˜ ë° ì„ í˜•íšŒê·€ì„  í™•ì¸ . import scipy.stats as stats # í”¼ì–´ìŠ¨ ìƒê´€ê³„ìˆ˜ ê²€ì • corr = stats.pearsonr(rated_videos_df['views'], rated_videos_df['likes']) print('Corr_Coefficient : %.3f \\np-value : %.3f' % (corr)) . Corr_Coefficient : 0.858 p-value : 0.000 . | ê°•í•œ ìƒê´€ê´€ê³„ë¥¼ ë³´ì´ëŠ” ê²ƒì„ í™•ì¸ (p-value &lt; 0.01) | . # viewì™€ likes ê°„ì˜ ê´€ê³„ í™•ì¸: ì„ í˜•íšŒê·€ì„  í™•ì¸ sns.lmplot(data=rated_videos_df, x='views', y='likes', height=5, aspect=1.5); ## default: ci=95 (95% ì‹ ë¢°êµ¬ê°„) . | ì¹´í…Œê³ ë¦¬ë³„ ì°¨ì´ í™•ì¸ . sns.lmplot(data=rated_videos_df, x='views', y='likes', hue='category_name', palette='Set3', height=5, aspect=1.5); . | ì¹´í…Œê³ ë¦¬ë³„ë¡œ ë‚˜ëˆ„ë©´ ë³´ë‹¤ ê°ê°ì— ì˜ ë§ëŠ” ì„ í˜•íšŒê·€ì„ ì„ ê·¸ë¦¬ëŠ” ê²ƒì´ ê°€ëŠ¥ | . ## Music ì¹´í…Œê³ ë¦¬ sns.heatmap(videos_df[videos_df['category_name'] == 'Music'][['views', 'likes', 'dislikes', 'comment_count']].corr(), annot=True, cmap='Blues') plt.yticks(rotation=0); . ## Entertainment ì¹´í…Œê³ ë¦¬ sns.heatmap(videos_df[videos_df['category_name'] == 'Entertainment'][['views', 'likes', 'dislikes', 'comment_count']].corr(), annot=True, cmap='Blues') plt.yticks(rotation=0); . ## Film &amp; Animation ì¹´í…Œê³ ë¦¬ sns.heatmap(videos_df[videos_df['category_name'] == 'Film &amp; Animation'][['views', 'likes', 'dislikes', 'comment_count']].corr(), annot=True, cmap='Blues') plt.yticks(rotation=0); . | ì¹´í…Œê³ ë¦¬ë³„ë¡œ ë‚˜ëˆ„ì–´ì„œ í™•ì¸í•˜ë©´, viewsì™€ likes ê°„ì˜ ìƒê´€ê´€ê³„ê°€ ë³´ë‹¤ ê°•í•˜ê²Œ í™•ì¸ëœë‹¤ | . | . viewsì™€ dislikes . : viewsê°€ ë§ì€ ì˜ìƒì¼ìˆ˜ë¡ dislikesë„ ë§ì„ê¹Œ? . | ì¹´í…Œê³ ë¦¬ë³„ë¡œ, viewsì™€ dislikesì˜ ê´€ê³„ë¥¼ í™•ì¸ . sns.lmplot(data=rated_videos_df, x='views', y='dislikes', hue='category_name', palette='Set3', height=5, aspect=1.5); . | â€˜Entertainmentâ€™ ì¹´í…Œê³ ë¦¬ì˜ 4ê°œ ì ì„ ì œì™¸í•˜ë©´ ëŒ€ì²´ë¡œ ìƒê´€ê´€ê³„ê°€ ìˆì–´ ë³´ì„ | . | dislikeê°€ ë§ì€ ì˜ìƒì´ ì–´ë–¤ ê²ƒì¸ì§€ í™•ì¸ . # dislikes ë§ì€ ì˜ìƒë¶€í„° ìˆœì„œëŒ€ë¡œ ì •ë ¬ videos_df.sort_values(by='dislikes', ascending=False)[['video_id', 'title', 'channel_title', 'trending_date', 'category_name']].head() . | Â  | video_id | title | channel_title | trending_date | category_name | Â  | . | 4716 | FlsCjmMhFmw | YouTube Rewind: The Shape of 2017 | #YouTubeRewind | YouTube Spotlight | 2017-12-11 | Entertainment | . | 4538 | FlsCjmMhFmw | YouTube Rewind: The Shape of 2017 | #YouTubeRewind | YouTube Spotlight | 2017-12-10 | Entertainment | . | 4313 | FlsCjmMhFmw | YouTube Rewind: The Shape of 2017 | #YouTubeRewind | YouTube Spotlight | 2017-12-09 | Entertainment | . | 4104 | FlsCjmMhFmw | YouTube Rewind: The Shape of 2017 | #YouTubeRewind | YouTube Spotlight | 2017-12-08 | Entertainment | . | 29013 | 7C2z4GqqS5E | BTS (ë°©íƒ„ì†Œë…„ë‹¨) â€˜FAKE LOVEâ€™ Official MV | ibighit | 2018-05-24 | Music | Â  | . | ê°€ì¥ dislikeê°€ ë§ì•˜ë˜ ì˜ìƒ 4ê°œëŠ” ëª¨ë‘ ê°™ì€ ë™ì˜ìƒ. | . | dislikeê°€ ë§ì€ Entertainment ì¹´í…Œê³ ë¦¬ì˜ ì˜ìƒ 1ê°œ (= 4ê°œ row)ë¥¼ ì œì™¸í•˜ê³  ìƒê´€ê´€ê³„ë¥¼ íŒŒì•… . # í”¼ì–´ìŠ¨ ìƒê´€ê³„ìˆ˜ ê²€ì • drop_index = rated_videos_df.sort_values(by='dislikes', ascending=False).head(4).index dl_outlier_removed_videos_df = rated_videos_df.drop(drop_index, axis='index') corr = stats.pearsonr(dl_outlier_removed_videos_df['views'], dl_outlier_removed_videos_df['dislikes']) print('Corr_Coefficient : %.3f \\np-value : %.3f' % (corr)) . Corr_Coefficient : 0.836 p-value : 0.000 . | dislikesì˜ outlier 4ê°œ ì ì„ ì œì™¸í•˜ê³  ë‚˜ë‹ˆ ìƒê´€ê³„ìˆ˜ê°€ ê°•í•´ì§„ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤ | . | outlierë¥¼ ì œì™¸í•œ ê´€ê³„ë¥¼ ì‹œê°í™”í•´ì„œ íŒŒì•… . sns.lmplot(data=dl_outlier_removed_videos_df, x='views', y='dislikes', hue='category_name', palette='Set3', height=5, aspect=1.5); . | outlier 4ê°œ ì ì„ ì œì™¸í•˜ê³  ë³´ë©´, ì¹´í…Œê³ ë¦¬ë³„ë¡œ í˜„ìƒì„ ì˜ ì„¤ëª…í•˜ëŠ” ì„ í˜•íšŒê·€ì„ ì„ ê·¸ë¦¬ëŠ” ê²ƒì´ ê°€ëŠ¥ | . | . ",
    "url": "https://chaelist.github.io/docs/kaggle/youtube_trending/#views%EC%99%80-%EB%8B%A4%EB%A5%B8-%EB%B3%80%EC%88%98-%EA%B0%84%EC%9D%98-%EA%B4%80%EA%B3%84",
    "relUrl": "/docs/kaggle/youtube_trending/#viewsì™€-ë‹¤ë¥¸-ë³€ìˆ˜-ê°„ì˜-ê´€ê³„"
  },"279": {
    "doc": "YouTube Trending Videos",
    "title": "ê¸°ëŠ¥ ì‚¬ìš© ìœ ë¬´ì— ë”°ë¥¸ ë¹„êµ",
    "content": ". | comments_disabledì™€ ratings_disabled ê¸°ëŠ¥ì„ ì‚¬ìš©í•œ ë¹„ìœ¨ í™•ì¸ &amp; ê¸°ëŠ¥ ì‚¬ìš© ì—¬ë¶€ê°€ viewsì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ í™•ì¸ | . comments/ratings_disabled ê¸°ëŠ¥ ì‚¬ìš© ë¹„ìœ¨ . | â€» â€˜trending ë™ì˜ìƒ ì¤‘ ëª‡%ê°€ ê° ê¸°ëŠ¥ì„ ì‚¬ìš©í–ˆëŠ”ì§€â€™ë¥¼ íŒŒì•…í•˜ê¸° ìœ„í•´ì„œëŠ” ê°™ì€ ë™ì˜ìƒ ë°ì´í„°ê°€ ì¤‘ë³µë˜ì–´ í¬í•¨ë˜ë©´ ì•ˆëœë‹¤ê³  ìƒê° â†’ ê°™ì€ ë™ì˜ìƒì¸ë° ì—¬ëŸ¬ ë²ˆ trendingí•´ì„œ ë°ì´í„°ê°€ ì—¬ëŸ¬ ë²ˆ í¬í•¨ëœ ê²½ìš°ëŠ” ì œì™¸ (ìœ„ì—ì„œ ì €ì¥í•´ë‘ì—ˆë˜ unique_videos_dfë¥¼ ì‚¬ìš©) | . comments_disabled_percentage = len(unique_videos_df[unique_videos_df['comments_disabled']]) / len(unique_videos_df) * 100 ratings_disabled_percentage = len(unique_videos_df[unique_videos_df['ratings_disabled']]) / len(unique_videos_df) * 100 print(f'comments_disabled ë¹„ìœ¨: {comments_disabled_percentage :.2f}%') print(f'ratings_disabled ë¹„ìœ¨: {ratings_disabled_percentage :.2f}%') . comments_disabled ë¹„ìœ¨: 1.53% ratings_disabled ë¹„ìœ¨: 4.57% . | trendingí•œ ì˜ìƒ ì¤‘ ratings_disabledë‚˜ comments_disabled ê¸°ëŠ¥ì„ ì‚¬ìš©í•œ ì˜ìƒì„ í˜„ì €íˆ ì ê¸´ í•˜ì§€ë§Œ, ì „ì²´ ë™ì˜ìƒ ì¤‘ ë¹„ìœ¨ì„ ëª¨ë¥´ê¸° ë•Œë¬¸ì— ì–´ë–¤ ê²°ë¡ ì„ ì´ëŒì–´ë‚´ëŠ” ê²ƒì€ ë¶ˆê°€ëŠ¥â€¦ | . comments/ratings_disabled ê¸°ëŠ¥ì˜ viewsì—ì˜ ì˜í–¥ . | â€» ì˜ìƒë³„ë¡œ dataë¥¼ 1ê°œë§Œ ë‚¨ê¸°ë˜, ê°€ì¥ ë§ˆì§€ë§‰ìœ¼ë¡œ trendingí•œ ë‚ ì˜ ë°ì´í„°(=ë°ì´í„°ì…‹ ë‚´ í•´ë‹¹ ì˜ìƒì˜ ìµœê³  views ë°ì´í„°)ë§Œ ë‚¨ê¸´ë‹¤ | íŠ¹ì • ê¸°ëŠ¥ì˜ ìœ ë¬´ì— ë”°ë¼ viewsê°€ ì–¼ë§ˆë‚˜ ë‹¬ë¼ì§€ëŠ”ì§€ë¥¼ í™•ì¸í•˜ê¸° ìœ„í•¨ì´ë¯€ë¡œ, ì˜ìƒë³„ ìµœê³  viewsë¡œ ë¶„í¬ íŒŒì•… (ex. ê°€ì„¤ ì˜ˆì‹œ: ratings_disabled ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ë©´ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ê²ƒë³´ë‹¤ viewsê°€ ë‚®ê²Œ í˜•ì„±ëœë‹¤) | . | ê¸°ëŠ¥ ìœ ë¬´ì— ë”°ë¥¸ views ë¶„í¬ í™•ì¸ . # comments_disabledê°€ viewsì— ì–´ë–¤ ì˜í–¥ì„ ë¯¸ì¹˜ëŠ”ì§€ fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(10, 5)) sns.stripplot(data=unique_recentf_df, x='comments_disabled', y='views', palette='Blues', ax=ax1) sns.boxplot(data=unique_recentf_df, x='comments_disabled', y='views', palette='Blues', showfliers=False, ax=ax2) plt.close(2) plt.close(3) plt.tight_layout() . # ratings_disabledê°€ viewsì— ì–´ë–¤ ì˜í–¥ì„ ë¯¸ì¹˜ëŠ”ì§€ fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(10, 5)) sns.stripplot(data=unique_recentf_df, x='ratings_disabled', y='views', palette='Blues', ax=ax1) sns.boxplot(data=unique_recentf_df, x='ratings_disabled', y='views', palette='Blues', showfliers=False, ax=ax2) plt.close(2) plt.close(3) plt.tight_layout() . | ê¸°ëŠ¥ ìœ ë¬´ì— ë”°ë¼ í‰ê·  viewsì— ì°¨ì´ê°€ ìˆëŠ”ì§€ í™•ì¸ . | í‰ê·  views ë¹„êµ í›„, t-testë¡œ ì°¨ì´ê°€ ìœ ì˜ë¯¸í•˜ë‹¤ê³  ë´ì•¼ í•  ì§€ í™•ì¸ | . print('--í‰ê·  views ë¹„êµ -------') print('comments_disabled: {:.0f}'.format(unique_recentf_df['views'][unique_recentf_df['comments_disabled']].mean())) print('comments_abled: {:.0f}\\n'.format(unique_recentf_df['views'][~unique_recentf_df['comments_disabled']].mean())) print('ratings_disabled: {:.0f}'.format(unique_recentf_df['views'][unique_recentf_df['ratings_disabled']].mean())) print('ratings_abled: {:.0f}'.format(unique_recentf_df['views'][~unique_recentf_df['ratings_disabled']].mean())) . --í‰ê·  views ë¹„êµ ------- comments_disabled: 384423 comments_abled: 369001 ratings_disabled: 209573 ratings_abled: 376942 . 1) comments_disabled ê¸°ëŠ¥ ìœ ë¬´ì— ë”°ë¥¸ ì°¨ì´ . comments_disabled = unique_recentf_df[unique_recentf_df['comments_disabled']] comments_abled = unique_recentf_df[~unique_recentf_df['comments_disabled']] # Leveneì˜ ë“±ë¶„ì‚° ê²€ì • lev_result = stats.levene(comments_disabled['views'], comments_abled['views']) print('LeveneResult(F) : %.2f \\np-value : %.3f' % (lev_result)) ## ëŒ€ì²´ë¡œ pê°’ì´ 0.05 ì´ìƒì´ë©´ ë“±ë¶„ì‚° ê°€ì •, ë“±ë¶„ì‚°ì¸ ë…ë¦½í‘œë³¸ t-testë¡œ ì§„í–‰ . LeveneResult(F) : 0.05 p-value : 0.819 . # ë“±ë¶„ì‚°ì¸ ë…ë¦½í‘œë³¸ t-test ì‹¤í–‰ t_result = stats.ttest_ind(comments_disabled['views'], comments_abled['views'], equal_var=True) print('t statistic : %.2f \\np-value : %.3f' % (t_result)) . t statistic : 0.12 p-value : 0.902 . | pê°’ &gt; 0.05ì´ê³ , ìœ„ì—ì„œ ì‹¤ì œ ê°’ìœ¼ë¡œ ì‚´í´ë´¤ì„ ë•Œì—ë„ í‰ê·  viewsì˜ ì°¨ì´ê°€ í¬ì§€ ì•Šìœ¼ë¯€ë¡œ, ìœ ì˜ë¯¸í•œ ì°¨ì´ëŠ” ì—†ë‹¤ê³  ë´ë„ ë¬´ë°©í•  ë“¯. | . 2) ratings_disabed ê¸°ëŠ¥ ìœ ë¬´ì— ë”°ë¥¸ ì°¨ì´ . ratings_disabled = unique_recentf_df[unique_recentf_df['ratings_disabled']] ratings_abled = unique_recentf_df[~unique_recentf_df['ratings_disabled']] # Leveneì˜ ë“±ë¶„ì‚° ê²€ì • lev_result = stats.levene(ratings_disabled['views'], ratings_abled['views']) print('LeveneResult(F) : %.2f \\np-value : %.3f' % (lev_result)) . LeveneResult(F) : 3.94 p-value : 0.047 . # ë“±ë¶„ì‚°ì´ ì•„ë‹Œ ë…ë¦½í‘œë³¸ t-test ì‹¤í–‰ t_result = stats.ttest_ind(ratings_disabled['views'], ratings_abled['views'], equal_var=False) print('t statistic : %.2f \\np-value : %.3f' % (t_result)) . t statistic : -2.78 p-value : 0.006 . | pê°’ &lt; 0.01ì´ê³ , ìœ„ì—ì„œ ì‹¤ì œ ê°’ìœ¼ë¡œ ì‚´í´ë´¤ì„ ë•Œë„ í‰ê·  viewsì˜ ì°¨ì´ê°€ ê½¤ ìˆë‹¤ê³  ë³´ì´ë¯€ë¡œ, ìœ ì˜ë¯¸í•œ ì°¨ì´ê°€ ìˆë‹¤ê³  ìƒê°. | . | . views ê¸°ì¤€ ì‚¬ë¶„ìœ„ ë¶„ë¥˜ â†’ ë¶„ìœ„ë³„ ë¹„ìœ¨ í™•ì¸ . # views_quartile ì¹¼ëŸ¼ì„ ìƒˆë¡œ ìƒì„±: ê°€ì¥ viewsê°€ ë‚®ì€ ì§‘ë‹¨ì´ 1st_q ~ ê°€ì¥ ë†’ì€ ì§‘ë‹¨ì´ 4th_q q1, q2, q3 = np.percentile(unique_recentf_df['views'], [25, 50, 75]) def get_quarter(view): if view &lt; q1: quarter = '1st_q' elif view &lt; q2: quarter = '2nd_q' elif view &lt; q3: quarter = '3rd_q' else: quarter = '4th_q' return quarter unique_recentf_df['views_quartile'] = unique_recentf_df['views'].apply(lambda view: get_quarter(view)) unique_recentf_df[['views', 'views_quartile']].head() . | Â  | views | views_quartile | . | 0 | 27352 | 1st_q | . | 1 | 345008 | 4th_q | . | 2 | 45444 | 2nd_q | . | 3 | 467546 | 4th_q | . | 4 | 9628 | 1st_q | . 1) views ì‚¬ë¶„ìœ„ë³„ comments_disabled ê¸°ëŠ¥ì„ ì‚¬ìš©í•œ ì˜ìƒì˜ ìˆ˜ . sns.countplot(data=unique_recentf_df[unique_recentf_df['comments_disabled']], y='views_quartile', order=['4th_q', '3rd_q', '2nd_q', '1st_q'], color='skyblue'); . 2) views ì‚¬ë¶„ìœ„ë³„ ratings_disabled ê¸°ëŠ¥ì„ ì‚¬ìš©í•œ ì˜ìƒì˜ ìˆ˜ . sns.countplot(data=unique_recentf_df[unique_recentf_df['ratings_disabled']], y='views_quartile', order=['4th_q', '3rd_q', '2nd_q', '1st_q'], color='skyblue'); . | views ê¸°ì¤€ 4ì‚¬ë¶„ìœ„ ì¤‘ 1ì‚¬ë¶„ìœ„(í•˜ìœ„ 25%)ì— ratings_disabled ê¸°ëŠ¥ì„ ì‚¬ìš©í•œ ì˜ìƒì˜ ìˆ˜ê°€ ìœ ë… ë§ì´ ë¶„í¬ (4ì‚¬ë¶„ìœ„ì˜ ì•½ 4ë°°) | . â†’ ê²°ë¡ : í‰ê· ì˜ ì°¨ì´ì™€, 4ì‚¬ë¶„ìœ„ ë‚´ ë¶„í¬ë¥¼ ê³ ë ¤í•´ë³´ë©´, ratings_disabled ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ê²ƒì´ ë³´ë‹¤ ë†’ì€ viewsë¥¼ ê¸°ë¡í•˜ëŠ” ë°ì— ìœ ë¦¬í•  ìˆ˜ ìˆë‹¤ê³  ìƒê°ë¨ . ",
    "url": "https://chaelist.github.io/docs/kaggle/youtube_trending/#%EA%B8%B0%EB%8A%A5-%EC%82%AC%EC%9A%A9-%EC%9C%A0%EB%AC%B4%EC%97%90-%EB%94%B0%EB%A5%B8-%EB%B9%84%EA%B5%90",
    "relUrl": "/docs/kaggle/youtube_trending/#ê¸°ëŠ¥-ì‚¬ìš©-ìœ ë¬´ì—-ë”°ë¥¸-ë¹„êµ"
  }
}
