{"0": {
    "doc": "Amazon Bestselling Books",
    "title": "Amazon Bestselling Books",
    "content": ". | 데이터 파악 . | 값의 분포 확인 | 데이터 사전 처리 | 중복값 확인 | . | 책별 bestselling 횟수 비교 . | bestselling 횟수 분포 확인 | bestselling 횟수가 많은 책 확인 | . | 책별 bestselling 횟수 비교 . | 작가명 표기 확인 | bestselling 횟수 분포 확인 | bestselling 횟수가 많은 작가 확인 | . | 연도별 변화 확인 . | 연도별 장르별 비중 | 연도별 Rating, Review, Price 평균의 변화 | . | Rating, Review, Price 상위권 도서 확인 . | User Rating이 높은 책 확인 | Review가 많은 책 확인 | Price가 높은 책 확인 | . | . *분석 대상 데이터셋: Amazon Bestselling Books . | 데이터셋 출처 | 2009년 ~ 2019년의 Amazon’s Top 50 bestselling books 데이터 (11년 * 50 = 550권) | 한 책이 여러 해에 걸친 bestseller였다면, 여러 번 중복되어 포함되어 있음 (중복을 제외하고 계산하면 총 351권) | 같은 책이여도 User Rating, Price, Reviews 정보가 달라지는 경우도 있음 | Columns (7개): ‘Name’, ‘Author’, ‘User Rating’, ‘Reviews’, ‘Price’, ‘Year’, ‘Genre’ | . ",
    "url": "https://chaelist.github.io/docs/kaggle/amazon_bestsellers/",
    "relUrl": "/docs/kaggle/amazon_bestsellers/"
  },"1": {
    "doc": "Amazon Bestselling Books",
    "title": "데이터 파악",
    "content": "# 필요한 라이브러리 import import pandas as pd import numpy as np from matplotlib import pyplot as plt import seaborn as sns import scipy.stats as stats . books_df = pd.read_csv('data/bestsellers with categories.csv') books_df.head() . |   | Name | Author | User Rating | Reviews | Price | Year | Genre | . | 0 | 10-Day Green Smoothie Cleanse | JJ Smith | 4.7 | 17350 | 8 | 2016 | Non Fiction | . | 1 | 11/22/63: A Novel | Stephen King | 4.6 | 2052 | 22 | 2011 | Fiction | . | 2 | 12 Rules for Life: An Antidote to Chaos | Jordan B. Peterson | 4.7 | 18979 | 15 | 2018 | Non Fiction | . | 3 | 1984 (Signet Classics) | George Orwell | 4.7 | 21424 | 6 | 2017 | Fiction | . | 4 | 5,000 Awesome Facts (About Everything!) (National Geographic Kids) | National Geographic Kids | 4.8 | 7665 | 12 | 2019 | Non Fiction | . 값의 분포 확인 . | null값 여부, data type 확인 . books_df.info() . &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 550 entries, 0 to 549 Data columns (total 7 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Name 550 non-null object 1 Author 550 non-null object 2 User Rating 550 non-null float64 3 Reviews 550 non-null int64 4 Price 550 non-null int64 5 Year 550 non-null int64 6 Genre 550 non-null object dtypes: float64(1), int64(3), object(3) memory usage: 30.2+ KB . | 숫자형 컬럼: 값의 분포를 확인 . books_df[['User Rating', 'Reviews', 'Price']].describe() . |   | User Rating | Reviews | Price | . | count | 550.00 | 550.00 | 550.00 | . | mean | 4.62 | 11953.28 | 13.10 | . | std | 0.23 | 11731.13 | 10.84 | . | min | 3.30 | 37.00 | 0.00 | . | 25% | 4.50 | 4058.00 | 7.00 | . | 50% | 4.70 | 8580.00 | 11.00 | . | 75% | 4.80 | 17253.25 | 16.00 | . | max | 4.90 | 87841.00 | 105.00 | . | Price의 최솟값이 0 → bestselling books 명단이므로 실제 가격이 0일리는 없다고 생각. (가격이 0 = 사은품이라고 간주할 수 있는데, 그렇다면 bestselling 명단에 올리지 않았을 것.) | . | unique한 값 수 확인 . books_df.nunique() . Name 351 Author 248 User Rating 14 Reviews 346 Price 40 Year 11 Genre 2 dtype: int64 . | . 데이터 사전 처리 . | Price 컬럼의 0값 채워주기 . | bestselling books 명단이므로 Price가 0일리는 없다고 생각. → 0인 값은 아마 조사 과정에서 누락된 것이라 생각됨 | 0값 = 결측치로 간주하고, 해당 책이 속한 연도 &amp; 장르의 Price 평균값으로 채워넣어준다 | . books_df[books_df['Price'] == 0] # Price가 0인 row: 총 12개 . |   | Name | Author | User Rating | Reviews | Price | Year | Genre | . | 42 | Cabin Fever (Diary of a Wimpy Kid, Book 6) | Jeff Kinney | 4.8 | 4505 | 0 | 2011 | Fiction | . | 71 | Diary of a Wimpy Kid: Hard Luck, Book 8 | Jeff Kinney | 4.8 | 6812 | 0 | 2013 | Fiction | . | 116 | Frozen (Little Golden Book) | RH Disney | 4.7 | 3642 | 0 | 2014 | Fiction | . | 193 | JOURNEY TO THE ICE P | RH Disney | 4.6 | 978 | 0 | 2014 | Fiction | . | 219 | Little Blue Truck | Alice Schertle | 4.9 | 1884 | 0 | 2014 | Fiction | . | 358 | The Constitution of the United States | Delegates of the Constitutional | 4.8 | 2774 | 0 | 2016 | Non Fiction | . | 381 | The Getaway | Jeff Kinney | 4.8 | 5836 | 0 | 2017 | Fiction | . | 461 | The Short Second Life of Bree Tanner: An Eclipse Novella (The Twilight Saga) | Stephenie Meyer | 4.6 | 2122 | 0 | 2010 | Fiction | . | 505 | To Kill a Mockingbird | Harper Lee | 4.8 | 26234 | 0 | 2013 | Fiction | . | 506 | To Kill a Mockingbird | Harper Lee | 4.8 | 26234 | 0 | 2014 | Fiction | . | 507 | To Kill a Mockingbird | Harper Lee | 4.8 | 26234 | 0 | 2015 | Fiction | . | 508 | To Kill a Mockingbird | Harper Lee | 4.8 | 26234 | 0 | 2016 | Fiction | . → 0값(결측치로 간주) 처리: 해당 책이 속한 연도 &amp; 장르의 Price 평균값으로 대체 . zero_price_index = books_df[books_df['Price'] == 0].index for index in zero_price_index: year = books_df.loc[index, 'Year'] genre = books_df.loc[index, 'Genre'] avg_price = books_df[(books_df['Genre'] == genre) &amp; (books_df['Year'] == year)]['Price'].mean() books_df.loc[index, 'Price'] = avg_price books_df.loc[zero_price_index] . |   | Name | Author | User Rating | Reviews | Price | Year | Genre | . | 42 | Cabin Fever (Diary of a Wimpy Kid, Book 6) | Jeff Kinney | 4.8 | 4505 | 11.619 | 2011 | Fiction | . | 71 | Diary of a Wimpy Kid: Hard Luck, Book 8 | Jeff Kinney | 4.8 | 6812 | 10.7083 | 2013 | Fiction | . | 116 | Frozen (Little Golden Book) | RH Disney | 4.7 | 3642 | 10.1724 | 2014 | Fiction | . | 193 | JOURNEY TO THE ICE P | RH Disney | 4.6 | 978 | 10.5232 | 2014 | Fiction | . | 219 | Little Blue Truck | Alice Schertle | 4.9 | 1884 | 10.8861 | 2014 | Fiction | . | 358 | The Constitution of the United States | Delegates of the Constitutional | 4.8 | 2774 | 13.5161 | 2016 | Non Fiction | . | 381 | The Getaway | Jeff Kinney | 4.8 | 5836 | 8.83333 | 2017 | Fiction | . | 461 | The Short Second Life of Bree Tanner: An Eclipse Novella (The Twilight Saga) | Stephenie Meyer | 4.6 | 2122 | 9.7 | 2010 | Fiction | . | 505 | To Kill a Mockingbird | Harper Lee | 4.8 | 26234 | 11.1545 | 2013 | Fiction | . | 506 | To Kill a Mockingbird | Harper Lee | 4.8 | 26234 | 11.2614 | 2014 | Fiction | . | 507 | To Kill a Mockingbird | Harper Lee | 4.8 | 26234 | 9.35294 | 2015 | Fiction | . | 508 | To Kill a Mockingbird | Harper Lee | 4.8 | 26234 | 12.6316 | 2016 | Fiction | . | string 데이터 처리 . | Name과 Author의 경우, 앞 뒤 공백 때문에 다르게 인식될 가능성을 없애기 위해 strip()을 해준다 | . books_df['Name'] = books_df['Name'].str.strip() books_df['Author'] = books_df['Author'].str.strip() . | . 중복값 확인 . | 중복값 확인 # 모든 열이 중복된 값이 포함되어 있나 확인 books_df.duplicated().sum() . 0 . | 같은 책이 여러 번 나온 경우도 확인 . # 제목과 작가만 중복되는 값이 몇 개인지 체크 books_df.duplicated(subset=['Name', 'Author']).sum() . 199 . | 같은 책이면 User Rating, Price, Reviews 정보가 같은지 확인 . nunique_df = books_df.groupby(['Name'])[['User Rating', 'Price', 'Reviews', 'Year']].nunique().reset_index() nunique_df[(nunique_df['User Rating'] &gt; 1) | (nunique_df['Price'] &gt; 1) | (nunique_df['Reviews'] &gt; 1)] . |   | Name | User Rating | Price | Reviews | Year | . | 104 | Gone Girl | 1 | 2 | 1 | 3 | . | 193 | Quiet: The Power of Introverts in a World That Can’t Stop Talking | 1 | 2 | 1 | 2 | . | 219 | The 7 Habits of Highly Effective People: Powerful Lessons in Personal Change | 2 | 2 | 2 | 7 | . | 240 | The Fault in Our Stars | 1 | 2 | 1 | 3 | . | 248 | The Girl on the Train | 1 | 2 | 1 | 2 | . | 258 | The Help | 1 | 3 | 1 | 3 | . | 263 | The Immortal Life of Henrietta Lacks | 1 | 2 | 1 | 3 | . | 322 | To Kill a Mockingbird | 1 | 5 | 1 | 5 | . | 328 | Unbroken: A World War II Story of Survival, Resilience, and Redemption | 1 | 2 | 1 | 4 | . | 같은 책이여도 해에 따라 User Rating, Price, Reviews 정보가 달라지는 경우도 있음 (To Kill a Mockingbird는 내가 임의로 Price를 채워넣었으므로 제외) | . | . ",
    "url": "https://chaelist.github.io/docs/kaggle/amazon_bestsellers/#%EB%8D%B0%EC%9D%B4%ED%84%B0-%ED%8C%8C%EC%95%85",
    "relUrl": "/docs/kaggle/amazon_bestsellers/#데이터-파악"
  },"2": {
    "doc": "Amazon Bestselling Books",
    "title": "책별 bestselling 횟수 비교",
    "content": ": 각 책별로, 몇 번 bestseller에 이름을 올렸는지 횟수를 계산 . yearly_count = books_df.groupby(['Name', 'Genre', 'Author'])[['Year']].count().reset_index() yearly_count.sort_values(by='Year', ascending=False, inplace=True) yearly_count.head() . |   | Name | Genre | Author | Year | . | 191 | Publication Manual of the American Psychological Association, 6th Edition | Non Fiction | American Psychological Association | 10 | . | 209 | StrengthsFinder 2.0 | Non Fiction | Gallup | 9 | . | 178 | Oh, the Places You’ll Go! | Fiction | Dr. Seuss | 8 | . | 310 | The Very Hungry Caterpillar | Fiction | Eric Carle | 7 | . | 219 | The 7 Habits of Highly Effective People: Powerful Lessons in Personal Change | Non Fiction | Stephen R. Covey | 7 | . bestselling 횟수 분포 확인 . | 최댓값, 최솟값, 사분위값 확인 . print(yearly_count.describe()) . Year count 351.000000 mean 1.566952 std 1.271868 min 1.000000 25% 1.000000 50% 1.000000 75% 2.000000 max 10.000000 . | 분포 시각화 plt.figure(figsize=(6, 4)) sns.countplot(data=yearly_count, x='Year', palette='Purples_r'); . | 한 번만 포함된 비율 확인 one_year_percentage = len(yearly_count[yearly_count['Year'] == 1]) / len(yearly_count) * 100 print(f'한 해만 bestseller에 포함된 책의 비율: {one_year_percentage :.0f}%') . 한 해만 bestseller에 포함된 책의 비율: 73% . | . &gt;&gt; 최대 10번이나 bestseller에 포함된 책도 있지만, 보통은 1~2번 bestseller에 포함되는 정도가 일반적. (73%가 1번만 이름을 올림) . bestselling 횟수가 많은 책 확인 . | Top 10 책 시각화: 가장 많이 bestseller에 올라온 책이 어떤 것인지 . plt.figure(figsize=(6, 5)) sns.barplot(data=yearly_count.head(10), x='Year', y='Name', hue='Genre', palette='Purples'); . | 가장 많이 bestseller에 포함된 책 10권 중 7권은 Non-Fiction | . | bestseller에 많이 올라온 책들의 연도별 bestselling 여부를 확인 . pivot_df = pd.pivot_table(books_df, index='Name', columns='Year', values='Author', fill_value=0, aggfunc='count').reset_index() pd.merge(pivot_df, yearly_count, on='Name').sort_values(by='Year', ascending=False).head(10) . | . |   | Name | 2009 | 2010 | 2011 | 2012 | 2013 | 2014 | 2015 | 2016 | 2017 | 2018 | 2019 | Genre | Author | Year | . | 191 | Publication Manual of the American Psychological Association, 6th Edition | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 0 | Non Fiction | American Psychological Association | 10 | . | 209 | StrengthsFinder 2.0 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 0 | 0 | Non Fiction | Gallup | 9 | . | 178 | Oh, the Places You’ll Go! | 0 | 0 | 0 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | Fiction | Dr. Seuss | 8 | . | 310 | The Very Hungry Caterpillar | 0 | 0 | 0 | 0 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | Fiction | Eric Carle | 7 | . | 219 | The 7 Habits of Highly Effective People: Powerful Lessons in Personal Change | 1 | 0 | 1 | 1 | 1 | 0 | 1 | 1 | 1 | 0 | 0 | Non Fiction | Stephen R. Covey | 7 | . | 243 | The Four Agreements: A Practical Guide to Personal Freedom (A Toltec Wisdom Book) | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 1 | 1 | 1 | 1 | Non Fiction | Don Miguel Ruiz | 6 | . | 140 | Jesus Calling: Enjoying Peace in His Presence (with Scripture References) | 0 | 0 | 1 | 1 | 1 | 1 | 1 | 1 | 0 | 0 | 0 | Non Fiction | Sarah Young | 6 | . | 281 | The Official SAT Study Guide | 0 | 1 | 1 | 1 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | Non Fiction | The College Board | 5 | . | 322 | To Kill a Mockingbird | 0 | 0 | 0 | 0 | 1 | 1 | 1 | 1 | 0 | 0 | 1 | Fiction | Harper Lee | 5 | . | 216 | The 5 Love Languages: The Secret to Love That Lasts | 0 | 1 | 1 | 1 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | Non Fiction | Gary Chapman | 5 | . | 연속으로 계속 bestseller에 올라온 경우만 있는 건 아니고, bestseller에 올라오지 않은 해가 중간에 끼어 있는 책들도 존재함 (ex. To Kill a Mockingbird) | . ",
    "url": "https://chaelist.github.io/docs/kaggle/amazon_bestsellers/#%EC%B1%85%EB%B3%84-bestselling-%ED%9A%9F%EC%88%98-%EB%B9%84%EA%B5%90",
    "relUrl": "/docs/kaggle/amazon_bestsellers/#책별-bestselling-횟수-비교"
  },"3": {
    "doc": "Amazon Bestselling Books",
    "title": "책별 bestselling 횟수 비교",
    "content": "작가명 표기 확인 . : 같은 작가인데 이름이 다르게 표기된 경우가 있는지 확인 . books_df['Author'].nunique() ## 작가명 표기를 정리하기 전에는 중복 제외하고 248개 . 248 . | 각 작가의 성(공백 기준으로 가장 뒷부분만 추출)만 분리해서 별도의 df로 저장 . books_df_copy = books_df[['Name', 'Author']] books_df_copy['Author_Last_Name'] = books_df_copy ['Author'].str.rsplit(n=1, expand=True)[1] books_df_copy.head() . |   | Name | Author | Author_Last_Name | . | 0 | 10-Day Green Smoothie Cleanse | JJ Smith | Smith | . | 1 | 11/22/63: A Novel | Stephen King | King | . | 2 | 12 Rules for Life: An Antidote to Chaos | Jordan B. Peterson | Peterson | . | 3 | 1984 (Signet Classics) | George Orwell | Orwell | . | 4 | 5,000 Awesome Facts (About Everything!) (National Geographic Kids) | National Geographic Kids | Kids | . | 같은 성에 2개 이상의 작가명이 존재하는 경우만 따로 저장 . authors_unique_df = books_df_copy.groupby(['Author_Last_Name'])[['Author']].nunique().reset_index() authors_unique_df[authors_unique_df['Author'] &gt; 1] . |   | Author_Last_Name | Author | . | 5 | Association | 2 | . | 20 | Brown | 4 | . | 26 | Campbell | 2 | . | 39 | Collins | 2 | . | 63 | Gaines | 2 | . | 121 | M.D. | 3 | . | 122 | MD | 3 | . | 125 | Martin | 3 | . | 164 | Press | 2 | . | 175 | Roth | 2 | . | 176 | Rowling | 2 | . | 191 | Smith | 3 | . | 221 | Young | 2 | . | last name이 2개 이상으로 나온 Author 이름만 알파벳 순으로 뽑아서 확인 (같은 작가인데 표기가 다른 경우가 있는지) . last_name_list = list(authors_unique_df[authors_unique_df['Author'] &gt; 1]['Author_Last_Name']) sorted(books_df_copy[books_df_copy['Author_Last_Name'].isin(last_name_list)]['Author'].unique()) . ['American Psychiatric Association', 'American Psychological Association', 'Bessel van der Kolk M.D.', 'Brené Brown', 'Chip Gaines', 'Craig Smith', 'Dan Brown', 'Daniel James Brown', 'David Perlmutter MD', 'Dr. Steven R Gundry MD', 'Emily Winfield Martin', 'Geneen Roth', 'George R. R. Martin', 'George R.R. Martin', 'Ian K. Smith M.D.', 'J. K. Rowling', 'J.K. Rowling', 'JJ Smith', 'Jennifer Smith', 'Jim Collins', 'Joanna Gaines', 'Joel Fuhrman MD', 'Margaret Wise Brown', 'Mark Hyman M.D.', 'Paper Peony Press', 'Pretty Simple Press', 'Rod Campbell', 'Sarah Young', 'Suzanne Collins', 'Thomas Campbell', 'Veronica Roth', 'William P. Young'] . | 공백 처리로 인해 다르게 인식되는 작가명을 수정해줌 . # J.K.Rowling과 George R.R.Martin의 경우, 중간 공백으로 인해 다르게 표기된 경우가 있음 → 수정해줌 books_df.replace({'Author': {'J.K. Rowling': 'J. K. Rowling', 'George R.R. Martin': 'George R. R. Martin'}}, inplace=True) . → 중복 제외 작가 수가 246명으로 줄어듦: . books_df['Author'].nunique() . 246 . | . bestselling 횟수 분포 확인 . yearly_count2 = books_df.groupby(['Author'])[['Year']].count().reset_index() yearly_count2.sort_values(by='Year', ascending=False, inplace=True) yearly_count2.head() . |   | Author | Year | . | 118 | Jeff Kinney | 12 | . | 224 | Suzanne Collins | 11 | . | 195 | Rick Riordan | 11 | . | 92 | Gary Chapman | 11 | . | 11 | American Psychological Association | 10 | . | 최댓값, 최솟값, 사분위값 확인 print(yearly_count2.describe()) . Year count 246.000000 mean 2.235772 std 2.080350 min 1.000000 25% 1.000000 50% 1.000000 75% 2.000000 max 12.000000 . | 분포 시각화 plt.figure(figsize=(6, 4)) sns.countplot(data=yearly_count2, x='Year', palette='Purples_r'); . | 1~2번 포함된 비율 확인 one_year_percentage2 = len(yearly_count2[yearly_count2['Year'] == 1]) / len(yearly_count2) * 100 print(f'한 해만 bestseller에 포함된 작가의 비율: {one_year_percentage2 :.0f}%') one_year_percentage2_2 = len(yearly_count2[yearly_count2['Year'] == 2]) / len(yearly_count2) * 100 print(f'두 해 동안 bestseller에 포함된 작가의 비율: {one_year_percentage2_2 :.0f}%') . 한 해만 bestseller에 포함된 작가의 비율: 53% 두 해 동안 bestseller에 포함된 작가의 비율: 24% . &gt;&gt; 최대 12번이나 bestseller에 포함된 작가도 있지만, 보통은 1~2번 bestseller에 포함되는 정도가 일반적. (과반수가 1번만 이름을 올렸고, 77%가 2번 이하로 이름을 올림) . | . bestselling 횟수가 많은 작가 확인 . | Top 10 작가 시각화: 가장 많이 bestseller에 올라온 작가가 누구인지 . plt.figure(figsize=(6, 5)) sns.barplot(data=yearly_count2.head(10), x='Year', y='Author', palette='Purples_r'); . | 유명한 시리즈물을 발표한 작가들이 주로 상위권에 포진. | Jeff Kinney: Diary of a Wimpy Kids 시리즈 | Suzanne Collins: The Hunger Game 시리즈 | Rick Riordan: The Heroes of Olympus &amp; The Kane Chronicles 시리즈 | J. K. Rowling: Harry Potter 시리즈 | Stephenie Meyer: Twilight 시리즈 | . | . | bestseller에 많이 올라온 작가들의 연도별 bestselling 분포를 확인 . pivot_df2 = pd.pivot_table(books_df, index='Author', columns='Year', values='Name', fill_value=0, aggfunc='count').reset_index() pd.merge(pivot_df2, yearly_count2, on='Author').sort_values(by='Year', ascending=False).head(10) . |   | Author | 2009 | 2010 | 2011 | 2012 | 2013 | 2014 | 2015 | 2016 | 2017 | 2018 | 2019 | Year | . | 118 | Jeff Kinney | 2 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 12 | . | 224 | Suzanne Collins | 0 | 3 | 4 | 4 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 11 | . | 195 | Rick Riordan | 1 | 4 | 2 | 2 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 11 | . | 92 | Gary Chapman | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 11 | . | 11 | American Psychological Association | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 0 | 10 | . | 73 | Dr. Seuss | 0 | 0 | 0 | 1 | 1 | 1 | 2 | 1 | 1 | 1 | 1 | 9 | . | 90 | Gallup | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 0 | 0 | 9 | . | 111 | J. K. Rowling | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 5 | 1 | 0 | 1 | 8 | . | 197 | Rob Elliott | 0 | 0 | 0 | 0 | 2 | 2 | 2 | 1 | 1 | 0 | 0 | 8 | . | 219 | Stephenie Meyer | 6 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 7 | . | Suzanne Collins, Rick Riordan, J. K. Rowling, Stephenie Meyer: 시리즈물로 유명한 작가여서인지, 대체로 특정 기간 내에 복수의 책이 한번에 bestseller에 올라옴 | . | . ",
    "url": "https://chaelist.github.io/docs/kaggle/amazon_bestsellers/#%EC%B1%85%EB%B3%84-bestselling-%ED%9A%9F%EC%88%98-%EB%B9%84%EA%B5%90-1",
    "relUrl": "/docs/kaggle/amazon_bestsellers/#책별-bestselling-횟수-비교-1"
  },"4": {
    "doc": "Amazon Bestselling Books",
    "title": "연도별 변화 확인",
    "content": "연도별 장르별 비중 . : 연도별로, 어떤 장르의 책이 bestseller로 이름을 더 많이 올렸는지 확인 . groupby_df1 = books_df.groupby(['Year', 'Genre'])[['Name']].count().reset_index() groupby_df1.rename(columns={'Name':'Count'}, inplace=True) groupby_df1.head(4) . |   | Year | Genre | Count | . | 0 | 2009 | Fiction | 24 | . | 1 | 2009 | Non Fiction | 26 | . | 2 | 2010 | Fiction | 20 | . | 3 | 2010 | Non Fiction | 30 | . → 시각화: . plt.figure(figsize=(12, 5)) sns.lineplot(data=groupby_df1, x='Year', y='Count', hue='Genre', palette='Purples_r'); . | 2014년을 제외하면 대체로 Non-Fiction이 더 많이 bestseller에 포함되는 경향 | . 연도별 Rating, Review, Price 평균의 변화 . | 연도별 장르별 User Rating 평균의 차이 plt.figure(figsize=(12, 5)) sns.lineplot(data=books_df, x='Year', y='User Rating', hue='Genre', palette='Purples'); . | 대체로 큰 차이가 없으나, 2017년 이후부터는 Fiction 장르의 평균 User Rating이 올라가면서 Non-Fiction 장르와 다소 차이가 벌어짐 | Fiction 장르가 대체로 User Rating의 편차가 큼 (보다 취향이 갈리는 장르이기 때문이라고 추정) | . | 연도별 장르별 Reviews 평균의 차이 plt.figure(figsize=(12, 5)) sns.lineplot(data=books_df, x='Year', y='Reviews', hue='Genre', palette='Purples'); . | 특히 2012 ~ 2016년 사이에 장르 간 평균 Review 수가 큰 차이를 보임 | 2017 ~ 2018년은 거의 평균의 차이가 없다고 봐도 무방하나, 2019년부터 다시 Fiction 장르의 평균 Review수가 증가하는 경향을 보여, 향후 추이를 지켜봐야 할 것으로 생각됨 | . | 연도별 장르별 Price 평균의 차이 plt.figure(figsize=(12, 5)) sns.lineplot(data=books_df, x='Year', y='Price', hue='Genre', palette='Purples'); . | 2010 ~ 2014년 사이의 bestseller들은 대체로 Non-Fiction이 Fiction보다 다소 높은 가격을 보이나, 2015년 이후에는 별 차이를 보이지 않음 | Non-Fiction 장르가 대체로 Price의 편차가 큼 (학술 도서 중에는 상당히 비싼 가격의 책들도 있기 때문이라고 추정) | . | . ",
    "url": "https://chaelist.github.io/docs/kaggle/amazon_bestsellers/#%EC%97%B0%EB%8F%84%EB%B3%84-%EB%B3%80%ED%99%94-%ED%99%95%EC%9D%B8",
    "relUrl": "/docs/kaggle/amazon_bestsellers/#연도별-변화-확인"
  },"5": {
    "doc": "Amazon Bestselling Books",
    "title": "Rating, Review, Price 상위권 도서 확인",
    "content": ": 전기간의 모든 bestselling 책 중, rating, review, price가 높은 책들이 어떤 것들인지 확인 . | ※ 대체로 같은 책은 rating, review, price가 동일한 경우가 많으므로, 같은 책의 경우 가장 최근 record 하나만 남겨서 분석에 활용 | . # 연도순으로 내림차순 정렬 books_df_sorted = books_df.sort_values(by='Year', ascending=False) # 중복된 행들 중 가장 위에 있는 행만 남기고 다 삭제 unique_books_df = books_df_sorted.drop_duplicates(subset=['Name', 'Author'], ignore_index=True) unique_books_df.reset_index(drop=True, inplace=True) print('books_df: ', len(books_df)) print('unique_books_df: ', len(unique_books_df)) . books_df: 550 unique_books_df: 351 . +) 평균 확인 . unique_books_df.groupby(['Genre'])[['User Rating', 'Reviews', 'Price']].mean() . | Genre | User Rating | Reviews | Price | . | Fiction | 4.61563 | 13111.1 | 12.0938 | . | Non Fiction | 4.60366 | 7001.66 | 13.7016 | . +) 표준편차 확인 . unique_books_df.groupby(['Genre'])[['User Rating', 'Reviews', 'Price']].std() . | Genre | User Rating | Reviews | Price | . | Fiction | 0.274388 | 13312.2 | 9.54815 | . | Non Fiction | 0.177815 | 7241.59 | 10.369 | . +) 변수간 상관관계 체크 . | Rating이 높은 책이 Review도 높고, Price도 높은 등의 관계가 있는지 파악하기 위함 | . sns.heatmap(unique_books_df[['User Rating', 'Reviews', 'Price']].corr(), annot=True, cmap='Purples') plt.yticks(rotation=0); . | 변수간 전혀 상관관계가 존재하지 않음 | . User Rating이 높은 책 확인 . | User Rating이 4.9(=최댓값)인 책들 중에 Fiction과 Non Fiction이 몇 권씩 포함되어 있는지 확인 . plot = sns.countplot(data=unique_books_df[unique_books_df['User Rating'] == 4.9], x='Genre', palette='Purples_r', order=unique_books_df[unique_books_df['User Rating'] == 4.9].groupby(['Genre'])[['Name']].nunique().index) # data label을 각각의 bar 위에 추가 for p, label in zip(plot.patches, unique_books_df[unique_books_df['User Rating'] == 4.9].groupby(['Genre'])[['Name']].nunique()['Name']): plot.annotate(label, (p.get_x() + 0.375, p.get_height() + 1)) plt.ylim(0, 25); . | 최고점인 4.9점을 받은 책 중에서는 약 78.6%가 Fiction | Fiction이 장르 특성상 호불호가 더 명확하기 때문인 듯 (Fiction이 더 표준편차가 큼) | . | 연도별로, User Rating이 4.9(=최댓값)인 책들이 몇 권씩 포함되어 있는지 확인 . # unique_books_df 대신 books_df를 활용 plot = sns.countplot(data=books_df[books_df['User Rating'] == 4.9], x='Year', palette='Purples', order=books_df[books_df['User Rating'] == 4.9].groupby(['Year'])[['Name']].nunique().index) # data label을 각각의 bar 위에 추가 for p, label in zip(plot.patches, books_df[books_df['User Rating'] == 4.9].groupby(['Year'])[['Name']].nunique()['Name']): plot.annotate(label, (p.get_x() + 0.27, p.get_height() + 0.7)) plt.ylim(0, 15); . | 해가 지날수록 bestseller들의 평균 Rating이 높아지는 경향이 있어서 그런 듯 | . | 4.9점 Rating의 bestseller를 2권 이상 보유한 작가들을 확인 . unique_books_df[unique_books_df['User Rating'] == 4.9].groupby(['Author'])[['Name']].count().sort_values(by='Name', ascending=False).head() . | Author | Name | . | Dav Pilkey | 6 | . | J. K. Rowling | 4 | . | Rush Limbaugh | 2 | . | Alice Schertle | 1 | . | Lin-Manuel Miranda | 1 | . | 2권 이상의 bestseller가 4.9점을 받은 작가의 경우, 모두 시리즈물 | . | . Review가 많은 책 확인 . ## Review 가장 많은 Top 10 책 시각화 plt.figure(figsize=(6, 5)) sns.barplot(data=unique_books_df.sort_values(by='Reviews', ascending=False).head(10), x='Reviews', y='Name', hue='Genre', palette='Purples_r'); . | Top 10 중 한 권만 Non-Fiction 장르 | 특히 2012 ~ 2016을 중심으로 Review가 높은 Fiction 장르 책이 많았기 때문인 듯 | . Price가 높은 책 확인 . | Price가 가장 높은 Top 10 책 시각화 plt.figure(figsize=(6, 5)) sns.barplot(data=unique_books_df.sort_values(by='Price', ascending=False).head(10), x='Price', y='Name', hue='Genre', palette='Purples'); . | 4권이 Fiction, 6권이 Non-Fiction | 다만, Fiction 중 가장 가격이 높은 2개의 상품은 Twilight과 Harry Potter 시리즈 세트 판매 상품 | . | Fiction 중 시리즈 세트 판매 상품들을 판별 . condition1 = unique_books_df['Name'].str.contains('Saga Collection') condition2 = unique_books_df['Name'].str.contains('Boxed Set') condition3 = unique_books_df['Name'].str.contains('Box Set') condition4 = unique_books_df['Name'].str.contains('Fifty Shades of Grey / ') condition5 = unique_books_df['Name'].str.contains('A Game of Thrones /') unique_books_df[condition1 | condition2 | condition3 | condition4 | condition5] . |   | Name | Author | User Rating | Reviews | Price | Year | Genre | . | 130 | Harry Potter Paperback Box Set (Books 1-7) | J. K. Rowling | 4.8 | 13471 | 52 | 2016 | Fiction | . | 187 | A Game of Thrones / A Clash of Kings / A Storm of Swords / A Feast of Crows / A Dance with Dragons | George R. R. Martin | 4.7 | 19735 | 30 | 2014 | Fiction | . | 212 | Game of Thrones Boxed Set: A Game of Thrones/A Clash of Kings/A Storm of Swords/A Feast for Crows | George R. R. Martin | 4.6 | 5594 | 5 | 2013 | Fiction | . | 228 | The Hunger Games Trilogy Boxed Set (1) | Suzanne Collins | 4.8 | 16949 | 30 | 2012 | Fiction | . | 251 | Fifty Shades Trilogy (Fifty Shades of Grey / Fifty Shades Darker / Fifty Shades Freed) | E L James | 4.5 | 13964 | 32 | 2012 | Fiction | . | 300 | Percy Jackson and the Olympians Paperback Boxed Set (Books 1-3) | Rick Riordan | 4.8 | 548 | 2 | 2010 | Fiction | . | 330 | The Twilight Saga Collection | Stephenie Meyer | 4.7 | 3801 | 82 | 2009 | Fiction | . | 시리즈 세트 판매 상품을 제외한 Price Top 10 temp = unique_books_df[~(condition1 | condition2 | condition3 | condition4 | condition5)] plt.figure(figsize=(6, 5)) sns.barplot(data=temp.sort_values(by='Price', ascending=False).head(10), x='Price', y='Name', hue='Genre', palette='Purples'); . | Fiction 3권, Non-Fiction 7권 | . | 시리즈 세트 판매 상품을 제외한 Price 분포를 파악 . sns.catplot(data=temp, x='Genre', y='Price', kind='box', palette='Purples'); . | 시리즈 세트 판매 상품을 제외한 장르별 평균 차이를 확인 . sns.barplot(data=temp, x='Genre', y='Price', palette='Purples'); . → 독립표본 t-test로 평균의 차이가 유의미한지 확인 . temp_fiction_df = temp[temp['Genre'] == 'Fiction'] temp_non_fiction_df = temp[temp['Genre'] == 'Non Fiction'] # Levene의 등분산 검정 lev_result = stats.levene(temp_fiction_df['Price'], temp_non_fiction_df['Price']) print('LeveneResult(F) : %.2f \\np-value : %.3f' % (lev_result)) . LeveneResult(F) : 2.50 p-value : 0.115 . # 등분산인 독립표본 t-test 실행 t_result = stats.ttest_ind(temp_fiction_df['Price'], temp_non_fiction_df['Price'], equal_var=True) print('t statistic : %.2f \\np-value : %.3f' % (t_result)) . t statistic : -2.67 p-value : 0.008 . | 소설 시리즈 세트 판매 상품을 제외하면, Non-Fiction의 가격이 대체로 다소 높은 편이라고 할 수 있을 듯 | . | . ",
    "url": "https://chaelist.github.io/docs/kaggle/amazon_bestsellers/#rating-review-price-%EC%83%81%EC%9C%84%EA%B6%8C-%EB%8F%84%EC%84%9C-%ED%99%95%EC%9D%B8",
    "relUrl": "/docs/kaggle/amazon_bestsellers/#rating-review-price-상위권-도서-확인"
  },"6": {
    "doc": "App Review 수집",
    "title": "App Review 수집",
    "content": ". | Google Play Store Scraping . | App Info 수집 | App Review 수집 | 특정 기간의 App Review만 수집 | . | App Store Scraping . | App Review 수집 | . | . ",
    "url": "https://chaelist.github.io/docs/webscraping/app_review/",
    "relUrl": "/docs/webscraping/app_review/"
  },"7": {
    "doc": "App Review 수집",
    "title": "Google Play Store Scraping",
    "content": ". | Google-Play-Scraper library를 활용 | pip install google-play-scraper로 설치해서 사용 | . App Info 수집 . from google_play_scraper import app # https://play.google.com/store/apps/details?id=com.frograms.wplay result = app( 'com.frograms.wplay', # app id (package name) lang='ko', # default: 'en' country='kr' # defaul: 'us' ) print(result) . {'title': '왓챠', 'description': \"- 영화/드라마/예능/애니메이션/다큐멘터리 무제한 스트리밍/다운로드 감상 앱.\\r\\n- 스마트폰, 태블릿, PC, 맥, 스마트 TV, 셋톱박스, 크롬캐스트에서 모두, 최고의 화질로.\\r\\n- 이제는 온 가족과 함께 즐기세요!\\r\\n- 왓챠플레이가 많은 분들이 불러주시는 그 이름 그대로 '왓챠'로 다시 태어났습니다.\\r\\n\\r\\n[제품 설명]\\r\\n\\r\\n• 스트리밍뿐 아니라, 다운로드 및 오프라인 재생 기능까지.\\r\\n• 스마트폰, 태블릿, PC, 맥, 스마트 TV, 셋톱박스, 크롬캐스트에서 모두.\\r\\n• HD부터 Ultra HD 4K까지 최고의 화질로.\\r\\n• 세계 최고 수준의 추천 엔진으로 내 취향에 맞는 작품만.\\r\\n• 신생아도 직관적으로 조작할 수 있는, 쉽고 편한 인터페이스.\\r\\n• 영화, 드라마, 예능뿐 아니라 다큐멘터리, 애니메이션까지.\\r\\n• 가입하고 둘러보는 건 평생 무료. 일단 다운로드해 보세요.\\r\\n\\r\\n\\r\\n (생략)} . App Review 수집 . from google_play_scraper import Sort, reviews result, continuation_token = reviews( 'com.frograms.wplay', lang='ko', # default: 'en' country='kr', # default: 'us' sort=Sort.MOST_RELEVANT, # default: Sort.MOST_RELEVANT count=3, # default: 100 filter_score_with=5 # default: None (모든 평점을 다 가져옴) ) result . [{'at': datetime.datetime(2021, 10, 16, 8, 2, 12), 'content': '짱구 극장판보며 우는 중..... 최신화 업데이트 해주시면 안될까요 보고싶은데ㅠㅠ', 'repliedAt': None, 'replyContent': None, 'reviewCreatedVersion': '1.9.97', 'reviewId': 'gp:AOqpTOFVBSK8EhoqhobkcZbFrDmaaLdIdsIVAC2-oKDVJSgRX62jWkQzMVbMvosS2-GjoA_BC96wOkYq0DkoVnw', 'score': 5, 'thumbsUpCount': 1, 'userImage': 'https://play-lh.googleusercontent.com/a/AATXAJzCaKwsYBHvCTY6ztFEq_F-WmRahkWIF1hhHQct=mo', 'userName': '슬구'}, {'at': datetime.datetime(2021, 10, 1, 16, 17, 43), 'content': '속도조절 생겨서 좋은데 좀 더 디테일한 속도 조절이였으면 좋겠습니다. 1.25배, 1.5배는 너무 빨라서 보기가 불편합니다. 1.1배부터 다양한 조절이 가능했으면 좋겠네요. 곰플도 되는데 왓챠에서 안될순없죠~ 부탁드립니다.', 'repliedAt': None, 'replyContent': None, 'reviewCreatedVersion': '1.9.96', 'reviewId': 'gp:AOqpTOFdH8PODeF-Al6Xo04_9IPHIDxdf3a5A2br0lHgtLrvi0ntmSW6hYmbUF9tv2x_6LcD5rIaAKSorWrBZZw', 'score': 5, 'thumbsUpCount': 20, 'userImage': 'https://play-lh.googleusercontent.com/a/AATXAJwiQJj1XEdXv4TfeilZfMufI8bKddTSyQFnKVGv=mo', 'userName': '안희수'}, {'at': datetime.datetime(2021, 10, 12, 8, 53, 16), 'content': '왓챠 잘사용하는 유저입니다 카드로결제한거는 해지해서 10월3일쯤 해지된다고하셔서 취소하고 구글기프트카드로 결제햇는데 왜 돈을 안돌려주시죠?돈돌려주시면 좋겟습니다', 'repliedAt': None, 'replyContent': None, 'reviewCreatedVersion': '1.9.96', 'reviewId': 'gp:AOqpTOEqFS_DVBEaph1OgYa9IwVZxtghfWKYxgBALGM_jN6Q92zsy1MdlnSDJIKXmUJ4HY_VOWEt_XXWlEJh21c', 'score': 5, 'thumbsUpCount': 3, 'userImage': 'https://play-lh.googleusercontent.com/a-/AOh14GgNhr4vwFU8hQyPbFLYGnSoDfJCJFUBCXhxU5aO', 'userName': '준튜브JUNTUBE'}] . +) continuation_token을 설정해주면 이어서 수집 가능 . result, _ = reviews( 'com.frograms.wplay', continuation_token=continuation_token # default: None (처음부터 수집) ) result . [{'at': datetime.datetime(2021, 8, 2, 14, 59, 45), 'content': '저 같은 경우에는 오류도 하나도 없고 콘텐츠도 다양해서 너무 좋습니다ㅜㅜㅜ 잘 알려지지 않은 영화라던가 단편이라던가 앞으로도 더욱 많이 들여와주시면 감사하겠구요ㅜ 그리고 혹시 각 작품마다 공유버튼을 만들어주시면 왓챠를 홍보하기 훨씬 편할것같아요!ㅎㅎ 앞으로도 소비자를 위한 왓챠가 되어주길 바랍니당♡', 'repliedAt': None, 'replyContent': None, 'reviewCreatedVersion': '1.9.85', 'reviewId': 'gp:AOqpTOEH6UJtiev2mz0oCU3vsn_N90fUIM1BYbLbxh6WU6Lu_elLAphl8J7mmFkOrF1DZ510b2AmURAuinSVncc', 'score': 5, 'thumbsUpCount': 10, 'userImage': 'https://play-lh.googleusercontent.com/a-/AOh14Gh5dTIVYgAtJZdgYj99tpKD3NMBaC7QBmRno8ZSng', 'userName': '정규반배하민'}, {'at': datetime.datetime(2021, 9, 21, 4, 52, 21), 'content': '좋은 컨텐츠 계속 제공해주셨으면 좋겠어요!!&gt;_&lt; 각종 영화제에서 상을 받거나 우수한 평가를 받았던 작품들도,,, 부탁드려요 ㅎㅅㅎ', 'repliedAt': None, 'replyContent': None, 'reviewCreatedVersion': '1.9.96', 'reviewId': 'gp:AOqpTOE1zthMN8AlbYemq_2IZpwlZ32xOK-46-s39ZSZZvcuZm_Ejasisx4EM1hHrd2nvcS9HzoNKEEVH54k0_Q', 'score': 5, 'thumbsUpCount': 5, 'userImage': 'https://play-lh.googleusercontent.com/a/AATXAJxrul7Th625uvpvsdeXIDcqQjmFlfw2_5t9y8ak=mo', 'userName': '방지영'}, {'at': datetime.datetime(2021, 9, 23, 13, 49, 42), 'content': '넷플릭스보다 볼게 많아서 아주 좋습니다 아쉬운건 애니들 언어 선택이 가능 했으면 좋겟습니다 원피스 원어로 보고 싶었는데 ㅜ', 'repliedAt': None, 'replyContent': None, 'reviewCreatedVersion': '1.9.96', 'reviewId': 'gp:AOqpTOE3L6Kscm9b2rdPafySeISRRREPHxa-qWa_N6HphWdcYqtqFVpU7F24zSk5i6xs-wb0AL5NUol29Coqi7U', 'score': 5, 'thumbsUpCount': 7, 'userImage': 'https://play-lh.googleusercontent.com/a-/AOh14GghVbKmfdpD-Ld65jdsw5k5l-oq05-IceGUJNnB', 'userName': '짬뽕짜장면'} . | 앞서 받아온 ‘continuation_token’을 넣어주면, 그 다음부터 이어서 수집이 가능하다. | 설정도 그대로 유지됨 (5점 리뷰만, 관련도순으로 3개 수집) | . 특정 기간의 App Review만 수집 . | ex) 2020 ~ 2021 기간의 Watcha 리뷰만 모두 수집해오기: | . from google_play_scraper import Sort, reviews import pandas as pd year = 2021 token = None review_list = [] while year &gt;= 2020: # 2020년 ~ 2021년만 수집 result, continuation_token = reviews( 'com.frograms.wplay', lang = 'ko', # default: 'en' country = 'kr', # default: 'us' continuation_token = token, sort = Sort.NEWEST, # default: Sort.MOST_RELEVANT count = 100, # default: 100 filter_score_with = None # default: None (모든 평점을 다 가져옴) ) token = continuation_token year = result[-1]['at'].year for review in result: if review['at'].year &gt;= 2020: # 2020년의 리뷰까지만 저장 temp_list = [review['score'], review['content'], review['at']] review_list.append(temp_list) review_df = pd.DataFrame(review_list, columns=['score', 'content', 'date']) print(len(review_df)) review_df.head() . 7378 . |   | score | content | date | . | 0 | 1 | 2주 무료체험을 하려고 등록해놓았다가 2주 무료체험이 안되서 하지도 않았는데 2주가… | 2021-10-19 09:29:50 | . | 1 | 5 | 진짜 | 2021-10-19 07:58:21 | . | 2 | 1 | 왓챠 앱지워도 결제계속떠서 결제안해도 될거에 계속해야하니 짜증나요. 해지도 않되고 | 2021-10-19 01:15:30 | . | 3 | 5 | 일본드라마가 많이 있어서 좋아요 | 2021-10-18 15:21:52 | . | 4 | 4 | 꼭 집을 수는 없지만 조금 아쉬움이….😝 | 2021-10-18 13:41:19 | . ",
    "url": "https://chaelist.github.io/docs/webscraping/app_review/#google-play-store-scraping",
    "relUrl": "/docs/webscraping/app_review/#google-play-store-scraping"
  },"8": {
    "doc": "App Review 수집",
    "title": "App Store Scraping",
    "content": ". | App-Store-Scraper library를 활용 | pip install app-store-scraper로 설치해서 사용 | . App Review 수집 . | app 정보를 입력해 객체 생성 from app_store_scraper import AppStore # https://apps.apple.com/kr/app/watcha/id1096493180 my_app = AppStore(country='kr', app_name='watcha', app_id='1096493180') # app_id는 optional print(my_app) # 객체 검증 . Country | kr Name | watcha ID | 1096493180 URL | https://apps.apple.com/kr/app/watcha/id1096493180 Review count | 0 . | 리뷰 수집 . | ex) 2020 ~ 2021 기간의 Watcha 리뷰만 모두 수집해오기: | . import datetime as dt import numpy as np my_app.review(after=dt.datetime(2020, 1, 1), sleep=np.random.randint(0, 2)) . | my_app.review(how_many, after, sleep)의 형태 | how_many = int (적어주지 않으면 모든 리뷰를 다 가져온다) | after = datetime (optional, 특정 시간 이후의 리뷰만 가져오겠다는 의미) | sleep = int (optional, parameter to specify seconds to sleep between each call) | . | 가져온 리뷰를 dataframe으로 정리 . fetched_reviews = my_app.reviews ios_review_df = pd.DataFrame(fetched_reviews) print(len(ios_review_df)) ios_review_df .head() . 5069 . |   | userName | isEdited | rating | review | title | date | developerResponse | . | 0 | 저내림 | False | 5 | 개발자 및 경영진분들 꼭 보시라고 별5개 답니다????? 꼭 읽으세요… | 버퍼링실화냐??? | 2020-05-26 12:27:32 | nan | . | 1 | dkqjfsqfe | False | 5 | 넷플릭스에서 왓챠로 넘어온지 2개월째입니당\\n음 영화를 좋아하시는 분이라면… | 너무좋아요❤️ | 2020-04-06 06:40:28 | nan | . | 2 | jsa38 | False | 5 | 안녕하세요! 코로나때문에 너무 심심해서 넷플로 먼저 시작했는데 너무 볼 것이 없어… | 왓챠 최고예요❤️ | 2020-08-25 01:03:45 | nan | . | 3 | 샌드위치냠냠냠 | False | 5 | 넷플릭스도 쓰고 왓챠도 쓰는데 개인적으로 넷플릭스는 오리지널이 있지만 왓챠가 볼건… | 너무 좋습니다 | 2020-04-26 16:13:03 | nan | . | 4 | dkjfew | False | 5 | 맥북으로 보시려면 앱스토어에서 아이패드용 앱으로 다운 받으시면 됩니다~ 그리고… | 맥북으로 왓챠 볼 수 있어요~! | 2021-03-01 10:45:08 | nan | . | . ",
    "url": "https://chaelist.github.io/docs/webscraping/app_review/#app-store-scraping",
    "relUrl": "/docs/webscraping/app_review/#app-store-scraping"
  },"9": {
    "doc": "Classification 1",
    "title": "Classification 1",
    "content": ". | Classification | KNN (K Nearest Neighbors) . | 기본 개념 | scikit-learn으로 구현하기 | . | Logistic Regression . | 기본 개념 | scikit-learn으로 구현하기 | . | . ",
    "url": "https://chaelist.github.io/docs/ml_basics/classification1/",
    "relUrl": "/docs/ml_basics/classification1/"
  },"10": {
    "doc": "Classification 1",
    "title": "Classification",
    "content": ": supervised learning 중, 종속변수가 범주형 변수인 문제를 해결하는 방식. | cf) 종속변수가 연속 변수인 문제는 Linear Regression 모형이 적합 | .   . *Classification problems . | Binary classification: 종속변수의 값이 2가지 → 보통 0과 1로 표현 . | ex) 어떤 사람이 감기인지 아닌지 | ex) 영화가 성공인지 실패인지 | ex) 소비자가 A제품을 사는지 안사는지 | . | Multiclass classification: 종속변수 값이 3가지 이상. | ex) 뉴스 기사의 카테고리 분류 | ex) 영화의 장르 분류 | ex) 학점 분류 (A ~ F) | . | . ",
    "url": "https://chaelist.github.io/docs/ml_basics/classification1/#classification",
    "relUrl": "/docs/ml_basics/classification1/#classification"
  },"11": {
    "doc": "Classification 1",
    "title": "KNN (K Nearest Neighbors)",
    "content": ": 가장 가까운 K개의 neighbor가 속한 class로 배정하는 classification 방식. 기본 개념 . | ex) K=1이라면, 새로운 datapoint의 class는 가장 가까운 하나의 neighbor의 class에 따라 배정된다. | ex) K=5라면, 새로운 datapoint의 class는 가장 가까운 5개의 neighbor 중 가장 많은 수가 속한 class에 따라 배정된다. | 보통, class(배정하게 되는 집단)의 수가 짝수일 경우, K는 홀수로 한다 | 최적의 K값은 dataset마다 다르므로, 다양한 K값일 때의 성능을 체크해보는 게 좋다 | . (출처: datacamp) .   . *vector간 유사도 계산 방법 . | Euclidean distance: xy = sqrt(sum((x - y)2)) . | x, y는 각각 하나의 벡터(데이터포인트)를 의미 | 가장 기본적인 거리 계산 방식 | 높은 차원에서의 계산에는 효과가 크지 않을 수 있다 (cosine 유사도가 더 좋은 결과를 낼 수 있음) | . | Cosine 유사도: cosθ = x∙y / lxllyl . | x∙y는 벡터 간 내적곱, lxl: 원점에서 벡터x까지의 거리 | cosθ 값이 1에 가까울수록 벡터 사이 유사도가 큰 것. (cosθ가 1에 가까울수록 사이각이 0에 가까움) | 정확히 ‘길이’의 차이를 반영하는 것은 아니라는 단점 (사이각이 작아도 두 벡터 간 거리가 멀 수 있음) | 하지만 normalization / standardization 과정을 거치고 나면 길이의 차이는 유의미하지 않기 때문에 cosine 유사도로 벡터 간 유사도를 충분히 구분할 수 있음. | . | . scikit-learn으로 구현하기 . from sklearn.datasets import load_iris # iris 데이터 분러오기 from sklearn.model_selection import train_test_split # 데이터셋을 training set / test set 나누기 위한 함수 from sklearn.neighbors import KNeighborsClassifier # KNN Classifier import pandas as pd . 1. 데이터 준비 . iris_dataset = load_iris() # 데이터셋을 가져와준다 . +) print(iris_dataset.DESCR)를 해주면 데이터셋에 대한 정보를 살펴볼 수 있음 . | 독립변수: Sepal(꽃받침) length, Sepal width, Petl(꽃잎) length, Petal width의 4개 | 결과변수: Iris-Setosa, Iris-Versicolour, Iris-Virginica 이렇게 3종류 . | 데이터셋에는 3종류의 붓꽃(iris)가 1/3씩(=50개씩) 포함되어 있음 | . | .   . → 데이터 정리 . # X에 boston dataset의 입력변수들 &amp; 해당 입력변수 명칭들 정리 X = pd.DataFrame(iris_dataset.data, columns=iris_dataset.feature_names) X.head() . |   | sepal length (cm) | sepal width (cm) | petal length (cm) | petal width (cm) | . | 0 | 5.1 | 3.5 | 1.4 | 0.2 | . | 1 | 4.9 | 3 | 1.4 | 0.2 | . | 2 | 4.7 | 3.2 | 1.3 | 0.2 | . | 3 | 4.6 | 3.1 | 1.5 | 0.2 | . | 4 | 5 | 3.6 | 1.4 | 0.2 | .   . # 목표변수도 dataframe으로 정리 y = pd.DataFrame(iris_dataset.target, columns=['Class']) y.head() . |   | Class | . | 0 | 0 | . | 1 | 0 | . | 2 | 0 | . | 3 | 0 | . | 4 | 0 | .   . y.Class.unique() ## 0: Iris-Setosa, 1: Iris-Versicolour, 2: Iris-Virginica . array([0, 1, 2]) . 2. train_test_split . X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=5) # 20%를 test set으로 선택 # 잘 나뉘었나 확인 print(X_train.shape) print(X_test.shape) print(y_train.shape) print(y_test.shape) . (120, 4) (30, 4) (120, 1) (30, 1) . 3. 모델 학습시키기 . y_train = y_train.values.ravel() #ravel(): 다차원 array를 1차원 array로 평평하게 펴주는 함수. 안써도 되지만 안쓰면 경고가 뜸. model = KNeighborsClassifier(n_neighbors=5) # 가장 근접한 5개 이웃의 class에 따라 분류하겠다는 뜻 model.fit(X_train, y_train) . KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski', metric_params=None, n_jobs=None, n_neighbors=5, p=2, weights='uniform') . | metric: 사용할 distance metric. (벡터 간 유사도(거리)를 어떤 방식으로 측정할 것인지) . | default=’minkowski’ (+. 가능한 metrics) | minkowski 방식에서 p=2 (default)면 standard Euclidean metric와 동일한 계산식 | . | . 4. test data로 성능 체크 . model.predict(X_test) # 어떻게 분류했나 확인 . array([1, 2, 2, 0, 2, 1, 0, 2, 0, 1, 1, 2, 2, 2, 0, 0, 2, 2, 0, 0, 1, 2, 0, 2, 1, 2, 1, 1, 1, 2]) . y_test['class'].to_numpy() ## 실제 y_test와 비교 . array([1, 2, 2, 0, 2, 1, 0, 1, 0, 1, 1, 2, 2, 2, 0, 0, 2, 2, 0, 0, 1, 2, 0, 1, 1, 2, 1, 1, 1, 2]) . # 몇 퍼센트가 올바르게 분류되었는지 확인 model.score(X_test, y_test) . 0.9333333333333333 . 약 93% 정도가 올바르게 분류되었다는 의미 . ",
    "url": "https://chaelist.github.io/docs/ml_basics/classification1/#knn-k-nearest-neighbors",
    "relUrl": "/docs/ml_basics/classification1/#knn-k-nearest-neighbors"
  },"12": {
    "doc": "Classification 1",
    "title": "Logistic Regression",
    "content": "기본 개념 . (출처: incredible.ai) . | 가설 함수: sigmoid 함수 (학습 = 데이터에 가장 잘 맞는 sigmoid 함수를 찾는 것) . | sigmoid 함수는 0과 1 사이의 연속적인 결과값을 갖기에 ‘regression(회귀)’라고 이름이 붙지만, 보통 sigmoid 함수의 결과값이 0.5보다 큰지 작은지를 보고 ‘분류’를 하는 방식으로 주로 사용한다 | . | 0과 1 어느 쪽에 가까운지를 판별하는 binary classification 모델이지만, 3개 이상의 분류에도 사용 가능하다 | *3개 이상의 Class를 분류하는 방법: (ex. 메일을 업무 메일 / 사적 메일 / 스팸 메일 3종류로 분류할 때) . | 업무 메일인지 아닌지 판별하는 활성 함수 → 업무 메일일 확률을 구함 | 사적 메일인지 아닌지 판별하는 활성 함수 → 사적 메일일 확률을 구함 | 스팸 메일인지 아닌지 판별하는 활성 함수 → 스팸 메일일 확률을 구함 | 각 datapoint를 가장 확률이 높은 쪽으로 분류. (ex. 스팸 메일일 확률이 가장 높다면, 스팸 메일로 분류) | . | 손실함수: 로그 손실 = log loss = cross entropy . (출처: codeit) . | y가 0인 경우: 0에 가깝게 분류할수록 로그 손실이 0에 가깝고, 1에 가깝게 분류할수록 로그 손실이 무한대에 가까워짐 | y가 1인 경우: 1에 가깝게 분류할수록 로그 손실이 0에 가깝고, 0에 가깝게 분류할수록 로그 손실이 무한대에 가까워짐 | sklearn의 Logistic Regression은 Gradient Descent 방식으로 손실함수가 0에 가까워지는 점을 찾는다 | . | . scikit-learn으로 구현하기 . from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression import pandas as pd . 1. 데이터 준비 . # KNN에서와 동일하게 iris data 사용 iris_data = load_iris() # dataframe으로 옮기기 X = pd.DataFrame(iris_data.data, columns=iris_data.feature_names) y = pd.DataFrame(iris_data.target, columns=['class']) . X.head() . |   | sepal length (cm) | sepal width (cm) | petal length (cm) | petal width (cm) | . | 0 | 5.1 | 3.5 | 1.4 | 0.2 | . | 1 | 4.9 | 3 | 1.4 | 0.2 | . | 2 | 4.7 | 3.2 | 1.3 | 0.2 | . | 3 | 4.6 | 3.1 | 1.5 | 0.2 | . | 4 | 5 | 3.6 | 1.4 | 0.2 | .   . y.head() . |   | Class | . | 0 | 0 | . | 1 | 0 | . | 2 | 0 | . | 3 | 0 | . | 4 | 0 | . 2. train_test_split &amp; 학습 . X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=5) y_train = y_train.values.ravel() #ravel(): 다차원 array를 1차원 array로 평평하게 펴주는 함수. model = LogisticRegression(solver='saga', max_iter=2000) . | solver: 모델을 최적화할 때 어떤 알고리즘을 사용할지 선택하는 것. (=어떤 경사하강법을 사용할지) . | SAGA: SAG(Stochastic Average Gradient) 알고리즘을 개선한 버전의 알고리즘. 유사하지만 더 성능이 좋다고 함 | +. 가능한 solver (dataset의 특성에 맞게 적절히 선택) | default는 lbfgs. | . | max_iter: 최적화 과정을 몇 번 반복할 지를 정해주는 것. | max_iter=2000이라고 해도 만약 2000번 돌기 전에 최적화되면 그 때 이미 멈춘다 | default는 100. | . | . model.fit(X_train, y_train) . LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True, intercept_scaling=1, l1_ratio=None, max_iter=2000, multi_class='auto', n_jobs=None, penalty='l2', random_state=None, solver='saga', tol=0.0001, verbose=0, warm_start=False) . | C: Inverse of regularization strength. 0 ~ 1 사이의 float 값을 지정할 수 있으며, 기본 세팅은 1.0 . | 숫자가 작을수록 penalty를 강하게 준다는 뜻 (= stronger regularization) | λ = 1 / C → C가 작을수록 λ (penalty)가 커진다 | λ는 θ값(변수별 가중치)에 붙는 penalty의 개념. (θ값이 너무 커지는 것을 방지해준다) | . | penalty: L1과 L2 Regularization 중 어느 것을 선택할지를 의미. (default는 L2) . | (Regularization에 대해서는 추후 자세히 설명 예정) | . | multi_class: default로 auto라고 되어 있는데, binary problem이거나 solver = ‘liblinear’인 경우에는 ‘ovr’로 선택하고, 그 외에는 ‘multinomial’로 선택해준다 . | 이 경우, multiclass problem이므로 알아서 ‘multinomial’로 선택되었을 것. | . | . 3. test data로 성능 체크 . model.predict(X_test) # 어떻게 분류했나 확인 . array([1, 2, 2, 0, 2, 1, 0, 2, 0, 1, 1, 2, 2, 2, 0, 0, 2, 2, 0, 0, 1, 2, 0, 1, 1, 2, 1, 1, 1, 2]) . y_test['class'].to_numpy() ## 실제 y_test와 비교 . array([1, 2, 2, 0, 2, 1, 0, 1, 0, 1, 1, 2, 2, 2, 0, 0, 2, 2, 0, 0, 1, 2, 0, 1, 1, 2, 1, 1, 1, 2]) . # 몇 퍼센트가 올바르게 분류되었는지 확인 model.score(X_test, y_test) . 0.9666666666666667 . 약 97% 정도가 올바르게 분류되었다는 의미 . ",
    "url": "https://chaelist.github.io/docs/ml_basics/classification1/#logistic-regression",
    "relUrl": "/docs/ml_basics/classification1/#logistic-regression"
  },"13": {
    "doc": "Classification 2",
    "title": "Classification 2",
    "content": ". | Decision Tree (결정 트리) . | 기본 개념 | scikit-learn으로 데이터 학습 | tree 구조 확인 | 속성 중요도 확인 | . | Random Forest . | 기본 개념 | scikit-learn으로 데이터 학습 | 속성 중요도 확인 | . | AdaBoost . | 기본 개념 | scikit-learn으로 데이터 학습 | 속성 중요도 확인 | . | . ",
    "url": "https://chaelist.github.io/docs/ml_basics/classification2/",
    "relUrl": "/docs/ml_basics/classification2/"
  },"14": {
    "doc": "Classification 2",
    "title": "Decision Tree (결정 트리)",
    "content": ": 예/아니오로 답할 수 있는 어떤 질문들이 있고, 그 질문들의 답을 따라가면서 데이터를 분류하는 알고리즘 . 기본 개념 . | 가장 위에 있는 질문 노드를 ‘root 노드’라고 하고, 트리의 가장 끝에 있는 분류 노드들을 ‘leaf 노드’라고 한다. | leaf node는 사망/생존, or 팽귄/돌고래 이런 식으로 특정 예측값을 가지고 있고 (= 분류 노드) , 나머지 노드들은 예/아니오(True/False)로 답할 수 있는 질문들을 가지고 있다 | . (출처: javatpoint.com) .   . *트리의 노드 만들기 . | 여러 질문 노드와 분류 노드의 ‘지니 불순도’를 계산해서, 지니 불순도가 가장 낮은 질문을 노드로 만들어준다. | 질문 후보들보다 분류 노드(ex. ‘독감으로 분류’)의 지니 불순도가 낮으면 그냥 바로 분류해주는 단계로 넘어간다 (leaf node) | ex) 독감 / 감기 분류: 특정 데이터셋을 1) 모두 독감으로 분류(분류 노드)할 때의 지니 불순도가 0.49, 2) ‘고열이 있나요?’ 질문 노드로 분류할 때의 지니 불순도가 0.34, 3) ‘몸살이 있나요?’ 질문 노드로 분류할 때의 지니 불순도가 0.33이라면 ‘몸살’ 질문 노드를 골라준다! | ※질문노드의 지니불순도는 이로 인해 분류된 2개의 데이터셋의 지니 불순도의 평균으로 계산 | 데이터가 숫자형인 경우(ex. 체온): 데이터를 정렬한 후, 각 연속된 데이터의 평균을 계산 → 이 평균을 이용해 질문을 하나씩 만듦 (ex. 36.4도가 넘나요?, 36.6도가 넘나요?,…) → 이 중 지니 불순도가 가장 낮은 질문을 선정 | .   . *지니 불순도(Gini Impurity) . | 데이터셋의 데이터들이 얼마나 혼합되어 있는지 나타내는 수치. | 지니 불순도가 작을수록 데이터셋이 순수하다는 뜻. | 독감 &amp; 일반 감기 데이터가 섞여 있는 데이터셋의 지니 불순도: 1 - pflu2 - pnot_flu2 (*p는 확률) . | ex) 데이터 100개 중 70개가 독감 30개가 일반 감기 → 1 - 0.72 - 0.32 = 0.42 | ex) 50개 독감 50개 일반 감기 → 1 - 0.52 - 0.52 = 0.5 | ex) 100개 모두 독감 → 1 - 1.02 - 0.02 = 0 | . | 세 가지 class가 섞여있는 경우: 1 - pclass12 - pclass22 - pclass32 | . scikit-learn으로 데이터 학습 . from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.tree import DecisionTreeClassifier import matplotlib.pyplot as plt import numpy as np import pandas as pd . 1. 데이터 준비 . # 데이터 준비: KNN, Logistic Regression에서와 동일하게 iris data 사용 iris_data = load_iris() X = pd.DataFrame(iris_data.data, columns=iris_data.feature_names) y = pd.DataFrame(iris_data.target, columns=['class']) . X.head() . |   | sepal length (cm) | sepal width (cm) | petal length (cm) | petal width (cm) | . | 0 | 5.1 | 3.5 | 1.4 | 0.2 | . | 1 | 4.9 | 3 | 1.4 | 0.2 | . | 2 | 4.7 | 3.2 | 1.3 | 0.2 | . | 3 | 4.6 | 3.1 | 1.5 | 0.2 | . | 4 | 5 | 3.6 | 1.4 | 0.2 | .   . y.head() . |   | Class | . | 0 | 0 | . | 1 | 0 | . | 2 | 0 | . | 3 | 0 | . | 4 | 0 | . 2. train_test_split &amp; 학습 . X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=5) y_train = y_train.values.ravel() #ravel(): 다차원 array를 1차원 array로 평평하게 펴주는 함수. model = DecisionTreeClassifier(max_depth=4) . | max_depth: maximum depth of the tree. (트리가 몇 층까지 내려 가는지 = depth of tree) . | max_depth를 설정해주지 않으면 모든 leaf가 pure해질 때까지 OR 모든 leaf가 min_samples_split보다 적은 양의 데이터를 가질 때까지 무한정 node가 생성된다 | depth가 너무 깊으면 training data에 과적합될 수 있기에, max_depth를 적절히 세팅해주면 좋다 | . | . model.fit(X_train, y_train) . DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini', max_depth=4, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, presort='deprecated', random_state=None, splitter='best') . | criterion: split의 qulaity를 측정하는 방식. ‘gini’(=Gini impurity)와 ‘entropy’(=information gain) 두 가지 옵션이 있으며, gini가 default. | gini와 entropy는 사실 계산의 차이가 크지는 않기에, 어떤 걸 사용하든지 큰 상관 없다 | . | . 3. test data로 성능 체크 . model.predict(X_test) # 어떻게 분류했나 확인 . array([1, 1, 2, 0, 2, 2, 0, 2, 0, 1, 1, 1, 2, 2, 0, 0, 2, 2, 0, 0, 1, 2, 0, 1, 1, 2, 1, 1, 1, 2]) . y_test['class'].to_numpy() ## 실제 y_test와 비교 . array([1, 2, 2, 0, 2, 1, 0, 1, 0, 1, 1, 2, 2, 2, 0, 0, 2, 2, 0, 0, 1, 2, 0, 1, 1, 2, 1, 1, 1, 2]) . # 몇 퍼센트가 올바르게 분류되었는지 확인 model.score(X_test, y_test) . 0.8666666666666667 . 약 87% 정도가 올바르게 분류되었다는 의미 . tree 구조 확인 . ※ 결정트리는 분류 과정을 직관적으로 이해할 수 있고, 속성별 중요도를 쉽게 해석할 수 있다 (정확도가 아주 높은 모델은 아니지만, 해석 및 적용이 쉽다는 것이 큰 장점!) . | sklearn.tree 활용 from sklearn import tree plt.figure(figsize=(7, 7)) tree.plot_tree(model, feature_names = iris_data.feature_names, class_names = iris_data.target_names, filled = True); # 색을 칠해서 구분하겠다는 뜻 . | dtreeviz 활용 . | install해야 사용 가능 https://github.com/parrt/dtreeviz | . from dtreeviz.trees import dtreeviz viz = dtreeviz(model, iris_data.data, # X값 iris_data.target, # y값: df 형태 말고 array 형태로 넣어주기 target_name = \"target\", feature_names = iris_data.feature_names, class_names = list(iris_data.target_names)) viz # +) viz.save(\"파일명.svg\") 이렇게 해서 저장 . | . 속성 중요도 확인 .   . *속성 중요도(Feature Importance) . | 속성의 ‘평균 지니 감소(Mean Gini Decrease)’라고 부르기도 함 | . *계산하는 법: . | 모든 질문 노드의 중요도(Node Importance)를 계산 | 특정 속성의 중요도: 해당 속성 질문 노드의 중요도 합 / 모든 노드의 중요도 합 . | 전체적으로 낮춰진 불순도에서, 특정 속성 하나가 불순도를 얼마나 낮췄는지 확인 → 그 속성의 중요한 정도를 계산하는 것! | ex) ‘고열 여부’ 변수의 중요도: 고열 질문을 갖는 모든 노드의 중요도 합 / 트리 안에 있는 모든 노드의 중요도 합 | . | . *노드 중요도(Node Importance) . | 특정 노드 전후로 불순도가 얼마나 낮아졌는지로 해당 노드의 중요도를 판단. | 나눠지는 데이터 셋들에 대해서 점점 더 알아간다, 또는 “더 많은 정보를 얻는다”라고 해서 이 수치를 정보 증가량 (information gain)이라고도 부름. (불순도가 낮아질수록 점점 데이터가 잘 나눠지고 있는 거니까) | . *계산하는 법: . | $ ni = \\dfrac{n}{m}GI - \\dfrac{n_{left}}{m}GI_{left} - \\dfrac{n_{right}}{m}GI_{right} $ . | n: 중요도를 계산하려는 노드까지 오는 학습 데이터의 수 | GI: 이 노드까지 오는 데이터 셋의 불순도 | m: 전체 학습 데이터의 수 | . | 한 노드에서 데이터를 두 개로 나눴을 때, 데이터 수에 비례해서 불순도가 얼마나 줄어들었는지를 계산하는 것! | .   . *python으로 계산 . # 속성들의 중요도를 확인 model.feature_importances_ # numpy 배열로 정리되어서 나옴 . array([0.04642857, 0. , 0. , 0.95357143]) . # df를 만들어 importance가 큰 순서대로 정렬 df = pd.DataFrame(list(zip(iris_data.feature_names, model.feature_importances_)), columns=['feature', 'importance']).sort_values('importance', ascending=False) df = df.reset_index(drop=True) df . |   | feature | importance | . | 0 | petal length (cm) | 0.953571 | . | 1 | petal width (cm) | 0.046429 | . | 2 | sepal length (cm) | 0.000000 | . | 3 | sepal width (cm) | 0.000000 | . +) zip(*iterable)은 동일한 개수로 이루어진 자료형을 묶어 주는 역할을 하는 함수 . list(zip(\"abc\", \"def\")) . [('a', 'd'), ('b', 'e'), ('c', 'f')] .   . import seaborn as sns plt.figure() plt.title('Feature Importances') sns.barplot(data=df, y='feature', x='importance', palette='RdPu') # palette 옵션: https://seaborn.pydata.org/generated/seaborn.color_palette.html#seaborn.color_palette plt.show(); . ",
    "url": "https://chaelist.github.io/docs/ml_basics/classification2/#decision-tree-%EA%B2%B0%EC%A0%95-%ED%8A%B8%EB%A6%AC",
    "relUrl": "/docs/ml_basics/classification2/#decision-tree-결정-트리"
  },"15": {
    "doc": "Classification 2",
    "title": "Random Forest",
    "content": ": 결정 트리 앙상블 알고리즘 중 하나. 수많은 트리들을 임의로 만들고, 이 모델들의 결과를 다수결 투표로 종합해서 예측하는 모델. (Bagging 방식의 앙상블 러닝) . | *Ensemble Learning: 하나의 모델을 쓰는 대신, 수많은 모델들을 만들고 이 모델들의 예측을 합쳐서 종합적인 예측을 하는 기법 | 결정 트리 자체는 아주 성능이 좋은 모델은 아니지만, 앙상블 기법으로 사용하면 성능이 좋은 모델을 만들 수 있음 | . 기본 개념 .   . *Random Forest 모델의 작동 방식: . | Bootstrapping으로 임의로 데이터셋을 만든다 . | Bootstrapping: 데이터셋에서 임의로 데이터를 골라와서 새로운 데이터셋을 만들어주는 방법. (※중복을 허용해서 데이터 임의 선택) | +) Bagging: Bootstrap Aggregating의 약어. (bootsrap 데이터 셋을 만들어내고, 이를 활용한 모델들의 결정을 종합(aggregate)해서 예측하는 앙상블 기법을 의미) | Bootstrap 데이터셋을 만드는 이유: 앙상블 기법을 사용할 때, 모델들을 다 똑같은 데이터셋으로 학습시키면 결과가 다 비슷하게 나와버릴 수도 있기에, 모델을 만들 때마다 각각 임의로 만든 bootstrap 데이터셋을 사용해서 학습시키는 것. | . | 임의로 수많은 결정 트리를 만든다 (질문 노드들을 어느 정도는 임의로 만듦) . | 각 속성을 사용한 질문들의 지니 불순도를 모두 구하고 가장 낮은 것으로 노드를 만드는 대신, 여러 속성 중 2개 정도를 임의로 선택 (속성이 많으면 더 많이 고를 수도 있음) | 2개 중 불순도가 더 낮은 것으로 root 노드의 질문 선택 | 그 다음에도 똑같이 속성 2개 정도를 임의로 선택해서 지니 불순도 낮을 걸 사용 … (반복) | . | 이렇게 1, 2단계를 반복하다보면, 서로 조금씩 다른 결정 트리들을 많이 많들 수 있다. → 이렇게 만든 트리들에 데이터를 넣은 후, 각 트리의 예측 값을 다수결 투표로 종합해서 최종 결정! (ex. 40개의 트리는 ‘독감’이라고 예측, 60개의 트리는 ‘일반 감기’라고 예측 → 일반 감기라고 최종 예측) | . (출처: dinhanhthi.com) . scikit-learn으로 데이터 학습 . from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.ensemble import RandomForestClassifier import matplotlib.pyplot as plt import numpy as np import pandas as pd . 1. 데이터 준비 . iris_data = load_iris() X = pd.DataFrame(iris_data.data, columns=iris_data.feature_names) y = pd.DataFrame(iris_data.target, columns=['class']) . X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=5) y_train = y_train.values.ravel() #ravel(): 다차원 array를 1차원 array로 평평하게 펴주는 함수 . 2. 학습 . model = RandomForestClassifier(n_estimators=100, max_depth=4) . | max_depth: 결정트리와 마찬가지로, 트리의 최대 깊이를 정하는 변수. 이 랜덤포레스트 모델이 만드는 모든 트리들의 최대 깊이를 정해줌. | . model.fit(X_train, y_train) . RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None, criterion='gini', max_depth=4, max_features='auto', max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None, oob_score=False, random_state=None, verbose=0, warm_start=False) . | bootstrap: False라고 하면 모든 tree에 다 whole dataset을 사용. (default=True) | max_samples: bootstrap=True인 경우, 각 tree 학습을 위해 X에서 몇 개씩의 sample을 추출할 것인지 . | None(default)인 경우, 자동으로 X.shape[0]개의 sample을 추출 (ex. 이 경우, X_train.shape[0]=120이므로 각각 120개의 element가 담긴 sample을 구성) | . | n_estimators: 몇 개의 결정트리를 만들어서 예측할 것인지 정해주는 변수. (default=100) | . 3. test data로 성능 체크 . model.predict(X_test) # 어떻게 분류했나 확인 . array([1, 1, 2, 0, 2, 1, 0, 2, 0, 1, 1, 1, 2, 2, 0, 0, 2, 2, 0, 0, 1, 2, 0, 1, 1, 2, 1, 1, 1, 2]) . y_test['class'].to_numpy() ## 실제 y_test와 비교 . array([1, 2, 2, 0, 2, 1, 0, 1, 0, 1, 1, 2, 2, 2, 0, 0, 2, 2, 0, 0, 1, 2, 0, 1, 1, 2, 1, 1, 1, 2]) . # 몇 퍼센트가 올바르게 분류되었는지 확인 model.score(X_test, y_test) . 0.9 . 약 90% 정도가 올바르게 분류되었다는 의미 . 속성 중요도 확인 . | 결정트리를 사용한 모델이기에, 결정 트리와 마찬가지로 평균 지니 감소를 이용해 속성 중요도 계산이 가능 | 랜덤 포레스트에서의 속성 중요도는 그 안의 수많은 결정 트리들의 속성 중요도의 평균값 | . # 속성 중요도 확인 model.feature_importances_ . array([0.09846022, 0.01962833, 0.35241878, 0.52949267]) . # df를 만들어 importance가 큰 순서대로 정렬 df = pd.DataFrame(list(zip(iris_data.feature_names, model.feature_importances_)), columns=['feature', 'importance']).sort_values('importance', ascending=False) df = df.reset_index(drop=True) df . |   | feature | importance | . | 0 | petal width (cm) | 0.529493 | . | 1 | petal length (cm) | 0.352419 | . | 2 | sepal length (cm) | 0.098460 | . | 3 | sepal width (cm) | 0.019628 | .   . import seaborn as sns plt.figure() plt.title('Feature Importances') sns.barplot(data=df, y='feature', x='importance', palette='RdPu') plt.show(); . | 보통, random forest 모델이 decision tree 모델보다 각 feature를 골고루 반영해서 예측한다. (does not depend highly on any specific set of features) | . ",
    "url": "https://chaelist.github.io/docs/ml_basics/classification2/#random-forest",
    "relUrl": "/docs/ml_basics/classification2/#random-forest"
  },"16": {
    "doc": "Classification 2",
    "title": "AdaBoost",
    "content": ": Adaptive Boosting. - Boosting 기법을 사용한 앙상블 러닝 알고리즘 중 하나. 기본 개념 .   . *Boosting 기법: . | 일부러 성능이 안좋은 모델(weak learner)을 사용 | 더 먼저 만든 모델의 성능에 따라 뒤에 있는 모델이 사용할 데이터셋을 바꾼다 | 모델별 성능의 차이를 반영해서 모델의 예측을 종합한다 (성능이 좋은 모델의 예측을 더 반영) | .   . *AdaBoost 작동 방식: . | 스텀프(stump)를 사용 . | 스텀프: root 노드 하나와 분류 노드 두 개를 갖는 얕은 결정 트리. 보통 50%보다 조금 나은 정도의 성능. | . | 특정 결정 스텀프가 분류한 결과를 보고, 맞게 분류한 애들은 중요도를 낮추고 틀리게 분류한 애들은 중요도를 높여준다 | 다음 스텀프는 앞의 스텀프의 분류 결과에 따라 중요도가 조정된 데이터셋을 사용해 학습한다 . | 뒤의 스텀프가 앞의 스텀프의 실수를 더 잘 맞추게 되는 방향으로 만들어지는 것. | . | 수많은 스텀프를 만들어준 후에, 각 스텀프의 성능을 고려해 종합적으로 결과를 예측한다 . | 다수결로 결과를 결정하되, 성능이 좋은 결정 스텀프일수록 예측 의견의 비중을 높게 반영 | . | .   . *stump의 성능 계산하기: . | $ \\dfrac{1}{2}\\log(\\dfrac{1- total \\, error}{total \\, error}) $ . | total error: 잘못 분류한 데이터들의 중요도의 합 | 늘 모든 데이터의 중요도의 합은 1로 유지되므로, total error의 최댓값은 1 | total error가 1인 경우 (=모든 데이터를 다 틀리게 예측한 경우) 성능이 무한하게 작아진다 | total error가 0인 경우 (=모든 데이터를 다 맞게 예측한 경우) 성능이 무한하게 커진다 | total error가 0.5인 경우 (= 딱 반만 맞게 예측한 경우) 성능은 0 | . | . (출처: codeit) .   . *stump 추가하기: . | 첫 스텀프는 결정트리를 만들 때처럼 지니불순도를 계산해서 root 노드를 고른다 | 그 후 스텀프 추가하기: . | 각 데이터의 중요도를 가지고 범위를 만들어준다. (ex. 첫번째 데이터는 중요도가 0.1 → 범위가 0 ~ 0.1, 두번째 데이터는 중요도가 0.2 → 범위가 0.1 ~ 0.3, 세번째 데이터는 중요도가 0.15 → 범위가 0.3 ~ 0.45, …) | 0과 1 사이의 임의의 숫자를 골라, 그 숫자가 속하는 범위의 데이터를 데이터셋에 추가한다 | ※ 중요도가 높은 데이터는 범위도 크기 때문에 선택될 확률이 높아지는 것! | . | . → 새로운 데이터 셋은 전 스텀프들이 틀린, 중요도가 높은 데이터들이 확률적으로 더 많이 들어있기 때문에 얘네를 더 잘 맞출 수 있게 됨.   . *데이터 중요도 바꾸기: . | 첫 스텀프를 만들 때는 모든 데이터의 중요도가 같다 (ex. 데이터가 10개면 각각의 중요도는 1/10) | 틀리게 분류한 데이터: $ weight_{new} = weight_{old} * e^{p_{tree}} $ . | $ e $: 자연상수. 2.71… | $ p_{tree} $: 스텀프의 성능 | . | 맞게 분류한 데이터: $ weight_{new} = weight_{old} * e^{-p_{tree}} $ | . | 성능이 0이면 $ weight_{new} $ = 1 | 틀린 데이터는 $ weight_{old} $에 1보다 큰 값을 곱하게 되므로 원래보다 중요도가 커짐 | 맞은 데이터는 $ weight_{old} $에 1보다 작은 값을 곱하게 되므로 원래보다 중요도가 작아짐 | . scikit-learn으로 데이터 학습 . from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.ensemble import AdaBoostClassifier import matplotlib.pyplot as plt import numpy as np import pandas as pd . 1. 데이터 준비 . iris_data = load_iris() X = pd.DataFrame(iris_data.data, columns=iris_data.feature_names) y = pd.DataFrame(iris_data.target, columns=['class']) . X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=5) y_train = y_train.values.ravel() #ravel(): 다차원 array를 1차원 array로 평평하게 펴주는 함수 . 2. 학습 . model = AdaBoostClassifier(n_estimators=100) . | n_estimators: 최대 몇 개의 결정 스텀프(stump)를 만들어서 예측할 것인지 정해주는 변수. 기본값은 50. | . model.fit(X_train, y_train) . AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0, n_estimators=100, random_state=None) . | base_estimator: 어떤 estimator를 바탕으로 boosted ensemble을 구축할 것인지. | default: None이며, None인 경우 max_depth=1의 DecisionTreeClassifier를 사용 (=결정 스텀프를 사용) | . | learning_rate: 각 classifier의 기여도를 낮춰줄 수 있음. (float number를 입력) | . 3. test data로 성능 체크 . model.predict(X_test) # 어떻게 분류했나 확인 . array([1, 1, 2, 0, 2, 2, 0, 2, 0, 1, 1, 1, 2, 2, 0, 0, 2, 2, 0, 0, 1, 2, 0, 1, 1, 2, 1, 1, 1, 2]) . y_test['class'].to_numpy() ## 실제 y_test와 비교 . array([1, 2, 2, 0, 2, 1, 0, 1, 0, 1, 1, 2, 2, 2, 0, 0, 2, 2, 0, 0, 1, 2, 0, 1, 1, 2, 1, 1, 1, 2]) . # 몇 퍼센트가 올바르게 분류되었는지 확인 model.score(X_test, y_test) . 0.8666666666666667 . 약 87% 정도가 올바르게 분류되었다는 의미 . 속성 중요도 확인 . | 결정트리를 사용한 모델이기에, 결정 트리와 마찬가지로 평균 지니 감소를 이용해 속성 중요도 계산이 가능 | 에다부스트에서의 속성 중요도는 각 결정 스텀프들의 속성 중요도의 weighted average (각 스텀프의 성능 차이를 반영해 평균냄) | . # 속성 중요도 확인 model.feature_importances_ . array([0.17, 0.03, 0.39, 0.41]) . # df를 만들어 importance가 큰 순서대로 정렬 df = pd.DataFrame(list(zip(iris_data.feature_names, model.feature_importances_)), columns=['feature', 'importance']).sort_values('importance', ascending=False) df = df.reset_index(drop=True) df . |   | feature | importance | . | 0 | petal width (cm) | 0.41 | . | 1 | petal length (cm) | 0.39 | . | 2 | sepal length (cm) | 0.17 | . | 3 | sepal width (cm) | 0.03 | .   . import seaborn as sns plt.figure() plt.title('Feature Importances') sns.barplot(data=df, y='feature', x='importance', palette='RdPu') plt.show(); . ",
    "url": "https://chaelist.github.io/docs/ml_basics/classification2/#adaboost",
    "relUrl": "/docs/ml_basics/classification2/#adaboost"
  },"17": {
    "doc": "Clustering",
    "title": "Clustering",
    "content": ". | Clustering | K-Means Clustering . | 기본 개념 | scikit-learn으로 구현 | 최적의 K값 찾기 | 최적의 K값 찾기: yellowbrick | . | Hierarchical Clustering . | 기본 개념 | Dendrogram 그려보기 | scikit-learn으로 구현 | 최적의 K값 찾기: yellowbrick | . | . ",
    "url": "https://chaelist.github.io/docs/ml_basics/clustering/",
    "relUrl": "/docs/ml_basics/clustering/"
  },"18": {
    "doc": "Clustering",
    "title": "Clustering",
    "content": ": 거리를 계산해 유사한 data point끼리 같은 Cluster로 묶어주는 것. | 비지도학습. 종속변수가 없고, 독립변수만 사용해서 답을 찾는 방식. | 대체로 탐색적 데이터 분석의 일부로 수행된다. (ex. 소비자 Cluster를 나눈 후 → 어떤 Cluster가 A제품을 많이 사는지 Regression 문제 풀기) | . ",
    "url": "https://chaelist.github.io/docs/ml_basics/clustering/",
    "relUrl": "/docs/ml_basics/clustering/"
  },"19": {
    "doc": "Clustering",
    "title": "K-Means Clustering",
    "content": "기본 개념 . | 데이터 포인트 (벡터) 간의 거리를 계산해, 거리가 짧은 애들끼리 묶어 K개의 Cluster를 형성해주는 방식. | K 값을 잘 정해주는 것이 중요하다! | 보통 유클리디안 방식으로 거리를 계산. | . (출처: tcpschool.com) .   . *K-Means Clustering 과정: . | K의 값을 정한다 (몇 개의 Cluster로 나눌 것인지) | 데이터셋에서 임의로 K개의 중심점을 선택. | 각 점을 K개의 중심점 중 가장 가까운 점이 속한 Cluster로 assign. | 각 그룹에 속하는 점들의 평균값을 새로운 중심점으로 함. | 색이 변하는 점이 없을 때까지 3, 4번을 계속 반복. | . scikit-learn으로 구현 . from sklearn.datasets import load_iris from sklearn.cluster import KMeans import pandas as pd import numpy as np import seaborn as sns import matplotlib.pyplot as plt . 1. 데이터 준비 . # 예시이므로, make_blobs를 사용해 clustering하기 쉬운 데이터를 준비 from sklearn.datasets import make_blobs X, y = make_blobs(n_samples=100, centers=3, n_features=2) # 변수 2개, 샘플 100개, 중심점 3개로 blob 만들기 X = pd.DataFrame(X, columns=['a', 'b']) X.head() . |   | a | b | . | 0 | 8.66962 | -2.95918 | . | 1 | -10.3818 | 0.959058 | . | 2 | 9.45125 | -2.50409 | . | 3 | -4.03838 | -9.18607 | . | 4 | 10.9778 | -2.85563 | .   . sns.scatterplot(data=X, x=\"a\", y=\"b\"); . 2. Clustering . model = KMeans(n_clusters=3) . | n_clusters: 몇 개의 cluster로 나눌 것인지 설정 (K) | . model.fit(X) . KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300, n_clusters=3, n_init=10, n_jobs=None, precompute_distances='auto', random_state=None, tol=0.0001, verbose=0) . | init: initialization 방법. k-means++, random, 혹은 직접 array 형태로 지정 . | default는 k-means++. 맨 처음 중심점을 보다 전략적으로 배치하는 방식. | 우선 데이터포인트 1개를 첫번째 중심점으로 선택하고, 이와 최대한 먼 곳에 있는 데이터포인트를 다음 중심점으로 선택, … | 맨 처음 중심점들이 서로 근접하게 위치하는 것을 방지해주기 때문에 단순히 random하게 고르는 것보다 더 최적의, 효율적인 clustering이 가능하다 | . | random: 맨 처음 중심점을 말 그대로 무작위로 K개 고르는 방식 | . | max_iter: 중심과 다른 데이터포인트 간의 거리를 계산해서 계속해서 cluster를 update해주는 것을 최대 몇 번 반복할 것인지 지정. max_iter 수를 작게 지정해주면 속도가 빨라지지만 정확도는 떨어짐. default=300 | n_init: 서로 다른 초기 중심점을 바탕으로 몇 번 알고리즘을 반복할 것인지 지정. default=10. 최종 결과는 inertia 계산값이 가장 잘 나오는 결과물로 출력됨. | inertia: 클러스터 내 오차제곱합. Sum of squared distances of samples to their closest cluster center. | . | . clusters = model.predict(X) # model.labels_라고 해도 동일한 결과 clusters # 각 점이 어느 cluster에 배정되었는지 확인 . array([0, 1, 0, 2, 0, 2, 2, 0, 2, 0, 0, 2, 0, 1, 0, 0, 1, 1, 1, 1, 2, 0, 1, 1, 1, 2, 2, 1, 0, 1, 1, 1, 2, 2, 1, 0, 0, 1, 2, 1, 2, 2, 0, 1, 2, 2, 1, 1, 2, 1, 2, 0, 2, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 1, 0, 0, 1, 2, 2, 1, 0, 2, 0, 2, 1, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 2, 1, 1, 1], dtype=int32) . | clusters = model.fit_predict(X)라고 하면 fit과 predict를 동시에 할 수 있음. (clustering은 사실 fit과 predict가 하나의 과정이라서) | . 3. Clustering 결과 확인 . result = X.copy() result[\"cluster\"] = clusters result.head() . |   | a | b | cluster | . | 0 | 8.66962 | -2.95918 | 0 | . | 1 | -10.3818 | 0.959058 | 1 | . | 2 | 9.45125 | -2.50409 | 0 | . | 3 | -4.03838 | -9.18607 | 2 | . | 4 | 10.9778 | -2.85563 | 0 | .   . sns.scatterplot(data=result, x=\"a\", y=\"b\", hue=\"cluster\", palette=\"Set2\"); . 최적의 K값 찾기 . | make_blobs로 만든 예시 데이터의 경우, 사전에 3개의 분류임을 알고 있었기에 K=3으로 지정해서 진행했지만, 보통은 몇 개의 cluster로 나눠야 할 지 쉽게 알 수 없다. | 2차원인 경우, 시각화해서 눈으로 판단하는 것도 가능하지만, 보통의 데이터는 3차원 이상이기에,,, 아래 방식들을 사용하면 좋음! | .   . 1. Elbow 방식 . | K값을 1부터 차례로 넣어보면서, 각 결과의 inertia(클러스터 내 오차제곱합)를 구한다 | K값에 따른 inertia의 변화를 보고, 그래프의 팔꿈치 부분에 해당하는 지점을 K값으로 선택 (intertia가 감소하는 정도가 낮아지는 지점) | ※ inertia는 cluster 수가 증가할 수록 감소함. (trade-off 관계.) | . inertias = [] for i in range(1, 11): kmeans = KMeans(n_clusters=i) kmeans.fit(X) inertias.append(kmeans.inertia_) plt.plot(range(1,11), inertias, marker='o') plt.xlabel('Num_Clusters') plt.ylabel('Inertia') plt.show() # 결과: 3이 최적의 Cluster . 2. Silhouette Score (Silhouette Coefficient) . | $ \\frac{(b-a)}{max(a, b)} $로 계산 . | a: 특정한 sample i로부터 같은 Class에 속한 다른 점들까지의 평균 거리 (mean intra-cluster distance) | b: 특정한 sample i로부터 가장 가까운 옆 Class에 속한 점들까지의 평균 거리 (mean nearest-cluster distance) | . | 숫자가 클수록 잘 분류된 것. (숫자가 크다는 것은 타 cluster와는 거리가 있고, 같은 cluster 내에서는 잘 모여 있다는 의미) | . from sklearn.metrics import silhouette_score silhouette = [] for i in range(2, 11): kmeans = KMeans(n_clusters=i) kmeans.fit(X) silhouette.append(silhouette_score(X, kmeans.labels_)) plt.plot(range(2,11), silhouette, marker='o') plt.xlabel('Num_Clusters') plt.ylabel('Silhouette Score') plt.show() # 결과: 3이 최적의 Cluster . 3. CH Index (Calinski-Harabasz Index) . | 잘 분류된 클러스터는 (1) 내부의 점들끼리 compact하게 모여 있고, (2) 나머지 cluster로부터는 멀리 떨어져있어야 한다는 점에 착안한 Index | $ \\frac{BCV}{k-1} * \\frac{n-k}{WCV} $로 계산 . | BCV: Between-Cluster Variation: 서로 다른 클러스터끼리 얼마나 떨어져있는지. – 클수록 좋음 | WCV: Within-Vluster Variation: 서로 같은 클러스터에 있는 점끼리 얼마나 떨어져있는지. – 작을수록 좋음 | k: # of clusters | n: # of datapoints | . | 숫자가 클수록 잘 분류된 것. (숫자가 크다는 것은 타 cluster와는 거리가 있고, 같은 cluster 내에서는 잘 모여 있다는 의미) | . from sklearn.metrics import calinski_harabasz_score ch_index = [] for i in range(2, 11): kmeans = KMeans(n_clusters=i) kmeans.fit(X) ch_index.append(calinski_harabasz_score(X, kmeans.labels_)) plt.plot(range(2, 11), ch_index, marker='o') plt.xlabel('Num_Clusters') plt.ylabel('CH Index') plt.show() # 결과: 3이 최적의 Cluster . 최적의 K값 찾기: yellowbrick . *yellowbrick: machine learning visualization library (https://www.scikit-yb.org/en/latest/) . | 최적의 cluster를 자동으로 찾아주고, clustering에 걸리는 시간 등도 간편하게 시각화할 수 있어서 편리하다 | . # 우선 설치해줘야 사용 가능 import sys !{sys.executable} -m pip install yellowbrick . 1. Elbow Method . # Import ElbowVisualizer from yellowbrick.cluster import KElbowVisualizer model = KMeans() visualizer = KElbowVisualizer(model, k=(1,30), timings=True) # k is range of number of clusters. visualizer.fit(X) # Fit the data to the visualizer visualizer.show(); . 2. Silhouette Score . model = KMeans() visualizer = KElbowVisualizer(model, k=(2,30), metric='silhouette', timings=True) visualizer.fit(X) visualizer.show(); . 3. CH Index . model = KMeans() visualizer = KElbowVisualizer(model, k=(2,30), metric='calinski_harabasz', timings=True) visualizer.fit(X) visualizer.show(); . ",
    "url": "https://chaelist.github.io/docs/ml_basics/clustering/#k-means-clustering",
    "relUrl": "/docs/ml_basics/clustering/#k-means-clustering"
  },"20": {
    "doc": "Clustering",
    "title": "Hierarchical Clustering",
    "content": "(계층적 군집 분석) . 기본 개념 . | K Means와 달리, 중심을 먼저 잡고 시작하는 게 아니라, 일단 모든 벡터의 거리를 다 계산 → 거리가 가장 짧은 것끼리 차근차근 묶어나감. (그룹의 수를 사전에 정하지 않음) | 계속 연결해나가서 하나의 완벽한 cluster로 묶일 때까지 묶는 작업을 계속함 | Dendrogram 보고 cluster 개수를 얼마 정도로 할 지 고려해서 적당히 잘라줌 | .   . *Cluster 간 거리를 계산하는 법: (※ 데이터포인트 간의 거리는 Euclidean이나 Cosign 방식 등으로 계산) . | Single: 각 클러스터를 구성하는 데이터포인트 중 가장 가까운 데이터포인트 간의 거리로 계산 | Complete: 각 클러스터를 구성하는 데이터포인트 중 가장 먼 데이터포인트 간의 거리로 계산 | Average: 각 클러스터를 구성하는 데이터포인트들의 평균점 간의 거리로 계산 | Ward: 두 개의 클러스터가 합쳐졌을 때의 데이터포인트들이 갖는 분산 (오차제곱합)이 가장 작은 클러스터끼리 묶어주는 방식 | . ※ 4개의 linkage type은 데이터셋의 분포에 따라 결과가 상이하므로, 데이터셋에 따라 적절히 선택 . Dendrogram 그려보기 . from sklearn.datasets import load_iris from sklearn.cluster import AgglomerativeClustering import pandas as pd import numpy as np import seaborn as sns import matplotlib.pyplot as plt from scipy.cluster.hierarchy import dendrogram, linkage . 1. 데이터 준비 . # iris dataset 사용 iris_data = load_iris() X = pd.DataFrame(iris_data.data, columns=iris_data.feature_names) X.head() . |   | sepal length (cm) | sepal width (cm) | petal length (cm) | petal width (cm) | . | 0 | 5.1 | 3.5 | 1.4 | 0.2 | . | 1 | 4.9 | 3 | 1.4 | 0.2 | . | 2 | 4.7 | 3.2 | 1.3 | 0.2 | . | 3 | 4.6 | 3.1 | 1.5 | 0.2 | . | 4 | 5 | 3.6 | 1.4 | 0.2 | . 2. Dendrogram 그려보기 . | Scipy 활용 | 어떻게 묶일지 시뮬레이션 + 몇 개 Cluster로 나눌지 고민 | 어떤 linkage type을 쓰면 좋을지, 몇 개의 Cluster를 쓰면 좋을지 고민 | . plt.figure(figsize=(15, 5)) L = linkage(X, 'single') dn = dendrogram(L) plt.title(\"Dendrograms: Single\") plt.show() . plt.figure(figsize=(15, 5)) L = linkage(X, 'ward') dn = dendrogram(L) plt.title(\"Dendrograms: Ward\") plt.show() . | single로 묶는다면 2개의 cluster로, ward로 묶는다면 2-3개의 cluster로 묶는 게 좋을 것 같다고 판단 | complete나 average 방식도 구현해보고 비교해보면 좋음 | . scikit-learn으로 구현 . 3. Clustering . model = AgglomerativeClustering(n_clusters=3) # iris dataset이므로, n_clusters=3으로 선택 model.fit(X) . AgglomerativeClustering(affinity='euclidean', compute_full_tree='auto', connectivity=None, distance_threshold=None, linkage='ward', memory=None, n_clusters=3) . | affinity: 데이터포인트 간의 거리를 어떻게 계산할지 결정. euclidean, cosine, l1, l2, manhattan 중에 고를 수 있으며, default는 euclidean. | linkage=’ward’로 하려면 euclidean밖에 선택할 수 없음 | . | linkage: ward, complete, average, single 중에 선택. (default는 ward) | . 4. Clustering 결과 구현 . result = X.copy() result[\"cluster\"] = model.labels_ result.head() . |   | sepal length (cm) | sepal width (cm) | petal length (cm) | petal width (cm) | cluster | . | 0 | 5.1 | 3.5 | 1.4 | 0.2 | 1 | . | 1 | 4.9 | 3 | 1.4 | 0.2 | 1 | . | 2 | 4.7 | 3.2 | 1.3 | 0.2 | 1 | . | 3 | 4.6 | 3.1 | 1.5 | 0.2 | 1 | . | 4 | 5 | 3.6 | 1.4 | 0.2 | 1 | . +) pairplot으로 clustering이 잘 되었나 살펴보기 . # 4차원으로 시각화할 수는 없지만, seaborn의 pairplot으로 어느 정도 다각도로 살펴볼 수는 있음 sns.pairplot(result, hue='cluster') plt.show() . 최적의 K값 찾기: yellowbrick . | 마찬가지로, iris data처럼 cluster 수를 알고 있는 상황이 아니라면, 아래와 같은 방식들을 사용해 최적의 K값을 찾아보고 clustering을 구현하는 것이 좋다 | . 1. Elbow Method . # Import ElbowVisualizer from yellowbrick.cluster import KElbowVisualizer model = AgglomerativeClustering() visualizer = KElbowVisualizer(model, k=(1,30), timings=True) visualizer.fit(X) visualizer.show(); . 2. Silhouette Score . model = AgglomerativeClustering() visualizer = KElbowVisualizer(model, k=(2,30), metric='silhouette', timings=True) visualizer.fit(X) visualizer.show(); . 3. CH Index . model = AgglomerativeClustering() visualizer = KElbowVisualizer(model, k=(2,30), metric='calinski_harabasz', timings=True) visualizer.fit(X) visualizer.show(); . | 대체로 2-3개의 cluster로 나누는 것이 좋다는 결론 (사실 iris data에서 virginica와 versicolor는 꽤 유사하기 때문) | . ",
    "url": "https://chaelist.github.io/docs/ml_basics/clustering/#hierarchical-clustering",
    "relUrl": "/docs/ml_basics/clustering/#hierarchical-clustering"
  },"21": {
    "doc": "Control Flow (제어문)",
    "title": "Control Flow (제어문)",
    "content": ". | if-else문 (조건문) . | Comparison tests | ‘pass’ | if - elif - else | . | for 반복문 . | range() function | ‘continue’와 ‘break’ | List Comprehension | . | while 반복문 . | ‘continue’와 ‘break’ | . | try - except문 (오류 예외 처리) | . ",
    "url": "https://chaelist.github.io/docs/python_basics/controlflow/",
    "relUrl": "/docs/python_basics/controlflow/"
  },"22": {
    "doc": "Control Flow (제어문)",
    "title": "if-else문 (조건문)",
    "content": ". | 특정한 조건이 충족될 때만(= True일 때만) 특정한 Python 코드들을 실행하게 함 | if 조건문의 결과로 실행되어야 하는 아래 코드들에는 indentation(들여쓰기)를 해주어야 함 | . *형식: . if condition1: body1 # body에는 여러 줄의 코드가 들어가도 됨. (들여쓰기로 구간이 잘 표현만 되면) else: # else는 안써도 됨 body2 . *예시: . num1 = -1 if num1 &gt; 0: # num1이 0보다 클 경우 아래 코드들이 실행 print(num1) print('positive') else: # num1이 0보다 크지 않을 경우 아래 코드들이 실행 print(num1) print('negative') . -1 negative . Comparison tests . | a &lt; b (b보다 작다), a &lt;= b (b보다 작거나 같다) | a &gt; b (b보다 크다), a &gt;= b (b보다 크거나 같다) | == (같다), != (다르다) | ‘in’ (포함되어 있다), ‘not in’ (포함되어 있지 않다) | . # equality test 예시 a = 2 b = 3 a != b # 둘이 다르므로 True . True . # in / not in x = [3, 5, 9] if 3 in x: print('included') else: print('not included') . included . ‘pass’ . : 조건이 True일 경우 아무 일도 일어나지 않게 설정하고 싶다면, ‘pass’를 사용하면 된다 . # 예시: bag = ['apple', 'water', 'cell phone'] if 'apple' in bag: pass # apple이 bag에 있기 때문에, 아무 결과값도 출력되지 않는다. else: print('need an apple') . if - elif - else . | for more than one condition, we use ‘if - elif - else’ statement | elif는 무수히 많이 사용 가능. | . *형식: . if condition1: body1 elif condition2: body2 elif condition3: body3 else: body4 . ** if-elif-else 구문은 위에서부터 점점 밑으로 내려가며 test되다가, 충족되는 condition을 만나면 해당 body가 실행되며 더 이상 아래 조건들은 test되지 않는다 . # 예시: a = 35 if a &gt; 30: # 첫 번째 조건이 충족되므로 여기서 process가 끝나고, 25가 출력됨 a= a-10 print(a) elif 20 &lt;a &lt;= 40: a = a-5 print(a) else: print(a) # 첫번째와 두번째 조건에 겹치는 부분이 있지만, 이미 첫번째 조건이 충족되어서 process가 끝나기에 더 이상 밑으로 진행되지 않는다. ## 하지만 물론 조건들은 서로 겹치지 않게 작성하는 게 더 좋음! . 25 . ** cf) if-elif 대신 if만 반복해서 쓰면, 독립적인 세트로 간주되어 모두 test됨. # if-elif 대신 if만 두 개 쓰는 경우 a = 3 if a &gt; 0: a = a+1 print(a) if a &gt; 2: print(a) # if가 두 개면, 서로 독립적인 두 개의 다른 세트로 간주됨. # 첫번재 조건에서 true여도 두번째 조건도 test되고, 두번째 조건도 또 true이면 둘 다 실행됨. 4 4 . +) 예시: 자동 학점 계산기 . ## 'input()' 함수로 점수 받아서 학점 계산해보기 num = input('Enter your score: ') x = int(num) # input()을 통해 받은 숫자는 string type으로 저장되므로, int()로 변환해야 계산 가능 if x &lt; 50: print('F') elif 50 &lt;= x &lt; 60: print('D') elif 60 &lt;= x &lt;70: print('C') elif 70 &lt;= x &lt; 80: print('B') else: print('A') . Enter your score: 70 B . ",
    "url": "https://chaelist.github.io/docs/python_basics/controlflow/#if-else%EB%AC%B8-%EC%A1%B0%EA%B1%B4%EB%AC%B8",
    "relUrl": "/docs/python_basics/controlflow/#if-else문-조건문"
  },"23": {
    "doc": "Control Flow (제어문)",
    "title": "for 반복문",
    "content": "*형식: . for item in list-like variable: body . | item은 해당 list-like variable의 각 값들을 취하는 temporary variable | 특정 list-like variable에 있는 값들을 하나씩 취해서 차례로 특정 body 구문을 실행 | . *예시: . | for문 예시1: list 안의 variable 차례로 print하기 x = ['a', 'b', 'c'] for k in x: print(k) . a b c . | for문 예시2: adding up all elements in a list x = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] total = 0 for k in x: total = total + k # x 안의 값을 하나씩 취해서 차례로 더해줌 print(total) . 55 . | for문 예시3: adding up even numbers only . # for문과 if문을 함께 사용해, 특정 조건의 값들만 더하기 x = [2, 3, 1, 4, 5] total = 0 for k in x: if k % 2 == 0: total = total + k print(total) . 6 . | . range() function . | for a number n, range(n) returns a sequence of 0, 1, … n-1 . | ex) range(5) –&gt; 0, 1, 2, 3, 4 차례로 접근 | . | . **추가: . | range(a, b): 시작값과 끝값을 지정 . | a, a+1, … b-1 | . | range(a, b, k): 시작값, 끝값, 간격을 지정 . | a, a+k, a+2k, … b-1 이런 식으로 k 간격으로 a ~ b-1 사이의 값들을 접근 | ex) range(2, 10, 2) -&gt; 2, 4, 6, 8 | . | . | range(n) 예시 for k in range(5): print(k) . 0 1 2 3 4 . | range(a, b, k) 예시 for a in range(3, 10, 2): print(a) . 3 5 7 9 . | 응용: range(len(s))를 통해 리스트 ‘s’에 있는 값들의 index number에 접근 s = ['a', 'b', 'c'] for k in range(len(s)): print(s[k]) ### 물론 `for k in s: print(k)` 이렇게 하는 거랑 결과는 똑같음,,, . a b c . | . ‘continue’와 ‘break’ . : 특정 condition 아래 ‘for’ loop를 멈추거나 escape하는 방법 . | continue: 특정 조건을 만족하는 element에 대해서는 나머지 body가 실행되지 X, 다음 element로 process가 넘어간다 (남아있는 element가 있다면 for문 자체는 계속되는 것) . x = [2, -1, 3] total = 0 for k in x: if k &lt; 0: # k가 음수일 경우에는 continue continue ## 아래 total = total + k 부분이 실행되지 않고, 다음 element로 넘어간다 total = total + k print(total) # 결국, 양수만 더하는 결과가 나옴. 5 . | break: 특정 조건이 충족되는 순간, for문이 아예 다 멈춰버린다 (다음 element가 남아있어도 넘어가지 않음) . x = [2, -1, 3] total = 0 for k in x: if k &lt; 0: # k가 음수일 경우에 아예 해당 'for' process가 다 멈춤. 다음 element로도 넘어가지 않음. break total = total + k print(total) ## 2는 일단 total = total + k까지 내려가서 더해지지만, -1에서 if가 충족되면서 break를 만나 for문이 멈추면서 그대로 total은 2에서 멈춤 . 2 . | . List Comprehension . | 리스트 내포 (List Comprehension): 리스트 안에 for문을 내포해 간결하게 작성 # list comprehension 예시 a = [1, 2, 3, 4] result = [num * 3 for num in a] print(result) . [3, 6, 9, 12] . | if 조건을 함께 내포하는 것도 가능 # List comprehension 예시2: if 조건도 함께 내포 a = [1, 2, 3, 4] result = [num * 3 for num in a if num % 2 == 0] print(result) . [6, 12] . | . ",
    "url": "https://chaelist.github.io/docs/python_basics/controlflow/#for-%EB%B0%98%EB%B3%B5%EB%AC%B8",
    "relUrl": "/docs/python_basics/controlflow/#for-반복문"
  },"24": {
    "doc": "Control Flow (제어문)",
    "title": "while 반복문",
    "content": ". | 특정 조건이 True인 동안 영원히 돌아간다. (조건이 False가 될 때까지) | 조건을 잘 설정하지 않으면 정말 ‘영원히’ 돌아가니, 조심! | . *형식: . while condition: body . *예시: . a = 10 while a &gt; 0: print(a) a = a - 1 . 10 9 8 7 6 5 4 3 2 1 . ‘continue’와 ‘break’ . : while문도 continue와 break를 활용해 특정 element를 skip하거나 전체 loop를 강제로 중단시킬 수 있다 . | continue 사용 예시 . a = 0 while a &lt; 10: a = a + 1 if a % 2 == 0: # 2의 배수일 경우에는 print(a)가 실행되지 않은 채 다시 맨 처음으로 돌아간다 continue print(a) . 1 3 5 7 9 . | break 사용 예시 . a = 0 while a &lt; 10: a = a + 1 if a % 3 == 0: # 2의 배수를 만나면 while loop가 아예 중단된다 break print(a) . 1 2 . | . ",
    "url": "https://chaelist.github.io/docs/python_basics/controlflow/#while-%EB%B0%98%EB%B3%B5%EB%AC%B8",
    "relUrl": "/docs/python_basics/controlflow/#while-반복문"
  },"25": {
    "doc": "Control Flow (제어문)",
    "title": "try - except문 (오류 예외 처리)",
    "content": ". | 기본 try - except문 . | 오류가 발생할 시 except 블록이 실행된다 | . try: 실행해야 하는 코드 except: 오류가 날 경우 실행할 코드 . | 발생 오류 종류를 포함한 except문 . | except문에 미리 정해둔 오류 이름과 일치할 때만 except 블록 실행 | . try: 실행해야 하는 코드 except 발생 오류: 오류가 날 경우 실행할 코드 . *예시: . try: a = [1,2] print(a[3]) except IndexError: print(\"인덱싱 불가능\") . 인덱싱 불가능 . +) 여러 개의 오류를 처리하는 것도 가능 . try: 실행해야 하는 코드 except 발생 오류1: 오류1이 날 경우 실행할 코드 except 발생 오류2: 오류2가 날 경우 실행할 코드 . | try … finally . | finally절은 try문 수행 도중 예외 발생 여부와 상관없이 항상 수행된다 | . try: 실행해야 하는 코드 finally: 오류 여부와 상관 없이 실행되어야 하는 코드 . *예시: . f = open('message.txt', 'w') try: 실행해야 하는 코드 finally: f.close() # try문 중 오류 여부와 상관없이 반드시 실행되어야 하는 코드 ## 예외 발생 여부와 상관없이 finally절에서 f.close()로 열린 파일을 닫을 수 있다. | ‘pass’로 오류 회피하기 try: 실행해야 하는 코드 except: pass # 오류가 나면 그냥 pass해버리기 . *예시: . # 오류 회피 예시 try: f = open(\"없는 파일\", 'r') except FileNotFoundError: pass ## 파일이 찾아지지 않으면 그냥 pass. | . ",
    "url": "https://chaelist.github.io/docs/python_basics/controlflow/#try---except%EB%AC%B8-%EC%98%A4%EB%A5%98-%EC%98%88%EC%99%B8-%EC%B2%98%EB%A6%AC",
    "relUrl": "/docs/python_basics/controlflow/#try---except문-오류-예외-처리"
  },"26": {
    "doc": "Telco Customer Churn",
    "title": "Telco Customer Churn",
    "content": ". | 데이터 파악 | 변수 간 상관관계 파악 | chrun 여부 예측에 주요한 변수 파악 . | Logistic Regression 모델 계산 | 변수별 coefficient 비교 | . | 변수별 비교 . | 주(state)별 이탈률 | plan별 이탈률 | 낮 통화시간에 따른 이탈률 | customer service calls에 따른 이탈률 | 보이스메일 메시지 수에 따른 이탈률 | . | . *분석 대상 데이터셋: Telecommunications Customer Churn . | 데이터셋 출처 | 고객이 telecommunications provider를 바꾸는 행동(churn)을 예측하는 징후를 파악하기 위한 dataset | 4250개 sample. 약 86%는 stay, 14%는 churn | .   . *변수 설명: . | “state”, string. 2-letter code of the US state of customer residence | “account_length”, numerical. Number of months the customer has been with the current telco provider | “area_code”, string=”area_code_AAA” where AAA = 3 digit area code. | “international_plan”, . The customer has international plan. | “voice_mail_plan”, . The customer has voice mail plan. | “number_vmail_messages”, numerical. Number of voice-mail messages. | “total_day_minutes”, numerical. Total minutes of day calls. | “total_day_calls”, numerical. Total minutes of day calls. | “total_day_charge”, numerical. Total charge of day calls. | “total_eve_minutes”, numerical. Total minutes of evening calls. | “total_eve_calls”, numerical. Total number of evening calls. | “total_eve_charge”, numerical. Total charge of evening calls. | “total_night_minutes”, numerical. Total minutes of night calls. | “total_night_calls”, numerical. Total number of night calls. | “total_night_charge”, numerical. Total charge of night calls. | “total_intl_minutes”, numerical. Total minutes of international calls. | “total_intl_calls”, numerical. Total number of international calls. | “total_intl_charge”, numerical. Total charge of international calls | “number_customer_service_calls”, numerical. Number of calls to customer service | “churn”, . Customer churn - target variable. | . ",
    "url": "https://chaelist.github.io/docs/kaggle/customer_churn/",
    "relUrl": "/docs/kaggle/customer_churn/"
  },"27": {
    "doc": "Telco Customer Churn",
    "title": "데이터 파악",
    "content": "# 필요한 라이브러리 import import pandas as pd import numpy as np from matplotlib import pyplot as plt import seaborn as sns import scipy.stats as stats . customer_df = pd.read_csv('data/customer_churn_2020.csv') customer_df.head() . |   | state | account_length | area_code | international_plan | voice_mail_plan | number_vmail_messages | total_day_minutes | total_day_calls | total_day_charge | total_eve_minutes | total_eve_calls | total_eve_charge | total_night_minutes | total_night_calls | total_night_charge | total_intl_minutes | total_intl_calls | total_intl_charge | number_customer_service_calls | churn | . | 0 | OH | 107 | area_code_415 | no | yes | 26 | 161.6 | 123 | 27.47 | 195.5 | 103 | 16.62 | 254.4 | 103 | 11.45 | 13.7 | 3 | 3.7 | 1 | no | . | 1 | NJ | 137 | area_code_415 | no | no | 0 | 243.4 | 114 | 41.38 | 121.2 | 110 | 10.3 | 162.6 | 104 | 7.32 | 12.2 | 5 | 3.29 | 0 | no | . | 2 | OH | 84 | area_code_408 | yes | no | 0 | 299.4 | 71 | 50.9 | 61.9 | 88 | 5.26 | 196.9 | 89 | 8.86 | 6.6 | 7 | 1.78 | 2 | no | . | 3 | OK | 75 | area_code_415 | yes | no | 0 | 166.7 | 113 | 28.34 | 148.3 | 122 | 12.61 | 186.9 | 121 | 8.41 | 10.1 | 3 | 2.73 | 3 | no | . | 4 | MA | 121 | area_code_510 | no | yes | 24 | 218.2 | 88 | 37.09 | 348.5 | 108 | 29.62 | 212.6 | 118 | 9.57 | 7.5 | 7 | 2.03 | 3 | no | . | null값 여부, data type 확인 . customer_df.info() . &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 4250 entries, 0 to 4249 Data columns (total 20 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 state 4250 non-null object 1 account_length 4250 non-null int64 2 area_code 4250 non-null object 3 international_plan 4250 non-null object 4 voice_mail_plan 4250 non-null object 5 number_vmail_messages 4250 non-null int64 6 total_day_minutes 4250 non-null float64 7 total_day_calls 4250 non-null int64 8 total_day_charge 4250 non-null float64 9 total_eve_minutes 4250 non-null float64 10 total_eve_calls 4250 non-null int64 11 total_eve_charge 4250 non-null float64 12 total_night_minutes 4250 non-null float64 13 total_night_calls 4250 non-null int64 14 total_night_charge 4250 non-null float64 15 total_intl_minutes 4250 non-null float64 16 total_intl_calls 4250 non-null int64 17 total_intl_charge 4250 non-null float64 18 number_customer_service_calls 4250 non-null int64 19 churn 4250 non-null object dtypes: float64(8), int64(7), object(5) memory usage: 664.2+ KB . | 숫자형 컬럼: 값의 분포를 확인 . customer_df.describe() . |   | account_length | number_vmail_messages | total_day_minutes | total_day_calls | total_day_charge | total_eve_minutes | total_eve_calls | total_eve_charge | total_night_minutes | total_night_calls | total_night_charge | total_intl_minutes | total_intl_calls | total_intl_charge | number_customer_service_calls | . | count | 4250 | 4250 | 4250 | 4250 | 4250 | 4250 | 4250 | 4250 | 4250 | 4250 | 4250 | 4250 | 4250 | 4250 | 4250 | . | mean | 100.236 | 7.63176 | 180.26 | 99.9073 | 30.6447 | 200.174 | 100.176 | 17.015 | 200.528 | 99.8395 | 9.02389 | 10.2561 | 4.42635 | 2.76965 | 1.55906 | . | std | 39.6984 | 13.4399 | 54.0124 | 19.8508 | 9.1821 | 50.2495 | 19.9086 | 4.27121 | 50.3535 | 20.0932 | 2.26592 | 2.7601 | 2.46307 | 0.745204 | 1.31143 | . | min | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . | 25% | 73 | 0 | 143.325 | 87 | 24.365 | 165.925 | 87 | 14.1025 | 167.225 | 86 | 7.5225 | 8.5 | 3 | 2.3 | 1 | . | 50% | 100 | 0 | 180.45 | 100 | 30.68 | 200.7 | 100 | 17.06 | 200.45 | 100 | 9.02 | 10.3 | 4 | 2.78 | 1 | . | 75% | 127 | 16 | 216.2 | 113 | 36.75 | 233.775 | 114 | 19.8675 | 234.7 | 113 | 10.56 | 12 | 6 | 3.24 | 2 | . | max | 243 | 52 | 351.5 | 165 | 59.76 | 359.3 | 170 | 30.54 | 395 | 175 | 17.77 | 20 | 20 | 5.4 | 9 | . | 중복값 확인 . customer_df.duplicated().sum() . 0 . | unique한 값 수 확인 . customer_df.nunique() . state 51 account_length 215 area_code 3 international_plan 2 voice_mail_plan 2 number_vmail_messages 46 total_day_minutes 1843 total_day_calls 120 total_day_charge 1843 total_eve_minutes 1773 total_eve_calls 123 total_eve_charge 1572 total_night_minutes 1757 total_night_calls 128 total_night_charge 992 total_intl_minutes 168 total_intl_calls 21 total_intl_charge 168 number_customer_service_calls 10 churn 2 dtype: int64 . | . ",
    "url": "https://chaelist.github.io/docs/kaggle/customer_churn/#%EB%8D%B0%EC%9D%B4%ED%84%B0-%ED%8C%8C%EC%95%85",
    "relUrl": "/docs/kaggle/customer_churn/#데이터-파악"
  },"28": {
    "doc": "Telco Customer Churn",
    "title": "변수 간 상관관계 파악",
    "content": "plt.figure(figsize=(14, 9)) sns.heatmap(customer_df.corr(), annot=True, cmap='Greens'); . | total_%_charge와 total_%_minutes는 모두 연관성이 아주 강함 (상관계수 = 1) | minute에 정확히 비례해서 charge가 결정되는 구조인 듯. | .   . → ‘total_%_charge’ 형태의 이름을 가진 컬럼은 삭제해줌 . | total_%_charge는 total_%_minutes에 비례해서 결정되는 값이기에, 두 종류의 컬럼을 모두 살펴볼 필요는 없다고 판단됨. customer_df.drop(['total_day_charge', 'total_eve_charge', 'total_night_charge', 'total_intl_charge'], axis='columns', inplace=True) customer_df.head() . | . |   | state | account_length | area_code | international_plan | voice_mail_plan | number_vmail_messages | total_day_minutes | total_day_calls | total_eve_minutes | total_eve_calls | total_night_minutes | total_night_calls | total_intl_minutes | total_intl_calls | number_customer_service_calls | churn | . | 0 | OH | 107 | area_code_415 | no | yes | 26 | 161.6 | 123 | 195.5 | 103 | 254.4 | 103 | 13.7 | 3 | 1 | no | . | 1 | NJ | 137 | area_code_415 | no | no | 0 | 243.4 | 114 | 121.2 | 110 | 162.6 | 104 | 12.2 | 5 | 0 | no | . | 2 | OH | 84 | area_code_408 | yes | no | 0 | 299.4 | 71 | 61.9 | 88 | 196.9 | 89 | 6.6 | 7 | 2 | no | . | 3 | OK | 75 | area_code_415 | yes | no | 0 | 166.7 | 113 | 148.3 | 122 | 186.9 | 121 | 10.1 | 3 | 3 | no | . | 4 | MA | 121 | area_code_510 | no | yes | 24 | 218.2 | 88 | 348.5 | 108 | 212.6 | 118 | 7.5 | 7 | 3 | no | . ",
    "url": "https://chaelist.github.io/docs/kaggle/customer_churn/#%EB%B3%80%EC%88%98-%EA%B0%84-%EC%83%81%EA%B4%80%EA%B4%80%EA%B3%84-%ED%8C%8C%EC%95%85",
    "relUrl": "/docs/kaggle/customer_churn/#변수-간-상관관계-파악"
  },"29": {
    "doc": "Telco Customer Churn",
    "title": "chrun 여부 예측에 주요한 변수 파악",
    "content": ": Logitsic Regression으로 각 변수의 coefficient를 계산해, 어떤 변수가 churn 여부를 예측하는 데에 중요하게 작용하는지 파악 . Logistic Regression 모델 계산 . # scikit-learn을 사용 from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression from sklearn import preprocessing . | churn = yes / no를 1과 0으로 변환한 칼럼을 별도로 저장 . customer_df['churn_0_1'] = customer_df['churn'].apply(lambda x: 1 if x == 'yes' else 0) customer_df[['churn', 'churn_0_1']].head() . |   | churn | churn_0_1 | . | 0 | no | 0 | . | 1 | no | 0 | . | 2 | no | 0 | . | 3 | no | 0 | . | 4 | no | 0 | . | 입력변수(X), 목표변수(y)를 나누어 저장 . X = customer_df.iloc[:, :-2] # ~ 'number_customer_service_calls' 컬럼까지 저장 y = customer_df.iloc[:, -1] # 'churn_0_1' 컬럼만 저장 . | 카테고리 변수를 더미변수화 . # 값이 3개 이상인 컬럼은 pd.get_dummies()로 더미변수화 X = pd.get_dummies(data=X, columns=['state', 'area_code']) # 값이 yes, no 2개인 컬럼: yes를 1, no를 0으로 변환 X['international_plan'] = X['international_plan'].apply(lambda x: 1 if x == 'yes' else 0) X['voice_mail_plan'] = X['voice_mail_plan'].apply(lambda x: 1 if x == 'yes' else 0) X.head() . |   | account_length | international_plan | voice_mail_plan | number_vmail_messages | total_day_minutes | total_day_calls | total_eve_minutes | total_eve_calls | total_night_minutes | total_night_calls | total_intl_minutes | total_intl_calls | number_customer_service_calls | state_AK | state_AL | state_AR | state_AZ | state_CA | state_CO | state_CT | state_DC | state_DE | state_FL | state_GA | state_HI | state_IA | state_ID | state_IL | state_IN | state_KS | state_KY | state_LA | state_MA | state_MD | state_ME | state_MI | state_MN | state_MO | state_MS | state_MT | state_NC | state_ND | state_NE | state_NH | state_NJ | state_NM | state_NV | state_NY | state_OH | state_OK | state_OR | state_PA | state_RI | state_SC | state_SD | state_TN | state_TX | state_UT | state_VA | state_VT | state_WA | state_WI | state_WV | state_WY | area_code_area_code_408 | area_code_area_code_415 | area_code_area_code_510 | . | 0 | 107 | 0 | 1 | 26 | 161.6 | 123 | 195.5 | 103 | 254.4 | 103 | 13.7 | 3 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | . | 1 | 137 | 0 | 0 | 0 | 243.4 | 114 | 121.2 | 110 | 162.6 | 104 | 12.2 | 5 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | . | 2 | 84 | 1 | 0 | 0 | 299.4 | 71 | 61.9 | 88 | 196.9 | 89 | 6.6 | 7 | 2 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | . | 3 | 75 | 1 | 0 | 0 | 166.7 | 113 | 148.3 | 122 | 186.9 | 121 | 10.1 | 3 | 3 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | . | 4 | 121 | 0 | 1 | 24 | 218.2 | 88 | 348.5 | 108 | 212.6 | 118 | 7.5 | 7 | 3 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | . | 입력변수 표준화(standardization) . | ※ X값을 표준화해주지 않으면, 계산되는 coefficient를 동일선상에서 비교할 수 없음 | . scaler = preprocessing.StandardScaler() X_scaled = scaler.fit_transform(X) . | train_test_split &amp; 학습 . X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2) y_train = y_train.values.ravel() #ravel(): 다차원 array를 1차원 array로 평평하게 펴주는 함수. logistic_model = LogisticRegression(solver='saga', max_iter=2000, penalty='l1') logistic_model.fit(X_train, y_train) logistic_model.score(X_test, y_test) # accuracy_score . 0.8670588235294118 . | . 변수별 coefficient 비교 . # df를 만들어 coefficient의 절대값이 큰 순서대로 정렬 temp = pd.DataFrame(list(zip(X.columns, np.absolute(logistic_model.coef_[0]))), columns=['feature', 'coefficient']).sort_values('coefficient', ascending=False).reset_index() # coefficient가 큰 Top 20 변수만 시각화 plt.figure(figsize=(8, 6)) sns.barplot(data=temp.head(20), y='feature', x='coefficient', palette='Greens_r'); . | voice_mail_plan, total_day_minutes, number_customer_service_calls, international_plan, number_vmail_messages가 고객 이탈 예측에 주요한 변수라고 판단됨 | . ",
    "url": "https://chaelist.github.io/docs/kaggle/customer_churn/#chrun-%EC%97%AC%EB%B6%80-%EC%98%88%EC%B8%A1%EC%97%90-%EC%A3%BC%EC%9A%94%ED%95%9C-%EB%B3%80%EC%88%98-%ED%8C%8C%EC%95%85",
    "relUrl": "/docs/kaggle/customer_churn/#chrun-여부-예측에-주요한-변수-파악"
  },"30": {
    "doc": "Telco Customer Churn",
    "title": "변수별 비교",
    "content": "주(state)별 이탈률 . | 주별 이탈률 비교 # churn: yes=1, no=0인 상태에서 평균을 구하면 churn rate와 같다 churn_state = customer_df.groupby(['state'])[['churn_0_1']].mean() * 100 churn_state.rename(columns={'churn_0_1':'churn rate (%)'}, inplace=True) churn_state.reset_index(inplace=True) # 이탈률 기준 내림차순 정렬 churn_state.sort_values(by='churn rate (%)', ascending=False, inplace=True) churn_state.head() . |   | state | churn rate (%) | . | 31 | NJ | 27.0833 | . | 4 | CA | 25.641 | . | 47 | WA | 22.5 | . | 20 | MD | 22.093 | . | 26 | MT | 21.25 | . → 주별 이탈률 시각화해서 확인 . plt.figure(figsize=(16, 6)) sns.barplot(data=churn_state, x='state', y='churn rate (%)', palette='Greens_r'); . | NJ(New Jersey)와 CA(California)가 특히 churn rate이 높은 주로 판명됨 | VA(Virginia)와 HI(Hawaii)는 특히 churn rate이 낮은 주로 판명됨 | .   . | 주별 이탈자 수 비교 . | ‘이탈률’이 높은 주가 실제 ‘이탈자 수’도 많은 것인지, 단순히 가입자 수가 적어서인지 확인하기 위함 | . state_churn_count = pd.pivot_table(data=customer_df, index='state', columns='churn', values='account_length', aggfunc='count').reset_index() state_churn_count.sort_values(by='yes', ascending=False, inplace=True) state_churn_count.head() . |   | state | no | yes | . | 31 | NJ | 70 | 26 | . | 43 | TX | 79 | 19 | . | 23 | MN | 89 | 19 | . | 49 | WV | 120 | 19 | . | 20 | MD | 67 | 19 | . → 주별 이탈자 수 시각화해서 확인 . plt.figure(figsize=(16, 6)) sns.barplot(data=state_churn_count, x='state', y='yes', palette='Greens_r'); . | NJ(New Jersey)는 실제로도 이탈자 수가 가장 많은 주로 판명됨. 특히 고객 경험 관리에 신경을 쓰면 좋은 주라고 생각. | VA(Virginia)와 HI(Hawaii)는 실제로도 이탈자 수가 매우 적은 주로 판명됨. | . | . plan별 이탈률 . : voice_mail_plan, international_plan 고객 타입별 이탈률 . | voice_mail_plan 사용 여부에 따른 이탈률 . # churn: yes=1, no=0인 상태에서 평균을 구하면 churn rate와 같다 sns.barplot(data=customer_df, x='voice_mail_plan', y='churn_0_1', palette='Greens'); . | voice_mail_plan을 사용하지 않는 고객의 이탈률을 약 16%, 사용하는 고객의 이탈률은 약 7% | . → 비율 차이가 유의미한지 t-test (사실 비율은 결국 평균과 같은 개념이라고 할 수 있기에, 평균 차이처럼 t-test를 활용해도 괜찮다) . temp1 = customer_df[customer_df['voice_mail_plan'] == 'yes'] temp2 = customer_df[customer_df['voice_mail_plan'] == 'no'] # Levene의 등분산 검정 lev_result = stats.levene(temp1['churn_0_1'], temp2['churn_0_1']) print('LeveneResult(F) : %.2f \\np-value : %.3f' % (lev_result)) . LeveneResult(F) : 56.57 p-value : 0.000 . # 이분산인 독립표본 t-test 실행 t_result = stats.ttest_ind(temp1['churn_0_1'], temp2['churn_0_1'], equal_var=False) print('t statistic : %.2f \\np-value : %.3f' % (t_result)) . t statistic : -8.84 p-value : 0.000 . | p &lt; 0.01이고, 시각화해서 살펴봤을 때도 꽤 차이가 나므로, voice_mail_plan을 사용하지 않는 고객이 더 이탈률이 높다고 할 수 있음 | .   . | international_plan 사용 여부에 따른 이탈률 . sns.barplot(data=customer_df, x='international_plan', y='churn_0_1', palette='Greens'); . | international_plan을 사용하지 않는 고객의 이탈률을 약 11%, 사용하는 고객의 이탈률은 약 42% | . → 비율 차이가 유의미한지 t-test . temp1 = customer_df[customer_df['international_plan'] == 'yes'] temp2 = customer_df[customer_df['international_plan'] == 'no'] # Levene의 등분산 검정 lev_result = stats.levene(temp1['churn_0_1'], temp2['churn_0_1']) print('LeveneResult(F) : %.2f \\np-value : %.3f' % (lev_result)) . LeveneResult(F) : 305.58 p-value : 0.000 . # 이분산인 독립표본 t-test 실행 t_result = stats.ttest_ind(temp1['churn_0_1'], temp2['churn_0_1'], equal_var=False) print('t statistic : %.2f \\np-value : %.3f' % (t_result)) . t statistic : 12.22 p-value : 0.000 . | p &lt; 0.01이고, 시각화해서 살펴봤을 때도 큰 차이가 나므로, international_plan을 사용하는 고객이 더 이탈률이 높다고 할 수 있음 | international_plan을 사용하는 고객의 이탈률은 42%나 되므로, 플랜의 불만족 요인을 전반적으로 점검해볼 필요가 있다고 생각됨. | . | . 낮 통화시간에 따른 이탈률 . | 이탈 여부에 따른 total_day_minutes 분포 &amp; 평균 비교 . fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(10, 5)) sns.boxplot(data=customer_df, x='churn', y='total_day_minutes', palette='Greens', ax=ax1) sns.barplot(data=customer_df, x='churn', y='total_day_minutes', palette='Greens', ax=ax2) plt.close(2) plt.close(3) plt.tight_layout() . | 이탈한 고객들이 전반적으로 total_day_minutes가 높은 편으로 보임 | . | total_day_minutes 4분위별 이탈률 . # 사분위를 계산해서 칼럼을 새로 생성 temp_df = customer_df[['total_day_minutes', 'churn', 'churn_0_1']] q1, q2, q3 = np.percentile(temp_df['total_day_minutes'], [25, 50, 75]) def get_quarter(x): if x &lt; q1: quarter = '1st_q' elif x &lt; q2: quarter = '2nd_q' elif x &lt; q3: quarter = '3rd_q' else: quarter = '4th_q' return quarter temp_df['total_day_minutes_quartile'] = temp_df['total_day_minutes'].apply(lambda x: get_quarter(x)) temp_df.head() . |   | total_day_minutes | churn | churn_0_1 | total_day_minutes_quartile | . | 0 | 161.6 | no | 0 | 2nd_q | . | 1 | 243.4 | no | 0 | 4th_q | . | 2 | 299.4 | no | 0 | 4th_q | . | 3 | 166.7 | no | 0 | 2nd_q | . | 4 | 218.2 | no | 0 | 4th_q | . → total_day_minutes 사분위별 churn rate 시각화해 비교 . sns.barplot(data=temp_df, y='total_day_minutes_quartile', x='churn_0_1', order=['4th_q', '3rd_q', '2nd_q', '1st_q'], palette='Greens'); . | 특히 total_day_minutes가 가장 많은 사분위에 속하는 고객들이 높은 이탈률을 보임. | . | . customer service calls에 따른 이탈률 . | 이탈 여부에 따른 number_customer_service_calls 분포 &amp; 평균 비교 . fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(10, 5)) sns.boxplot(data=customer_df, x='churn', y='number_customer_service_calls', palette='Greens', ax=ax1) sns.barplot(data=customer_df, x='churn', y='number_customer_service_calls', palette='Greens', ax=ax2) plt.close(2) plt.close(3) plt.tight_layout() . | 이탈한 고객들이 전반적으로 number_customer_service_calls가 많았던 것으로 보임 | . | number_customer_service_calls별 이탈자와 비이탈자 분포 . plt.figure(figsize=(8, 6)) sns.countplot(data=customer_df, x='number_customer_service_calls', hue='churn', palette='Greens_r'); . | number_customer_service_calls 그룹별 이탈률 . # 전화 횟수를 4번 이상 / 3번 이하로 나누어 flag를 붙임 temp_df = customer_df[['number_customer_service_calls', 'churn', 'churn_0_1']] temp_df['customer_service_calls_flag'] = temp_df['number_customer_service_calls'].apply(lambda x: 'more than 4' if x &gt;= 4 else 'less than 3') # number_customer_service_calls 그룹별 churn rate 비교 sns.barplot(data=temp_df, y='customer_service_calls_flag', x='churn_0_1', palette='Greens'); . | customer service call을 4번 이상 한 고객의 50%가 이탈 → customer service call을 4번 이상 한 고객은 특별한 고객 경험 관리가 필요하다고 생각됨. | . | . 보이스메일 메시지 수에 따른 이탈률 . | 이탈 여부에 따른 number_vmail_messages 분포 &amp; 평균 비교 . fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(10, 5)) sns.boxplot(data=customer_df, x='churn', y='number_vmail_messages', palette='Greens', ax=ax1) sns.barplot(data=customer_df, x='churn', y='number_vmail_messages', palette='Greens', ax=ax2) plt.close(2) plt.close(3) plt.tight_layout() . | 이탈한 고객 중 voice_mail_plan을 사용하지 않는 경우가 많아서 vmail_messages가 0인 경우가 많은 듯. | . | ‘voice_mail_plan = no’인 경우를 제외하고 비교 . | voice_mail_plan을 사용하지 않는 경우, vmail_messages가 0이기 때문 | . vmail_customer = customer_df[customer_df['voice_mail_plan'] == 'yes'] fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(10, 5)) sns.boxplot(data=vmail_customer, x='churn', y='number_vmail_messages', palette='Greens', ax=ax1) sns.barplot(data=vmail_customer, x='churn', y='number_vmail_messages', palette='Greens', ax=ax2) plt.close(2) plt.close(3) plt.tight_layout() . | voice_mail_plan을 사용한 고객만으로 비교하면, 이탈 여부에 따라 number_vmail_messages가 크게 달라지지는 않는 것으로 확인됨. | . | number_vmail_messages별 이탈자와 비이탈자 분포 . | number_vmail_messsages = 0인 경우는 제외하고 시각화 (0인 경우가 압도적으로 많아서) | . plt.figure(figsize=(15, 6)) sns.countplot(data=customer_df[customer_df['number_vmail_messages'] != 0], x='number_vmail_messages', hue='churn', palette='Greens_r'); . | number_vmail_messages별 이탈률 비교 . plt.figure(figsize=(15, 6)) sns.barplot(data=customer_df, x='number_vmail_messages', y ='churn_0_1', palette='Greens_r', ci=None); . | number_vmail_messages = 0인 경우를 제외하면, number_vmail_messages가 낮다고 이탈자가 많은 것은 아님 | number_vmail_messages만 가지고는 이탈할 고객인지 판별하기 어려울 듯 | . | . ",
    "url": "https://chaelist.github.io/docs/kaggle/customer_churn/#%EB%B3%80%EC%88%98%EB%B3%84-%EB%B9%84%EA%B5%90",
    "relUrl": "/docs/kaggle/customer_churn/#변수별-비교"
  },"31": {
    "doc": "Data Handling",
    "title": "Data Handling",
    "content": " ",
    "url": "https://chaelist.github.io/docs/data_handling",
    "relUrl": "/docs/data_handling"
  },"32": {
    "doc": "Datetime 다루기",
    "title": "Datetime 다루기",
    "content": ". | datetime 객체 생성 | datetime 정보 출력 . | ※ strftime(), strptime() 포맷 코드 | . | datetime 연산 | . ",
    "url": "https://chaelist.github.io/docs/data_handling/datetime/",
    "relUrl": "/docs/data_handling/datetime/"
  },"33": {
    "doc": "Datetime 다루기",
    "title": "datetime 객체 생성",
    "content": ". | 직접 정보를 입력해 생성 . import datetime as dt # import해서 사용. 별도의 설치는 필요 없다 custom_date = dt.datetime(2020, 11, 12) # 2020년 11월 22일 print(custom_date) # 시간 정보를 추가하지 않으면 자동으로 00시 00분 00초로 생성됨 . 2020-11-12 00:00:00 . +) 시간 정보 추가: . custom_date = dt.datetime(2020, 11, 12, 10, 11, 12) print(custom_date) . 2020-11-12 10:11:12 . | 현재 datetime으로 생성 . now = dt.datetime.now() print(now) . 2022-01-09 06:16:51.301662 . +) 시간까지 세세하게 가져오는 대신, 오늘의 날짜만 가져오기: . now = dt.date.today() print(now) . 2022-01-09 . | 특정 str 포맷으로부터 생성 . | datetime.datetim.strptime('str 형태의 날짜', '포맷')의 구조로 작성 | . str_date = '2020-11-12' custom_date = dt.datetime.strptime(str_date, '%Y-%m-%d') print(custom_date) . 2020-11-12 00:00:00 . | . ",
    "url": "https://chaelist.github.io/docs/data_handling/datetime/#datetime-%EA%B0%9D%EC%B2%B4-%EC%83%9D%EC%84%B1",
    "relUrl": "/docs/data_handling/datetime/#datetime-객체-생성"
  },"34": {
    "doc": "Datetime 다루기",
    "title": "datetime 정보 출력",
    "content": ". | 날짜, 시간 정보 추출 . now = dt.datetime.now() print(now.date()) print(now.time()) . 2022-01-09 06:16:51.301662 . +) 연도, 월, 일, 시간, 분, 초 모두 구분해서 추출: . print(now.year) print(now.month) print(now.day) print(now.hour) print(now.minute) print(now.second) . 2022 1 9 6 16 51 . | 요일 정보 추출 . print(now.weekday()) # 0: 월요일 ~ 6: 일요일 print(now.isoweekday()) # ISO 달력 형식: 1: 월요일 ~ 7: 일요일 . 6 7 . | 특정 포맷으로 정보 출력하기 . print('isoformat:', now.isoformat()) # yyyy-mm-dd'T'HH:mm:ss.SSSXXX 형태 print('ctime:', now.ctime()) # 요일 월 일자 HH:mm:ss 연도 형태 . isoformat: 2022-01-09T06:16:51.301662 ctime: Sun Jan 9 06:16:51 2022 . | 원하는 str 포맷으로 정보 출력하기 . | strptime()과 반대의 개념 | datetime_object.strftime('포맷')의 구조로 작성 | . print(now.strftime('%Y.%m.%d')) print(now.strftime('%Y/%m/%d %H시 %M분 %S초')) . 2022.01.09 2022/01/09 06시 16분 51초 . | . ※ strftime(), strptime() 포맷 코드 . | 포맷코드 | 설명 | 예 | . | %a | 요일 줄임말 | Sun, Mon, … Sat | . | %A | 요일 | Sunday, Monday, …, Saturday | . | %w | 요일을 숫자로 표시, 월요일~일요일, 0~6 | 0, 1, …, 6 | . | %d | 일 | 01, 02, …, 31 | . | %b | 월 줄임말 | Jan, Feb, …, Dec | . | %B | 월 | January, February, …, December | . | %m | 숫자 월 | 01, 02, …, 12 | . | %y | 두 자릿수 연도 | 01, 02, …, 99 | . | %Y | 네 자릿수 연도 | 0001, 0002, …, 2017, 2018, 9999 | . | %H | 시간(24시간) | 00, 01, …, 23 | . | %I | 시간(12시간) | 01, 02, …, 12 | . | %p | AM, PM | AM, PM | . | %M | 분 | 00, 01, …, 59 | . | %S | 초 | 00, 01, …, 59 | . | %Z | 시간대 | (비어있음)1), UTC, GMT | . | %j | 1월 1일부터 경과한 일수 | 001, 002, …, 366 | . | %U | 1년중 주차, 일요일이 한 주의 시작으로 | 00, 01, …, 53 | . | %W | 1년중 주차, 월요일이 한 주의 시작으로 | 00, 01, …, 53 | . | %c | 날짜, 요일, 시간을 출력, 현재 시간대 기준 | Sat May 19 11:14:27 2018 | . | %x | 날짜를 출력, 현재 시간대 기준 | 05/19/18 | . | %X | 시간을 출력, 현재 시간대 기준 | ‘11:44:22’ | . 1) naive datetime object의 경우(timezone 정보가 들어 있지 않은 경우), 아무것도 출력되지 않는다 . ",
    "url": "https://chaelist.github.io/docs/data_handling/datetime/#datetime-%EC%A0%95%EB%B3%B4-%EC%B6%9C%EB%A0%A5",
    "relUrl": "/docs/data_handling/datetime/#datetime-정보-출력"
  },"35": {
    "doc": "Datetime 다루기",
    "title": "datetime 연산",
    "content": ". | datetime 간 비교 . | datetime object는 int처럼 &gt;, &lt;, ==를 사용해 비교할 수 있다 | . date1 = dt.datetime(2020, 11, 12) date2 = dt.datetime(2021, 1, 19) if date1 &gt; date2: print('date1이 더 나중') elif date1 &lt; date2: print('date2가 더 나중') . date2가 더 나중 . | datetime - datetime . | 두 시점 사이의 기간이 timedelta 형태로 저장됨 | . period = date2 - date1 # 두 시점 사이의 기간이 timedelta object로 저장됨 print(period) print(type(period)) print(period.days) . 68 days, 0:00:00 &lt;class 'datetime.timedelta'&gt; 68 . +) datetime ± timedelta: . timedelta1 = dt.timedelta(days=2, hours=5) print(date1 + timedelta1) print(date1 - timedelta1) . 2020-11-14 05:00:00 2020-11-09 19:00:00 . | . ",
    "url": "https://chaelist.github.io/docs/data_handling/datetime/#datetime-%EC%97%B0%EC%82%B0",
    "relUrl": "/docs/data_handling/datetime/#datetime-연산"
  },"36": {
    "doc": "데이터베이스 & 테이블 구축",
    "title": "데이터베이스 &amp; 테이블 구축",
    "content": ". | 데이터베이스 생성 | 테이블 생성 . | 칼럼의 속성 | 칼럼의 데이터 타입 | . | 테이블에 row 추가/업데이트/삭제 | . ",
    "url": "https://chaelist.github.io/docs/sql/db_table_create/#%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B2%A0%EC%9D%B4%EC%8A%A4--%ED%85%8C%EC%9D%B4%EB%B8%94-%EA%B5%AC%EC%B6%95",
    "relUrl": "/docs/sql/db_table_create/#데이터베이스--테이블-구축"
  },"37": {
    "doc": "데이터베이스 & 테이블 구축",
    "title": "데이터베이스 생성",
    "content": ". | CREATE DATABASE로 생성 CREATE DATABASE 데이터베이스명; . | +) 이미 저장되어 있는 데이터베이스명으로 또 생성할 경우의 error 피하기: CREATE DATABASE IF NOT EXISTS 데이터베이스명; . | . ",
    "url": "https://chaelist.github.io/docs/sql/db_table_create/#%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B2%A0%EC%9D%B4%EC%8A%A4-%EC%83%9D%EC%84%B1",
    "relUrl": "/docs/sql/db_table_create/#데이터베이스-생성"
  },"38": {
    "doc": "데이터베이스 & 테이블 구축",
    "title": "테이블 생성",
    "content": ". | CREATE TABLE로 생성 CREATE TABLE `데이터베이스명`.`테이블명` ( `id` INT NOT NULL AUTO_INCREMENT, `name` VARCHAR(50) NOT NULL, `follower_count` INT NULL, PRIMARY KEY (id)); . | 테이블 역시, CREATE TABLE IF NOT EXISTS로 작성하면 이미 저장된 이름으로 중복 생성할 때의 error를 피할 수 있다 | 어떤 데이터베이스 내에 생성할지 명확히 표시했다면, CREATE TABLE 테이블명이라고만 적는 것도 가능 CREATE TABLE IF NOT EXISTS `테이블명` ( `id` INT NOT NULL AUTO_INCREMENT, `name` VARCHAR(50) NOT NULL, `follower_count` INT NULL, PRIMARY KEY (id)); . | ※ backtick(`): 식별자(identifier)임을 나타내기 위해 사용하는 기호 . | identifier: 데이터베이스, 테이블, 칼럼 등의 object에 붙는 이름 | 식별자를 `로 감싸주지 않아도 실행에는 문제가 없음. 이름임을 확실히 드러내고 싶어서 써주는 경우가 많을 뿐. | cf) ‘(작은 따옴표)나 “(큰 따옴표)는 문자열 값을 나타낼 때 사용되니, `와 구분해서 사용 | . | .   . 칼럼의 속성 . | PRIMARY KEY(PK): 해당 테이블에서 각 row를 식별할 수 있게 해주는 칼럼. 테이블별로 1개씩만 지정 가능하다 | NOT NULL(NN): 칼럼에 NULL값을 허용하지 않겠다는 제한 조건 PRIMARY KEY는 반드시 NOT NULL이여야 한다 | AUTO_INCREMENT(AI): 따로 값을 입력해주지 않아도 자동으로 칼럼의 값이 1씩 늘어난다. | PRIMARY KEY로 쓰는 칼럼(ex.id 칼럼)의 경우, AUTO_INCREMENT 속성을 달아주는 것이 편하다 | . | .   . 칼럼의 데이터 타입 .   . 1. Numeric types (숫자형 타입) . 1) 정수형 타입 . | TINYINT: 작은 범위의 정수를 저장할 때 쓰는 데이터 타입 . | 최소 -128 ~ 쵀대 127까지의 정수를 저장 가능 | ※ SIGNED: 양수-0-음수 / UNSIGNED: 0-양수를 의미 | TINYINT SIGNED: -128 ~ 127 | TINYINT UNSIGNED: 0 ~ 255 | *그냥 TINYINT라고만 하면 SIGNED가 붙은 것으로 간주하는 것이 default. | . | SMALLINT . | SMALLINT SIGNED: -32768 ~ 32767 | SMALLINT UNSIGNED : 0 ~ 65535 | . | MEDIUMINT . | MEDIUMINT SIGNED : -8388608 ~ 8388607 | MEDIUMINT UNSIGNED : 0 ~ 16777215 | . | INT . | INT SIGNED : -2147483648 ~ 2147483647 | INT UNSIGNED : 0 ~ 4294967295 | . | BIGINT . | BIGINT SIGNED : -9223372036854775808 ~ 9223372036854775807 | BIGINT UNSIGNED : 0 ~ 18446744073709551615 | . | . 2) 실수형 타입 . | DECIMAL: 보통 DECIMAL(M, D)의 형식으로 나타내는데, M은 최대로 쓸 수 있는 전체 숫자의 자리수, D는 최대로 쓸 수 있는 소수점 뒤 자리수를 의미 . | ex) DECIMAL (5, 2)라면 -999.99 부터 999.99 까지의 실수가 가능 | M은 최대 65, D는 30까지의 값을 가질 수 있다 | DECIMAL이라는 단어 대신 DEC, NUMERIC, FIXED를 써도 된다 | . | FLOAT . | -3.402823466E+38 ~ -1.175494351E-38, 0, 1.175494351E-38 ~ 3.402823466E+38 범위의 실수들을 나타낼 수 있는 데이터 타입 | fyi) -3.402823466E+38 은 (-3.402823466) X (10의 38제곱), -1.175494351E-38 은 (-1.175494351) X (10의 38제곱 분의 1) | . | DOUBLE . | -1.7976931348623157E+308 ~ -2.2250738585072014E-308, 0, .2250738585072014E-308 ~ 1.7976931348623157E+308 범위의 실수들을 나타낼 수 있는 데이터 타입 | . | . 2. 날짜 및 시간 타입 (Date and Time Types) . | DATE: 날짜를 저장하는 데이터 타입 . | YYYY-MM-DD 형식으로 표기 - ex. ‘2020-03-26’ | . | DATETIME: 날짜와 시간을 저장하는 데이터 타입 . | YYYY-MM-DD HH:MM:SS 형식으로 표기 - ex. ‘2020-03-26 09:30:27’ | . | TIMESTAMP: 날짜와 시간을 저장하는 데이터 타입 . | DATETIME과 마찬가지로 YYYY-MM-DD HH:MM:SS 형식으로 표기 | . ※ DATETIME과 TIMESTAMP의 차이 . | TIMESTAMP는 타임 존(time_zone) 정보도 함께 저장한다는 점에서 DATETIME과 차별화된다 | ex) SET time_zone = '-11:00'; 이렇게 시간대를 UTC-11로 바꿔주면 DATETIME으로 저정한 시간은 그대로지만, TIMESTAMP로 저장한 시간은 바뀜 (TIMESTAMP로 저장할 당시에는 UTC+9 시간대로 저장됨) | *UTC (Coordinated Universal Time): 국제 사회에서 통용되는 표준 시간 체계로 ‘국제 표준시’라고도 함. 영국 런던이 기준. → 한국은 UTC+9 시간대 | . | TIME: 시간을 나타내는 데이터 타입. | HH:MM:SS의 형식으로 표기 - ex. ‘09:27:31’ | . | . 3. 문자열 타입 (String type) . | CHAR: 문자열을 나타내는 기본 타입으로, Character의 줄임말 . | CHAR(30) 이런식으로 표기하는데, 괄호 안의 숫자는 문자를 최대 몇 자까지 저장할 수 있는지를 의미 (30이라고 쓰면 최대 30자의 문자열을 저장할 수 있다는 뜻) | () 안에는 0부터 255까지의 숫자를 적을 수 있음 | . | VARCHAR: CHAR처럼 문자열의 최대 길이를 지정할 수 있는 문자열 타입 - ex. VARCHAR(30) . | () 안에 최소 0부터 최대 65,535 (216 − 1)를 쓸 수 있다 | . ※ CHAR와 VARCHAR의 차이 . | VARCHAR는 ‘가변 길이 타입’ – VARCHAR이라는 단어 자체가 가변 문자열(varying character)의 줄임말 | CHAR는 ‘고정 길이 타입’ | CHAR(10)은 어떤 길이의 문자열이 저장되더라도 항상 그 값이 10만큼의 저장 용량을 차지하는 반면, VARCHAR(10)의 경우 만약 값이 ‘Hello’와 같이 5자라면 저장 용량도 5만큼만 차지한다 (저장 용량이 설정된 최대 길이에 맞게 고정되는 게 아니라 실제 저장된 값에 맞게 최적화되는 것!) | 대신, VARCHAR 타입으로 값이 저장될 때는 해당 값의 사이즈를 나타내는 부분(1byte 또는 2byte)이 저장 용량에 추가된다 → 값의 길이가 크게 변하지 않을 컬럼에는 CHAR 타입을 사용하고, 길이가 들쑥날쑥할 컬럼에는 VARCHAR 타입을 쓰는 게 좋다 | . | TEXT: 문자열을 저장하는 데이터 타입으로 최대 65,535자까지 저장 가능 . | VARCHAR와 내부 구현 면에서 일부 차이가 있어서, 보통 길이가 긴 문자열은 TEXT계열의 타입으로 저장 (메시지, 댓글 등) | . | MEDIUM TEXT: 16,777,215 (224 − 1) 자까지 저장 가능 | LONGTEXT: 4,294,967,295 (232 − 1) 자까지 저장 가능 | .   . ",
    "url": "https://chaelist.github.io/docs/sql/db_table_create/#%ED%85%8C%EC%9D%B4%EB%B8%94-%EC%83%9D%EC%84%B1",
    "relUrl": "/docs/sql/db_table_create/#테이블-생성"
  },"39": {
    "doc": "데이터베이스 & 테이블 구축",
    "title": "테이블에 row 추가/업데이트/삭제",
    "content": "  . 1. 테이블에 row 추가 . | INSERT INTO 테이블명 (칼럼명 나열) VALUES (추가할 값들 나열);의 구조로 작성 . INSERT INTO members (id, name, gender, email, phone, sign_up_date) VALUES (1, 'Chaelist', 'F', 'chaechae@chaelist.com', '010-1234-5678', '2015-05-22'); . | 모든 칼럼에 값이 있는 row를 추가한다면 아래와 같이 칼럼명은 제외하고 적어도 된다: . INSERT INTO members VALUES (1, 'Chaelist', 'F', 'chaechae@chaelist.com', '010-1234-5678', '2015-05-22'); . | 일부 칼럼에만 값을 추가해줄 경우, 해당 row의 나머지 column은 NULL값으로 채워진다. (NOT NULL 제약이 없는 경우) . INSERT INTO members (id, name, gender, sign_up_date) VALUES (1, 'Chaelist', 'F', '2015-05-22'); . | AUTO_INCREMENT 속성을 갖는 칼럼 ‘id’의 경우, 값을 입력해주지 않아도 자동으로 이전 값보다 1 큰 수가 할당된다 . INSERT INTO members (name, gender, email, sign_up_date) VALUES ('Honu', 'M', 'honu@racoon.com', '2016-01-19'); . | 여러 row를 삽입할 경우, INSERT INTO문을 계속 적어주는 대신 각각을 tuple로 묶어서 나열해줄 수 있다 . INSERT INTO food_menu (menu, price, ingredient) VALUES ('떡볶이', 4500, '어묵, 떡, 양파..'), ('참치김밥', 3000, '참치, 김, 단무지..'), ('오므라이스', 7000, '달걀, 양파, 애호박..'); . | INSERT INTO문을 여러 번 반복해서 써주는 것보다, 각각을 tuple로 묶어서 하나의 쿼리로 넣어주는 것이 속도가 빠르다 (한번에 다량의 데이터를 넣을 때는 확연한 속도 차이를 보임) | . | . 2. row 정보 업데이트 . | UPDATE 테이블명 SET 변경할 컬럼 정보 WHERE 조건;의 구조로 작성 . UPDATE members SET phone = '010-8765-4321' WHERE id = 1; . | 같은 row의 정보를 한 번에 2개 이상 바꿀 때는 SET 뒤에 나열해주면 됨 . UPDATE members SET phone = '010-8765-4321', name = 'ChaeChae' WHERE id = 1; . | ※ 정보를 업데이트할 때는, WHERE절을 잘 적어주는 것이 중요하다 . | ex) UPDATE members SET name = 'ChaeChae'라고만 적으면 ‘name’ 칼럼의 모든 값이 다 변경됩 | . | +) workbench에서 safe update 모드가 설정되어 있으면, primary key 외의 컬럼을 WHERE 절에서 사용하는 UPDATE문은 실행이 안된다. | MySQL인 경우, [MYSQLWorkbench &gt; Preferences 클릭 &gt; SQL Editor 선택 &gt; 아래 Others 부분에 ‘Safe Updates’에 체크되어 있는 것을 해제]한 후 재접속하면 primary key 외의 컬럼으로도 접근해서 UPDATE 가능. | . | . +) 컬럼의 기존 값을 기준으로 값 업데이트하기 . | ex) 기말고사 성적에 채점 오류가 있었어서, 모든 row의 score에 +3을 해줘야 하는 상황: . UPDATE final_exam_result SET score = score + 3; -- 이렇게 해주면 모든 row의 점수가 3씩 더해져서 저장됨 . | . 3. row 삭제 . | DELETE FROM 테이블명 WHERE 조건;의 구조로 작성 . DELETE FROM members WHERE id = 4; . | ※ 업데이트할 때와 마찬가지로, DELETE할 때에도 WHERE절을 신경써야 한다 . | ex) DELETE FROM memebers;라고만 하면 테이블의 모든 row가 삭제되어 버린다 | . | . +) 물리 삭제 vs 논리 삭제 . | 물리 삭제: 삭제해야 할 row를 물리적으로 ‘DELETE’해서 없애버리는 것 . | ex) DELETE FROM order WHERE id = 4; | . | 논리 삭제: DELETE 대신, ‘삭제 여부’ 컬럼을 별도로 만들어서, ‘YES’와 같이 값을 업데이트해줘서 row가 삭제되었음을 표시해주는 것 . | ex) UPDATE order SET cancelled = 'Y' WHERE id = 4; | . | 추후 분석에 활용할 수 있도록 데이터를 남겨두고 싶을 때는 논리 삭제를 이용하는 것이 좋다 . | ex) 사용자가 주문을 취소한 경우, 아예 row 자체를 삭제해버리기보다 row는 그대로 두고 취소된 주문임을 명시해주는 방식을 사용하면 추후에 취소된 주문 데이터를 따로 모아서 분석 가능 | . | 다만, 논리 삭제 방식으로 삭제하면 1) 삭제되지 않은 유효한 row들만 조회할 때 WHERE절에 매번 별도 조건을 넣어줘야 하며, 2) 삭제된 row여도 실제 DELETE된 것은 아니기에 DB 용량을 계속 차지한다는 단점이 있다 | . ",
    "url": "https://chaelist.github.io/docs/sql/db_table_create/#%ED%85%8C%EC%9D%B4%EB%B8%94%EC%97%90-row-%EC%B6%94%EA%B0%80%EC%97%85%EB%8D%B0%EC%9D%B4%ED%8A%B8%EC%82%AD%EC%A0%9C",
    "relUrl": "/docs/sql/db_table_create/#테이블에-row-추가업데이트삭제"
  },"40": {
    "doc": "데이터베이스 & 테이블 구축",
    "title": "데이터베이스 & 테이블 구축",
    "content": " ",
    "url": "https://chaelist.github.io/docs/sql/db_table_create/",
    "relUrl": "/docs/sql/db_table_create/"
  },"41": {
    "doc": "Deep Learning 기초",
    "title": "Deep Learning 기초",
    "content": ". | 인공신경망 이론 . | Neural network의 구조 | 가설함수와 손실함수 | 활성 함수 | . | Keras로 구현하기 (기초) . | 데이터 준비 | Normalizer 준비 | 모델 형성 | 학습 &amp; 평가 | . | . *Deep Learning: ML 연구방법 중 하나로, Neural Network를 활용하는 방식.   . *python으로 Deep Learning을 할 때 사용할 수 있는 framework: . | Tensorflow: Google에서 개발 | Keras: tensorflow를 기반으로 만든, 보다 쉬운 framework . | 입문자용으로 적합. 전문 연구자가 아니고 본인 필드에 가볍게 응용하는 정도면 Keras로 충분 | TensorFlow v1.10.0부터 tf.keras로 텐서플로우 안에서 케라스를 사용 | . | PyTorch: Facebook에서 개발 | . ",
    "url": "https://chaelist.github.io/docs/ml_basics/deep_learning/",
    "relUrl": "/docs/ml_basics/deep_learning/"
  },"42": {
    "doc": "Deep Learning 기초",
    "title": "인공신경망 이론",
    "content": "Neural network의 구조 . ※ layer의 수를 셀 때, 보통 hidden layer와 output layer 수로 표현. (input layer는 0번째 층으로 간주) → L = 3이면 hidden layer 2개에 output layer 1개인 구조 . ※ 각 layer에 포함된 node들을 neuron이라고도 함 (인공 뉴런) .   . (출처: dzone.com) .   . 1. Input layer : 입력층. 가장 앞에 있는 레이어로, 독립변수를 데이터로 받는다. | 독립변수(feature)의 수만큼의 input node로 구성 (각 node는 각 독립변수의 값을 입력받음) . | ex) 두 개의 독립변수(평수, 연식)을 통해 아파트 가격을 예측할 경우, input node는 2개 | . | . 2. Hidden layer : 은닉층. 추가적인 작업을 통해 종속변수의 정확한 예측에 기여하는 node들을 뽑아낸다. | 앞 layer에 존재하는 정보들 중 종속변수 예측에 중요한 정보들을 추출해주는 역할을 한다. | hidden layer는 1개 이상 존재할 수 있다. (연구자가 결정하는 것) | hidden layer가 너무 많아도 오히려 성능이 떨어지기에, 보통 1-3개 사이 중에서 고르는 듯? | 보통 hidden layer 수가 많은 경우를 ‘deep neural networks’ = ‘deep learning’이라고 부르는 것. | hidden layer의 node 개수도 연구자 재량에 따라 결정. | 첫번째 hidden layer의 node 수와 두번째 hidden layer의 node 수가 같지 않아도 됨. 하지만 보통은 그 전 layer의 node 수와 같거나 적은 수를 사용한다고 함 | hidden layer의 node 수는 보통 input layer의 node 수와 output layer의 node 수 사이로 많이 설정 | . 3. Output layer : 출력층. 종속변수에 대한 예측치가 출력되는 레이어. | 종속변수의 형태에 따라 output node 수가 달라짐 . | 이분적 분류 문제: output node를 2개로 하거나 / 1개로 두고, 시그모이드 함수를 output layer의 활성 함수로 사용해 0에 가까운지 1에 가까운지를 가지고 판단 | 다중 분류 문제: 결과변수(DV)가 취할 수 있는 값 (카테고리의 수) = output node의 수 | 회귀 문제: output node는 1개. (예측치 = output node) | . | . +) Bias Node . | bias node = intercept (상수항)의 개념 (선형회귀에서의 intercept의 기능과 같다) | input layer와 hidden layer에만 bias node가 하나씩 들어갈 수 있으며, bias node가 들어가면 모델의 flexibility가 증가한다. | . +) Weights (Parameters) . | 독립변수와 종속변수 간의 관계를 설정 (선형회귀에서의 theta값과 동일한 기능) | node와 node를 잇는 각 선마다 별도의 weight가 존재. | ex) 1번째 layer의 모든 node와 2번째 layer의 모든 node 간에는 별도의 weight가 존재 | . | .   . 가설함수와 손실함수 . | 가설함수: 주어진 가중치(weight)와 편향(bias)에 따라 output layer 뉴런들의 출력을 계산해내는 함수. | 손실함수(비용함수): . | 회귀 문제: 보통 MSE(평균제곱오차)를 많이 사용 | 분류 문제: 보통 로그 손실(cross entropy)를 많이 사용 | . | . → 경사하강법을 사용해서 손실함수의 최소점을 찾는다. (손실함수를 최소화하는 weight 값들을 구한다) .   . +) 다양한 경사 하강법 . | 배치 경사 하강법 (batch gradient descent): . | 한 번 경사 하강을 할 때 모든 학습 데이터를 사용 → 데이터가 많으면 너무 오래 걸린다! | . | 확률적 경사 하강법 (stochastic gradient descent): . | 한 번 경사 하강을 할 때 하나의 학습 데이터만 사용 → 빠르게 계산이 가능하지만, 가장 경사가 가파른 방향으로 하강하지 않으며 극소점 근처에서도 계속 주변을 맴돌며 쉽게 수렴하지 않을 수 있다는 단점도 있음 | . | 미니 배치 경사 하강법 (mini batch gradient descent): . | 위 두가지 방법의 타협점. 데이터셋을 임의로 같은 크기의 여러 데이터셋으로 나눠준 후, 한 번 경사 하강을 할 때 ‘mini batch’ 학습 데이터를 사용 (ex. 학습 데이터를 50개씩 나눠 놓은 후, 한 번의 경사하강에 하나의 mini batch만 사용) | . | . ※ 가장 좋은 경사 하강법이 정해져 있는 건 아니지만, 대부분의 경우 미니 배치 경사 하강법을 가장 많이 사용한다 .   . 활성 함수 . ※ hidden &amp; output node들은 보통 입력값 z (이전 층에서 전달하는 입력값)을 그대로 출력하지 않고, 활성화 함수로 변환해 f(z)를 출력한다. (각 node 안에서 이러한 변환이 이루어짐) . 1. Hidden Layer의 활성 함수 . (출처: adilmoujahid.com) . | 시그모이드 함수( = logistic function) . | $ \\frac{1}{(1 + e^{-z})} $로 계산. input을 0~1 사이의 숫자로 바꿔서 출력해주게 됨 | . | Hyperbolic tangent (tanh) . | $ \\frac{sinh(z)}{cosh(z)} = \\frac{(e^z - e^{-z})}{(e^z + e^{-z})} $로 계산. input을 -1 ~ 1 사이의 값으로 바꿔서 출력해줌 | . | ReLU (Rectified Linear Unit) . | max(0, z)로 계산. input이 0보다 크면 그대로 출력, 0보다 작거나 같으면 0을 출력. | 경사 계산 속도가 매우 빠르다는 장점. (z가 0보다 크면 경사가 1, 작거나 같으면 0) | . | Leaky ReLU . | ReLU를 약간 변형한 형태로, 사라지는 기울기 문제가 덜해진다. | max(εz, z)로 계산. ε(입실론)은 보통 0.01 정도의 작은 상수를 사용 | . | . ※ 최근에는 hidden layer의 활성 함수로 ReLU가 가장 많이 사용된다. ※ 활성화 함수가 비선형이면 신경망도 비선형 함수, 활성화 함수가 선형이면 신경망도 결국 선형 함수 (선형적인 결정경계만 찾아낼 수 있음) → 은닉층의 활성 함수로는 선형 함수를 사용하지 않는다! . 2. Output Layer의 활성 함수 . | 분류문제 (DV가 분류문제) . | 이분적 분류 문제라면 보통 output layer의 활성함수로 ‘시그모이드 함수’를 사용. | output node를 1개만 두고, 0에 가까운지 1에 가까운지의 ‘확률’에 따라 분류 | . | 다중 분류 문제라면 보통 ‘Softmax 함수’를 사용. | $ \\frac{e^z}{각 node의 e^z의 합} $으로 계산 (e는 자연상수. 2.71…) → 그리고 가장 값이 큰 쪽으로 분류 | ※ Softmax 함수는 늘 출력값의 합이 1이 되기에, 다중 분류에서 보다 확률적으로 의미가 명확하다 (ex. 강아지일 확률 70%) | . | . | 회귀문제 (DV가 연속변수): output layer의 활성함수로 ‘선형함수’를 사용. output node를 1개만 두고, output node가 받는 z값을 그대로 출력. (output layer에 활성함수가 없다고 이해해도 무방) | . ",
    "url": "https://chaelist.github.io/docs/ml_basics/deep_learning/#%EC%9D%B8%EA%B3%B5%EC%8B%A0%EA%B2%BD%EB%A7%9D-%EC%9D%B4%EB%A1%A0",
    "relUrl": "/docs/ml_basics/deep_learning/#인공신경망-이론"
  },"43": {
    "doc": "Deep Learning 기초",
    "title": "Keras로 구현하기 (기초)",
    "content": ". | 먼저, 설치해야 사용이 가능하다: https://www.tensorflow.org/install/pip#virtual-environment-install | google colaboratory에서는 별도의 설치 없이 바로 import해서 사용 가능 | . import tensorflow as tf # google colab에서는 설치 따로 안해도 import해서 사용 가능 from tensorflow import keras from tensorflow.keras import layers from tensorflow.keras.layers.experimental import preprocessing import numpy as np import pandas as pd import matplotlib.pyplot as plt # Make numpy printouts easier to read. np.set_printoptions(precision=2, suppress=True) . | precision=2: 소수점 두번째 자리까지만 출력 | suppress=True: e-04와 같은 scientific notation을 제거 | . 데이터 준비 . | 기초 예시이기에, 기본으로 제공되는 boston_housing 데이터를 사용 | . from tensorflow.keras.datasets import boston_housing (X_train, y_train), (X_test, y_test) = boston_housing.load_data() # test_split=0.2이 default. Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/boston_housing.npz 57344/57026 [==============================] - 0s 0us/step . print(X_train.shape, y_train.shape, X_test.shape, y_test.shape) ## train:test가 대략 4:1 비율로 나뉘었고, IV는 13개, DV는 1개 . (404, 13) (404,) (102, 13) (102,) . Normalizer 준비 . | preprocessing을 위한 normalization layer를 준비해서 모델에 넣게 됨 | . normalizer = preprocessing.Normalization() normalizer.adapt(np.array(X_train)) # 아래와 같이 normalizer에 13개 IV 각각의 평균(mean)과 분산(variance) 정보가 저장됨 print(normalizer.mean.numpy()) print(normalizer.variance.numpy()) . [ 3.75 11.48 11.1 0.06 0.56 6.27 69.01 3.74 9.44 405.9 18.48 354.78 12.74] [ 85.18 563.51 46.28 0.06 0.01 0.5 778.75 4.11 75.47 27611.97 4.83 8834.99 52.5 ] . ## 예시로, 데이터를 넣었을 때 잘 normalize되는지 확인 ex_array = np.array(X_train[:1]) # X_train의 첫번째 값 (첫번째 도시에 대한 13개 정보 vector) print('Original:', ex_array) print('Normalized:', normalizer(ex_array).numpy()) # 원래 서로 다른 scale의 값이였지만 잘 normalize된 것을 확인 가능! . Original: [[ 1.23 0. 8.14 0. 0.54 6.14 91.7 3.98 4. 307. 21. 396.9 18.72]] Normalized: [[-0.27 -0.48 -0.44 -0.26 -0.17 -0.18 0.81 0.12 -0.63 -0.6 1.15 0.45 0.83]] . 모델 형성 . model = keras.Sequential([ normalizer, layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)), # hidden layer 1 layers.Dense(32, activation='relu'), # hidden layer 2 layers.Dense(1) # output layer: 회귀 문제이므로 node 1개 &amp; 활성 함수 지정 X ]) model.summary() . Model: \"sequential\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= normalization (Normalization (None, 13) 27 _________________________________________________________________ dense_6 (Dense) (None, 64) 896 _________________________________________________________________ dense_7 (Dense) (None, 32) 2080 _________________________________________________________________ dense_8 (Dense) (None, 1) 33 ================================================================= Total params: 3,036 Trainable params: 3,009 Non-trainable params: 27 _________________________________________________________________ . *Prameter 수 계산: . | 첫번째 normalization layer는 13*2 + 1 = 27 | 두번째: 14 * 64 = 896 (bias node가 하나씩 추가되니까 13이 아니라 14) | 세번째: 65 * 32 = 2080 | 마지막: 33 * 1 = 33 | . model.compile(loss=keras.losses.MeanSquaredError(), optimizer=tf.optimizers.Adam(0.01)) . | 손실함수: 보통 ‘mean_absolute_error’나 keras.losses.MeanSquaredError() 사용 | optimizer로 어떤 경사 하강법을 사용할지 결정. 보통 Adam(Adaptive Moment Estimation)을 많이 사용 | Adam(0.01): learning_rate를 0.01로 지정한 것. (default = 0.001) | Adam 경사하강법은 파라미터에 따라 learning rate를 자동으로 조절해주는 기능이 있으나, 초기 learning rate는 설정을 해줘야 함 | . 학습 &amp; 평가 . %%time history = model.fit( X_train, y_train, validation_split = 0.2, # 20%의 trainig data를 validation data로 사용 -- 사실 이미 train_test_split을 해뒀기 때문에 꼭 할 필요는 없다. epochs=100, # 전체 데이터를 몇 번 반복해서 업데이트할 것인지 batch_size = 16, # 한 번 업데이트할 때 데이터포인트를 몇 개 사용할지 verbose=0 # 학습 과정을 아래에 출력하지 않겠다는 의미 ) . CPU times: user 10.2 s, sys: 432 ms, total: 10.6 s Wall time: 9.49 s . | batch_size: default는 32. 보통 2의 배수를 많이 사용하는 듯. | verbose: 0, 1, 2 중 하나 선택. 0 = silent, 1 = progress bar, 2 = one line per epoch | . +) batch_size와 epoch: . | ex) 학습 데이터가 48개 있고, batch_size=16, epochs=100이라면, 총 (48/16) * 100 = 300번 업데이트가 일어남 . | 48개의 데이터를 16개씩 쪼개서 3번 나눠서 업데이트를 해주는데, 이렇게 전체 데이터를 업데이트하는 과정을 100번 반복하는 것! | . | . def plot_loss(history): plt.plot(history.history['loss'], label='loss') ## 실제 training에 사용했던 data에서 어느 정도 오차가 발생하는지 plt.plot(history.history['val_loss'], label='val_loss') ## validation data로 사용했던 20%에서 어느 정도 오차가 발생하는지 plt.ylim([0, 50]) plt.xlabel('Epoch') plt.ylabel('Loss') plt.legend() plt.grid(True) plot_loss(history) . # test data로 예측했을 때의 손실 확인 model.evaluate(X_test, y_test) . 4/4 [==============================] - 0s 2ms/step - loss: 13.1361 13.136137008666992 . from sklearn.metrics import r2_score y_test_pred = model.predict(X_test) r2_score(y_test, y_test_pred) # r스퀘어값 확인 . 0.8421969063118736 . ",
    "url": "https://chaelist.github.io/docs/ml_basics/deep_learning/#keras%EB%A1%9C-%EA%B5%AC%ED%98%84%ED%95%98%EA%B8%B0-%EA%B8%B0%EC%B4%88",
    "relUrl": "/docs/ml_basics/deep_learning/#keras로-구현하기-기초"
  },"44": {
    "doc": "Dictionary, Tuple, Set",
    "title": "Dictionary, Tuple, Set",
    "content": ". | Dictionary . | Dictionary 기초 | Main Dictionary Functions | . | Tuple . | Tuple 기초 | Tuple의 활용법 예시 | . | Set (집합자료형) . | Set 기초 | 교집합, 합집합, 차집합 | . | . ",
    "url": "https://chaelist.github.io/docs/python_basics/dictionary_tuple_set/",
    "relUrl": "/docs/python_basics/dictionary_tuple_set/"
  },"45": {
    "doc": "Dictionary, Tuple, Set",
    "title": "Dictionary",
    "content": "{Key : Value}와 같은 모양으로 표현됨 . | {key:value, key2:value2, …} 이런 식으로 pair들 나열 | 이렇게 키와 값이 연결되는 개념을 보통 ‘연관 배열 (Associative Arrays)’이라고 한다. | list는 값들에 순서대로 index가 매겨지지만, dictionary는 가방에 이것저것 라벨을 붙여서 넣는 것과 같다. (dictionaries are like bags - no order!) | ‘Key’ is unique in the dictionary and must be immutable | . # 빈 Dictionary를 생성하는 방법 a = {} b = dict() print(a, b) # 두 방법 모두 동일 . Dictionary 기초 . | key로 value에 접근하기 dict1 = {'Tom':23, 'John':34, 'Bob':12} print(dict1['Tom']) # 'Tom'의 value를 추출 . 23 . | Dictionary에 값 넣기 purse = dict() # 빈 dictionary 새로 생성 purse['money'] = 12 # key가 'money'고 value가 12인 pair 저장 purse['candy'] = 3 # key가 'candy'고 value가 3인 pair 저장 print(purse) . {'money': 12, 'candy': 3} . *주의: 특정 key는 하나뿐이며(unique), 변경불가능하다(immutable) . # 같은 key에 또 값을 assign해주면 그냥 그 key의 value가 대체될 뿐이다. (a key must be unique!) dict1 = {'Tom': 23, 'John': 34, 'Bob': 12, 'Sarah': 35} dict1['Sarah'] = 54 ## {'Sarah':54}가 새로 추가되는 대신, 원래 있던 'Sarah' key의 value가 54로 변경됨. print(dict1) . {'Tom': 23, 'John': 34, 'Bob': 12, 'Sarah': 54} . # 또한, 특정 key는 변경 불가능하기에, {'Sarah': 35}를 {'SARAH':35}로 바꾸는 방법은 없다 . | pair의 수 (=key의 수) 세기: len() # len() dict1 = {'Tom':23, 'John':34, 'Bob':12} len(dict1) # returns the number of keys (= number of pairs) . 3 . | dictionary 삭제하기: del # dictionary의 특정 element 삭제 dict1 = {'Tom':23, 'John':34, 'Bob':12} del dict1['Tom'] print(dict1) # dictionary 자체를 삭제하는 것도 가능 del dict1 print(dict1) # dict1 자체가 아예 사라졌으므로, 존재하지 않는다고 error가 남. {'John': 34, 'Bob': 12} --------------------------------------------------------------------------- NameError Traceback (most recent call last) &lt;ipython-input-15-ecedb4a10042&gt; in &lt;module&gt;() 5 6 del dict1 ----&gt; 7 print(dict1) NameError: name 'dict1' is not defined . | . Main Dictionary Functions . | dict1.clear(): dictionary 안의 모든 데이터를 삭제 # dict1.clear() dict1 = {'Tom':23, 'John':34, 'Bob':12} dict1.clear() # dict1 안의 모든 값을 제거. dict1이라는 것 자체는 남아 있음 ## cf) del dict1을 하면 해당 dictionary 자체가 사라짐 print(dict1) . {} . | dict1.keys(): dictionary 안의 모든 key들을 list-like variable로 반환 # dict1.keys() dict1 = {'Tom':23, 'John':34, 'Bob':12} dict1.keys() . dict_keys(['Tom', 'John', 'Bob']) . *dict1.keys()의 결과로 반환되는 결과물은 list 형태로 변환이 가능 (list-like variable: ‘list(x)’ function을 사용해서 list로 변경 가능) . # list(dict1.keys()) key_list = list(dict1.keys()) print(key_list) # list 형태로 출력됨 . ['Tom', 'John', 'Bob'] . cf) list(dict1)을 해도 key만 list로 반환됨 . # list(dict1) print(list(dict1)) . ['Tom', 'John', 'Bob'] . | dict1.values(): dictionary 안의 모든 value들을 list-like variable로 반환 # dict1.values() dict1 = {'Tom':23, 'John':34, 'Bob':12} dict1.values() . dict_values([23, 34, 12]) . | values() function은 별로 안 중요. keys() function만 기억해도 된다. (key는 unique하니까, key만 알면 그 value에 access 가능. 하지만 value는 겹칠 수 있어서, value만 아는 건 useless.) | . | dict1.update(another_dicationary): 다른 dictionary의 내용을 가져와서 dictionary를 update. # dict1.update(another_dicationary) dict1 = {'Tom':21, 'Kai':20} dict2 = {'Sarah':22} dict1.update(dict2) # dict2의 정보를 dict1에 추가 print(dict1) . {'Tom': 21, 'Kai': 20, 'Sarah': 22} . cf) update할 때, 두 개의 dictionary가 중복되는 key를 갖고 있다면? . dict1 = {'Tom':21, 'Kai':20} dict2 = {'Sarah':22, 'Tom':34} dict1.update(dict2) print(dict1) # 'Tom'의 value가 dict2의 값으로 업데이트됨 . {'Tom': 34, 'Kai': 20, 'Sarah': 22} . | dict1.get(name, default_value) . | dictionary에 존재하는 키인지 여부에 따라 값을 다르게 처리 | name이라는 key가 dict1에 있으면 그 값을 가져오고, 아니면 따로 입력한 default value를 반환 | . # 'get' method 이용 예시 ## 'names' 리스트에 각 이름이 몇 개씩 담겨있는지 세어서 'counts'라는 딕셔너리에 넣는 것. counts = dict() names = ['csev', 'cwen', 'csev', 'zqian', 'cwen'] for name in names: counts[name] = counts.get(name, 0) + 1 # counts.get(name, 0): counts 딕셔너리에 name 키가 존재할 경우 이의 value를 불러오고, # 그렇지 않을 경우에는 counts 딕셔너리에 {name : 0} 데이터를 추가하라는 의미 print(counts) . {'csev': 2, 'cwen': 2, 'zqian': 1} . | dict1.items(): (key, value)의 tuple이 list-like variable로 출력됨 # dict1.items() dict1 = {'Tom':23, 'John':34, 'Bob':12} dict1.items() . dict_items([('Tom', 23), ('John', 34), ('Bob', 12)]) . +) dict1.items() 활용 예시 . jjj = {'chuck':1, 'fred':42, 'jan':100} for aaa, bbb in jjj.items(): print(aaa, bbb) . chuck 1 fred 42 jan 100 . | . +) get()과 item() 함수 이용 예시: . dictionary의 get()과 item() 함수를 이용해, 예시 텍스트에서 가장 많이 나온 단어를 출력하는 코드 . text = \"love live love start hands shake you feel love lower less dry deny follow hi love live like yogurt timeline loyal love\" # 연습을 위해, 랜덤한 단어를 나열한 단순한 text를 사용 counts = dict() words = text.split() for word in words: counts[word] = counts.get(word, 0) + 1 bigcount = None bigword = None for word, count in counts.items(): if bigword is None or count &gt; bigcount: bigword = word bigcount = count print(bigword, bigcount) # 가장 많이 나온 단어와 그 사용 빈도를 출력 . love 5 . ",
    "url": "https://chaelist.github.io/docs/python_basics/dictionary_tuple_set/#dictionary",
    "relUrl": "/docs/python_basics/dictionary_tuple_set/#dictionary"
  },"46": {
    "doc": "Dictionary, Tuple, Set",
    "title": "Tuple",
    "content": ". | list와 비슷하나, [ ] 대신 ( )로 구성됨 | list처럼, 순서가 있어서 index number로 접근 가능 (indexing) | 하지만 list와 달리 immutable. (한 번 만든 tuple은 modify할 수 없다) | 변경불가능이기에 더 simple, efficient하다는 장점 -&gt; preferred when making ‘temporary variables’ | . Tuple 기초 . | tuple에 적용 가능한 함수들 (tuple은 list와 비슷하기에, 몇몇 list 함수들은 tuple에도 동일하게 적용된다) # tuple 예시 x = ('Glenn', 'Sally', 'Joseph') print(x[0]) # indexing print(x[:2]) # slicing print(max(x)) # max, min 기능 print(x.count('Glenn')) # x.count 기능 (특정 element의 개수 세기) print(x.index('Glenn')) # x.index 기능 (특정 element의 index number 찾기) . Glenn ('Glenn', 'Sally') Sally 1 0 . | tuple is immutable; 한 번 만든 tuple은 변경할 수 없다 x = (9, 8, 7) x[2] = 6 ## index number가 2인 element를 6으로 바꾸려 시도 --&gt; error. --------------------------------------------------------------------------- TypeError Traceback (most recent call last) &lt;ipython-input-4-0ca6b872377d&gt; in &lt;module&gt;() 1 x = (9, 8, 7) ----&gt; 2 x[2] = 6 ## index number가 2인 element를 6으로 바꾸려 시도 --&gt; error. TypeError: 'tuple' object does not support item assignment . *주의: tuple은 한 번 생성된 후에는 변경 불가하기에, x.sort(), x.append(), x.extend(), x.reverse() 등의 기능이 없다. (list에 쓰는 method 중, list를 modify하는 속성의 아이들은 tuple에는 모두 적용 불가!) . | . Tuple의 활용법 예시 . | tuple을 사용해 여러 개 variable를 한 번에 assign 가능 (x, y) = (4, 'fred') # 각각 x=4, y='fred'로 저장됨 print(x) a, b = (99, 98) # 이렇게 괄호를 지우고 assign해도 된다 print(a) a, b = 99, 98 # 이 것도 가능 print(b) ## 이 특성을 이용해 dictionary.items()의 결과로 나온 (key, value) tuple에 쉽게 접근 가능 ## (ex. for k, v in di.items(): 와 같은 코드 이용) . 4 99 98 . | tuple 간 비교 . | 첫번째부터 순서대로 비교; 만약 첫번째 element이 같다면 다음 element끼리 비교하고,..이런 식. | . print((0, 1, 2) &lt; (5, 1, 2)) print((0, 1, 3000) &lt; (0, 3, 4)) print(('Jones', 'Sally') &lt; ('Jones', 'Sam')) print(('Jones', 'Sally') &gt; ('Jones', 'Sam')) . True True True False . | dictionary 정리1: sorting by keys d = {'a':10, 'b':1, 'c':22} print(d.items()) print(sorted(d.items())) # list 속 tuple들의 첫번재 element를 기준으로 비교해서 sort해준다 ## sorted()를 하면 dict_items 대신 list variable로 출력됨 . dict_items([('a', 10), ('b', 1), ('c', 22)]) [('a', 10), ('b', 1), ('c', 22)] . | dictionary 정리2: sorting by values (instead of keys) d = {'a':10, 'b':1, 'c':22} tmp = list() for k, v in d.items(): tmp.append((v, k)) # 반대로 뒤집어서 list에 넣어줌 print(tmp) tmp = sorted(tmp, reverse=True) # 역순으로 정렬 print(tmp) . [(10, 'a'), (1, 'b'), (22, 'c')] [(22, 'c'), (10, 'a'), (1, 'b')] . +) List Comprehension (참고) . # 위에서 썼던 다음과 같은 코드를 tmp = list() for k, v in d.items(): tmp.append((v, k)) # --&gt; 아래와 같이 간결하게 작성 가능 tmp = [(v, k) for k, v in d.items()] . | . ",
    "url": "https://chaelist.github.io/docs/python_basics/dictionary_tuple_set/#tuple",
    "relUrl": "/docs/python_basics/dictionary_tuple_set/#tuple"
  },"47": {
    "doc": "Dictionary, Tuple, Set",
    "title": "Set (집합자료형)",
    "content": ". | { } 안에 값들이 나열된 형태. (cf. dictionary는 { } 안에 ‘key:value’의 “조합”들이 담긴 형태) | 중복을 허용하지 않는다 | 순서가 없다 (unordered) -&gt; indexing 불가. (cf. dictionary도 순서가 없어서 indexing 불가) | 하지만, 쉽게 list나 tuple로 변환할 수 있기에, list나 tuple로 변환 후 indexing하면 됨 | . Set 기초 . | Set(집합) 만들기 # 1. list 입력해서 set 만들기 s1 = set([1, 2, 3]) print(s1) # 2. string 입력해서 set 만들기 s2 = set(\"Hello\") print(s2) # 중복이 제거되어 나오고, 순서도 뒤죽박죽으로 나옴 (순서가 없는 자료형이라) . {1, 2, 3} {'H', 'o', 'l', 'e'} . | ‘중복을 허용하지 않는다’ » 중복 제거 목적으로 종종 사용 l1 = [1, 2, 3, 4, 1, 5, 2, 3, 6, 9] s1 = set(l1) print(s1) # 중복되는 숫자가 다 제거됨 . {1, 2, 3, 4, 5, 6, 9} . | add(): 값 1개 추가하기 # add s1 = set([1, 2, 3]) s1.add(4) # list의 append()와 유사 s1 . {1, 2, 3, 4} . | update(): 값 여러 개 추가하기 # update s1 = set([1, 2, 3]) s1.update([4, 5, 6]) # list의 extend()와 유사 s1 . {1, 2, 3, 4, 5, 6} . | remove(): 값 1개 제거하기 s1 = set([1, 2, 3]) s1.remove(2) s1 . {1, 3} . | . 교집합, 합집합, 차집합 . | 교집합 구하기: &amp; 기호 or intersection() 함수 # 교집합 s1 = set([1, 2, 3, 4]) s2 = set([3, 4, 5, 6]) print(s1 &amp; s2) # \"&amp;\" 기호로 간단히 교집합을 구할 수 있다 print(s1.intersection(s2)) # intersection 함수를 써도 동일한 결과 . {3, 4} {3, 4} . | 합집합 구하기: | 기호 or union() 함수 # 합집합 s1 = set([1, 2, 3, 4]) s2 = set([3, 4, 5, 6]) print(s1 | s2) # \"|\" 기호로 간단히 합집합을 구할 수 있다 print(s1.union(s2)) # union 함수를 써도 동일한 결과 . {1, 2, 3, 4, 5, 6} {1, 2, 3, 4, 5, 6} . | 차집합 구하기: - 기호 or difference() 함수 # 차집합 s1 = set([1, 2, 3, 4]) s2 = set([3, 4, 5, 6]) # s1에 대한 s2의 차집합 print(s1 - s2) # \"-\" 기호 사용 print(s1.difference(s2)) # difference 함수 사용 # s2에 대한 s1의 차집합 print(s2 - s1) # \"-\" 기호 사용 print(s2.difference(s1)) # difference 함수 사용 . {1, 2} {1, 2} {5, 6} {5, 6} . | . ",
    "url": "https://chaelist.github.io/docs/python_basics/dictionary_tuple_set/#set-%EC%A7%91%ED%95%A9%EC%9E%90%EB%A3%8C%ED%98%95",
    "relUrl": "/docs/python_basics/dictionary_tuple_set/#set-집합자료형"
  },"48": {
    "doc": "빈도 분석 (English)",
    "title": "빈도 분석 (English)",
    "content": ". | nltk 준비 &amp; text 수집 | 전처리 (Preprocessing) . | Text Cleaning | Case Conversion | Tokenization | POS tagging | Lemmatization | Stopwords Removal | . | Frequency Analysis . | Counter | WordCloud | . | . ",
    "url": "https://chaelist.github.io/docs/text_analysis/english_text/",
    "relUrl": "/docs/text_analysis/english_text/"
  },"49": {
    "doc": "빈도 분석 (English)",
    "title": "nltk 준비 &amp; text 수집",
    "content": "*nltk: natural language tool kit (English text anlaysis에 필요한 python module) . | pip install nltk로 설치 | 처음 사용할 때는 아래와 같이 다운받아야 사용 가능 import nltk nltk.download('all') ## 한 컴퓨터에 한 번만 해두면 된다 . | . *Frequency Analysis에 사용할 text 수집해오기 . import requests from bs4 import BeautifulSoup url = 'https://www.nytimes.com/2017/06/12/well/live/having-friends-is-good-for-you.html' r = requests.get(url) soup = BeautifulSoup(r.text, 'lxml') title = soup.title.text.strip() text_list = soup.select('p.css-axufdj') content = '' for text in text_list: content = content +' '+ text.text.strip() print(title, '\\n') print(content) . Social Interaction Is Critical for Mental and Physical Health - The New York Times Hurray for the HotBlack Coffee cafe in Toronto for declining to offer Wi-Fi to its customers. There are other such cafes, to be sure, including seven of the eight New York City locations of Café Grumpy. But it’s HotBlack’s reason for the electronic blackout that is cause for hosannas. As its president, Jimson Bienenstock, explained, his aim is to get customers to talk with one another instead of being buried in their portable devices. “It’s about creating a social vibe,” he told a New York Times reporter. “We’re a vehicle for human interaction, otherwise it’s just a commodity.” What a novel idea! Perhaps Mr. Bienenstock instinctively knows what medical science has been increasingly demonstrating for decades: Social interaction is a critically important contributor to good health and longevity. Personally, I don’t need research-based evidence to appreciate... (생략) . ",
    "url": "https://chaelist.github.io/docs/text_analysis/english_text/#nltk-%EC%A4%80%EB%B9%84--text-%EC%88%98%EC%A7%91",
    "relUrl": "/docs/text_analysis/english_text/#nltk-준비--text-수집"
  },"50": {
    "doc": "빈도 분석 (English)",
    "title": "전처리 (Preprocessing)",
    "content": "  . *전처리 과정 (English): . | Text Cleaning: 불필요한 기호 / 표현 없애기(예, !, ., “, ; 등) | Case Conversion: 대소문자 변환. 소문자 ↔ 대문자 | Tokenization: 단어 (혹은 Token) 단위로 잘라주기 | POS tagging: 단어의 품사 찾기 | 원하는 품사의 단어들만 선택 | Lemmatization: (=Stemming) 단어의 원형(혹은 어근) 찾기 | Stopwords Removal: 불용어 제거 | . 필요에 따라서는 위 과정의 순서가 바뀔수도 있고, 같은 과정을 반복 수행할 수도 있다 . Text Cleaning . : 불필요한 symbol / mark 등 제거 . | .replace() 혹은 정규식 활용 | ※ 분석 목적에 따라, ‘어떠한 mark 나 symbol 들을 이 단계에서 제거할 것인가?’를 고민하는 것이 중요함 . | ex) 텍스트 분석이 문장 단위로 이루어지는 경우, 문장의 끝을 나타내는 기호들(마침표(.) 물음표(?) 느낌표(!) 등)을 없애지 않아야 한다 | . | . # 불필요한 기호 없애기 - 정규식 사용 import re filtered_content = re.sub('[^,.?!\\w\\s]','', content) ## ,.?!와 문자+숫자+_(\\w)와 공백(\\s)만 남김 filtered_content . Hurray for the HotBlack Coffee cafe in Toronto for declining to offer WiFi to its customers. There are other such cafes, to be sure, including seven of the eight New York City locations of Café Grumpy. But its HotBlacks reason for the electronic blackout that is cause for hosannas. As its president, Jimson Bienenstock, explained, his aim is to get customers to talk with one another instead of being buried in their portable devices. Its about creating a social vibe, he told a New York Times reporter. Were a vehicle for human interaction, otherwise its just a commodity. What a novel idea! Perhaps Mr. Bienenstock instinctively knows what medical science has been increasingly demonstrating for decades Social interaction is a critically important contributor to good health and longevity. Personally, I dont need researchbased evidence to appreciate the value of making and maintaining social connections. I experience it daily during my morning walk with up to three women, then... (생략) . +) 아래와 같이 replace를 사용해서 제거할 수도 있다 . filtered_content = content.replace('[','').replace(']','').replace('\"','') filtered_content = filtered_content.replace('“','').replace('”','').replace('’',\"\") # +) 아래와 같이 특정 text 특성상 불필요한 부분도 추가로 제거 가능 filtered_content = filtered_content.replace('The New York Times', '') . Case Conversion . | 영어의 경우, python에서는 같은 단어라도 대문자와 소문자를 다르게 인식하므로 통일해줘야 한다 | . filtered_content = filtered_content.lower() # .lower()를 사용해서 소문자로 변환 filtered_content . hurray for the hotblack coffee cafe in toronto for declining to offer wi-fi to its customers. there are other such cafes, to be sure, including seven of the eight new york city locations of café grumpy. but its hotblacks reason for the electronic blackout that is cause for hosannas. as its president, jimson bienenstock, explained, his aim is to get customers to talk with one another instead of being buried in their portable devices. its about creating a social vibe, he told a new york times reporter. were a vehicle for human interaction, otherwise its just a commodity. what a novel idea! perhaps mr. bienenstock instinctively knows what medical science has been increasingly demonstrating for decades: social interaction is a critically important contributor to good health and longevity. personally, i dont need research-based evidence to appreciate the value of making and maintaining social connections. i experience it daily during my morning walk with up to three women, then... (생략) . Tokenization . | Token: 뜻을 갖고 사용될 수 있는 가장 작은 글의 단위. 보통 하나의 단어(word)라고 생각하면 된다 | Tokenization:토큰(단어) 단위로 잘라주는 것 | . *두 가지 방법이 가능: . | nltk.word_tokenize() 함수 사용 | .split() 함수로 whitespace 기준으로 text split (영어의 경우 split()으로만 잘라줘도 단어 단위로 잘 잘림) | . # nltk.word_tokenize() 함수를 사용해서 tokenize import nltk # import해서 사용 word_tokens = nltk.word_tokenize(filtered_content) print(word_tokens) ## 단어의 list로 나옴 . ['hurray', 'for', 'the', 'hotblack', 'coffee', 'cafe', 'in', 'toronto', 'for', 'declining', 'to', 'offer', 'wi-fi', 'to', 'its', 'customers', '.', 'there', 'are', 'other', 'such', 'cafes', ',', 'to', 'be', 'sure', ',', 'including', 'seven', 'of', 'the', 'eight', 'new', 'york', 'city', 'locations', 'of', 'café', 'grumpy', '.', 'but', 'its', 'hotblacks', 'reason', 'for', 'the', 'electronic', 'blackout', 'that', 'is', 'cause', 'for', 'hosannas', '.', 'as', 'its', 'president', ',', 'jimson', 'bienenstock', ',', 'explained', ',', 'his', 'aim', 'is', 'to', 'get', 'customers', 'to', 'talk', 'with', 'one', 'another', 'instead', 'of', 'being', 'buried', 'in', 'their', 'portable', 'devices', '.', 'its', 'about', 'creating', 'a', 'social', 'vibe', ',', 'he', 'told', 'a', 'new', 'york', 'times', 'reporter', '.', 'were', 'a', 'vehicle', 'for', 'human', 'interaction', ',', 'otherwise', 'its', 'just', 'a', 'commodity', '.', 'what', 'a', 'novel', 'idea', (생략)] . POS tagging . : (=Parts-of-Speech tagging) 각 단어의 품사를 찾아서 태깅. (ex. ‘this’: pronoun(대명사), ‘class’: noun, ‘is’: be동사, ‘a’: article(관사), …) . | tag된 품사의 종류 해석하는 법: https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html | . # nltk.pos_tag() 함수를 사용해서 품사 태깅 tokens_pos = nltk.pos_tag(word_tokens) # pos_tag()의 입력값으로는 단어의 리스트가 들어가야 한다 print(tokens_pos) . [('hurray', 'NN'), ('for', 'IN'), ('the', 'DT'), ('hotblack', 'NN'), ('coffee', 'NN'), ('cafe', 'NN'), ('in', 'IN'), ('toronto', 'NN'), ('for', 'IN'), ('declining', 'VBG'), ('to', 'TO'), ('offer', 'VB'), ('wi-fi', 'JJ'), ('to', 'TO'), ('its', 'PRP$'), ('customers', 'NNS'), ('.', '.'), ('there', 'EX'), ('are', 'VBP'), ('other', 'JJ'), ('such', 'JJ'), ('cafes', 'NNS'), (',', ','), ('to', 'TO'), ('be', 'VB'), ('sure', 'JJ'), (',', ','), ('including', 'VBG'), ('seven', 'CD'), ('of', 'IN'), ('the', 'DT'), ('eight', 'CD'), ('new', 'JJ'), ('york', 'NN'), ('city', 'NN'), ('locations', 'NNS'), ('of', 'IN'), ('café', 'NN'), ('grumpy', 'NN'), ('.', '.'), ('but', 'CC'), ('its', 'PRP$'), ('hotblacks', 'NNS'), ('reason', 'NN'), ('for', 'IN'), ('the', 'DT'), ('electronic', 'JJ'), ('blackout', 'NN'), ('that', 'WDT'), ('is', 'VBZ'), ('cause', 'NN'), ('for', 'IN'), ('hosannas', 'NN'), ('.', '.'), ('as', 'IN'), ('its', 'PRP$'), ('president', 'NN'), (생략)] . +) 특정 PoS의 단어들만 추출하기 . | 보통, 주제/키워드를 분석하고자 할 때는 noun만 사용해서 분석 | 하지만 감정 분석에는 형용사/부사도 중요 → 목적에 따라 적절히 필요한 품사 선택! | . # 명사 단어만 추출하기 NN_words = [] for word, pos in tokens_pos: if 'NN' in pos: ## Noun 종류 4개는 모두 'NN'을 포함하고 있어서 (NN, NNS, NNP, NNPS) NN_words.append(word) print(NN_words) . ['hurray', 'hotblack', 'coffee', 'cafe', 'toronto', 'customers', 'cafes', 'york', 'city', 'locations', 'café', 'grumpy', 'hotblacks', 'reason', 'blackout', 'cause', 'hosannas', 'president', 'jimson', 'bienenstock', 'aim', 'customers', 'devices', 'vibe', 'york', 'times', 'vehicle', 'interaction', 'commodity', 'idea', 'bienenstock', 'science', 'decades', 'interaction', 'contributor', 'health', 'longevity', 'evidence', 'value', 'connections', 'experience', 'morning', 'walk', 'women', 'swim', (생략)] . Lemmatization . : lemma=원형. 단어를 원형으로 바꿔줌 (ex. is -&gt; be, ate -&gt; eat, methods -&gt; method) . | 동사는 동사원형으로, 명사는 singular(단수형)으로 변환 | . # nltk.WordNetLemmatizer() 사용 wlem = nltk.WordNetLemmatizer() lemmatized_words = [] for word in NN_words: new_word = wlem.lemmatize(word) lemmatized_words.append(new_word) print(lemmatized_words) . ['hurray', 'hotblack', 'coffee', 'cafe', 'toronto', 'customer', 'cafe', 'york', 'city', 'location', 'café', 'grumpy', 'hotblacks', 'reason', 'blackout', 'cause', 'hosanna', 'president', 'jimson', 'bienenstock', 'aim', 'customer', 'device', 'vibe', 'york', 'time', 'vehicle', 'interaction', 'commodity', 'idea', 'bienenstock', 'science', 'decade', (생략)] . Stopwords Removal . : 불용어 제거. articles(a, an, the,…), pronouns(this, that,…) 등 별 의미가 없는 단어들을 제거해준다. # 1차적으로 nltk에서 제공하는 불용어사전을 이용해서 불용어를 제거 from nltk.corpus import stopwords stopwords_list = stopwords.words('english') # nltk에서 제공하는 영어 불용어사전 unique_NN_words = set(lemmatized_words) ## 중복을 제거하기 위해 set(집합형)으로 변환 final_NN_words = lemmatized_words # 불용어 제거 for word in unique_NN_words: if word in stopwords_list: while word in final_NN_words: final_NN_words.remove(word) print(final_NN_words) . ['hurray', 'hotblack', 'coffee', 'cafe', 'toronto', 'customer', 'cafe', 'york', 'city', 'location', 'café', 'grumpy', 'hotblacks', 'reason', 'blackout', 'cause', 'hosanna', 'president', 'jimson', 'bienenstock', 'aim', 'customer', 'device', 'vibe', 'york', 'time', 'vehicle', 'interaction', 'commodity', 'idea', 'bienenstock', 'science', 'decade', 'interaction', 'contributor', (생략)] . +) 제거하고자 하는 단어가 nltk 제공 사전에 포함되어 있지 않다면, 아래와 같이 직접 불용어 사전을 만들어 추가로 단어를 제거해도 된다 . customized_stopwords = ['be', 'today', 'yesterday'] # 직접 만든 불용어 사전 unique_NN_words1 = set(final_NN_words) for word in unique_NN_words1: if word in customized_stopwords: while word in final_NN_words: final_NN_words.remove(word) . ",
    "url": "https://chaelist.github.io/docs/text_analysis/english_text/#%EC%A0%84%EC%B2%98%EB%A6%AC-preprocessing",
    "relUrl": "/docs/text_analysis/english_text/#전처리-preprocessing"
  },"51": {
    "doc": "빈도 분석 (English)",
    "title": "Frequency Analysis",
    "content": "Counter . : 단어 사용 빈도를 계산해주는 모듈. from collections import Counter # import해서 사용 c = Counter(final_NN_words) ## 단어 개수를 세어준다 print(c) . Counter({'health': 11, 'people': 11, 'researcher': 7, 'study': 6, 'tie': 6, 'interaction': 5, 'friend': 4, 'others': 4, 'exercise': 4, 'york': 3, 'time': 3, 'connection': 3, 'woman': 3, 'problem': 3, 'lifestyle': 3, 'smoking': 3, 'lack': 3, 'heart': 3, 'death': 3, 'connectedness': 3, 'disease': 3, 'loneliness': 3, 'research': 3, 'blood': 3, 'inflammation': 3, 'texas': 3, 'seppala': 3, 'cafe': 2, 'customer': 2, 'reason': 2, 'bienenstock': 2, 'device': 2, 'longevity': 2, 'evidence': 2, (생략)}) . → 가장 많이 나온 단어 10개 추출 . print(c.most_common(10)) . [('health', 11), ('people', 11), ('researcher', 7), ('study', 6), ('tie', 6), ('interaction', 5), ('friend', 4), ('others', 4), ('exercise', 4), ('york', 3)] . WordCloud . : 단어의 빈도를 크기로 표현해 워드클라우드를 만들어주는 모듈 . from wordcloud import WordCloud import matplotlib.pyplot as plt total_words = '' for word in final_NN_words: total_words = total_words+' '+word wordcloud = WordCloud(max_font_size=40, relative_scaling=.5).generate(total_words) plt.figure() plt.imshow(wordcloud) plt.axis(\"off\") plt.show() . | max_font_size: 가장 크기가 크게 나올 (=가장 빈도가 높은) 단어의 크기에 제한을 둠 | relative_scaling: 빈도 차이에 따른 scaling 정도를 지정. | 1일 경우, 2배로 빈도가 높으면 2배로 크게 그려짐. 0에 가까울수록 크기 차이가 덜 나게 그리는 것. | . | . +) colormap, 배경색 지정 . wordcloud = WordCloud(relative_scaling=.2, background_color='white', colormap='summer').generate(total_words) plt.imshow(wordcloud) plt.axis('off') plt.show() . +) 워드클라우드를 파일로 저장하기 . wordcloud.to_file('WordCloud.png') . ",
    "url": "https://chaelist.github.io/docs/text_analysis/english_text/#frequency-analysis",
    "relUrl": "/docs/text_analysis/english_text/#frequency-analysis"
  },"52": {
    "doc": "파일 & 폴더 다루기",
    "title": "파일 &amp; 폴더 다루기",
    "content": ". | 조회 &amp; 정보 탐색 . | 운영체제 &amp; 경로 | 파일/폴더 정보 탐색 | 파일/폴더 조회 | 폴더/파일 여부 판별 | . | 복사 &amp; 이동 &amp; 삭제 | 압축 파일 생성 . | shutil 활용 | zipfile 활용 | . | . *사용할 모듈: os, shutil, zipfile (모두 별도의 설치 없이 import해서 사용 가능) . import os import shutil import zipfile . ",
    "url": "https://chaelist.github.io/docs/data_handling/file_folder_handling/#%ED%8C%8C%EC%9D%BC--%ED%8F%B4%EB%8D%94-%EB%8B%A4%EB%A3%A8%EA%B8%B0",
    "relUrl": "/docs/data_handling/file_folder_handling/#파일--폴더-다루기"
  },"53": {
    "doc": "파일 & 폴더 다루기",
    "title": "조회 &amp; 정보 탐색",
    "content": "운영체제 &amp; 경로 . | 현재의 운영체제 확인 . os.name . | windows면 ‘nt’, macOS나 Linux라면 ‘posix’라고 출력됨 | . | working directory 확인 (현재 이 코드 파일이 존재하는 directory 위치) . print(os.getcwd()) # 아래와 같이 절대 경로로 표현되어 나온다 . C:\\Users\\admin\\Desktop\\python . | windows면 \\로, macOS면 /로 구분된다 | . | 특정 파일/폴더의 절대 경로 확인하기 . print(os.path.abspath('Python 기초')) . C:\\Users\\admin\\Desktop\\python\\Python 기초 . | 현재 운영체제에서 사용하는 경로 구분 기호 확인 . print(os.path.sep) . | windows면 \\, macOS면 /가 출력됨 | . | os.path.join을 사용한 경로 구분 . | os.path.join을 사용하면 내가 사용하는 운영체제에 알맞게 구분 기호를 붙여서 경로를 만들어줌 (운영체제별 경로 구분 구애받지 않고 사용하는 방법) | . path = os.path.join('User', 'Folder', 'image.jpg') print(path) . User\\Folder\\image.jpg . | 경로 검증하기 (실제 해당 경로에 해당 파일/폴더가 존재하는지 확인) . # 파일 / 폴더 모두 동일하게 검증 가능 print(os.path.exists('Python 기초')) print(os.path.exists('없는 파일.png')) . True False . | 경로를 다룰 때에는, if os.path.exists('path'):처럼 if문으로 경로의 존재 여부를 먼저 검증하고 그 다음 코드를 써주면 더 안전하다 | . | .   . +) 경로 관련 정보: . | windows의 경우, 단순히 ‘\\‘로만 경로를 구분하기보다, ‘\\\\’ 이렇게 두 번 써주는 게 좋다. – ex) ‘user\\nine’ 이런 경우에는 ‘\\n’ = 엔터로 인식되어버리기 때문. | 아무것도 쓰지 않거나 ‘.’이라고 쓰면 working directory를 의미 | ’..’이라고 쓰면 wd의 상위 디렉토리를 의미, ‘../..’라고 쓰면 한 번 더 거슬러 올라간 상위 디렉토리를 의미 | . 파일/폴더 정보 탐색 . | 용량 확인 . | 파일 용량: 컴퓨터의 저장장치(HDD, SSD)에 차지하는 공간의 양 | . os.path.getsize('파일/폴더명') . 4096 . | 이 때 출력되는 값은 byte 단위. | 1kB = 1000byte, 1MB = 1000kB, 1GB = 1000MB | . | 확장자 정보 분리 . | 일반적인 파일명의 경우 ‘.’은 잘 안쓰니까 split(‘.’)으로 나눠도 되지만, os.path.splittext를 사용하면 아래와 같은 특수 경우에도 안전하게 확장자 정보를 얻을 수 있음 | . file_ex = 'quarter.report.pdf' filename, extension = os.path.splitext(file_ex) print('파일명:', filename) print('확장자:', extension) . 파일명: quarter.report 확장자: .pdf . | 생성한 날짜 확인 . import datetime birthtimestamp = os.path.getctime('Python/test.txt') # 파일/폴더에 모두 적용 가능 birthtime = datetime.datetime.fromtimestamp(birthtimestamp) print(birthtime) . 2020-04-01 00:13:57.907304 . | os.path.getctime('path')의 결과는 timestamp 형태로 return되므로, datetime.datetime.fromtimestamp를 사용해 변환해줘야 한다 (*timestamp: 1970년 1월 1일부터 지난 시간을 초로 환산한 것) | os.stat('path').st_ctime을 사용해도 동일한 결과 | . | 마지막으로 수정한 날짜 확인 . mtimestamp = os.path.getmtime('Python/test.txt') mtime = datetime.datetime.fromtimestamp(mtimestamp) print(mtime) . 2021-04-01 14:12:23.278385 . | os.stat('path').st_atime을 사용해도 동일한 결과 | . | 마지막으로 access한 날짜 확인 . atimestamp = os.path.getatime('Python/test.txt') atime = datetime.datetime.fromtimestamp(atimestamp) print(atime) . 2021-04-01 14:09:57.907304 . | os.stat('path').st_mtime을 사용해도 동일한 결과 | . | . 파일/폴더 조회 . | 특정 directory 내 모든 element 조회 . os.listdir('.') # walking directory를 조회할 경우, os.listdir()이라고만 써도 됨 . ['Python 기초', '데이터 사이언스', '파일 자동화.ipynb'] . | os.listdir('path')를 활용하면 해당 경로 내의 모든 파일/폴더 정보가 list로 반환됨 | . | 특정 경로 내의 모든 폴더/파일 돌아보기 . | 경로에 들어 있는 모든 폴더와 파일을 조회해주고, 또 그 안의 폴더의 폴더와 파일을 조회해주고….. (더 이상 조회할 폴더가 없을 때까지 반복) | . for path, dirs, files in os.walk('datascience_intro'): print('Path:', path) print('Folders:', dirs) print('Files:', files) print('-----------') . Path: datascience_intro Folders: ['.ipynb_checkpoints', 'data'] Files: ['Exploratory Data Analysis.ipynb', 'Pandas 기초.ipynb', '데이터 정제.ipynb'] ----------- Path: datascience_intro\\.ipynb_checkpoints Folders: [] Files: ['데이터 정제-checkpoint.ipynb'] ----------- Path: datascience_intro\\data Folders: ['empty_folder'] Files: ['iris.csv', 'test_scores.csv', 'titanic.csv'] ----------- Path: datascience_intro\\data\\empty_folder Folders: [] Files: [] ----------- . | os.walk('path')는 path, folders, files 순서로 3개의 element를 반복적으로 return해줌 | . | . 폴더/파일 여부 판별 . # dir인지 판별 print(os.path.isdir('Python/test.txt')) print(os.path.isdir('Python')) print(os.path.isdir('.ipynb_checkpoints')) # 파일인지 판별 print(os.path.isfile('Python/test.txt')) print(os.path.isfile('파일 자동화.ipynb')) print(os.path.isfile('Python')) . False True True True True False . | ※ 주의: 이름이 틀린 경우(해당 경로가 존재하지 않는 경우)에도 에러가 나지는 않고, 그냥 False가 반환됨! | . ",
    "url": "https://chaelist.github.io/docs/data_handling/file_folder_handling/#%EC%A1%B0%ED%9A%8C--%EC%A0%95%EB%B3%B4-%ED%83%90%EC%83%89",
    "relUrl": "/docs/data_handling/file_folder_handling/#조회--정보-탐색"
  },"54": {
    "doc": "파일 & 폴더 다루기",
    "title": "복사 &amp; 이동 &amp; 삭제",
    "content": ". | os.rename: 이름 변경 / 경로 이동 . | 단순히 이름만 바꿔서 써주면 그대로 같은 dir 내에 이름만 바뀌어서 저장됨 os.rename('기존이름', '새이름') . | 경로 자체를 바꿔서 써주면 위치도 새 경로로 이동시킬 수 있음 os.rename('기존경로/기존이름.txt', '새경로/새이름.txt') . | . +) shutil.move: 파일 이동 (os.rename과 거의 동일하게 동작) . shutil.move('기존경로/기존이름.txt', '새경로/새이름.txt') . | os.rename과의 차이: shutil.move를 활용하면 다른 드라이브 / 다른 파일 시스템을 갖는 위치로도 이동 가능 (ex. C드라이브에서 D드라이브로 이동) | 같은 드라이브 내에서 이동하는 경우, os.rename만 사용해도 충분 | . | shutil.copy: 파일 복사 . shutil.copy('원본 파일 이름', '복제본 파일 이름') . | 물론 shutil 모듈 없이도, 그냥 open으로 복제하는 것도 가능: . | read 모드로 복제할 파일을 읽어와서 f.read()로 전체 내용을 저장 후, 다시 write 모드로 새 파일을 만들어서 f.write()으로 저장햐둔 내용을 적어주면 됨 | . | . ※ 주의: ‘복제본 파일 이름’과 같은 경로/이름의 파일이 이미 존재할 경우, 새 파일이 기존 파일을 덮어씌우게 된다 (if os.path.extist(‘path’)로 검증하고 복제하도록 코드를 적으면 더 안전) . | shutil.copytree: 폴더 복사 . shutil.copytree('기존폴더명', '새폴더명') . | os.remove: 파일 삭제 . os.remove('파일명.txt') . | 폴더 삭제 . | 빈 폴더 삭제 os.rmdir('폴더명') # os.rmdir를 사용하면 내용물이 없는 폴더만 삭제 가능하다 # 내용물이 있는 폴더를 os.rmdir로 삭제하려고 하면 error . | 내용물이 있는 폴더 삭제 shutil.rmtree('폴더명') . | . | 폴더 생성 . | os.makedirs(‘폴더명’): 여러 디렉토리를 한 번에 생성하는 것도 가능 os.makedirs(r'years/2021') # 현재 경로에 years라는 폴더조차 없는 상황 → 이 코드를 실행하면 years 디렉토리가 생기고 그 안에 2021 디렉토리도 함께 생김 . | os.mkdir(‘폴더명’): 하나의 디렉토리만 생성 가능 . os.mkdir(r'years/2021') # 현재 경로에 years라는 폴더가 없다면 이 코드는 error를 발생시킴 os.mkdir('2021') # 위와 같은 코드는 정상적으로 실행됨 (폴더 하나만 생성) . | . | . ",
    "url": "https://chaelist.github.io/docs/data_handling/file_folder_handling/#%EB%B3%B5%EC%82%AC--%EC%9D%B4%EB%8F%99--%EC%82%AD%EC%A0%9C",
    "relUrl": "/docs/data_handling/file_folder_handling/#복사--이동--삭제"
  },"55": {
    "doc": "파일 & 폴더 다루기",
    "title": "압축 파일 생성",
    "content": "shutil 활용 . | 하나의 폴더 내 내용물을 한번에 압축 . shutil.make_archive('archive', 'zip', 'folder') # 두 번째 인자로는 압축 방식을 넣어주면 됨 # folder라는 폴더의 모든 내용물이 archive.zip이라는 압축파일로 저장됨 . | . | 압축 해제 . shutil.unpack_archive('archive.zip', 'unpacked_folder') # archive.zip 압축파일의 내용물이 unpacked_folder라는 폴더로 압축해제가 되어 저장됨 . | . zipfile 활용 . | 각각의 파일을 선별해서 압축 . with zipfile.ZipFile('archive.zip', 'w', compression=zipfile.ZIP_DEFLATED) as z: z.write('압축할 파일 1') z.write('압축할 파일 2') z.write('압축할 파일 3') # 파일 1~3이 archive.zip이라는 압축파일로 저장됨 . | compression=zipfile.ZIP_DEFLATED을 안쓰면 파일 1~3이 zip 파일로 합쳐지긴 하지만, 용량이 줄지는 않음 | . | 압축 해제 . with zipfile.ZipFile('archive.zip', 'r') as z: z.extractall('unpacked_folder') # archive.zip 압축파일의 내용물이 unpacked_folder라는 폴더로 압축해제가 되어 저장됨 . | . ",
    "url": "https://chaelist.github.io/docs/data_handling/file_folder_handling/#%EC%95%95%EC%B6%95-%ED%8C%8C%EC%9D%BC-%EC%83%9D%EC%84%B1",
    "relUrl": "/docs/data_handling/file_folder_handling/#압축-파일-생성"
  },"56": {
    "doc": "파일 & 폴더 다루기",
    "title": "파일 & 폴더 다루기",
    "content": " ",
    "url": "https://chaelist.github.io/docs/data_handling/file_folder_handling/",
    "relUrl": "/docs/data_handling/file_folder_handling/"
  },"57": {
    "doc": "파일 읽고 쓰기",
    "title": "파일 읽고 쓰기",
    "content": ". | Accessing a file | Reading a file . | f.read() | f.readlines() | f.readline() | 활용 Tip | . | Writing a file . | 내용 append하기 | . | r+, w+, a+ . | r+ 모드 | w+ 모드 | a+ 모드 | . | . ",
    "url": "https://chaelist.github.io/docs/data_handling/file_input_output/",
    "relUrl": "/docs/data_handling/file_input_output/"
  },"58": {
    "doc": "파일 읽고 쓰기",
    "title": "Accessing a file",
    "content": ". | 우선 open('file_name', 'mode')로 파일에 access | ‘mode’의 종류: ‘r’: read, ‘w’: write, ‘a’: append | . # 파일이 같은 폴더에 있을 경우 f = open('example.txt', 'r') ## 이렇게 파일명만 적어주면 됨 # 파일이 다른 폴더에 있을 경우 f = open('C:/Users/admin/Desktop/chaelist_blog/example.txt', 'r') ## 절대 경로로 접근 # 파일이 현재 python 파일을 실행시킨 폴더 내 다른 폴더에 있을 경우 f = open('data/example.txt', 'r') ## data라는 폴더 내에 파일이 있는 경우 # 파일이 현재 python 파일을 실행시킨 폴더 밖의 폴더에 있을 경우 f = open('../example.txt', 'r') ## ../를 통해 traverse up the directory . ",
    "url": "https://chaelist.github.io/docs/data_handling/file_input_output/#accessing-a-file",
    "relUrl": "/docs/data_handling/file_input_output/#accessing-a-file"
  },"59": {
    "doc": "파일 읽고 쓰기",
    "title": "Reading a file",
    "content": "f.read() . : 파일 전체를 하나의 string value로 읽어준다 . f = open('example.txt', 'r') # 'r'은 default mode이기 때문에 안써줘도 결과는 동일 f.read() # 아래처럼, 하나의 string value로 읽어옴 . '1\\n2\\n3\\n4\\n5' . f.close() # 특정 파일을 open한 다음 read / write하고 나면 닫아줘야 한다 . f.readlines() . : read the entire file, as a list type (각 줄을 각각의 list variable로 받아서, 하나의 list로 반환) . | 가장 흔히 사용하는 file reading method. | . f = open('example.txt', 'r') f.readlines() # 각 줄을 element로 하는 list를 반환 . ['1\\n', '2\\n', '3\\n', '4\\n', '5'] . f.close() . f.readline() . : read the file line by line . | 한줄씩 차례대로 readline해줘야 해서, 잘 안 쓰는 method. | . f = open('example.txt', 'r') f.readline() # 이런 식으로 한줄씩 읽힌다 . '1\\n' . f.readline() # 이런 식으로 한줄씩 읽힌다 . '2\\n' . f.close() . 활용 Tip . | 보통, readlines()에 for문을 결합해서 이용한다. f = open('example.txt', 'r') for line in f.readlines(): print(line.strip()) # 보통 이렇게 for문을 사용해서 한 줄 한 줄 접근 f.close() . 1 2 3 4 5 . | readlines()를 쓰지 않아도, for문을 사용하면 한 줄 한 줄 접근이 가능하다 . f = open('example.txt', 'r') for line in f: print(line.strip()) f.close() . 1 2 3 4 5 . | txt 파일에서 읽어온 값들은 일단 다 string 형태로 저장된다. f = open('example.txt', 'r') lines_list = f.readlines() # 이렇게 저장해두면 불러오기 편하다 f.close() line = lines_list[0].strip() print(line, type(line)) # type을 출력해보면, txt 파일에서 읽어온 value들은 다 일단 string 형태. num_line = int(line) # '' 사이의 값이 숫자일 경우, 이렇게 int()를 적용해 숫자로 바꿔줄 수 있다 print(num_line, type(num_line)) # 이제 type이 integer로 바뀌어서, 사칙연산 가능 . 1 &lt;class 'str'&gt; 1 &lt;class 'int'&gt; . | file open &amp; close를 한 줄로 해결하는 방법: . with open('파일명.txt', 'r', encoding='utf-8') as file: file.read() . | 이렇게 써주면 open()과 close()를 모두 써준 것과 동일 (with 아래의 코드가 모두 실행되고 나면 알아서 파일이 닫힌다) | . | 커서 옮기기 (※ 파일을 읽고 쓰는 과정에서 일종의 ‘커서’가 계속 줄을 따라 이동하게 되기 때문에, 커서 위치를 유의해야 할 때가 있음) . | f.readline()으로 한 줄을 이미 읽어준 후라서, 커서의 위치는 두번째 줄로 이동함. 그 상태에서 f.read()로 읽어주면 커서 위치부터 가장 끝까지의 내용이 출력됨. f = open('sample.txt') print(f.readline()) print('---------') print(f.read()) . 1 --------- 2 3 4 5 . | seek()을 활용하면 커서 위치를 옮길 수 있음 (seek(0): 파일의 가장 처음으로 커서를 옮겨줌) . f = open('sample.txt') print(f.readline()) print('---------') f.seek(0) print(f.read()) . 1 --------- 1 2 3 4 5 . | . | . ",
    "url": "https://chaelist.github.io/docs/data_handling/file_input_output/#reading-a-file",
    "relUrl": "/docs/data_handling/file_input_output/#reading-a-file"
  },"60": {
    "doc": "파일 읽고 쓰기",
    "title": "Writing a file",
    "content": ". | f.write('내용')의 형태로 작성 | 다 쓴 file은 close해주어야 제대로 정보가 저장된다 | . # 예시 f1 = open('name.txt', 'w') ## 해당 python 파일이 실행된 동일한 폴더에 저장됨 f1.write('Chung &amp; Cho') # 쓰고 싶은 내용을 작성 f1.close() . | write하는 내용은 무조건 string value여야 한다! . f2 = open('numbers.txt', 'w') num = 3 f2.write(num) # 이렇게 int 형태의 데이터를 write하려고 하면 error f2.close . --------------------------------------------------------------------------- TypeError Traceback (most recent call last) &lt;ipython-input-15-7b27a5c24a65&gt; in &lt;module&gt;() 1 f2 = open('numbers.txt', 'w') 2 num = 3 ----&gt; 3 f2.write(num) 4 f2.close TypeError: write() argument must be str, not int . cf) int 형태의 데이터는 str(num)으로 string으로 바꿔주면 write 가능 . f2 = open('numbers.txt', 'w') num = 3 f2.write(str(num)) f2.close() . | 한글 처리 방법: encoding='utf-8' f3 = open('한글.txt', 'w', encoding='utf-8') # encoding 방법을 지정 가능 f3.write('김태희') f3.close() . | 여러 내용 이어서 write f_test = open('test.txt', 'w') f_test.write('test1'+'\\n') f_test.write('test2') # 이런식으로 이어서 f.write()를 또 해주면 계속 이어서 써짐 ## f.close()를 해주기 전까지는 계속해서 한 줄씩 추가 가능 f_test.close() . | . 내용 append하기 . (이미 만든 파일에 내용 추가로 더 쓰기) . f_test = open('test.txt', 'a') # 'a': append f_test.write('\\n'+'test3') # 'a'로 open한다음에 write 해주면 이미 존재하는 파일에 이어서 쓸 수 있음. f_test.close() . ※ open('이미 있는 파일명', 'w') 이렇게 이미 있는 파일을 ‘w’(write)로 열면 그 위에 overwrite하게 되기 때문에, 이어서 계속 쓰고 싶다면 ‘a’(append)로 열어줘야 한다 . ",
    "url": "https://chaelist.github.io/docs/data_handling/file_input_output/#writing-a-file",
    "relUrl": "/docs/data_handling/file_input_output/#writing-a-file"
  },"61": {
    "doc": "파일 읽고 쓰기",
    "title": "r+, w+, a+",
    "content": ". | open() 함수에는 +모드들도 존재. 읽기 &amp; 쓰기 기능을 한번에 처리하고 싶은 경우에 유용하다 | 총 6개의 모드: r, w, a, r+, w+, a+ | . r+ 모드 . | r 모드로 열면 reading만 가능하지만, r+ 모드로 열면 reading과 writing이 모두 가능하다 . | ‘r’ 모드로 열고 ‘write’ 함수를 사용하면 ‘UnsupportedOperation: not writable’이라고 error가 남 | . | r+ 모드로 열고 ‘write’하는 경우, 새로 적는 내용은 커서 위치부터 작성된다. 파일을 연 직후에는 커서 위치가 0 (가장 앞)이므로, 그 상태에서 write하는 경우 파일의 맨 처음부터 새로운 내용이 덮어씌워진다 . | ※ r+ 모드로 열어서 먼저 ‘read’를 한 번 하고 나서 ‘write’를 하는 경우, 커서가 가장 뒤에 가 있으므로 뒤에 이어서 새로운 내용을 추가할 수 있다 | . | . # 샘플 파일 준비 with open('sample.txt', 'r') as f: print(f.read()) . first line second line . with open('sample.txt', 'r+') as f: ## 새로운 내용이 파일의 가장 앞부분부터 덮어씌워짐 f.write('write new text') f.seek(0) # 처음부터 읽어주기 위해서는 커서 위치를 0으로 바꿔줘야 함 print(f.read()) . write new textcond line . w+ 모드 . | w+ 모드로 열면 기존 내용을 지우고 새롭게 write하는 것 외에도, read 기능들도 사용 가능하다 (ex. 내용을 적고 바로 확인하는 것이 가능) | . # 샘플 파일 준비 with open('sample.txt', 'r') as f: print(f.read()) . first line second line . with open('sample.txt', 'w+') as f: # 기존 내용은 지워지고, 새로운 내용이 적힌다 f.write('write new text') f.seek(0) print(f.read()) . write new text . +) ‘w+’ 모드로 여는 순간 file은 백지화된다. w+ 모드로 열자마자 f.read()으로 읽어들이는 것은 의미가 없음 . with open('sample.txt', 'w+') as f: print(f.read()) # 아무것도 출력되지 않음 . a+ 모드 . | a+ 모드로 열면 기존 내용에 새롭게 내용을 덧붙여 write하는 것 외에도, read 기능들도 사용 가능하다 | . # 샘플 파일 준비 with open('sample.txt', 'r') as f: print(f.read()) . first line second line . with open('sample.txt', 'a+') as f: # 파일의 기존 내용은 유지하고, 맨 뒤에 새로운 내용이 추가된다 f.write('\\nwrite new text') # write한 후의 text 내용 출력 f.seek(0) print(f.read()) . first line second line write new text . +) ‘a+’ 모드로 열면 커서 위치가 파일 맨 끝부터 시작한다. 그러므로 a+모드로 열자마자 read해주려면 seek(0)으로 커서 위치를 조정해줘야 한다 . with open('sample.txt', 'a+') as f: print(f.read()) # 아무것도 출력되지 않음 # seek(0) 하고 f.read()를 해줘야 전체 내용을 출력 가능 . ",
    "url": "https://chaelist.github.io/docs/data_handling/file_input_output/#r-w-a",
    "relUrl": "/docs/data_handling/file_input_output/#r-w-a"
  },"62": {
    "doc": "Foreign Key & Python 연결",
    "title": "Foreign Key &amp; Python 연결",
    "content": ". | Foreign Key . | 참조 무결성(Referential Integrity) | Foreign Key 설정하기 | Foreign Key 삭제하기 | . | Foreign Key 정책 . | ON DELETE | ON UPDATE | . | Python에서 DB 연결 . | PyMySQL | . | . ",
    "url": "https://chaelist.github.io/docs/sql/foreign_key_pymysql/#foreign-key--python-%EC%97%B0%EA%B2%B0",
    "relUrl": "/docs/sql/foreign_key_pymysql/#foreign-key--python-연결"
  },"63": {
    "doc": "Foreign Key & Python 연결",
    "title": "Foreign Key",
    "content": ": 두 개의 테이블을 연결해주는 다리 역할을 하는 key. | Foreign Key가 있는 테이블: child table(자식 테이블) 혹은 referencing table이라고 지칭 | Foreign Key에 의해 참조당하는 테이블: parent table(부모 테이블) 혹은 referenced table이라고 지칭 | . (출처: SQLShack) . 참조 무결성(Referential Integrity) . : 두 테이블 간에 참조 관계가 있을 때 각 데이터 간에 유지되어야 하는 정확성과 일관성을 의미 . | DBMS 상에서 두 테이블 간의 Foreign Key 관계를 설정해두면 참조 무결성을 지킬 수 있다 | ex) item_id = 10인 리뷰들이 있는데 정작 item 테이블에는 id = 10인 상품이 없다면 참조 무결성이 깨진 것! | .   . *논리적 Foreign Key와 물리적 Foreign Key . | 실무에서는 논리적으로 두 테이블 간에 Foreign Key 관계가 있어도 DBMS 상에서 설정해두지 않는 경우도 있다 (ex. 성능 / Legacy data 때문…) | 논리적(Logical) Foreign Key: 논리적으로 성립하는 Foreign Key | 물리적(Physical) Foreign Key: DBMS 상에서 실제로 Foreign Key로 설정해서 참조 무결성을 보장할 수 있게 된 것 | . Foreign Key 설정하기 . | SQL문으로 설정: . | 형식: (Foreign Key도 일종의 Constraint이므로, ADD CONSTRAINT문을 사용) ALTER TABLE 테이블명 ADD CONSTRAINT 제약이름 FOREIGN KEY (foreign_key로_설정할_컬럼) REFERENCES 참조하는_테이블명 (참조할_컬럼) ON DELETE 정책 ON UPDATE 정책; . | 예시: review 테이블의 course_id 컬럼이 foreign key, 참조되는 컬럼은 course 테이블의 id 컬럼. ALTER TABLE `course_rating`.`review` ADD CONSTRAINT `fk_review_table` FOREIGN KEY (`course_id`) REFERENCES `course_rating`.`course` (`id`) ON DELETE RESTRICT ON UPDATE RESTRICT; . | 위와 같이 Foreign Key를 설정해두면 참조 무결성이 보장되도록 제약이 걸린다. ex) course 테이블에 id = 10인 row가 없는 상황에서, review 테이블에 course_id = 10인 row를 삽입하려고 하면 error 발생. | .   . | workbench를 통해 직접 클릭해서 설정하는 것도 가능: | .   . +) 테이블에 더해진 Constraint 확인하기 . | SHOW CREATE TABLE 테이블명;: 특정 테이블이 어떻게 생성되었는지를 알려주는 SQL문 (특정 테이블을 다시 생성한다고 하면 어떤 CREATE TABLE문을 작성해야 하는지 알려준다) | 이를 활용하면 테이블에 더해져 있는 Constraint (Foreign Key 포함) 종류를 파악할 수 있다 | . SHOW CREATE TABLE review; . → 아래와 같이 결과가 나오고, 그 내용을 복사해서 다른 곳에 붙여넣으면 전체 SQL문을 확인할 수 있다: . Foreign Key 삭제하기 . | ALTER TABLE 테이블명 DROP FOREIGN KEY 제약이름;의 구조로 작성 | cf) MySQL말고 다른 DB에서는 DROP CONSTRAINT를 사용하기도 함 | . -- review 테이블에 있는 'fk_review_table'이라는 Foreign Key 조건을 삭제 ALTER TABLE review DROP FOREIGN KEY fk_review_table; . ",
    "url": "https://chaelist.github.io/docs/sql/foreign_key_pymysql/#foreign-key",
    "relUrl": "/docs/sql/foreign_key_pymysql/#foreign-key"
  },"64": {
    "doc": "Foreign Key & Python 연결",
    "title": "Foreign Key 정책",
    "content": "ON DELETE . : 부모 테이블의 row가 삭제되는 경우에 대한 정책 . | RESTRICT 정책 . | 자식 테이블의 row에 의해 참조되고 있는 부모 테이블의 row는 삭제 불가능 | ex) course 테이블이 review 테이블에 의해 참조되고 있고, review 테이블에 course_id = 5인 row들이 존재할 때, course 테이블에서 id = 5인 row를 삭제하려고 하면 error 발생 DELETE FROM course WHERE id = 5; | ※ 해당 row를 참조하고 있는 자식 테이블의 row들을 모두 삭제한 후에야 삭제가 가능하다! | +) MySQL에서 ‘No Action’이라고 설정하는 것도 RESTRICT와 동일한 정책 | . | CASCADE 정책 . | 부모 테이블의 row를 삭제하면, 그 row를 참조하고 있던 자식 테이블의 row도 모두 함께 삭제됨 | ex) course 테이블이 review 테이블에 의해 참조되고 있고, review 테이블에 course_id = 5인 row들이 존재할 때, course 테이블에서 id = 5인 row를 삭제하면 1) 정상적으로 삭제되고, 2) review 테이블에서 course_id = 5인 row들도 함께 삭제된다 | . | SET NULL 정책 . | 부모 테이블의 row를 삭제하면, 그 row를 참조하고 있던 자식 테이블의 row의 foreign_key 자리가 모두 NULL값으로 대체됨 | ex) course 테이블이 review 테이블에 의해 참조되고 있고, review 테이블에 course_id = 5인 row들이 존재할 때, course 테이블에서 id = 5인 row를 삭제하면 1) 정상적으로 삭제되고, 2) review 테이블에서 course_id = 5인 row들은 course_id가 NULL값으로 바뀐다 | . | . ON UPDATE . : 부모 테이블의 row에서 참조되는 컬럼이 업데이트되는 경우에 대한 정책 . | RESTRICT 정책 . | 부모 테이블의 row에서, 자식테이블의 row에 의해 참조되고 있는 컬럼은 업데이트 불가능 | ex) course 테이블이 review 테이블에 의해 참조되고 있고, review 테이블에 course_id = 1인 row들이 존재할 때, course 테이블에서 id = 1인 row의 id를 100으로 업데이트하려고 하면 error 발생 UPDATE course SET id = 100 WHERE id = 1; | . | CASCADE 정책 . | 부모 테이블의 row에서, 자식테이블의 row에 의해 참조되고 있는 컬럼을 업데이트하면 이를 참조하던 자식 테이블 row의 foreign_key도 함께 업데이트됨 | ex) course 테이블이 review 테이블에 의해 참조되고 있고, review 테이블에 course_id = 1인 row들이 존재할 때, course 테이블에서 id = 1인 row의 id를 100으로 업데이트하려고 하면 1) 정상적으로 업데이트되고, 2) review 테이블에서 course_id = 1인 row들도 다 course_id = 100으로 변경된다 | . | SET NULL 정책 . | 부모 테이블의 row에서, 자식테이블의 row에 의해 참조되고 있는 컬럼을 업데이트하면 이를 참조하던 자식 테이블 row의 foreign_key는 모두 NULL값으로 대체됨 | ex) course 테이블이 review 테이블에 의해 참조되고 있고, review 테이블에 course_id = 1인 row들이 존재할 때, course 테이블에서 id = 1인 row의 id를 100으로 업데이트하려고 하면 1) 정상적으로 업데이트되고, 2) review 테이블에서 course_id = 1인 row들도 다 course_id가 NULL값으로 바뀐다 | . | .   . +) ON DELETE와 ON UPDATE를 서로 다른 정책으로 설정하는 것도 가능! . | ex) ON DELETE RESTRICT &amp; ON UPDATE CASCADE | . ",
    "url": "https://chaelist.github.io/docs/sql/foreign_key_pymysql/#foreign-key-%EC%A0%95%EC%B1%85",
    "relUrl": "/docs/sql/foreign_key_pymysql/#foreign-key-정책"
  },"65": {
    "doc": "Foreign Key & Python 연결",
    "title": "Python에서 DB 연결",
    "content": "PyMySQL . : python에서 MySQL 데이터베이스에 연결할 수 있게 해주는 library. | pip install pymsql로 설치해서 사용 | . | 데이터베이스와 연결하기 import pymysql # import해서 사용 conn = pymysql.connect(host='localhost', user='root', password='password', db='database') curs = conn.cursor(pymysql.cursors.DictCursor) . | 테이블 생성하기 # curs.execute() 안에 원하는 SQL문을 써주면 된다 curs.execute('''CREATE TABLE IF NOT EXISTS `users` ( id INT NOT NULL AUTO_INCREMENT, name VARCHAR(50) NOT NULL, follower_count INT NULL PRIMARY KEY (id));''') conn.commit() # commit을 해야 DB에 반영됨 . | 데이터 INSERT # 아래와 같이 f string을 써서 SQL문을 구성하는 것도 가능 curs.execute(f\"\"\"INSERT INTO users (name, follower_count) VALUES ('{artist_name}', {artist_follower_count});\"\"\") conn.commit() . | 그 외, 데이터 삭제/업데이트 등도 그냥 cusr.execute(\"SQL문\")으로 적어준 후 conn.commit()하면 DB에 반영된다 | . | 데이터 SELECT . | curs.fetchall()을 사용하면 SELECT문으로 가져온 데이터 전체를 한 번에 읽어올 수 있다 . curs.execute('SELECT * FROM users WHERE follower_count &gt; 10') curs.fetchall() . | curs.fetchone()을 사용하면 SELECT문으로 가져온 데이터를 한 줄 한 줄 읽어오게 된다 (*주로 한 줄만 읽어오면 될 때 사용) . curs.execute('SELECT * FROM users ORDER BY follower_count DESC LIMIT 1') curs.fetchone() . | . | 데이터베이스 연결 닫기 # 원하는 작업을 모두 수행한 후에 close로 닫아주면 된다 conn.close . | . ",
    "url": "https://chaelist.github.io/docs/sql/foreign_key_pymysql/#python%EC%97%90%EC%84%9C-db-%EC%97%B0%EA%B2%B0",
    "relUrl": "/docs/sql/foreign_key_pymysql/#python에서-db-연결"
  },"66": {
    "doc": "Foreign Key & Python 연결",
    "title": "Foreign Key & Python 연결",
    "content": " ",
    "url": "https://chaelist.github.io/docs/sql/foreign_key_pymysql/",
    "relUrl": "/docs/sql/foreign_key_pymysql/"
  },"67": {
    "doc": "Function & Module",
    "title": "Function &amp; Module",
    "content": ". | Function (함수) . | 함수 만들기 | global 변수 설정 | lambda: 함수의 간결한 버전 | lambda와 map(), filter(), reduce() | . | Module . | Module 중 일부 함수만 import | Module 이름을 다르게 import | . | . ",
    "url": "https://chaelist.github.io/docs/python_basics/function_module/#function--module",
    "relUrl": "/docs/python_basics/function_module/#function--module"
  },"68": {
    "doc": "Function & Module",
    "title": "Function (함수)",
    "content": ": 해당 함수에 저장된 특정 task를 수행해서 그 결과를 반환 . | built-in functions: ex) print(), min() 등… | user-created functions: create when you want to repeat a task (여러 줄의 코드를 반복하고 싶을 때 하나의 function으로 저장해두고 사용하면 효율적) | . 함수 만들기 . : def를 사용해 함수 생성 . *형식: . def 함수명(매개변수): 수행할 task . *예시: . # 특정 문장의 단어 수를 구하는 함수. (공백 기준으로 나누어서 count) def count_words(s): ## 함수의 특성을 잘 드러내는 이름을 설정해주면 좋다 words = s.split() return len(words) z = 'today is a good day.' count_words(z) # 위에서 만든 함수를 사용 . 5 . global 변수 설정 . | 일반적으로, 함수 안의 변수는 함수 안에서만 효력을 갖는다 . a = 1 def add_one(a): a = a + 1 add_one(a) print(a) # 함수 안에서 a를 변경시켜도 global한 a는 변함이 없으므로 1이 출력됨 . 1 . cf) return을 통해 변경된 a를 내보내면 밖에서 접근/출력 가능 . a = 1 def add_one(a): a = a + 1 return a a = add_one(a) # 이렇게 해야 a가 2로 바뀐다. +1이 반영된 결과를 내보내서, a에 값으로 넣어주는 것. print(a) # 2가 출력됨 . 2 . | global 명령어를 사용하면 함수 밖 변수에 효력을 미칠 수 있다 . a = 1 def add_one(): global a # 함수 밖의 a (=global한 a) a = a + 1 add_one() # global한 a에 1을 더한 것이기에, 실제 함수 밖의 a가 2로 변함 print(a) # 2가 출력됨 . 2 . | . ※ 하지만 global 명령어는 가급적 사용하지 않는 것이 좋다. (함수는 독립적으로 존재하는 것이 좋기 때문)      return을 사용해서 값을 밖에서 받을 방법을 만들어 주는 방법이 가장 좋음! . lambda: 함수의 간결한 버전 . : 보통 함수를 한 줄로 간결하게 표현하고 싶을 때 def 대신 lambda를 사용한다 . | def를 사용할 정도로 복잡하지 않거나, def를 사용할 수 없을 때 주로 사용 | . *형식: . lambda 매개변수: 표현식 . *예시: . sq = lambda a: a**2 print(sq(2)) # 2**2의 결과인 4가 출력됨 . 4 . +) 매개변수를 2개 이상 넣어도 된다 . add = lambda a, b: a + b print(add(1, 2)) # 1 + 2의 결과인 3이 출력됨 . 3 . lambda와 map(), filter(), reduce() . (lambda의 가치를 극대화해주는 함수들) . | map(함수, 리스트): 리스트에서 원소를 하나씩 꺼내 함수를 적용하고, 그 결과를 새로운 리스트로 반환 # assign된 리스트의 각 원소의 제곱을 원소들로 하는 새로운 리스트를 반환 list(map(lambda x: x**2, [0, 1, 2, 3, 4])) . [0, 1, 4, 9, 16] . | filter(함수, 리스트): 리스트의 각 원소들에 함수를 적용해, 결과가 True인 값들만 넣은 새로운 리스트를 반환 # assign된 리스트의 각 원소 중 2로 나누어 떨어지는 값, 즉 짝수만을 원소로 하는 새로운 리스트를 생성 list(filter(lambda x: x % 2 == 0, [0, 1, 2, 3, 4])) . [0, 2, 4] . ## 다른 예시: 3보다 작은 수만 가지고 리스트 생성 list(filter(lambda x: x &lt; 3, [0, 1, 2, 3, 4])) . [0, 1, 2] . | reduce(함수, 순서형 자료): 순서형 자료(list/string/tuple)의 원소들을 함수에 누적 적용해, 최종 결과를 반환 . | map(), filter()와 달리, 기본 내장 함수가 아니라서 import해주어야 사용 가능. | . from functools import reduce ## reduce는 내장 함수가 아니라 import해야 함 reduce(lambda x, y: x + y, [0, 1, 2, 3, 4] # 1) 0+1의 결과인 1이 x가 되고, 다음 원소인 2가 y가 되어 1+2=3 # 2) 1+2의 결과인 3이 x가 되고, 다음 원소인 3이 y가 되어 3+3=6 # 3) 3+3의 결과인 6이 x가 되고, 다음 원소인 4가 y가 되어 6+4=10 ## 결국, &lt;모든 원소를 더한 결과&gt;인 10이 반환됨. 10 . | . ",
    "url": "https://chaelist.github.io/docs/python_basics/function_module/#function-%ED%95%A8%EC%88%98",
    "relUrl": "/docs/python_basics/function_module/#function-함수"
  },"69": {
    "doc": "Function & Module",
    "title": "Module",
    "content": ": a file containing codes . | 특정 module은 특정 목적에 맞는 function, class 등을 담고 있다 . | ex) ‘math’ 모듈은 math에 필요한 기능들을, ‘matplotlib’ 모듈은 visualization에 필요한 기능들을 포함 | . | import를 통해 모듈을 가져와서 사용한다 | 이미 존재하는 모듈을 사용하는 것 외에, 직접 모듈을 만들어서 사용할 수도 있다 . | .py 파일에 원하는 코드들을 저장해두고, 다른 .py / .ipynb 파일에서 import해주면 된다 | ※ import 하려는 모듈이 같은 디렉토리에 있어야 한다 | ※ import filename.py와 같이 파일명을 그대로 쓰는 것이 아니라, import filename이라고만 써야 한다 | . | . import math # 'import'를 통해 모듈을 가져옴 dir(math) ## dir()를 통해 모듈에 포함된 함수 등 목록을 확인 가능 . ['__doc__', '__loader__', '__name__', '__package__', '__spec__', 'acos', 'acosh', 'asin', 'asinh', 'atan', 'atan2', 'atanh', 'ceil', 'copysign', 'cos', 'cosh', 'degrees', 'e', 'erf', 'erfc', 'exp', 'expm1', 'fabs', 'factorial', 'floor', 'fmod', 'frexp', 'fsum', 'gamma', 'gcd', 'hypot', 'inf', 'isclose', 'isfinite', 'isinf', 'isnan', 'ldexp', 'lgamma', 'log', 'log10', 'log1p', 'log2', 'modf', 'nan', 'pi', 'pow', 'radians', 'sin', 'sinh', 'sqrt', 'tan', 'tanh', 'tau', 'trunc'] . Module 중 일부 함수만 import . from math import sqrt math.sqrt(4) # sqrt: square route. 루트 씌운 값 출력 . 2.0 . +) 여러 개를 가져오고 싶으면 ,로 구분 . from math import sqrt, pow, trunc . Module 이름을 다르게 import . import math as ma ma.sqrt(4) . 2.0 . | import pandas as pd, import numpy as np 등 관행적으로 줄여서 사용하는 이름이 존재하는 모듈도 많음 | . ",
    "url": "https://chaelist.github.io/docs/python_basics/function_module/#module",
    "relUrl": "/docs/python_basics/function_module/#module"
  },"70": {
    "doc": "Function & Module",
    "title": "Function & Module",
    "content": " ",
    "url": "https://chaelist.github.io/docs/python_basics/function_module/",
    "relUrl": "/docs/python_basics/function_module/"
  },"71": {
    "doc": "Image 수집 & API 활용",
    "title": "Image 수집 &amp; API 활용",
    "content": ". | Image 수집 | API로 데이터 수집 . | KOFIC(영화진흥위원회) API | . | . ",
    "url": "https://chaelist.github.io/docs/webscraping/image_api/#image-%EC%88%98%EC%A7%91--api-%ED%99%9C%EC%9A%A9",
    "relUrl": "/docs/webscraping/image_api/#image-수집--api-활용"
  },"72": {
    "doc": "Image 수집 & API 활용",
    "title": "Image 수집",
    "content": "(downloading images from the web) . import requests from bs4 import BeautifulSoup import shutil # 파일/폴터 관련 작업할 때 쓰는 함수 ## 이미지 저장 함수 만들어두기 def get_picture(picture_url, name): # name은 이미지를 저장할 이름 with requests.get(picture_url, stream=True) as r: # stream=True로 설정하면 connection이 계속 열려 있는다 f = open('{}.jpg'.format(name), 'wb') # wb= write binary (0101...) shutil.copyfileobj(r.raw, f) f.close() ## 아래 nytimes 기사에서 이미지 url을 가져온 후, 해당 이미지를 파일로 저장하기 r = requests.get('https://www.nytimes.com/2018/05/06/us/politics/gina-haspel-cia.html') soup = BeautifulSoup(r.text, 'lxml') picture_url = soup.find('meta', attrs={'name':'image'}).get('content') # 기사 속 이미지 url을 가져온다 get_picture(picture_url, 'nytimes_image') ## 함수 실행 -&gt; 'nytimes_image.jpg'라는 이름으로 이미지 저장 . | By default, when you make a request, the body of the response is downloaded immediatedly) | 하지만 stream=True를 옵션으로 써주면 내용이 다운받아지는 것이 미뤄지고, connection이 계속 열려 있는다. (모든 데이터를 다 받아오거나 response를 닫아주기 전까지 connection이 지속) | 그렇기에, with requests.get(url, stream=True) as r: 와 같은 형태로 stream=True를 사용하면 connection이 계속 닫혀 있다가 필요할 때만 열리도록 해주므로 효율적. | . ",
    "url": "https://chaelist.github.io/docs/webscraping/image_api/#image-%EC%88%98%EC%A7%91",
    "relUrl": "/docs/webscraping/image_api/#image-수집"
  },"73": {
    "doc": "Image 수집 & API 활용",
    "title": "API로 데이터 수집",
    "content": "*API (Application Program Interface): 특정 사이트에서 특정 방식으로 특정한 양식의 데이터를 받아올 수 있는 일종의 매개체. (사이트마다 자사 API를 사용할 수 있는 방식 및 요건이 안내되어 있음) . | API로 받아오는 데이터는 보통 XML이나 JSON 형태인데, JSON 형태가 XML보다 심플하기 때문에 더 널리 사용된다 | JSON은 구조가 python dictionary와 유사하기 때문에, dictionary 사용할 때의 방법으로 접근 가능하다 | . +) REST 방식과 SOAP 방식? . | REST(Representational State Transfer): 네트워크를 통해서 컴퓨터들끼리 통신할 수 있게 해주는 아키텍처 스타일로, HTTP 프로토콜로 데이터를 전달한다. 대부분의 public API는 REST 방식을 따른다. | SOAP(Simple Object Access Protocol): XML 기반의 메시지를 네트워크 상에서 교환하는 프로토콜. 보안이나 메시지 전송 등에 있어서 REST보다 더 많은 표준들이 정해져있기 때문에 조금 더 복잡하다. | . KOFIC(영화진흥위원회) API . : 아래 링크의 사이트에서 key를 발급받고, 사용 방법 가이드를 확인할 수 있다. https://www.kobis.or.kr/kobisopenapi/homepg/apiservice/searchServiceInfo.do?serviceId=searchWeeklyBoxOffice . * Weekly Boxoffice 데이터 받아오기 . | 원하는 주간의 boxoffice 데이터를 받아오는 url 만들기 key = 'My_Key' ## 발급받은 key를 넣어준다 target_date = '20210104' # 20210104(월)부터 시작하는 주간 (01.04~01.10) weekchoice = '0' # 0: 주간(월~일), 1: 주말(금~일)(default), 2: 주중(월~목) url_basic = 'http://www.kobis.or.kr/kobisopenapi/webservice/rest/boxoffice/searchWeeklyBoxOfficeList.json?key={}&amp;targetDt={}&amp;weekGb={}' url = url_basic.format(key, target_date, weekchoice) . | requests로 데이터를 받아 json parsing 가능 형태로 변환 . | json library의 ‘loads’(=load from string을 의미) 사용 → string 형태로 담겨 있는 json data를 json parsing이 가능한 형태로 변환해준다. | . import requests import json # requsts로 데이터를 가여온 후,json 형태로 변환해서 parsing할 수 있게 만든다 r = requests.get(url) text = r.text j = json.loads(text) ## json library의 'loads' 사용. -- loads는 load from string을 의미 (string 형태로 담겨 있는 json data를 json parsing이 가능한 형태로 변환) . cf) r.text에는 이런 식으로 데이터가 담겨 있다 . text . {\"boxOfficeResult\":{\"boxofficeType\":\"주간 박스오피스\",\"showRange\":\"20210104~20210110\",\"yearWeekTime\":\"202101\",\"weeklyBoxOfficeList\":[{\"rnum\":\"1\",\"rank\":\"1\",\"rankInten\":\"0\",\"rankOldAndNew\":\"OLD\",\"movieCd\":\"20192567\",\"movieNm\":\"원더 우먼 1984\",\"openDt\":\"2020-12-23\",\"salesAmt\":\"428613900\",\"salesShare\":\"33.9\",\"salesInten\":\"-952680560\",\"salesChange\":\"-69.0\",\"salesAcc\":\"4573697330\",\"audiCnt\":\"45689\",\"audiInten\":\"-111904\",\"audiChange\":\"-71.0\",\"audiAcc\":\"507121\",\"scrnCnt\":\"1321\",\"showCnt\":\"14083\"},{\"rnum\":\"2\",\"rank\":\"2\",\"rankInten\":\"0\",\"rankOldAndNew\":\"OLD\",\"movieCd\":\"20040725\",\"movieNm\":\"화양연화\",\"openDt\":\"2000-10-20\",\"salesAmt\":\"147512030\",\"salesShare\":\"11.7\",\"salesInten\":\"-88550420\",\"salesChange\":\"-37.5\",\"salesAcc\":\"654243280\",\"audiCnt\":\"15999\",\"audiInten\":\"-11936\",\"audiChange\":\"-42.7\",\"audiAcc\":\"75797\",\"scrnCnt\":\"447\",\"showCnt\":\"4064\"}, (생략) . | weeklyBoxOfficeList의 첫번째 element만 먼저 parsing해봄 j['boxOfficeResult']['weeklyBoxOfficeList'][0] . {'audiAcc': '507121', 'audiChange': '-71.0', 'audiCnt': '45689', 'audiInten': '-111904', 'movieCd': '20192567', 'movieNm': '원더 우먼 1984', 'openDt': '2020-12-23', 'rank': '1', 'rankInten': '0', 'rankOldAndNew': 'OLD', 'rnum': '1', 'salesAcc': '4573697330', 'salesAmt': '428613900', 'salesChange': '-69.0', 'salesInten': '-952680560', 'salesShare': '33.9', 'scrnCnt': '1321', 'showCnt': '14083'} . | 해당 기간 Top10 영화의 이름, 관객수, 매출액, 상영횟수 데이터 가져와서 정리하기 . | 관객수, 매출액, 상영횟수는 모두 누적이 아닌, 해당 기간의 수치 | . import pandas as pd import numpy as np # DataFrame 준비 df = pd.DataFrame(columns=['Movie', '#_of_Tickets_Sold', 'Revenue', '#_of_Screenings']) for item in j['boxOfficeResult']['weeklyBoxOfficeList']: srs = pd.Series([item['movieNm'], int(item['audiCnt']), int(item['salesAmt']), int(item['showCnt'])], index=['Movie', '#_of_Tickets_Sold', 'Revenue', '#_of_Screenings']) df = df.append(srs, ignore_index=True) df . |   | Movie | #_of_Tickets_Sold | Revenue | #_of_Screenings | . | 0 | 원더 우먼 1984 | 45689 | 428613900 | 14083 | . | 1 | 화양연화 | 15999 | 147512030 | 4064 | . | 2 | 조제 | 9242 | 84387220 | 2559 | . | 3 | 도굴 | 6916 | 62909780 | 1831 | . | 4 | 뱅가드 | 6307 | 56071400 | 2116 | . | 5 | 미스터 존스 | 5618 | 50406750 | 1258 | . | 6 | 완벽한 가족 | 5229 | 46994840 | 1793 | . | 7 | 빅풋 주니어2: 패밀리가 떴다 | 5193 | 44030440 | 1218 | . | 8 | 라라랜드 | 3275 | 23033000 | 368 | . | 9 | 걸 | 2830 | 26387710 | 1046 | . | 추가) 상영횟수 - 티켓판매량 간의 상관관계를 확인 df.plot(kind='scatter', x='#_of_Screenings', y='#_of_Tickets_Sold') . | 대체로 상영횟수가 많을수록 티켓 판매량도 함께 늘어난다는 것을 알 수 있다. | . | . ",
    "url": "https://chaelist.github.io/docs/webscraping/image_api/#api%EB%A1%9C-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%88%98%EC%A7%91",
    "relUrl": "/docs/webscraping/image_api/#api로-데이터-수집"
  },"74": {
    "doc": "Image 수집 & API 활용",
    "title": "Image 수집 & API 활용",
    "content": " ",
    "url": "https://chaelist.github.io/docs/webscraping/image_api/",
    "relUrl": "/docs/webscraping/image_api/"
  },"75": {
    "doc": "이미지 / 동영상 처리",
    "title": "이미지 / 동영상 처리",
    "content": ". | 이미지 / 동영상 데이터 읽기 . | 이미지 데이터 읽기 | 동영상 데이터 읽기 | 동영상을 이미지로 나눠 저장 | . | 이미지 속 사람 인식 . | 사람 검출 | 사람 얼굴 검출 | 사람 얼굴 방향 확인 | . | 사람 인식: 영상 예시 . | 타임랩스 만들기 | 거리의 사람 수 변화 측정 | 이동 평균 계산 후 비교 | . | . ",
    "url": "https://chaelist.github.io/docs/ml_application/image_processing/",
    "relUrl": "/docs/ml_application/image_processing/"
  },"76": {
    "doc": "이미지 / 동영상 처리",
    "title": "이미지 / 동영상 데이터 읽기",
    "content": "*OpenCV 사용 . | OpenCV: 실시간 이미지 프로세싱에 중점을 둔 라이브러리 | 설치해야 사용 가능: pip install opencv-python | . 이미지 데이터 읽기 . import cv2 img = cv2.imread(\"img/img_students.jpg\") # 정보 읽기 height, width = img.shape[:2] print(f\"이미지 가로: {width}\") print(f\"이미지 세로: {height}\") # 창에 이미지 띄우기 cv2.imshow(\"img\", img) cv2.waitKey(0) # 0을 지정하면 윈도우를 닫을 때까지 계속해서 보여줌 cv2.destroyAllWindows() ## 이미지 창이 별도의 윈도우로 열리고, 아무 키나 누르면 이미지 창이 닫힘 . 이미지 가로: 1920 이미지 세로: 1281 . (이미지가 너무 커서 PC에서 잘림) . | imread: cv2.imread(“img.jpg”, cv2.IMREAD_GRAYSCALE) 이런식으로 옵션을 넣어줄 수도 있는데, 아무것도 안넣으면 default는 그냥 color로 읽어오는 것. | cv2.IMREAD_COLOR: BGR 컬러 적용 (3 채널, default) | cv2.IMREAD_UNCHANGED: 원본 사용 (이미지 파일을 alpha channel까지 포함하여 읽어 들임) | cv2.IMREAD_GRAYSCALE: 그레이스케일 적용 (1 채널) | . | img.shape: (높이, 너비, 채널)을 return. | 채널은 색상 정보로, 3이면 다색(BGR), 1이면 단색(grayscale) | . | waitKey(): 몇 초 동안 이미지를 표시할 지를 밀리초(ms) 단위로 지정 . | ex. waitKey(1000)이라고 하면 1초간 표시됨 | 윈도우를 닫을 때까지 계속해서 보여주고 싶으면 0을 지정하면 됨 | . | .   . → image resizing (이미지 크기 바꾸기) . # 이 경우, 이미지가 너무 크기 때문에 사이즈를 줄여준다 resize_img = cv2.resize(img, dsize=(0, 0), fx=0.5, fy=0.5, interpolation=cv2.INTER_AREA) cv2.imshow(\"resize img\", resize_img) cv2.waitKey(0) # 0을 지정하면 윈도우를 닫을 때까지 계속해서 보여줌 cv2.destroyAllWindows() . (이제는 PC에서 잘리지 않고 잘 보임) . | dsize=(0, 0), fx=0.5, fx=0.5라고 하면 원본 크기에서 가로, 세로를 각각 0.5배로 변경하겠다는 뜻 . | +) Manual sizing: dsize=(640, 480) 이런식으로 숫자를 넣어주면 가로=640, 세로=480으로 변경 | . | interpolation=cv2.INTER_AREA는 이미지 축소 시에 주로 사용하는 보간법. | +) 이미지를 확대하는 경우에는 cv2.INTER_LINEAR이나 cv2.INTER_CUBIC 보간법을 주로 사용 | . | . 동영상 데이터 읽기 . cap = cv2.VideoCapture(\"vid/vid01.avi\") ## 데이터 출처: https://github.com/wikibook/pyda100 # 정보 읽기 width = cap.get(cv2.CAP_PROP_FRAME_WIDTH) height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT) count = cap.get(cv2.CAP_PROP_FRAME_COUNT) fps = cap.get(cv2.CAP_PROP_FPS) print(f\"가로: {width}\") print(f\"세로: {height}\") print(f\"총 프레임수: {count}\") print(f\"FPS: {fps}\") # Frame Per Second # 창에 영상 띄우기 while(cap.isOpened()): ret, frame = cap.read() # cap.read()의 결과로 (retval값(bool), 프레임이미지)가 return되는데, 프레임이 잘 읽히면 retval값은 True if ret: cv2.imshow(\"frame\", frame) if cv2.waitKey(1) &amp; 0xFF == ord(\"q\"): # 각 프레임을 1밀리초 동안 표시하고 다음 프레임으로 이동, 'q' 키를 클릭하면 종료 break else: break # 모든 프레임이 처리되면 종료 cap.release() # 파일을 닫아준다 cv2.destroyAllWindows() # 생성한 모든 window 제거 . 가로: 1920.0 세로: 1440.0 총 프레임수: 401.0 FPS: 30.0 . | What’s 0xFF for in cv2.waitKey(1)? https://stackoverflow.com/questions/35372700/whats-0xff-for-in-cv2-waitkey1 | . 동영상을 이미지로 나눠 저장 . cap = cv2.VideoCapture(\"vid/vid01.avi\") num = 0 while(cap.isOpened()): ret, frame = cap.read() if ret: cv2.imshow(\"frame\", frame) filepath = f\"snapshots/snapshot_{num}.jpg\" # 경로+파일명 cv2.imwrite(filepath, frame) if cv2.waitKey(1) &amp; 0xFF == ord('q'): break else: break num = num + 1 cap.release() cv2.destroyAllWindows() . | ‘snapshots’ 폴더에 ‘snapshot_0.jpg’부터 ‘snapshot_400.jpg’까지 저장됨 | . ",
    "url": "https://chaelist.github.io/docs/ml_application/image_processing/#%EC%9D%B4%EB%AF%B8%EC%A7%80--%EB%8F%99%EC%98%81%EC%83%81-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%9D%BD%EA%B8%B0",
    "relUrl": "/docs/ml_application/image_processing/#이미지--동영상-데이터-읽기"
  },"77": {
    "doc": "이미지 / 동영상 처리",
    "title": "이미지 속 사람 인식",
    "content": "사람 검출 . | HOG 특징량을 활용 | HOG: Histogram of Oriented Gradients. 이미지의 지역적인 Gradient(기울기 정보)를 특징으로 사용해 사람 형태를 판별 (주로 전신 판별에 이용) | . # 준비 hog = cv2.HOGDescriptor() hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector()) # 미리 훈련된 특징 벡터를 가져오는 것 hogParams = {'winStride': (8, 8), 'padding': (16, 16), 'scale': 1.05} # 검출 img = cv2.imread(\"img/img_pedestrians.jpg\") gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) # 이미지를 흑백으로 변환: 컬러나 흑백이나 사람 인식에 차이가 없기 때문에, 흑백으로 넣어줌 human, r = hog.detectMultiScale(gray, **hogParams) # 검출된 사람의 위치 정보는 human에 저장됨 if len(human) &gt; 0: for (x, y, w, h) in human: color = (255, 255, 255) # BGR color - (255, 255, 255)는 white pen_w = 3 cv2.rectangle(img, (x, y, w, h), color, thickness = pen_w) # rectangle: 이미지에 사각형을 그려주는 함수 cv2.namedWindow(\"img\", cv2.WINDOW_NORMAL) cv2.imshow(\"img\", img) cv2.imwrite(\"img_pedstrians_hogdetect.jpg\", img) # 파일로 저장 cv2.waitKey(0) cv2.destroyAllWindows() . | detectMultiScale(): input image에서 다양한 크기의 객체를 인식 . | *인식 대상이 되는 img만 제대로 넣어주면, 다른 parameter는 넣어주지 않아도 문제는 없다 | winStride: 객체 인식을 위해 네모난 모양의 sliding window가 이미지 위로 지나가게 되는데, 이 탐색 window의 크기를 결정하는 것이 winStride 값. | winStride를 크게 하면 탐지가 빨라지지만 정확도가 떨어지고, 반대로 winStride를 작게 하면 정확도가 높아지지만 탐지 속도가 느려진다. | (4, 4) 정도로 시작해서 조금씩 올리면서 정확도와 속도 사이의 적절한 포인트를 찾는 것을 추천. | . | padding: 탐색 window에 더해지는 padding . | (8, 8), (16, 16), (24, 24), (32, 32) 등의 수치를 많이 사용한다고 함 | . | scale: 이미지 layer의 수축 scale. 다양한 크기의 객체를 인식하기 위해, 이미지 크기를 여러 layer로 줄여 나가면서 탐색을 진행하는데 (image pyramid), 이 때 scale 값을 작게 설정하면 더 많은 layer를 탐색해야 한다 . | scale 값을 줄이면 정확도가 높아질 수 있지만 계산하는 시간이 길어진다. | default값은 1.05 (5%씩 사이즈를 줄이면서 탐색한다는 뜻) | . | hitThreshold, finalThreshold: optional. 보통의 경우 그냥 default값으로 두어도 괜찮다 (default: hitThreshold=0, finalThreshold=2) | . | . 사람 얼굴 검출 . | CascadeClassifier를 사용 | . # 준비 cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_alt2.xml\") # 검출 img = cv2.imread(\"img/img_meeting.jpg\") gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) face_list = cascade.detectMultiScale(gray, minSize=(50, 50)) # 검출한 얼굴 표시하기 for (x, y, w, h) in face_list: color = (0, 0, 225) # BGR color - (0, 0, 255)는 red pen_w = 3 cv2.rectangle(img, (x, y, w, h), color, thickness = pen_w) cv2.namedWindow(\"img\", cv2.WINDOW_NORMAL) cv2.imshow(\"img\", img) cv2.imwrite(\"img_meeting_facedetect.jpg\", img) # 파일로 저장 cv2.waitKey(0) cv2.destroyAllWindows() . | 얼굴 4개 검출. (대체로 정면 얼굴에 가까워야 검출됨) | haarcascade 인식 모델 종류 확인: GitHub_opencv . | frontalface_alt, frontalface_alt2, frontalface_alt_tree 등 사용 가능 | . | . 사람 얼굴 방향 확인 . | dlib 라이브러리를 이용: 얼굴을 눈, 코, 입, 얼굴라인 윤곽의 68개의 특징점으로 표현 → 얼굴 방향 등 세세한 정보를 검출 가능 | dlib은 설치해야 사용 가능: conda install -c conda-forge dlib | . import cv2 import dlib import math # 준비 # predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\") # 68개의 얼굴 landmark를 표시해주는 모델 # https://github.com/davisking/dlib-models에서 다운받아서 경로에 저장 detector = dlib.get_frontal_face_detector() # 정면 얼굴 검출 모델 # 검출 # img = cv2.imread(\"img/img_couple_.jpg\") dets = detector(img, 1) for k, d in enumerate(dets): shape = predictor(img, d) # 얼굴 영역 표시 color_f = (0, 0, 225) # 정면 얼굴 검출: red color_l_out = (255, 0, 0) # 얼굴 윤곽 표시: blue color_l_in = (0, 255, 0) # 눈코잎 윤곽 표시: green line_w = 3 # 굵기 circle_r = 3 # radian(반지름) fontType = cv2.FONT_HERSHEY_SIMPLEX fontSize = 1 cv2.rectangle(img, (d.left(), d.top()), (d.right(), d.bottom()), color_f, line_w) # 중심을 계산할 사각형 준비 num_of_points_out = 17 num_of_points_in = shape.num_parts - num_of_points_out gx_out = 0 gy_out = 0 gx_in = 0 gy_in = 0 for shape_point_count in range(shape.num_parts): shape_point = shape.part(shape_point_count) # 얼굴 랜드마크마다 그리기 if shape_point_count &lt; num_of_points_out: cv2.circle(img, (shape_point.x, shape_point.y), circle_r, color_l_out, line_w) gx_out = gx_out + shape_point.x / num_of_points_out gy_out = gy_out + shape_point.y / num_of_points_out else: cv2.circle(img, (shape_point.x, shape_point.y), circle_r, color_l_in, line_w) gx_in = gx_in + shape_point.x / num_of_points_in gy_in = gy_in + shape_point.y / num_of_points_in # 중심 위치 표시 cv2.circle(img, (int(gx_out), int(gy_out)), circle_r, (0,0,255), line_w) # 얼굴 윤곽(blue로 표현)의 중심점을 잡아서 red 점으로 표현 cv2.circle(img, (int(gx_in), int(gy_in)), circle_r, (0,0,0), line_w) # 눈코잎 윤곽(green로 표현)의 중심점을 잡아서 black 점으로 표현 # 얼굴 방향 계산 theta = math.asin(2 * (gx_in - gx_out) / (d.right() - d.left())) radian = theta * 180 / math.pi print(f\"얼굴 방향: {theta} (각도: {radian}도)\") # 얼굴 방향 표시 (사진 위에 적기) if radian &lt; 0: textPrefix = \" left \" else: textPrefix = \" right \" textShow = str(k) + textPrefix + str(round(abs(radian),1)) + \" deg.\" cv2.putText(img, textShow, (d.left(), d.top()), fontType, fontSize, color_f, line_w) cv2.namedWindow(\"img\",cv2.WINDOW_NORMAL) cv2.imshow(\"img\",img) cv2.imwrite(\"09_data/temp3.jpg\",img) cv2.waitKey(0) cv2.destroyAllWindows() . 얼굴 방향: 0.06456096931747406 (각도: 3.6990710631648662도) . | cv2로 rectangle, circle 그리기: 참고 | . ",
    "url": "https://chaelist.github.io/docs/ml_application/image_processing/#%EC%9D%B4%EB%AF%B8%EC%A7%80-%EC%86%8D-%EC%82%AC%EB%9E%8C-%EC%9D%B8%EC%8B%9D",
    "relUrl": "/docs/ml_application/image_processing/#이미지-속-사람-인식"
  },"78": {
    "doc": "이미지 / 동영상 처리",
    "title": "사람 인식: 영상 예시",
    "content": "타임랩스 만들기 . | 타임랩스: “빠르게 재생하는 것”. 일정 시간의 프레임 중 1 프레임씩만 꺼내, 간단하게 경향을 파악 | 타임랩스로 일부 프레임만을 꺼내 사람을 인식해, 사람의 수 증감을 빠르게 파악할 수 있게 함 | . import cv2 print(\"타임랩스 생성 시작\") # 동영상 읽어오기 cap = cv2.VideoCapture(\"vid/vid01.avi\") width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)) height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)) # HOG로 시간별 사람 수 검출 hog = cv2.HOGDescriptor() hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector()) hogParams = {'winStride': (8, 8), 'padding': (32, 32), 'scale': 1.05, 'hitThreshold': 0, 'finalThreshold': 5} # 타임랩스 작성 movie_name = \"timelapse.avi\" fourcc = cv2.VideoWriter_fourcc(*'DIVX') # Codec 정보. OS마다 지원되는 Codec이 다른데, Windows는 DIVX video = cv2.VideoWriter(movie_name, fourcc, 30, (width, height)) # 30: 초당 저장될 frame num = 0 while(cap.isOpened()): ret, frame = cap.read() if ret: if (num % 10 == 0): # FPS=30이었으니까, 1초에 3개의 프레임만 사용하는 것 # 약 0.333333초에 한 번씩 프레임을 꺼내 사람 인식 후 video에 저장 gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) human, r = hog.detectMultiScale(gray, **hogParams) if (len(human) &gt; 0): for (x, y, w, h) in human: cv2.rectangle(frame, (x, y, w, h), (255,255,255), 3) video.write(frame) cv2.imshow(\"frame\", frame) cv2.waitKey(1) else: break num = num + 1 video.release() cap.release() cv2.destroyAllWindows() print(\"타임랩스 생성 완료\") . | cv2.VideoWriter로 영상을 저장: 참고 . | VideoWriter_fourcc 함수로 동영상 데이터 포맷 지정 (네 개의 동영상 데이터 포맷을 지정하는 거라 FourCC) | cv2.VideoWriter(output_file_path, fourcc, frame, size)로 파일을 생성한 다음, | video.write(frame)으로 저장하고 싶은 프레임을 저장 | 마지막으로 video.release()를 실행하면 동영상 생성 완료 | . | . 거리의 사람 수 변화 측정 . | vid01에 대해 사람 수 변화를 계산 . import pandas as pd # 함수로 생성해두기 def video_hog_detect_people(video_file): print(\"분석 시작\") # 동영상 읽어오기 cap = cv2.VideoCapture(video_file) fps = cap.get(cv2.CAP_PROP_FPS) # HOG로 시간별 사람 수 검출 hog = cv2.HOGDescriptor() hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector()) hogParams = {'winStride': (8, 8), 'padding': (32, 32), 'scale': 1.05, 'hitThreshold':0, 'finalThreshold':5} # 1초에 3개 프레임씩 (약 0.33333초에 한번씩) 꺼내서 사람을 인식, df에 사람 수를 저장 df = pd.DataFrame(columns=['time(sec.)', 'num_of_people']) num = 0 while(cap.isOpened()): ret, frame = cap.read() if ret: if (num % 10 == 0): gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) human, r = hog.detectMultiScale(gray, **hogParams) if (len(human) &gt; 0): for (x, y, w, h) in human: cv2.rectangle(frame, (x, y, w, h), (255,255,255), 3) tmp_se = pd.Series([num / fps, len(human)], index=df.columns) # num / fps로 경과 시간 계산 (fps: 1초당 프레임 수) df = df.append(tmp_se, ignore_index=True) if cv2.waitKey(1) &amp; 0xFF == ord('q'): break else: break num = num + 1 cap.release() cv2.destroyAllWindows() print(\"분석 종료\") return df # vid01에 대해 분석 진행 list_df1 = video_hog_detect_people(\"vid/vid01.avi\") ## 데이터 출처: https://github.com/wikibook/pyda100 list_df1.head() . 분석 시작 분석 종료 . |   | time(sec.) | num_of_people | . | 0 | 0 | 5 | . | 1 | 0.333333 | 11 | . | 2 | 0.666667 | 11 | . | 3 | 1 | 7 | . | 4 | 1.33333 | 9 | . → 시각화해서 추이 확인 . import matplotlib.pyplot as plt import seaborn as sns sns.lineplot(data=list_df1, x='time(sec.)', y='num_of_people') plt.ylim(0, 15); . | vid02에 대해 사람 수 변화를 계산 . list_df2 = video_hog_detect_people(\"vid/vid02.avi\") ## 데이터 출처: https://github.com/wikibook/pyda100 list_df2.head() . 분석 시작 분석 종료 . |   | time(sec.) | num_of_people | . | 0 | 0 | 13 | . | 1 | 0.333333 | 14 | . | 2 | 0.666667 | 10 | . | 3 | 1 | 8 | . | 4 | 1.33333 | 10 | . → 시각화해서 추이 확인 . import matplotlib.pyplot as plt import seaborn as sns sns.lineplot(data=list_df1, x='time(sec.)', y='num_of_people') plt.ylim(0, 15); . | . 이동 평균 계산 후 비교 . | HOG로 분석한 데이터에는 오검출된 값도 들어가 있기 때문에, 오검출의 영향을 줄여서 비교하는 것이 좋다 | 노이즈로 인한 fluctuation을 제거하고 보다 공정하게 비교하기 위해, 이동평균(moving average)을 계산 | . | vid01에 대해 이동 평균을 계산 . import numpy as np # 함수로 생성해두기 def moving_average(x, y): y_conv = np.convolve(y, np.ones(5) / float(5), mode='valid') # 데이터를 순서대로 5개씩 잡아서 평균내줌 x_dat = np.linspace(np.min(x), np.max(x), np.size(y_conv)) # y_conv의 요소 개수만큼 x 최솟값 ~ 최댓값 사이의 숫자 array 만들어줌 return x_dat, y_conv . | np.convolve(array1, array2, mode=’valid’): array2가 array1 위를 지나가면서 곱한 결과를 쭉 array로 만들어줌. | ex) np.convolve([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], [0.2, 0.2, 0.2, 0.2, 0.2], mode='valid')이면 결과로 [3, 4, 5, 6, 7, 8]의 array가 만들어짐. (array1이 원소 10개, array2가 원소 5개 → 결과 array는 원소 6개 (len(array1) - len(array2) + 1) . | 1*0.2 + 2*0.2 + 3*0.2 + 4*0.2 + 5*0.2 = 3 이런 식…. (→ 앞에서부터 5개씩 묶어서 평균낸 것과 동일) | . | np.ones(5) / float(5)= [0.2, 0.2, 0.2, 0.2, 0.2]이기에, np.convolve(y, np.ones(5) / float(5), mode='valid')도 첫번째-5번째 평균낸 값, 두번째-6번째 평균낸 값, … 이런식으로 값이 나열된 array가 반환된다. | . | np.linspace(a, b, c): a부터 b까지 c개의 요소를 갖는 배열을 만들라는 뜻 (default로 endpoint=True이기에 b도 포함) . | ex) np.linspace(0, 10, 11)이라고 하면 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]의 array가 만들어짐 | . | . plt.plot(list(list_df1[\"time(sec.)\"]), list(list_df1[\"num_of_people\"]), label=\"raw\") ma_x, ma_y = moving_average(list_df1[\"time(sec.)\"], list_df1[\"num_of_people\"]) # list_dt['num_of_people']의 원소가 41개였으니, ma_y의 원소는 37개 plt.plot(ma_x, ma_y, label=\"average\") plt.xlabel('time(sec.)') plt.ylabel('num_of_people') plt.ylim(0, 15) plt.legend() plt.show(); . | vid02에 대해 이동 평균을 계산 . plt.plot(list(list_df2[\"time(sec.)\"]), list(list_df2[\"num_of_people\"]), label=\"raw\") ma_x2, ma_y2 = moving_average(list_df2[\"time(sec.)\"], list_df2[\"num_of_people\"]) plt.plot(ma_x2, ma_y2, label=\"average\") plt.xlabel('time(sec.)') plt.ylabel('num_of_people') plt.ylim(0, 15) plt.legend() plt.show(); . | viod01, vid02 비교 . plt.plot(ma_x, ma_y, label=\"1st\") plt.plot(ma_x2, ma_y2, label=\"2nd\") plt.xlabel('time(sec.)') plt.ylabel('num_of_people') plt.ylim(0, 15) plt.legend() plt.show(); . | vid01보다 vid02가 전반적으로 사람 수가 많이 찍혀 있다고 판단됨 | 해당 거리에서 가게를 한다면, vid02의 시간대에 조금 더 많은 잠재 고객이 있다고 판단 가능 | . | . ",
    "url": "https://chaelist.github.io/docs/ml_application/image_processing/#%EC%82%AC%EB%9E%8C-%EC%9D%B8%EC%8B%9D-%EC%98%81%EC%83%81-%EC%98%88%EC%8B%9C",
    "relUrl": "/docs/ml_application/image_processing/#사람-인식-영상-예시"
  },"79": {
    "doc": "Home",
    "title": "Chaelist",
    "content": "Data Analytics Blog . Email LinkedIn . UK Ecommerce Dataset (EDA) . YouTube Trending Videos Dataset (EDA) . Amazon Bestselling Books Dataset (EDA) . Telco Customer Churn Dataset (EDA) . Kiva Crowdfunding Dataset (EDA) . STEM Salaries Dataset (EDA) . . Frequency Analysis (단어 빈도 분석) . 뉴스 기사를 수집해 전처리를 거쳐 단어별 빈도를 파악 &amp; 기사의 내용을 워드클라우드로 압축적으로 표현 . Harry Potter Network Analysis (인물 네트워크 분석) . 소설 &lt;Harry Potter&gt; 시리즈 속 인물들 간 연결 관계 및 권별 인물의 중요도 변화를 분석 .   . Sementic Network Analysis (언어 네트워크 분석) . 뉴스 기사 속 주요 단어들 간의 연결 관계를 파악해, 기사의 핵심 내용을 유추 . Movie Review Sentiment Analysis (영화 리뷰 감성 분석) . 영화 100개의 평점-리뷰 데이터를 수집해, 리뷰의 감성(긍정/부정)을 예측하는 모델을 구축 . News Clustering (뉴스 기사 군집화) . 서로 다른 토픽의 뉴스 기사들을 Clustering을 통해 유사한 기사들끼리 묶어줌 . Time Series Data Forecasting (시계열 데이터 예측) . Facebook의 Prophet 라이브러리를 활용해 시계열 데이터를 예측 &amp; 파라미터 튜닝 .   . Topic Modeling (토픽모델링) . Google Play Store의 ‘Netflix’ 앱 리뷰 데이터를 활용해 잠재 디리클레 할당(Latent Dirichlet Allocation, LDA) 기법으로 토픽 모델링 구현 . Logistics Optimization (물류 최적화) . PuLP, ortoolpy 라이브러리를 활용해 최적화된 운송 경로 &amp; 생산 계획을 계산 .   . Image Processing (이미지 데이터 처리) . OpenCV 라이브러리를 활용해 이미지 / 동영상 데이터를 처리 &amp; 이미지 속 사람 검출 .   . ",
    "url": "https://chaelist.github.io/#chaelist",
    "relUrl": "/#chaelist"
  },"80": {
    "doc": "Home",
    "title": "Home",
    "content": " ",
    "url": "https://chaelist.github.io/",
    "relUrl": "/"
  },"81": {
    "doc": "조인, 서브쿼리, 뷰",
    "title": "조인, 서브쿼리, 뷰",
    "content": ". | 테이블 합치기 . | 결합연산 (JOIN) | 기타 다양한 JOIN들 | 집합연산 (UNION 등) | . | 서브쿼리 (SubQuery) . | 비상관 서브쿼리(Non-correlated Subquery) | 상관 서브쿼리(Correlated Subquery) | . | 뷰 (View) | . ",
    "url": "https://chaelist.github.io/docs/sql/join_subq_view/",
    "relUrl": "/docs/sql/join_subq_view/"
  },"82": {
    "doc": "조인, 서브쿼리, 뷰",
    "title": "테이블 합치기",
    "content": "결합연산 (JOIN) . : 테이블을 가로 방향으로 붙이는 연산 . | LEFT OUTER JOIN: 왼쪽 테이블을 기준으로 합쳐짐 (왼쪽 테이블에 존재하는 row만 보여짐) . | ex. 만약 왼쪽 테이블 값(ex. 꽃무늬 원피스)와 연결되는 값이 오른쪽에 2개 이상이라면(ex. 리뷰1, 리뷰2) → 왼쪽 테이블에는 해당 값이 여러 번 표시될 수 있음 | . | RIGHT OUTER JOIN: 오른쪽 테이블을 기준으로 합쳐짐 (오른쪽 테이블에 존재하는 row만 보여짐) | INNER JOIN: 왼쪽, 오른쪽 테이블 모두에 존재하는 row만 추려서 합쳐짐 (교집합 개념) | . -- ex) item 테이블을 기준으로, stock 테이블을 join -- item 테이블의 id 칼럼과 stock 데이블의 item_id 칼럼을 key로 해서 join SELECT item.id, item.name, stock.item_id, stock.inventory_count FROM item LEFT OUTER JOIN stock ON item.id = stock.item_id; . +) 테이블에 alias 붙이기 . | JOIN할 때는 테이블에 alias를 붙여서 사용하는 경우가 많다. (간결하게 작성하기 위함) | ※주의: 한 번 테이블에 alias를 붙였으면, 다른 모든 절에서 그 테이블은 그 alias로만 표현해야 함. (원래의 테이블 이름이랑 섞어서 나타낼 수 없음) | . SELECT i.id, i.name, s.item_id, s.inventory_count FROM item AS i LEFT OUTER JOIN stock AS s ON i.id = s.item_id; . +) 테이블 3개 JOIN하기 . SELECT i.name, i.id, r.item_id, r.star, r.comment, r.mem_id, m.id, m.email FROM item AS i LEFT OUTER JOIN review AS r ON r.item_id = i.id LEFT OUTER JOIN member AS m ON r.mem_id = m.id ORDER BY i.name ASC; . 기타 다양한 JOIN들 . | NATURAL JOIN . | 두 테이블에서 같은 이름의 컬럼을 찾아서 자동으로 그것들을 조인 조건으로 설정하고, INNER JOIN을 해준다 | 다만, NATURAL JOIN을 할 수 있는 경우라도 직관적으로 해석 가능한 SQL문을 위해서 ON으로 조인 조건을 명시해주는 것이 더 좋다 | . SELECT p.id, p.player_name, p.team_name, t.team_name, t.region FROM player AS p INNER JOIN team AS t ON p.team_name = t.team_name; --위와 같은 INNER JOIN을 아래와 같이 구현하는 것도 가능: SELECT p.id, p.player_name, p.team_name, t.team_name, t.region FROM player AS p NATURAL JOIN team AS t . | CROSS JOIN . | 두 테이블의 row들의 모든 조합을 보여주는 JOIN (두 테이블의 카르테시안 곱(Cartesian Product)을 구하는 것) | . -- 가능한 모든 shirts와 pants의 조합을 보고 싶은 경우: SELECT * FROM shirts CROSS JOIN pants . | Self Join . | 자기 자신과 JOIN하는 방식. (한 테이블을 자기 자신과 다시 JOIN해주는 것) | . -- 회사 구성원 정보가 담긴 테이블 → 자기 자신과 join해서 각 직원의 상사 정보를 찾아본다 SELECT employee AS e1 LEFT OUTER JOIN employee AS e2 ON e1.boss = e2.id; . | FULL OUTER JOIN . | 두 테이블의 LEFT OUTER JOIN 결과와 RIGHT OUTER JOIN 결과를 합쳐주는 JOIN. 두 결과에 모두 존재하는 row들(두 테이블에 공통으로 존재하던 row들)은 한번만 표현해준다 (합집합의 개념) | . SELECT * FROM player AS p FULL OUTER JOIN team AS t ON p.team_name = t.team_name; . ※ Oracle에서는 위와 같이 FULL OUTER JOIN 연산자가 지원되지만, MySQL에서는 지원되지 않는다. → LEFT OUTER JOIN과 RIGHT OUTER JOIN을 UNION해서 계산하면 됨! . SELECT * FROM player AS p LEFT OUTER JOIN team AS t ON p.team_name = t.team_name UNION SELECT * FROM player AS p RIGHT OUTER JOIN team AS t ON p.team_name = t.team_name . | Non-Equi Join . | Equi: Equality Condition (동등 조건) | Non-Equi Join: 등호(=)가 아닌 다른 조건을 사용하는 JOIN. | . -- 특정 회원이 가입한 날(sign_up_date)보다 이후에 올라온 상품들을 확인 SELECT * FROM member AS m LEFT OUTER JOIN item AS i ON m.sign_up_date &lt; i.registration_date -- 부등호(&lt;)로 비교 ORDER BY m.sign_up_date ASC; . | . 집합연산 (UNION 등) . : 테이블을 세로 방향으로 합치는 연산 (※ 같은 종류의 테이블끼리만 가능) +) 서로 다른 종류의 테이블의 경우, 조회하는 컬럼을 일치시키면 집합 연산이 가능해진다 . | A ∩ B (INTERSECT): A와 B에 모두 존재하는 row만 출력 SELECT * FROM item -- 기존 item 테이블 INTERSECT SELECT * FROM item_new -- 새로 업데이트된 item 테이블 . | A - B (MINUS): A에는 존재하지만 B에는 존재하지 않는 row만 출력 . | MINUS 연산자 대신 EXCEPT 연산자를 사용해도 된다 | . SELECT * FROM item MINUS SELECT * FROM item_new . | A ∪ B (UNION): A와 B에 존재하는 row를 모두 출력하되, 공통으로 존재하는 row는 중복을 제거하고 하나만 표시 (합집합의 개념) SELECT * FROM item UNION SELECT * FROM item_new . | UNION ALL: A와 B에 존재하는 row를 모두 출력 (중복을 제거하지 않고 그냥 합쳐줌) SELECT * FROM item UNION ALL SELECT * FROM item_new . | . ※ MySQL 8.0에서는 UNION, UNION ALL 연산자만 지원 (Oracle에서는 모두 지원) . +) JOIN으로도 집합연산을 유사하게 수행 가능 . SELECT old.id AS old_id, old.name AS old_name, new.id AS new_id, new.name AS new_name FROM item AS old LEFT OUTER JOIN item_new AS new ON old.id = new.id WHERE new.id IS NULL; . | 이렇게 하면 왼쪽 테이블에는 있지만 오른쪽 테이블에는 없는 row를 확인 가능 (기존 item 테이블엔 정보가 있었지만 오른쪽 새 테이블에선 누락된 것들 확인) | +) 위 예시처럼 JOIN할 때의 key가 되는 두 테이블 내 칼럼의 이름이 같으면 ON 대신 USING을 써도 된다! (ON old.id = new.id 대신 USING(id)라고 사용 가능) | . ",
    "url": "https://chaelist.github.io/docs/sql/join_subq_view/#%ED%85%8C%EC%9D%B4%EB%B8%94-%ED%95%A9%EC%B9%98%EA%B8%B0",
    "relUrl": "/docs/sql/join_subq_view/#테이블-합치기"
  },"83": {
    "doc": "조인, 서브쿼리, 뷰",
    "title": "서브쿼리 (SubQuery)",
    "content": ": SQL문 안에 부품처럼 들어가는 SELECT문 (전체 SQL문 안에 있는 또 다른 SQL문의 개념) . | ※ 서브쿼리는 꼭 ( ) 안에 써줘야 함! | . 비상관 서브쿼리(Non-correlated Subquery) . : 서브쿼리의 SQL절만 독립적으로 써도 실행되는 쿼리. | 단일값을 반환하는 서브쿼리 (스칼라 서브쿼리) . | ex1) SELECT절에서 사용: . -- 각 상품의 price와 쉽게 비교할 수 있도록, 별도로 AVG(price) 값을 가져와서 출력 SELECT id, name, price, (SELECT AVG(price) FROM item) AS avg_price FROM item; . | ex2) WHERE절에서 사용: . -- 가장 높은 가격을 가진 상품을 출력 SELECT id, name, price FROM item WHERE price = (SELECT MAX(price) FROM item); . | ex3) HAVING절에서 사용: . -- 전체 star 평균보다 작은 avg_star를 갖는 group만 출력 SELECT i.id, i.name, AVG(star) AS avg_star FROM item as i LEFT OUTER JOIN review AS r ON r.item_id = i.id GROUP BY i.id, i.name HAVING avg_star &lt; (SELECT AVG(star) FROM review) . | . | 리스트가 결과로 나오는 서브쿼리 (하나의 column &amp; 여러 row의 형태가 반환되는 서브쿼리) . | 리스트 형태의 결과가 나오는 서브쿼리는 IN, ANY, ALL 등과 함께 사용 가능 | . -- 리뷰 수가 3개 이상인 item의 정보만 출력 SELECT * FROM item WHERE id IN (SELECT item_id FROM review GROUP BY item_id HAVING COUNT(*) &gt;= 3); . | 테이블이 결과로 나오는 서브쿼리 . | ※ 이런 식으로 서브쿼리로 가져온 ‘derived table‘을 사용할 때는, 반드시 alias를 붙여줘야 한다! | . -- 지역별 리뷰 수를 count한 후, 평균값, 최댓값, 최솟값을 출력 SELECT AVG(review_count), MAX(review_count), MIN(review_count) FROM (SELECT SUBSTRING(address, 1, 2) AS region, COUNT(*) AS review_count FROM review as r LEFT OUTER JOIN member AS m ON r.mem_id = m.id GROUP BY SUBSTRING(address, 1, 2) HAVING region IS NOT NULL) AS review_count_summary; . | . 상관 서브쿼리(Correlated Subquery) . : 독립적으로는 실행되지 않고, 바깥 SQL절과 관계를 맺고 있는 서브쿼리. | EXISTS와 함께 사용 . -- 리뷰가 달린 상품들만 조회 SELECT * FROM item WHERE EXISTS (SELECT * FROM review WHERE review.item_id = item.id); . | 해석: (1) 일단 item 테이블의 첫 번째 row를 생각 (2) 그 row의 id(item.id) 값과 같은 값을 item_id(review.item_id) 컬럼에 가진 review 테이블의 row가 있는지 조회 (3) 만약에 존재하면(EXISTS) (4) WHERE 절은 True가 되고, (1)에서 생각했던 item 테이블의 row는 조회 대상이 된다 (5) item 테이블의 모든 row에 대해 순차적으로 (2)~(4)를 반복한 후 대상 row들만 출력해준다 | . | NOT EXISTS 사용 (EXISTS의 반대 경우) . -- 리뷰가 달리지 않은 상품들만 조회 SELECT * FROM item WHERE NOT EXISTS (SELECT * FROM review WHERE review.item_id = item.id); . | EXISTS, NOT EXISTS 없는 상관 서브쿼리 . -- member 테이블을 조회하면서, 특정 회원과 같은 해에 태어난 회원들 중 가장 작은 키를 가진 회원의 키 정보를 담은 컬럼을 오른쪽 끝에 추가 SELECT *, (SELECT MIN(height) FROM member AS m2 WHERE birthday IS NOT NULL AND height IS NOT NULL AND YEAR(m1.birthday) = YEAR(m2.birthday)) AS min_height_in_the_year FROM member AS m1 ORDER BY min_height_in_the_year ASC; . | *해석: (1) 일단 birthday 컬럼과 height 컬럼에 둘다 값이 있는 회원들만 대상으로 해야하기 때문에 ‘birthday IS NOT NULL AND height IS NOT NULL’ 조건을 걸어둠 (2) member 테이블의 첫 번째 row를 생각 (3) 그 row에 대해서 같은 YEAR(birthday) 값을 가진 row들을 찾는다 (4) 그 다음 해당 row들 중 height의 최솟값을 구한다 | . | . +) 서브쿼리를 중첩해서 사용하는 것도 가능하지만 (서브쿼리 안에 또 다른 서브쿼리), 가독성이 떨어지므로 ‘뷰’를 활용하는 것이 나을 수 있다 . ",
    "url": "https://chaelist.github.io/docs/sql/join_subq_view/#%EC%84%9C%EB%B8%8C%EC%BF%BC%EB%A6%AC-subquery",
    "relUrl": "/docs/sql/join_subq_view/#서브쿼리-subquery"
  },"84": {
    "doc": "조인, 서브쿼리, 뷰",
    "title": "뷰 (View)",
    "content": ". | CREATE VIEW 뷰_이름 AS 특정한_SQL문의 구조로 ‘뷰’를 생성해두고, 필요할 때마다 원래 있는 테이블인 것처럼 자연스럽게 가져다 쓰면 된다 | 매번 특정한 방식으로 JOIN / 서브쿼리를 사용해서 구성해야 하는 테이블이 있다면, 미리 VIEW로 저장해두고 쓰면 좋다 | . -- VIEW 생성해두기 CREATE VIEW three_tables_joined AS SELECT i.id, i.name, AVG(star) AS avg_star, COUNT(*) AS count_star FROM item AS i LEFT OUTER JOIN review AS r ON r.item_id = i.id LEFT OUTER JOIN member AS m ON r.mem_id = m.id WHERE m.gender = 'f' GROUP BY i.ithree_tables_joinedd, i.name HAVING COUNT(*) &gt;= 2 ORDER BY AVG(star) DESC, COUNT(*) DESC; . → 위와 같이 view를 생성해두면, 서브쿼리를 중첩해서 써야 했던 구문을 아래와 같이 짧게 쓸 수 있다: . SELECT * FROM three_tables_joined WHERE avg_star = ( SELECT MAX(avg_star) FROM three_tables_joined); -- VIEW는 다른 평범한 table처럼 가져다 사용하면 된다 . ",
    "url": "https://chaelist.github.io/docs/sql/join_subq_view/#%EB%B7%B0-view",
    "relUrl": "/docs/sql/join_subq_view/#뷰-view"
  },"85": {
    "doc": "Kaggle Dataset EDA",
    "title": "Kaggle Dataset EDA",
    "content": " ",
    "url": "https://chaelist.github.io/docs/kaggle",
    "relUrl": "/docs/kaggle"
  },"86": {
    "doc": "Kiva Crowdfunding 1",
    "title": "Kiva Crowdfunding 1",
    "content": ". | 데이터 파악 . | 데이터 정보 파악 | datetime 형식 처리 | . | funded amount 파악 . | funded amount 분포 | funded amount 추이 | funded amount와 loan amount의 차이 비교 | . | 조건별 funded amount 확인 . | 국가별 | Sector별 | Activity별 | . | 조건별 분포 확인 . | borrower genders | term in months | repayment interval | lender count | funding에 걸리는 기간 | . | . *분석 대상 데이터셋: Kiva Crowdfunding Data . | 데이터셋 출처 | kiva.org: 세계 각국의 경제적으로 어려운 사람들에게 돈을 빌려주는 online crowdfunding platform | kiva_loans.csv: 2014.01.01~2017.07.26 사이에 kiva에 올라온 loan 정보 데이터. (671,205개의 row) | . ",
    "url": "https://chaelist.github.io/docs/kaggle/kiva_crowdfunding/",
    "relUrl": "/docs/kaggle/kiva_crowdfunding/"
  },"87": {
    "doc": "Kiva Crowdfunding 1",
    "title": "데이터 파악",
    "content": "# 필요한 라이브러리 import import pandas as pd import numpy as np from matplotlib import pyplot as plt import seaborn as sns import scipy.stats as stats . kiva_df = pd.read_csv('data/kiva_loans.csv') kiva_df.head() . |   | id | funded_amount | loan_amount | activity | sector | use | country_code | country | region | currency | partner_id | posted_time | disbursed_time | funded_time | term_in_months | lender_count | tags | borrower_genders | repayment_interval | date | . | 0 | 653051 | 300 | 300 | Fruits &amp; Vegetables | Food | To buy seasonal, fresh fruits to sell. | PK | Pakistan | Lahore | PKR | 247 | 2014-01-01 06:12:39+00:00 | 2013-12-17 08:00:00+00:00 | 2014-01-02 10:06:32+00:00 | 12 | 12 | nan | female | irregular | 2014-01-01 | . | 1 | 653053 | 575 | 575 | Rickshaw | Transportation | to repair and maintain the auto rickshaw used in their business. | PK | Pakistan | Lahore | PKR | 247 | 2014-01-01 06:51:08+00:00 | 2013-12-17 08:00:00+00:00 | 2014-01-02 09:17:23+00:00 | 11 | 14 | nan | female, female | irregular | 2014-01-01 | . | 2 | 653068 | 150 | 150 | Transportation | Transportation | To repair their old cycle-van and buy another one to rent out as a source of income | IN | India | Maynaguri | INR | 334 | 2014-01-01 09:58:07+00:00 | 2013-12-17 08:00:00+00:00 | 2014-01-01 16:01:36+00:00 | 43 | 6 | user_favorite, user_favorite | female | bullet | 2014-01-01 | . | 3 | 653063 | 200 | 200 | Embroidery | Arts | to purchase an embroidery machine and a variety of new embroidery materials. | PK | Pakistan | Lahore | PKR | 247 | 2014-01-01 08:03:11+00:00 | 2013-12-24 08:00:00+00:00 | 2014-01-01 13:00:00+00:00 | 11 | 8 | nan | female | irregular | 2014-01-01 | . | 4 | 653084 | 400 | 400 | Milk Sales | Food | to purchase one buffalo. | PK | Pakistan | Abdul Hakeem | PKR | 245 | 2014-01-01 11:53:19+00:00 | 2013-12-17 08:00:00+00:00 | 2014-01-01 19:18:51+00:00 | 14 | 16 | nan | female | monthly | 2014-01-01 | . | funded_amount: amount disbursed by Kiva to the field agent (USD) | loan_amount: amount disbursed by the field agent to the borrower (USD) | country_code: ISO country code | posted_time: the time at which the loan is posted on Kiva by the field agent | disbursed_time: the time at which the loan is disbursed by the field agent to the borrower | funded_time: the time at which the loan posted to Kiva gets funded by lenders completely | term_in_months: the duration for which the loan was disbursed in months | date: posted_time에서 date만 추출한 것. (Kiva에 올라온 날짜) | . 데이터 정보 파악 . | 중복값 확인 . # 중복값이 포함되어 있나 확인 (모든 열의 데이터가 같은 경우) kiva_df.duplicated().sum() . 0 . | null값 여부, data type 확인 . kiva_df.info() . &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 671205 entries, 0 to 671204 Data columns (total 20 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 id 671205 non-null int64 1 funded_amount 671205 non-null float64 2 loan_amount 671205 non-null float64 3 activity 671205 non-null object 4 sector 671205 non-null object 5 use 666973 non-null object 6 country_code 671197 non-null object 7 country 671205 non-null object 8 region 614405 non-null object 9 currency 671205 non-null object 10 partner_id 657698 non-null float64 11 posted_time 671205 non-null object 12 disbursed_time 668809 non-null object 13 funded_time 622874 non-null object 14 term_in_months 671205 non-null float64 15 lender_count 671205 non-null int64 16 tags 499789 non-null object 17 borrower_genders 666984 non-null object 18 repayment_interval 671205 non-null object 19 date 671205 non-null object dtypes: float64(4), int64(2), object(14) memory usage: 102.4+ MB . | funded_amount와 loan_amount의 관계를 확인 . print('funded = loan: ', len(kiva_df[kiva_df['funded_amount'] == kiva_df['loan_amount']])) print('funded &lt; loan: ', len(kiva_df[kiva_df['funded_amount'] &lt; kiva_df['loan_amount']])) print('funded &gt; loan: ', len(kiva_df[kiva_df['funded_amount'] &gt; kiva_df['loan_amount']])) . funded = loan: 622875 funded &lt; loan: 48328 funded &gt; loan: 2 . | funded_amount: Kiva → field agent // loan_amount: field_agent → borrower | 보통 field agent가 먼저 borrower에게 필요한 금액을 빌려주고, 그 다음 Kiva에 post헤서 자금을 모으는 형태인 듯. | loan_amount만큼 funded_amount가 찬 경우가 가장 많고, 아직 loan_amount만큼 차지 않는 경우는 48,328건 | loan_amount보다 funded_amount가 많은 경우도 이례적으로 2건 있으나, 오기입인지 실제로 loan_amount를 초과해서 자금을 모을 수 있는지는 불분명. | . | funded_time의 null값을 확인 . funded_time_na = kiva_df[kiva_df['funded_time'].isna()] print('funded = loan: ', len(funded_time_na[funded_time_na['funded_amount'] == funded_time_na['loan_amount']])) print('funded &lt; loan: ', len(funded_time_na[funded_time_na['funded_amount'] &lt; funded_time_na['loan_amount']])) print('funded &gt; loan: ', len(funded_time_na[funded_time_na['funded_amount'] &gt; funded_time_na['loan_amount']])) . funded = loan: 1 funded &lt; loan: 48328 funded &gt; loan: 2 . | funded_time: Kiva에 post된 loan이 완전히 모금된 시간을 기록 | loan_amount만큼 아직 funded_amount가 다 차지 않은 경우, funded_time이 아직 기록될 수 없어서 null값으로 남아 있음. | 이례적으로 funded_amount &gt;= loan_amount인 경우도 3건 있음: 모금이 완료되었는데도 해당 시간을 미처 기록하지 못한 듯. | . | . datetime 형식 처리 . | datetime 정보를 담은 컬럼들을 datetime type으로 변환해줌 kiva_df['posted_time'] = pd.to_datetime(kiva_df['posted_time']) kiva_df['disbursed_time'] = pd.to_datetime(kiva_df['disbursed_time']) kiva_df['funded_time'] = pd.to_datetime(kiva_df['funded_time']) # 잘 변환되었나 확인 kiva_df[['posted_time', 'disbursed_time', 'funded_time']].dtypes . posted_time datetime64[ns, UTC] disbursed_time datetime64[ns, UTC] funded_time datetime64[ns, UTC] dtype: object . | posted_time 정보: 미리 가공해둠 . # posted_time → Month, Year, Quarter로도 묶어둠 (추이를 시각화해서 확인하기 편하도록) df['posted_month'] = df['posted_time'].dt.strftime('%Y%m') df['posted_year'] = df['posted_time'].dt.year df['posted_quarter'] = df['posted_time'].dt.to_period(\"Q\").astype('str') df[['posted_time', 'posted_month', 'posted_year', 'posted_quarter']].head() . |   | posted_time | posted_month | posted_year | posted_quarter | . | 0 | 2014-01-01 06:12:39+00:00 | 201401 | 2014 | 2014Q1 | . | 1 | 2014-01-01 06:51:08+00:00 | 201401 | 2014 | 2014Q1 | . | 2 | 2014-01-01 09:58:07+00:00 | 201401 | 2014 | 2014Q1 | . | 3 | 2014-01-01 08:03:11+00:00 | 201401 | 2014 | 2014Q1 | . | 4 | 2014-01-01 11:53:19+00:00 | 201401 | 2014 | 2014Q1 | . | . ",
    "url": "https://chaelist.github.io/docs/kaggle/kiva_crowdfunding/#%EB%8D%B0%EC%9D%B4%ED%84%B0-%ED%8C%8C%EC%95%85",
    "relUrl": "/docs/kaggle/kiva_crowdfunding/#데이터-파악"
  },"88": {
    "doc": "Kiva Crowdfunding 1",
    "title": "funded amount 파악",
    "content": "# 당장 필요하지는 않은 column을 제외하고 df 구성 df = kiva_df.copy() df.drop(['country_code', 'tags', 'date'], axis='columns', inplace=True) . funded amount 분포 . plt.figure(figsize=(12, 5)) sns.kdeplot(data=df, x='funded_amount', color='#C88686'); . → outlier가 많아서 알아보기 어려우므로 특정 구간만 확대해서 다시 그려봄: . # 3사분위값 + 1.5IQR = 1875 plt.figure(figsize=(12, 5)) sns.kdeplot(data = df[df['funded_amount'] &lt;= 1875], x='funded_amount', color='#C88686'); . | 대체로 250 USD 정도의 funding이 많고, 75%가 900 USD 이하의 규모. | . df['funded_amount'].describe() . count 671205.000000 mean 785.995061 std 1130.398941 min 0.000000 25% 250.000000 50% 450.000000 75% 900.000000 max 100000.000000 Name: funded_amount, dtype: float64 . funded amount 추이 . | 분기별 총 funded_amount . | 월별로 그리기에는 기간이 너무 길어 시각화해서 제대로 표현하기 어려우므로, 분기별 추이로 시각화 | 2017Q3은 2017.07.26까지의 기록밖에 없으므로 제외하고 시각화 | . plt.figure(figsize=(16, 6)) sns.lineplot(data=df[df['posted_quarter'] != '2017Q3'], x='posted_quarter', y='funded_amount', color='#C88686', estimator='sum', ci=None) plt.ylim(30000000, 45000000); . | 분기별 총 post된 fund수 . plt.figure(figsize=(16, 6)) sns.lineplot(data=df[df['posted_quarter'] != '2017Q3'], x='posted_quarter', y='funded_amount', color='#C88686', estimator='count', ci=None) plt.ylim(30000, 60000); . | 분기별 평균 funded_amount . plt.figure(figsize=(16, 6)) sns.lineplot(data=df[df['posted_quarter'] != '2017Q3'], x='posted_quarter', y='funded_amount', color='#C88686') # estimator='mean'이 default값 plt.ylim(600, 1000); . | 분기별 총 funded_amount는 증가하는 추세 | 평균 funded_amount는 감소세이지만, post되는 fund의 수가 증가 추세여서 총 amount가 함께 증가. | 다만, 2017Q2에는 post된 fund의 수가 증가했음에도 평균 funded_amount가 크게 감소해 총 funded_amount도 크게 감소. | . | . funded amount와 loan amount의 차이 비교 . | funded_amount와 loan_amount의 변화 추이를 비교해서 시각화 . plt.figure(figsize=(16, 6)) sns.lineplot(data=df[df['posted_quarter'] != '2017Q3'], x='posted_quarter', y='funded_amount', color='#C88686', estimator='sum', ci=None) sns.lineplot(data=df[df['posted_quarter'] != '2017Q3'], x='posted_quarter', y='loan_amount', color='#D7CCAD', estimator='sum', ci=None) plt.ylim(30000000, 50000000); . | loan_amount - funded_amount 차액 변화를 확인 . | loan_amount에서 funded_amount를 뺀 금액이 어떻게 변화하는지 시각화 | . quarterly_amount = df.groupby(['posted_quarter'])[['funded_amount', 'loan_amount']].sum().reset_index() quarterly_amount['loan-funded'] = quarterly_amount['loan_amount'] - quarterly_amount['funded_amount'] plt.figure(figsize=(16, 6)) sns.barplot(data=quarterly_amount[quarterly_amount['posted_quarter'] != '2017Q3'], x='posted_quarter', y='loan-funded', color='#C88686'); . | 2017Q2에는 loan_amount 자체도 조금 감소하긴 했으나 별 문제는 없어 보임. | 2017Q2에 funded_amount 총액이 크게 감소한 것은 아직 loan_amount만큼 다 채워지지 않은 건들이 남아 있기 때문인 듯. 조금 더 시간이 있으면 funding이 늘어날 수 있으므로 시간을 두고 추이를 확인해볼 필요가 있다고 판단됨. | 특이하게도 연도별 Q1(1분기)에는 loan_amount와 funded_amount 사이의 차이가 유독 작게 나타남. 주로 연초에 crowdfunding 등 새로운 계획을 실천하는 사람이 많이 때문? | . | . ",
    "url": "https://chaelist.github.io/docs/kaggle/kiva_crowdfunding/#funded-amount-%ED%8C%8C%EC%95%85",
    "relUrl": "/docs/kaggle/kiva_crowdfunding/#funded-amount-파악"
  },"89": {
    "doc": "Kiva Crowdfunding 1",
    "title": "조건별 funded amount 확인",
    "content": "국가별 . | funded amount가 가장 높은 국가 Top 10 확인 . # funded_amount 합산 금액 내림차순으로 Top 10 국가만 시각화 loans_country = df.groupby('country')[['funded_amount']].sum().reset_index() loans_country.sort_values(by='funded_amount', ascending=False, inplace=True) loans_country.head(10) plt.figure(figsize=(12, 5)) sns.barplot(data=loans_country.head(10), x='country', y='funded_amount', palette='pink'); . | kiva에서 fund 받은 금액이 가장 많은 나라는 Philippines | . | 분기별 국가별 funded amount . quarterly_loans_country = df.groupby(['posted_quarter', 'country'])[['funded_amount']].sum().reset_index() # 2017Q3은 7월밖에 없고, 그나마 7월도 26일까지밖에 없으므로 제외 quarterly_loans_country = quarterly_loans_country[quarterly_loans_country['posted_quarter'] != '2017Q3'] # 각 기간별로, funded amount top 5에 포함되는 국가들만 따로 저장 quarters = quarterly_loans_country['posted_quarter'].unique() top_countries_set = set() for q in quarters: temp = quarterly_loans_country[quarterly_loans_country['posted_quarter'] == q] temp_set = set(temp.sort_values(by='funded_amount', ascending=False).head()['country'].to_list()) top_countries_set.update(temp_set) # 위에서 정리한 top_countries_set에 포함된 국가에 한해서 시각화 plt.figure(figsize=(16, 7)) sns.lineplot(data=quarterly_loans_country[quarterly_loans_country['country'].isin(top_countries_set)], x='posted_quarter', y='funded_amount', hue='country', palette='Set2'); # legend를 box 밖으로 빼 줌 plt.legend(bbox_to_anchor=(1.01, 1), borderaxespad=0); . | 2014Q1을 제외한 모든 기간에 Philippines이 fund 받은 금액이 가장 많음 | Kenya의 경우, 특히 Q1(1분기)를 위주로 funded amount가 증가하는 트렌드를 보임 | Cambodia의 경우 2017Q1, 2017Q2에 큰 funded amount 상승세를 보여, 추후 지켜봐야 할 듯. | . +) Kenya의 sector별 funded_amount 확인 . kenya_sector = df[df['country'] == 'Kenya'].groupby(['sector'])[['funded_amount']].sum().sort_values(by='funded_amount', ascending=False) kenya_sector['percentage(%)'] = kenya_sector['funded_amount'] / kenya_sector['funded_amount'].sum() * 100 kenya_sector.head() . | sector | funded_amount | percentage(%) | . | Agriculture | 16484185 | 51.12 | . | Food | 4603345 | 14.27 | . | Retail | 4232640 | 13.13 | . | Services | 1896535 | 5.88 | . | Clothing | 1547850 | 4.80 | . | Kenya의 경우 funded amount 중 약 51%가 Agriculture(농업) 관련이여서 1분기에 특히 fund가 증가하는 추세가 나타나는 듯. | . | . Sector별 . | funded amount가 가장 높은 sector Top 10 확인 . # funded_amount 합산 금액 내림차순으로 정렬 후 시각화 loans_sector = df.groupby('sector')[['funded_amount']].sum().reset_index() loans_sector.sort_values(by='funded_amount', ascending=False, inplace=True) plt.figure(figsize=(12, 5)) sns.barplot(data=loans_sector, x='sector', y='funded_amount', palette='pink') plt.xticks(rotation=40); . | 가장 많은 funded amount를 보이는 sector는 Agriculture, Food, Retail. | . | 분기별 sector별 funded amount . quarterly_loans_sector = df.groupby(['posted_quarter', 'sector'])[['funded_amount']].sum().reset_index() # 2017Q3은 7월밖에 없고, 그나마 7월도 26일까지밖에 없으므로 제외 quarterly_loans_sector = quarterly_loans_sector[quarterly_loans_sector['posted_quarter'] != '2017Q3'] # 각 기간별로, funded amount top 5에 포함되는 sector들만 따로 저장 quarters = quarterly_loans_sector['posted_quarter'].unique() top_sectors_set = set() for q in quarters: temp = quarterly_loans_sector[quarterly_loans_sector['posted_quarter'] == q] temp_set = set(temp.sort_values(by='funded_amount', ascending=False).head()['sector'].to_list()) top_sectors_set.update(temp_set) # 위에서 정리한 top_sectors_set에 포함된 sector에 한해서 시각화 plt.figure(figsize=(16, 7)) sns.lineplot(data=quarterly_loans_sector[quarterly_loans_sector['sector'].isin(top_sectors_set)], x='posted_quarter', y='funded_amount', hue='sector', palette='Set2'); # legend를 box 밖으로 빼 줌 plt.legend(bbox_to_anchor=(1.01, 1), borderaxespad=0); . | Agriculture 섹터의 경우, 주로 Q1 ~ Q2의 기간에 특히 funded amount가 증가하는 트렌드를 보임 | Food, Retail 섹터는 꾸준히 많은 funded amount를 보임 | . | 국가별 sector별 funded amount . # funded amount Top 10 country만 따로 저장 top_countries = df.groupby('country')[['funded_amount']].sum().sort_values(by='funded_amount', ascending=False).head(10).index # funded amount 내림차순으로 sector 순서 정렬 sectors_order = df.groupby('sector')[['funded_amount']].sum().sort_values(by='funded_amount', ascending=False).index loans_country_sector = pd.pivot_table(df, index='country', columns='sector', values='funded_amount', fill_value=0, aggfunc='sum') loans_country_sector = loans_country_sector[loans_country_sector.index.isin(top_countries)] # top 10 country만 대상으로 자름 loans_country_sector = loans_country_sector[sectors_order] # 전체 loan amount가 가장 높은 sector부터 내림차순 정렬 loans_country_sector = loans_country_sector.T loans_country_sector = loans_country_sector[top_countries] # 전체 loan amount가 가장 높은 country부터 내림차순 정렬 # 값을 normalize(정규화)해서 비교 (heatmap으로 보다 명확하게 비교할 수 있게 표현하기 위함) from sklearn import preprocessing scaler = preprocessing.MinMaxScaler() normalized_data = scaler.fit_transform(loans_country_sector) loans_cntry_sctr_normalized = pd.DataFrame(normalized_data, columns=loans_country_sector.columns, index=loans_country_sector.index) # heatmap으로 시각화 plt.figure(figsize=(14, 9)) sns.heatmap(loans_cntry_sctr_normalized, annot=True, cmap='pink_r', fmt='.2f'); . | funded amount Top 10 국가 중 Kenya, El Salvador, Cambodia, Ecuador는 Agriculture 섹터로 funding 받은 금액이 가장 많음 | funded amount Top 10 국가 중 Peru, Paraguay, Bolivia, Rwanda는 Food 섹터로 funding 받은 금액이 가장 많음 | 가장 funded amount가 많은 국가인 Philippines은 Retail 섹터로 받은 금액이 가장 많음 | United States는 이례적으로 Services 섹터로 받은 금액이 가장 많음 | . | . Activity별 . | funded amount가 가장 높은 activity Top 10 확인 . # funded_amount 합산 금액 내림차순으로 Top 10 activity만 확인 loans_activity = df.groupby(['sector', 'activity'])[['funded_amount']].sum().reset_index() loans_activity.sort_values(by='funded_amount', ascending=False, inplace=True) loans_activity.head(10) . |   | sector | activity | funded_amount | . | 7 | Agriculture | Farming | 47900500 | . | 94 | Retail | General Store | 35555135 | . | 0 | Agriculture | Agriculture | 25530735 | . | 54 | Food | Food Production/Sales | 24326140 | . | 112 | Retail | Retail | 23911950 | . | 24 | Clothing | Clothing Sales | 22569725 | . | 68 | Housing | Personal Housing Expenses | 20166525 | . | 37 | Education | Higher education costs | 19012400 | . | 57 | Food | Grocery Store | 14299895 | . | 10 | Agriculture | Livestock | 13699275 | . → 시각화 . plt.figure(figsize=(14, 5)) sns.barplot(data=loans_activity.head(10), x='activity', y='funded_amount', palette='pink') plt.xticks(rotation=20); . | Agriculture 섹터에 속하는 ‘Farming’ activity가 가장 많은 funded amount를 보임 | . | 분기별 activity별 funded amount . quarterly_loans_activity = df.groupby(['posted_quarter', 'activity'])[['funded_amount']].sum().reset_index() # 2017Q3은 7월밖에 없고, 그나마 7월도 26일까지밖에 없으므로 제외 quarterly_loans_activity = quarterly_loans_activity[quarterly_loans_activity['posted_quarter'] != '2017Q3'] # 각 기간별로, funded amount top 5에 포함되는 activity들만 따로 저장 quarters = quarterly_loans_activity['posted_quarter'].unique() top_activities_set = set() for q in quarters: temp = quarterly_loans_activity[quarterly_loans_activity['posted_quarter'] == q] temp_set = set(temp.sort_values(by='funded_amount', ascending=False).head()['activity'].to_list()) top_activities_set.update(temp_set) # 위에서 정리한 top_sectors_set에 포함된 activity에 한해서 시각화 plt.figure(figsize=(16, 7)) sns.lineplot(data=quarterly_loans_activity[quarterly_loans_activity['activity'].isin(top_activities_set)], x='posted_quarter', y='funded_amount', hue='activity', palette='Set2'); # legend를 box 밖으로 빼 줌 plt.legend(bbox_to_anchor=(1.01, 1), borderaxespad=0); . | Farming activity의 funded amount는 Q4에 가장 적고 주로 Q1 ~ Q2 기간에 증가하는 추세를 보임 (Seasonality 존재) | General Store activity의 꾸준히 높은 funded amount를 보이는 편 | . | 국가별 activity별 funded amount . # funded amount Top 10 country만 따로 저장 top_countries = df.groupby('country')[['funded_amount']].sum().sort_values(by='funded_amount', ascending=False).head(10).index # funded amount 내림차순으로 activity 순서 정렬 activity_order = df.groupby('activity')[['funded_amount']].sum().sort_values(by='funded_amount', ascending=False).index # Top 10 country 각각의 Top 3 activity만 따로 저장 temp_df = df.groupby(['country', 'activity'])[['funded_amount']].sum().reset_index() top_activities = set() for country in top_countries: temp = temp_df[temp_df['country'] == country].sort_values(by='funded_amount', ascending=False).head(3) top_activities.update(set(temp['activity'].to_list())) loans_country_activity = pd.pivot_table(df, index='country', columns='activity', values='funded_amount', fill_value=0, aggfunc='sum') loans_country_activity = loans_country_activity[loans_country_activity.index.isin(top_countries)] # top 10 country만 대상으로 자름 loans_country_activity = loans_country_activity[activity_order] # 전체 loan amount가 가장 높은 activity부터 내림차순 정렬 loans_country_activity = loans_country_activity.T loans_country_activity = loans_country_activity[loans_country_activity.index.isin(top_activities)] # top_activities로 한정 loans_country_activity = loans_country_activity[top_countries] # 전체 loan amount가 가장 높은 country부터 내림차순 정렬 # 값을 normalize(정규화)해서 비교 (heatmap으로 보다 명확하게 비교할 수 있게 표현하기 위함) scaler = preprocessing.MinMaxScaler() normalized_data = scaler.fit_transform(loans_country_activity) loans_country_activity_normalized = pd.DataFrame(normalized_data, columns=loans_country_activity.columns, index=loans_country_activity.index) # heatmap으로 시각화 plt.figure(figsize=(14, 9)) sns.heatmap(loans_country_activity_normalized, annot=True, cmap='pink_r', fmt='.2f'); . | Kenya와 Cambodia는 Farming으로 받은 금액이 가장 많음 | Philipines는 General Store 관련 금액이 가장 많음 | . | . ",
    "url": "https://chaelist.github.io/docs/kaggle/kiva_crowdfunding/#%EC%A1%B0%EA%B1%B4%EB%B3%84-funded-amount-%ED%99%95%EC%9D%B8",
    "relUrl": "/docs/kaggle/kiva_crowdfunding/#조건별-funded-amount-확인"
  },"90": {
    "doc": "Kiva Crowdfunding 1",
    "title": "조건별 분포 확인",
    "content": "borrower genders . | 결측치는 무시하고 계산 | . | borrower_genders 칼럼을 type별로 묶어서 저장 . | female 또는 male 한 명인 경우 그대로 적고, female 여러 명일 경우 female_group, male 여러 명일 경우 male_group, 그리고 female과 male이 섞여 있는 그룹일 경우 mixed_group으로 기재 | . def borrower_type(x): if type(x) != str: borrower = 'N/A' elif x == 'female': borrower = 'female' elif x == 'male': borrower = 'male' else: borr_set = set(x.split(', ')) if 'female' in borr_set: if 'male' in borr_set: borrower = 'mixed_group' else: borrower = 'female_group' else: borrower = 'male_group' return borrower df['borrower_type'] = df['borrower_genders'].apply(lambda x: borrower_type(x)) df[['borrower_genders', 'borrower_type']].head() . |   | borrower_genders | borrower_type | . | 0 | female | female | . | 1 | female, female | female_group | . | 2 | female | female | . | 3 | female | female | . | 4 | female | female | . | borrower type별 총 funded_amount . plt.figure(figsize=(9, 5)) sns.barplot(data=df, x='borrower_type', y='funded_amount', palette='pink', estimator=np.sum, order = ['female', 'female_group', 'male', 'male_group', 'mixed_group', 'N/A']); . | borrower type별 fund count . plt.figure(figsize=(9, 5)) sns.countplot(data=df, x='borrower_type', palette='pink', order = ['female', 'female_group', 'male', 'male_group', 'mixed_group', 'N/A']); . | borrower type별 평균 funded_amount . plt.figure(figsize=(9, 5)) sns.barplot(data=df, x='borrower_type', y='funded_amount', palette='pink', order = ['female', 'female_group', 'male', 'male_group', 'mixed_group', 'N/A']); . | female 혼자 빌리는 경우는 평균 funded_amount는 적지만 수 자체가 가장 많기 때문에 총 funded_amount가 가장 많음 | group으로 빌리는 경우는 수 자체는 적지만 평균 funded_amount가 많다 | . | 분기별 borrower type별 funded amount . borrower_loan = df.groupby(['posted_quarter', 'borrower_type'])[['funded_amount']].sum().reset_index() borrower_loan = borrower_loan[borrower_loan['posted_quarter'] != '2017Q3'] # 2017Q3은 7월밖에 없고, 그나마 7월도 26일까지밖에 없으므로 제외 plt.figure(figsize=(16, 6)) sns.lineplot(data=borrower_loan, x='posted_quarter', y='funded_amount', hue='borrower_type', palette='pink', hue_order=['female', 'female_group', 'male', 'male_group', 'mixed_group', 'N/A']) # legend를 box 밖으로 빼 줌 plt.legend(bbox_to_anchor=(1.01, 1), borderaxespad=0); . | 모든 기간에 female 혼자 빌리는 형태의 funded_amount 총액이 가장 많다 | . | borrower type별 activity별 funded amount . # borrower_type 중 'N/A'만 제외 borrower_type_list = list(df['borrower_type'].unique()) borrower_type_list.remove('N/A') # funded amount 내림차순으로 activity 순서 정렬 activity_order = df.groupby('activity')[['funded_amount']].sum().sort_values(by='funded_amount', ascending=False).index # Top 10 country 각각의 Top 5 activity만 따로 저장 temp_df = df.groupby(['borrower_type', 'activity'])[['funded_amount']].sum().reset_index() top_activities = set() for borrower in borrower_type_list: temp = temp_df[temp_df['borrower_type'] == borrower].sort_values(by='funded_amount', ascending=False).head(3) top_activities.update(set(temp['activity'].to_list())) loans_borrower_activity = pd.pivot_table(df, index='borrower_type', columns='activity', values='funded_amount', fill_value=0, aggfunc='sum') loans_borrower_activity = loans_borrower_activity[activity_order] # 전체 loan amount가 가장 높은 activity부터 내림차순 정렬 loans_borrower_activity = loans_borrower_activity[loans_borrower_activity.index.isin(borrower_type_list)] loans_borrower_activity = loans_borrower_activity.T loans_borrower_activity = loans_borrower_activity[loans_borrower_activity.index.isin(top_activities)] # top_activities로 한정 # 값을 normalize(정규화)해서 비교 (heatmap으로 보다 명확하게 비교할 수 있게 표현하기 위함) scaler = preprocessing.MinMaxScaler() normalized_data = scaler.fit_transform(loans_borrower_activity) loans_borrower_activity_normalized = pd.DataFrame(normalized_data, columns=loans_borrower_activity.columns, index=loans_borrower_activity.index) # heatmap으로 시각화 plt.figure(figsize=(14, 9)) sns.heatmap(loans_borrower_activity_normalized, annot=True, cmap='pink_r', fmt='.2f'); . | 다른 타입은 모두 ‘Farming’ 관련 funded amount가 가장 많은 반면, female은 ‘General Store’ 관련 funded amount가 가장 많음 | male과 male_group은 Farming과 Agriculture 관련 funded amount가 많음 | female_group의 경우, 가장 funded amount가 많은 두 activity는 Farming과 Retail | . | . term in months . | term_in_months: 돈을 몇 개월에 걸쳐 나눠서 지급하는지 나타내는 지표 (The duration for which the loan was disbursed in months) | . | term_in_months와 funded_amount 사이의 상관관계 확인 . plt.figure(figsize=(12, 6)) sns.scatterplot(data = df[df['posted_quarter'] != '2017Q3'], x='funded_amount', y='term_in_months', color='#C88686'); . +) 상관계수 계산: . # 피어슨 상관계수 검정 corr = stats.pearsonr(df['term_in_months'], df['funded_amount']) print('Corr_Coefficient : %.3f \\np-value : %.3f' % (corr)) . Corr_Coefficient : 0.149 p-value : 0.000 . | funded_amount와 term_in_months 사이에 큰 관계는 보이지 않음 | . | funded amount를 구간별로 나눠서 비교 . # funded_amount를 기준으로 1사분위 ~ 4사분위로 나눠서 flag를 붙임 q1, q2, q3 = np.percentile(df['funded_amount'], [25, 50, 75]) def get_flag(x): if x&lt; q1: quarter = '1st_q' elif x &lt; q2: quarter = '2nd_q' elif x&lt; q3: quarter = '3rd_q' else: quarter = '4th_q' return quarter df['funded_amount_flag'] = df['funded_amount'].apply(lambda x: get_flag(x)) df[['funded_amount', 'funded_amount_flag']].head() . |   | funded_amount | funded_amount_flag | . | 0 | 300 | 2nd_q | . | 1 | 575 | 3rd_q | . | 2 | 150 | 1st_q | . | 3 | 200 | 1st_q | . | 4 | 400 | 2nd_q | . → funded_amount_flag별 평균 term_in_months를 비교 . sns.barplot(data=df, x='funded_amount_flag', y='term_in_months', palette='pink', order=['4th_q', '3rd_q', '2nd_q', '1st_q']); . | loan_amount가 가장 많은 그룹(4사분위)이 지급 기간도 평균적으로 긴 편 | . | 분기별 그룹별 평균 term in months 비교 . plt.figure(figsize=(16, 6)) sns.lineplot(data=df[df['posted_quarter'] != '2017Q3'], x='posted_quarter', y='term_in_months', hue='funded_amount_flag', palette='pink', hue_order=['4th_q', '3rd_q', '2nd_q', '1st_q']) # legend를 box 밖으로 빼 줌 plt.legend(bbox_to_anchor=(1.01, 1), borderaxespad=0); . | 대체로 loan_amount가 많은 그룹(3~4사분위)이 지급 기간도 평균적으로 긴 편 | . | 분기별 평균 term in months 추이 확인 . plt.figure(figsize=(16, 6)) sns.lineplot(data=df[df['posted_quarter'] != '2017Q3'], x='posted_quarter', y='term_in_months', color='#C88686') plt.ylim(12, 15); . | 특히 Q1 ~ Q2 기간에 평균 term_in_months가 짧은 편으로 나타나는 경향이 있는 듯 | 전체적으로 봤을 때, 평균 term_in_months가 감소하는 추세라고 판단됨. (추후 수치 트래킹 필요) | . | sector별 평균 term in months 비교 . # term_in_months 기준 내림차순으로 index 순서를 저장해둠 index_order = df.groupby('sector')[['term_in_months']].mean().sort_values(by='term_in_months', ascending=False).index plt.figure(figsize=(16, 7)) sns.barplot(data = df, x='sector', y='term_in_months', palette='pink', order=index_order) plt.xticks(rotation=40); . | 각 sector의 평균 funded_amount와는 큰 관계가 없음 | 평균 term_in_months가 큰 sector는 Education, Housing, Health | . | . repayment interval . | repayment interval별 총 funded_amount . sns.barplot(data=df, x='repayment_interval', y='funded_amount', estimator=np.sum, ci=None, palette='pink', order=['bullet', 'irregular', 'monthly', 'weekly']); . | repayment interval별 평균 funded_amount . sns.barplot(data=df, x='repayment_interval', y='funded_amount', palette='pink', order=['bullet', 'irregular', 'monthly', 'weekly']); . | repayment interval별 loan 수 . sns.countplot(data=df, x='repayment_interval', palette='pink', order=['bullet', 'irregular', 'monthly', 'weekly']); . | ‘monthly’로 상환하는 loan이 가장 많음 | monthly로 상환하는 loan 수가 많고, 평균 funded_amount도 높은 편이기 때문에 monthly로 상환되는 총 funded_amount가 가장 많음 | . | 분기별 repayment interval별 funded amount . quarter_interval = df.groupby(['posted_quarter', 'repayment_interval'])[['funded_amount']].sum().reset_index() quarter_interval = quarter_interval[quarter_interval['posted_quarter'] != '2017Q3'] plt.figure(figsize=(16, 6)) sns.lineplot(data=quarter_interval, x='posted_quarter', y='funded_amount', hue='repayment_interval', palette='pink') # legend를 box 밖으로 빼 줌 plt.legend(bbox_to_anchor=(1.01, 1), borderaxespad=0); . | 2015Q3 이후로는 weekly 상환 방식은 없어짐 | monthly 혹은 bullet 상환 방식이 더 안정적인 자금 계획에 유리하겠지만, irregular 상환 방식도 지나치게 증가하고 있는 추세는 아니므로 괜찮다고 생각됨 | 다만, irregular 상환 방식의 비중이 너무 높아지면 좋지 않을 수 있으므로 향후 꾸준히 트래킹할 필요가 있다고 생각됨 | . print('*2014Q1 대비 2017Q1 funded_amount 증감*') for interval in ['monthly', 'irregular', 'bullet']: temp = quarter_interval[quarter_interval['repayment_interval'] == interval] loan_2014 = int(temp[temp['posted_quarter'] == '2014Q1']['funded_amount']) loan_2017 = int(temp[temp['posted_quarter'] == '2017Q1']['funded_amount']) growth_rate = (loan_2017 / loan_2014 - 1) * 100 print(f'{interval}: {growth_rate :.0f}% ({loan_2014} -&gt; {loan_2017})') . *2014Q1 대비 2017Q1 funded_amount 증감* monthly: 15% (21201250 -&gt; 24314025) irregular: 30% (10370950 -&gt; 13451700) bullet: 41% (3173205 -&gt; 4460625) . | . lender count . | lender_count와 funded_amount 사이의 상관관계 확인 . plt.figure(figsize=(12, 6)) sns.scatterplot(data=df, x='funded_amount', y='lender_count', color='#C88686'); . +) 상관계수 계산: . # 피어슨 상관계수 검정 corr = stats.pearsonr(df['funded_amount'], df['lender_count']) print('Corr_Coefficient : %.3f \\np-value : %.3f' % (corr)) . Corr_Coefficient : 0.849 p-value : 0.000 . | lender_count와 funded_amount는 꽤 강한 상관관계를 보임 | . | lender_count의 분포 확인 . plt.figure(figsize=(16, 6)) sns.kdeplot(data = df, x='lender_count', color='#C88686'); . → outlier로 인해 값이 몰려 있는 부분을 구체적으로 확인하기 어려우므로, 주요 구간을 확대해서 확인: . # Q3 + 1.5IQR = 49.5 plt.figure(figsize=(16, 6)) sns.countplot(data = df[df['lender_count'] &lt; 50], x='lender_count', color='#C88686'); . | 주로 1명 혹은 5~9명 정도가 하나의 funding에 참여하는 경우가 많다 | . | 분기별 평균 lender_count . plt.figure(figsize=(16, 6)) sns.barplot(data = df[df['posted_quarter'] != '2017Q3'], x='posted_quarter', y='lender_count', palette='pink'); . | 평균적인 lender 수가 점점 감소하는 추세를 보임 | 점점 post되는 loan의 수가 많아지면서 개당 lender수, funded_amount가 감소하는 것인라고 추정됨 | . | . funding에 걸리는 기간 . | posted_time ~ funded_time 사이 기간 (post한 후 완전히 funding 완료되기까지) | . | funding 미완료 post의 분포를 확인 . # funded_time이 null값인 post를 funding 미완료라고 분류 funding_incomplete = df[df['funded_time'].isna()] plt.figure(figsize=(16, 6)) sns.countplot(data=funding_incomplete, x='posted_quarter', color='#C88686'); . | posted_time ~ funded_time 사이 기간을 계산해서 저장 . # day(일) 단위로 저장 df['post_to_funded'] = (df['funded_time'] - df['posted_time']).dt.days df[['funded_time', 'posted_time', 'post_to_funded']].head() . |   | funded_time | posted_time | post_to_funded | . | 0 | 2014-01-02 10:06:32+00:00 | 2014-01-01 06:12:39+00:00 | 1 | . | 1 | 2014-01-02 09:17:23+00:00 | 2014-01-01 06:51:08+00:00 | 1 | . | 2 | 2014-01-01 16:01:36+00:00 | 2014-01-01 09:58:07+00:00 | 0 | . | 3 | 2014-01-01 13:00:00+00:00 | 2014-01-01 08:03:11+00:00 | 0 | . | 4 | 2014-01-01 19:18:51+00:00 | 2014-01-01 11:53:19+00:00 | 0 | . +) null값 수 파악 . df['post_to_funded'].isnull().sum() . 48331 . +) 잘못된 값 파악 . df[df['funded_time'] &lt; df['posted_time']][['funded_time', 'posted_time']] . |   | posted_time | funded_time | . | 636606 | 2017-05-15 00:00:00+00:00 | 2017-04-27 11:52:24+00:00 | . | posted_time은 kiva에 loan이 post된 시각이고, funded_time은 kiva에 post된 loan이 완전히 funded된 시각이므로 posted_time이 funded_time보다 뒤일 수는 없다 | posted_time &lt; posted_time으로 나오는 row는 잘못 기입된 것이라고 생각됨 | . → 마이너스 값을 보이는 칼럼 &amp; N/A값(아직 funding 완료 안됨)을 제외한 데이터를 별도로 저장 . post_to_fund_notna = df[df['post_to_funded'] &gt;= 0] . | post된 후 funding 완료까지의 기간 분포 확인 . plt.figure(figsize=(16, 6)) sns.kdeplot(data = post_to_fund_notna, x=\"post_to_funded\", color='#C88686'); . → outlier로 인해 값이 몰려 있는 부분을 구체적으로 확인하기 어려우므로, 주요 구간을 확대해서 확인: . # Q3 + 1.5IQR = 47.5 plt.figure(figsize=(20, 6)) sns.countplot(data = post_to_fund_notna[post_to_fund_notna['post_to_funded'] &lt; 50], x=\"post_to_funded\", palette='pink'); . | 주로 funding 완료까지 5일 정도 걸리는 경우가 많으며, 전체의 50% 이상이 9일 안에 funding이 완료됨 | . | 분기별 평균 posted ~ funded 기간 . plt.figure(figsize=(16, 6)) sns.barplot(data = post_to_fund_notna[post_to_fund_notna['posted_quarter'] != '2017Q3'], x='posted_quarter', y='post_to_funded', palette='pink'); . | 특히 Q1에 post 후 funding 완료까지의 기간이 평균적으로 짧은 경향을 보임 | . | sector별 평균 posted ~ funded 기간 . plt.figure(figsize=(16, 6)) sns.barplot(data = post_to_fund_notna, x='sector', y='post_to_funded', palette='pink') plt.xticks(rotation=40); . | Arts, Manufacturing, Education, Personal Use 섹터가 특히 post 후 funding 완료까지의 기간이 비교적 짧은 것으로 보임 | . | funded_amount와 posted ~ funded 기간의 상관관계 확인 . plt.figure(figsize=(12, 6)) sns.scatterplot(data=post_to_fund_notna, x='funded_amount', y='post_to_funded', color='#C88686'); . +) 상관계수 계산: . # 피어슨 상관계수 검정 corr = stats.pearsonr(post_to_fund_notna['post_to_funded'], post_to_fund_notna['funded_amount']) print('Corr_Coefficient : %.3f \\np-value : %.3f' % (corr)) . Corr_Coefficient : 0.140 p-value : 0.000 . | funded_amount와 funding 완료되기까지의 기간 사이에 관계가 있을 수 있다고 생각했으나, 별로 관계가 없는 것으로 확인됨 | . | . ",
    "url": "https://chaelist.github.io/docs/kaggle/kiva_crowdfunding/#%EC%A1%B0%EA%B1%B4%EB%B3%84-%EB%B6%84%ED%8F%AC-%ED%99%95%EC%9D%B8",
    "relUrl": "/docs/kaggle/kiva_crowdfunding/#조건별-분포-확인"
  },"91": {
    "doc": "Kiva Crowdfunding 2",
    "title": "Kiva Crowdfunding 2",
    "content": ". | 데이터 정리 | Funds for Philippines . | 분기별 추이 | borrower 성별 | borrower 지역 | repayment interval | 주요 sector, activity, use | ‘use’ 단어 빈도 분석 | . | 데이터 결합해 분석 . | 데이터 정리 | kiva만을 위한 loan 비중 확인 | country, sector별 forkiva 비중 | Field Partner와의 협업 관계 확인 | 국가별 funded amount와 MPI | . | . *분석 대상 데이터셋: Kiva Crowdfunding Data . | 데이터셋 출처 | kiva.org: 세계 각국의 경제적으로 어려운 사람들에게 돈을 빌려주는 online crowdfunding platform | kiva_loans.csv: 2014.01.01~2017.07.26 사이에 kiva에 올라온 loan 정보 데이터. (671,205개의 row) | kiva_mpi_region_locations.csv: 2,772개의 row. 각 지역의 geolocation &amp; poverty level 정보 | loan_theme_ids.csv: 779,092개의 row. 각 loan의 theme에 대한 정보. | loan_theme_by_region.csv: 15,736개의 row. 각 loan theme과 연결된 regional 정보들 | . ",
    "url": "https://chaelist.github.io/docs/kaggle/kiva_crowdfunding2/",
    "relUrl": "/docs/kaggle/kiva_crowdfunding2/"
  },"92": {
    "doc": "Kiva Crowdfunding 2",
    "title": "데이터 정리",
    "content": "# 필요한 라이브러리 import import pandas as pd import numpy as np from matplotlib import pyplot as plt import seaborn as sns import scipy.stats as stats . kiva_df = pd.read_csv('data/kiva_loans.csv') kiva_df.head(3) . |   | id | funded_amount | loan_amount | activity | sector | use | country_code | country | region | currency | partner_id | posted_time | disbursed_time | funded_time | term_in_months | lender_count | tags | borrower_genders | repayment_interval | date | . | 0 | 653051 | 300 | 300 | Fruits &amp; Vegetables | Food | To buy seasonal, fresh fruits to sell. | PK | Pakistan | Lahore | PKR | 247 | 2014-01-01 06:12:39+00:00 | 2013-12-17 08:00:00+00:00 | 2014-01-02 10:06:32+00:00 | 12 | 12 | nan | female | irregular | 2014-01-01 | . | 1 | 653053 | 575 | 575 | Rickshaw | Transportation | to repair and maintain the auto rickshaw used in their business. | PK | Pakistan | Lahore | PKR | 247 | 2014-01-01 06:51:08+00:00 | 2013-12-17 08:00:00+00:00 | 2014-01-02 09:17:23+00:00 | 11 | 14 | nan | female, female | irregular | 2014-01-01 | . | 2 | 653068 | 150 | 150 | Transportation | Transportation | To repair their old cycle-van and buy another one to rent out as a source of income | IN | India | Maynaguri | INR | 334 | 2014-01-01 09:58:07+00:00 | 2013-12-17 08:00:00+00:00 | 2014-01-01 16:01:36+00:00 | 43 | 6 | user_favorite, user_favorite | female | bullet | 2014-01-01 | . | 3 | 653063 | 200 | 200 | Embroidery | Arts | to purchase an embroidery machine and a variety of new embroidery materials. | PK | Pakistan | Lahore | PKR | 247 | 2014-01-01 08:03:11+00:00 | 2013-12-24 08:00:00+00:00 | 2014-01-01 13:00:00+00:00 | 11 | 8 | nan | female | irregular | 2014-01-01 | . | 4 | 653084 | 400 | 400 | Milk Sales | Food | to purchase one buffalo. | PK | Pakistan | Abdul Hakeem | PKR | 245 | 2014-01-01 11:53:19+00:00 | 2013-12-17 08:00:00+00:00 | 2014-01-01 19:18:51+00:00 | 14 | 16 | nan | female | monthly | 2014-01-01 | . | funded_amount: amount disbursed by Kiva to the field agent (USD) | loan_amount: amount disbursed by the field agent to the borrower . | funded_amount &lt; loan_amount인 경우: 48328건 | funded_amount &gt; loan_amount인 경우: 2건 | . | country_code: ISO country code | posted_time: the time at which the loan is posted on Kiva by the field agent | disbursed_time: the time at which the loan is disbursed by the field agent to the borrower | funded_time: the time at which the loan posted to Kiva gets funded by lenders completely . | null값: 48331개 (funded_amount &gt;= loan_amount인 경우 중 3건도 funded_time이 null로 나옴…) | . | term_in_months: the duration for which the loan was disbursed in months | date: posted_time에서 date만 추출한 것. (Kiva에 올라온 날짜) | . | datetime 데이터 정리 . # datetime 형식 변환 kiva_df['posted_time'] = pd.to_datetime(kiva_df['posted_time']) kiva_df['disbursed_time'] = pd.to_datetime(kiva_df['disbursed_time']) kiva_df['funded_time'] = pd.to_datetime(kiva_df['funded_time']) # posted_time -&gt; Month, Quarter 단위로 저장해둠 kiva_df['posted_month'] = kiva_df['posted_time'].dt.strftime('%Y%m') kiva_df['posted_quarter'] = kiva_df['posted_time'].dt.to_period(\"Q\").astype('str') . | borrower type 정리 . ## borrower를 성별 / 단체 유무로 정리 def borrower_type(x): if type(x) != str: borrower = 'N/A' elif x == 'female': borrower = 'female' elif x == 'male': borrower = 'male' else: borr_set = set(x.split(', ')) if 'female' in borr_set: if 'male' in borr_set: borrower = 'mixed_group' else: borrower = 'female_group' else: borrower = 'male_group' return borrower kiva_df['borrower_type'] = kiva_df['borrower_genders'].apply(lambda x: borrower_type(x)) . | . ",
    "url": "https://chaelist.github.io/docs/kaggle/kiva_crowdfunding2/#%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%A0%95%EB%A6%AC",
    "relUrl": "/docs/kaggle/kiva_crowdfunding2/#데이터-정리"
  },"93": {
    "doc": "Kiva Crowdfunding 2",
    "title": "Funds for Philippines",
    "content": ": kiva를 통해 받은 funding 금액이 가장 많은 국가인 Philippines에 대해 집중 탐구 . # philippines 데이터만 따로 추출 ph_df = kiva_df[kiva_df['country'] == 'Philippines'] ph_df.info() . &lt;class 'pandas.core.frame.DataFrame'&gt; Int64Index: 160441 entries, 51 to 670445 Data columns (total 23 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 id 160441 non-null int64 1 funded_amount 160441 non-null float64 2 loan_amount 160441 non-null float64 3 activity 160441 non-null object 4 sector 160441 non-null object 5 use 160361 non-null object 6 country_code 160441 non-null object 7 country 160441 non-null object 8 region 160360 non-null object 9 currency 160441 non-null object 10 partner_id 160441 non-null float64 11 posted_time 160441 non-null datetime64[ns, UTC] 12 disbursed_time 160441 non-null datetime64[ns, UTC] 13 funded_time 157607 non-null datetime64[ns, UTC] 14 term_in_months 160441 non-null float64 15 lender_count 160441 non-null int64 16 tags 94094 non-null object 17 borrower_genders 160361 non-null object 18 repayment_interval 160441 non-null object 19 date 160441 non-null object 20 posted_quarter 160441 non-null object 21 borrower_type 160441 non-null object 22 posted_month 160441 non-null object dtypes: datetime64[ns, UTC](3), float64(4), int64(2), object(14) memory usage: 29.4+ MB . 분기별 추이 . | 분기별 loan count . plt.figure(figsize=(16, 6)) # 2017Q3은 2017.07.26까지의 기록밖에 없으므로 제외하고 시각화 sns.lineplot(data=ph_df[ph_df['posted_quarter'] != '2017Q3'], x='posted_quarter', y='funded_amount', estimator='count', ci=None, color='#C88686') plt.ylim(5000, 18000); . | 분기별 평균 funded_amount . plt.figure(figsize=(16, 6)) sns.lineplot(data=ph_df[ph_df['posted_quarter'] != '2017Q3'], x='posted_quarter', y='funded_amount', color='#C88686') plt.ylim(250, 450); . | 분기별 총 funded_amount . plt.figure(figsize=(16, 6)) sns.lineplot(data=ph_df[ph_df['posted_quarter'] != '2017Q3'], x='posted_quarter', y='funded_amount', estimator='sum', ci=None, color='#C88686') plt.ylim(1500000, 5500000); . | 2014년 이후 post당 평균 funded_amount는 줄고 있지만, 올라온 loan post 수 자체가 계속 증가세를 보임에 따라 총 funded_amount도 증가 추세. | . | . borrower 성별 . | borrower type별 loan count . sns.countplot(data=ph_df, x='borrower_type', palette='pink'); . | 여러 borrower가 그룹으로 빌리는 경우는 없음 (female_group, male_group, mixed_group) | female 혼자 빌리는 경우가 압도적 (전체 loan의 약 95%) | . ph_df.groupby('borrower_type')['funded_amount'].count() . borrower_type N/A 80 female 151984 male 8377 Name: funded_amount, dtype: int64 . | borrower type별 평균 funded amount . sns.barplot(data=ph_df, x='borrower_type', y='funded_amount', palette='pink'); . | 평균 funded amount는 오히려 female이 많은 편은 아님 | . | 분기별 총 funded amount 비교 . plt.figure(figsize=(16, 6)) sns.lineplot(data=ph_df[ph_df['posted_quarter'] != '2017Q3'], x='posted_quarter', y='funded_amount', hue='borrower_type', estimator='sum', ci=None, palette='pink'); . | 총 funded amount 자체도 female이 모든 기간에 가장 압도적. | female borrower를 중심으로 funded_amount가 크게 증가세 | . | . borrower 지역 . | funded_amount Top 10 지역 . ph_region = ph_df.groupby('region')[['funded_amount']].sum().reset_index() ph_region.sort_values(by='funded_amount', ascending=False, inplace=True) # Top 10까지만 시각화 plt.figure(figsize=(7, 5)) sns.barplot(data=ph_region.head(10), x='funded_amount', y='region', palette='pink'); . | Palawan과 Negros Occidental의 지역들이 상위권에 많이 포진되어 있음 | .   . | region의 뒷부분을 따로 ‘province’로 저장 . | region에 적힌 구조가 대체로 ‘Narra, Palawan’처럼 ‘municipality, province’의 구조로 되어 있다고 생각해, 쉼표(,) 뒷부분을 분리해서 저장 | . # 쉼표가 없는 region명이 몇 개인지 확인 print(len(ph_region)) print(len(ph_region[ph_region['region'].str.contains(',')])) . 3638 3249 . → 쉼표로 분리된 region명이 대부분이므로, 쉼표를 기준으로 나누기로 함. # 쉼표(,)를 기준으로 뒷부분 반을 province로 저장하고, 쉼표 없이 하나가 통째로 region명인 경우 그대로 province에 저장. ph_region_temp = ph_region['region'].str.rsplit(',', 1, expand=True) for i in ph_region_temp.index: if ph_region_temp.loc[i, 1]: ## 1이 None이 아니면 실행됨 (None이면 else로) ph_region.loc[i, 'province'] = ph_region_temp.loc[i, 1] else: ph_region.loc[i, 'province'] = ph_region_temp.loc[i, 0] ph_region.head() . |   | region | funded_amount | province | . | 2000 | Narra, Palawan | 1234625.0 | Palawan | . | 2556 | Quezon, Palawan | 1210875.0 | Palawan | . | 583 | Brookes Point, Palawan | 1180100.0 | Palawan | . | 1381 | Kabankalan, Negros Occidental | 1040700.0 | Negros Occidental | . | 1282 | Hinigaran, Negros Occidental | 956950.0 | Negros Occidental | . | province별 funded_amount 정리, 비교 . ph_province = ph_region.groupby('province')[['funded_amount']].sum().reset_index() ph_province.sort_values(by='funded_amount', ascending=False, inplace=True) # Top 10까지만 시각화 plt.figure(figsize=(7, 5)) sns.barplot(data=ph_province.head(10), x='funded_amount', y='province', palette='pink'); . | Negros Occidental이 가장 압도적 | . | 각 province별로, 어떤 sector의 funded_amount가 많은지 확인 . # ph_df에 'province' 칼럼를 더해줌 ph_df = pd.merge(ph_df, ph_region[['region', 'province']], how='left', on='region') . # funded amount Top 10 province만 따로 저장 top_countries = ph_df.groupby('province')[['funded_amount']].sum().sort_values(by='funded_amount', ascending=False).head(10).index # funded amount 내림차순으로 sector 순서 정렬 sectors_order = ph_df.groupby('sector')[['funded_amount']].sum().sort_values(by='funded_amount', ascending=False).index loans_province_sector = pd.pivot_table(ph_df, index='province', columns='sector', values='funded_amount', fill_value=0, aggfunc='sum') loans_province_sector = loans_province_sector[loans_province_sector.index.isin(top_countries)] # top 10 province만 대상으로 자름 loans_province_sector = loans_province_sector[sectors_order] # 전체 loan amount가 가장 높은 sector부터 내림차순 정렬 loans_province_sector = loans_province_sector.T loans_province_sector = loans_province_sector[top_countries] # 전체 loan amount가 가장 높은 province부터 내림차순 정렬 # heatmap으로 시각화 plt.figure(figsize=(14, 9)) sns.heatmap(loans_province_sector, annot=True, cmap='pink_r', fmt='.0f') plt.xticks(rotation=40); . | Negros Occidental은 Retail sector로 빌린 금액이 가장 많음 | Palawan은 Retail과 Food sector가 비슷하게 많음 | Negros Occidental과 Negros Oriental은 Agriculture sector로 빌린 금액도 많은 편 | . | . repayment interval . | repayment interval별 총 funded_amount . sns.barplot(data=ph_df, x='repayment_interval', y='funded_amount', estimator=np.sum, ci=None, palette='pink'); . | repayment interval별 평균 funded_amount . sns.barplot(data=ph_df, x='repayment_interval', y='funded_amount', palette='pink'); . | repayment interval별 loan 수 . sns.countplot(data=ph_df, x='repayment_interval', palette='pink'); . | irregular 상환 방식의 loan이 가장 많음 | irregular 상환 방식의 loan이 평균 funded amount가 많은 편은 아니지만, 수 자체가 압도적이기 때문에 전체 funded amount 중 irregular 상환 방식인 금액이 압도적 | 전체 kiva loan 중에서는 monthly로 상환되는 금액이 가장 많은 것을 생각하면, irregular 상환 금액이 많다는 것은 Philippines의 특징인 듯 | . | 분기별 repayment interval별 funded amount . quarter_interval = ph_df.groupby(['posted_quarter', 'repayment_interval'])[['funded_amount']].sum().reset_index() quarter_interval = quarter_interval[quarter_interval['posted_quarter'] != '2017Q3'] plt.figure(figsize=(16, 6)) sns.lineplot(data=quarter_interval, x='posted_quarter', y='funded_amount', hue='repayment_interval', palette='pink', hue_order=['irregular', 'monthly', 'bullet']) # legend를 box 밖으로 빼 줌 plt.legend(bbox_to_anchor=(1.01, 1), borderaxespad=0); . | irregular 상환 방식의 금액이 가장 많이 증가하는 추세 | irregular 상환보다는 monthly 상환이 더 안정적인 자금 계획에 유리하므로, irregular 상환 방식보다는 monthly 상환 방식을 늘리는 것이 좋다고 생각됨 | . | . 주요 sector, activity, use . | sector별 funded amount . # funded amount 순으로 내림차순 정렬해 시각화 ph_sectors = ph_df.groupby('sector')[['funded_amount']].sum().sort_values(by='funded_amount', ascending=False).reset_index() plt.figure(figsize=(12, 5)) sns.barplot(data=ph_sectors, x='sector', y='funded_amount', palette='pink') plt.xticks(rotation=40); . | Retail, Food, Agriculture 관련 funded amount가 가장 많음 | . | funded amount Top 10 activities . ph_activities = ph_df.groupby(['activity', 'sector'])[['funded_amount']].sum().sort_values(by='funded_amount', ascending=False).reset_index() ph_activities.head(10) . |   | activity | sector | funded_amount | . | 0 | General Store | Retail | 15025550.0 | . | 1 | Pigs | Agriculture | 6039975.0 | . | 2 | Farming | Agriculture | 4063150.0 | . | 3 | Fishing | Food | 3129675.0 | . | 4 | Fish Selling | Food | 2969850.0 | . | 5 | Food Production/Sales | Food | 2659425.0 | . | 6 | Personal Housing Expenses | Housing | 1710425.0 | . | 7 | Fruits &amp; Vegetables | Food | 1563675.0 | . | 8 | Retail | Retail | 1366075.0 | . | 9 | Motorcycle Transport | Transportation | 1067450.0 | . → Top 10 activities 시각화 . plt.figure(figsize=(7, 5)) sns.barplot(data=ph_activities.head(10), x='funded_amount', y='activity', palette='pink'); . | ‘Retail’ sector의 ‘General Store’ 목적의 funded amount가 압도적으로 많고, 그 다음은 오히려 ‘Agriculture’ sector의 ‘Pigs’와 ‘Farming’ 목적의 funded amount가 많음 | 여러 섬으로 구성된 국가인만큼, ‘Food’ sector 중에서는 ‘Fishing’과 ‘Fish Selling’ 목적의 funded amount가 많음 | . | funded amount Top 10 use . ph_use = ph_df.groupby(['use'])[['funded_amount']].sum().sort_values(by='funded_amount', ascending=False).reset_index() plt.figure(figsize=(7, 5)) sns.barplot(data=ph_use.head(10), x='funded_amount', y='use', palette='pink'); . | 같은 내용이라도 사용한 단어에 따라 다르게 집계되므로 정확히 파악하기 어렵지만, ‘grocery’, ‘pig’, ‘food production business’ 등의 단어들이 눈에 띔 | . | ‘General Store’ 관련 Top 10 use . # 우선, activity, sector, use별 funded amount를 정리 ph_use_activity = ph_df.groupby(['use', 'activity', 'sector'])[['funded_amount']].sum().sort_values(by='funded_amount', ascending=False).reset_index() ph_use_activity.head() . |   | use | activity | sector | funded_amount | . | 0 | to purchase more groceries to sell. | General Store | Retail | 501400.0 | . | 1 | to buy feed and other supplies to raise her pigs. | Pigs | Agriculture | 431850.0 | . | 2 | to buy fertilizers and other farm supplies. | Farming | Agriculture | 427725.0 | . | 3 | to buy feed and vitamins for her pigs. | Pigs | Agriculture | 374800.0 | . | 4 | to buy ingredients for her food production business | Food Production/Sales | Food | 344125.0 | . → ‘General Store’ Top 10 use 시각화 . plt.figure(figsize=(7, 5)) sns.barplot(data=ph_use_activity[ph_use_activity['activity'] == 'General Store'].head(10), x='funded_amount', y='use', palette='pink'); . | 같은 내용이라도 사용한 단어에 따라 다르게 집계되므로 정확히 파악하기 어렵지만, General Store를 위한 fund로는 ‘grocery 구매’ 관련 목적이 많다고 생각됨 | . | . ‘use’ 단어 빈도 분석 . | ‘use’는 같은 내용이라도 단어 사용 등에 따라 다르게 집계되므로, 단순 집계 후 시각화보다 단어별로 쪼개서 파악하는 것이 도움이 될 수 있다고 생각 | 사유를 단어별로 나눠서 분석해, 어떤 단어가 빈번하게 등장하는지 확인하고, 이로부터 필리핀의 전반적인 loan needs를 파악 | . ## 단어 빈도 계산 &amp; wordcloud 생성 &amp; 빈번하게 사용된 단어 bar graph로 시각화하는 코드를 미리 함수로 다 만들어 둠 ## 각 loan의 funded amount는 고려하지 않고, count만 고려 import nltk from wordcloud import WordCloud from collections import Counter # wordcloud mask x, y = np.ogrid[:1200, :1200] mask = (x - 600) ** 2 + (y - 600) ** 2 &gt; 520 ** 2 mask = 255 * mask.astype(int) def count_words(sector_name): # Text Cleaning &amp; Case Conversion &amp; Tokenization words = [] for content in ph_df[ph_df['sector'] == sector_name]['use']: content_cleaned = str(content).replace('.', '').replace(',','').strip().lower() # ., 정도만 제거, strip, lower words.extend(content_cleaned.split()) # 간단하게 공백 기준으로만 쪼개줌 # POS Tagging tokens_pos = nltk.pos_tag(words) # Noun, Verb만 추출 &amp; Lemmatization wlem = nltk.WordNetLemmatizer() lemm_words = [] for word, pos in tokens_pos: if ('NN' in pos) | ('VB' in pos): lemm_words.append(wlem.lemmatize(word)) # Counter로 빈도 계산 c = Counter(lemm_words) return c def generate_wordcloud(c): # generate wordcloud wordcloud = WordCloud(relative_scaling=.2, background_color='white', colormap='pink', mask=mask).generate_from_frequencies(c) plt.figure(figsize=(7, 7)) plt.imshow(wordcloud) plt.axis('off') def visualize_top10_words(c): word_count = pd.DataFrame.from_dict(c, orient='index', columns=['count']).reset_index().rename(columns={'index':'word'}) word_count.sort_values(by='count', ascending=False, inplace=True) plt.figure(figsize=(6, 4)) sns.barplot(data=word_count.head(10), x='count', y='word', palette='pink') . | Retail sector의 use . retail_c = count_words('Retail') generate_wordcloud(retail_c) . | canned goods, grocery 등의 item들을 사기 위한 목적이 많다고 파악됨 | . visualize_top10_words(retail_c) . | Food sector의 use . food_c = count_words('Food') generate_wordcloud(food_c) . | ‘food’ sector도 단순히 자신의 식량을 사기 위한 것보다, ‘food production business’의 재료 구매 &amp; 판매할 식료품 구매 목적이 많다고 판단됨 | . visualize_top10_words(food_c) . | Agriculture sector의 use . agri_c = count_words('Agriculture') generate_wordcloud(agri_c) . | pig를 위한 식량 등을 구매하는 목적이 많고, 그 다음으로는 농업을 위한 fertilizer 등의 구매 목적도 많다고 판단됨 | 돼지 농장이 필리핀의 주요 business 중 하나라고 추정됨 | . visualize_top10_words(agri_c) . | . ",
    "url": "https://chaelist.github.io/docs/kaggle/kiva_crowdfunding2/#funds-for-philippines",
    "relUrl": "/docs/kaggle/kiva_crowdfunding2/#funds-for-philippines"
  },"94": {
    "doc": "Kiva Crowdfunding 2",
    "title": "데이터 결합해 분석",
    "content": "데이터 정리 . | kiva_mpi_region_locations.csv . location_df = pd.read_csv('data/kiva_mpi_region_locations.csv') location_df.head(3) . |   | LocationName | ISO | country | region | world_region | MPI | geo | lat | lon | . | 0 | Badakhshan, Afghanistan | AFG | Afghanistan | Badakhshan | South Asia | 0.387 | (36.7347725, 70.81199529999999) | 36.7348 | 70.812 | . | 1 | Badghis, Afghanistan | AFG | Afghanistan | Badghis | South Asia | 0.466 | (35.1671339, 63.7695384) | 35.1671 | 63.7695 | . | 2 | Baghlan, Afghanistan | AFG | Afghanistan | Baghlan | South Asia | 0.3 | (35.8042947, 69.2877535) | 35.8043 | 69.2878 | . | ISO: unique ID for country | MPI: multidimensional poverty index – 수치가 높을수록 poverty level이 높은 것 | . → 중복값 제거 . print('중복 제거 이전: ', len(location_df)) location_df.drop_duplicates(inplace=True, ignore_index=True) print('중복 제거 이후: ', len(location_df)) . 중복 제거 이전: 2772 중복 제거 이후: 1009 . → LocationName이 null값인 행 제외 (LocationName이 null이면 region, MPI도 다 null임) . print('결측치 제거 이전: ', len(location_df)) location_df = location_df[~location_df['LocationName'].isna()] location_df.reset_index(inplace=True, drop=True) print('결측치 제거 이후: ', len(location_df)) . 결측치 제거 이전: 1009 결측치 제거 이후: 984 . | loan_theme_ids.csv . loan_themes_df = pd.read_csv('data/loan_theme_ids.csv') loan_themes_df.head(3) . |   | id | Loan Theme ID | Loan Theme Type | Partner ID | . | 0 | 638631 | a1050000000skGl | General | 151 | . | 1 | 640322 | a1050000000skGl | General | 151 | . | 2 | 641006 | a1050000002X1ij | Higher Education | 160 | . → id 제외하고 다 null값인 경우 삭제 (Loan Theme ID가 null이면 Loan Theme Type, Partner ID도 다 null) . print('결측치 제거 이전: ', len(loan_themes_df)) loan_themes_df = loan_themes_df[~loan_themes_df['Loan Theme ID'].isna()] loan_themes_df.reset_index(inplace=True, drop=True) print('결측치 제거 이후: ', len(loan_themes_df)) . | loan_themes_by_region.csv . themes_region_df = pd.read_csv('data/loan_themes_by_region.csv') themes_region_df.head(3) . |   | Partner ID | Field Partner Name | sector | Loan Theme ID | Loan Theme Type | country | forkiva | region | geocode_old | ISO | number | amount | LocationName | geocode | names | geo | lat | lon | mpi_region | mpi_geo | rural_pct | . | 0 | 9 | KREDIT Microfinance Institution | General Financial Inclusion | a1050000000slfi | Higher Education | Cambodia | No | Banteay Meanchey | (13.75, 103.0) | KHM | 1 | 450 | Banteay Meanchey, Cambodia | [(13.6672596, 102.8975098)] | Banteay Meanchey Province; Cambodia | (13.6672596, 102.8975098) | 13.6673 | 102.898 | Banteay Mean Chey, Cambodia | (13.6672596, 102.8975098) | 90 | . | 1 | 9 | KREDIT Microfinance Institution | General Financial Inclusion | a10500000068jPe | Vulnerable Populations | Cambodia | No | Battambang Province | nan | KHM | 58 | 20275 | Battambang Province, Cambodia | [(13.0286971, 102.989615)] | Battambang Province; Cambodia | (13.0286971, 102.989615) | 13.0287 | 102.99 | Banteay Mean Chey, Cambodia | (13.6672596, 102.8975098) | 90 | . | 2 | 9 | KREDIT Microfinance Institution | General Financial Inclusion | a1050000000slfi | Higher Education | Cambodia | No | Battambang Province | nan | KHM | 7 | 9150 | Battambang Province, Cambodia | [(13.0286971, 102.989615)] | Battambang Province; Cambodia | (13.0286971, 102.989615) | 13.0287 | 102.99 | Banteay Mean Chey, Cambodia | (13.6672596, 102.8975098) | 90 | . | forkiva: Was this loan theme created specifically for Kiva? | geocode_old: 옛날 geocoding system | ISO: unique ID for country | number: Number of loans funded in this LocationName and this loan theme | amount: Dollar value of loans funded in this LocationName and this loan theme | LocationName: “{region}, {country}” | names: All placenames that the Gmaps API associates with LocationName | mpi_region: MPI Region where we think this loan theme is located | mpi_geo: Lat-Lon pair where we think this MPI region is located | rural_pct: The percentage of this field partners’ borrowers that are in rural areas | . | . kiva만을 위한 loan 비중 확인 . | kiva_df와 loan_themes_df를 merge . kiva_loan_themes = kiva_df.copy() kiva_loan_themes.drop(['loan_amount', 'country_code', 'disbursed_time', 'tags', 'date', 'borrower_genders'], axis='columns', inplace=True) kiva_loan_themes = pd.merge(kiva_loan_themes, loan_themes_df, how='left', on='id') kiva_loan_themes.head(3) . |   | id | funded_amount | activity | sector | use | country | region | currency | partner_id | posted_time | funded_time | term_in_months | lender_count | repayment_interval | posted_month | posted_quarter | borrower_type | Loan Theme ID | Loan Theme Type | Partner ID | . | 0 | 653051 | 300 | Fruits &amp; Vegetables | Food | To buy seasonal, fresh fruits to sell. | Pakistan | Lahore | PKR | 247 | 2014-01-01 06:12:39+00:00 | 2014-01-02 10:06:32+00:00 | 12 | 12 | irregular | 201401 | 2014Q1 | female | nan | nan | nan | . | 1 | 653053 | 575 | Rickshaw | Transportation | to repair and maintain the auto rickshaw used in their business. | Pakistan | Lahore | PKR | 247 | 2014-01-01 06:51:08+00:00 | 2014-01-02 09:17:23+00:00 | 11 | 14 | irregular | 201401 | 2014Q1 | female_group | a1050000000sjEC | Underserved | 247 | . | 2 | 653068 | 150 | Transportation | Transportation | To repair their old cycle-van and buy another one to rent out as a source of income | India | Maynaguri | INR | 334 | 2014-01-01 09:58:07+00:00 | 2014-01-01 16:01:36+00:00 | 43 | 6 | bullet | 201401 | 2014Q1 | female | a1050000002VkWz | Underserved | 334 | . → 결합 후의 null값 파악 . kiva_loan_themes.info() . &lt;class 'pandas.core.frame.DataFrame'&gt; Int64Index: 671205 entries, 0 to 671204 Data columns (total 17 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 id 671205 non-null int64 1 funded_amount 671205 non-null float64 2 activity 671205 non-null object 3 sector 671205 non-null object 4 country 671205 non-null object 5 region 614405 non-null object 6 currency 671205 non-null object 7 partner_id 657698 non-null float64 8 posted_time 671205 non-null datetime64[ns, UTC] 9 funded_time 622874 non-null datetime64[ns, UTC] 10 term_in_months 671205 non-null float64 11 lender_count 671205 non-null int64 12 borrower_genders 666984 non-null object 13 repayment_interval 671205 non-null object 14 Loan Theme ID 657692 non-null object 15 Loan Theme Type 657692 non-null object 16 Partner ID 657692 non-null float64 dtypes: datetime64[ns, UTC](2), float64(4), int64(2), object(9) memory usage: 92.2+ MB . | 위에서 만든 kiva_loan_themes에 themes_region_df를 merge . temp1 = kiva_loan_themes[['funded_amount', 'posted_quarter', 'Loan Theme ID', 'borrower_type', 'activity', 'sector', 'use', 'country']] temp2 = themes_region_df[['Loan Theme ID', 'Loan Theme Type', 'forkiva', 'Partner ID', 'Field Partner Name']] temp2.drop_duplicates(inplace=True) kiva_themes_df = pd.merge(temp1, temp2, how='left', on='Loan Theme ID') kiva_themes_df.head(3) . |   | funded_amount | posted_quarter | Loan Theme ID | borrower_type | activity | sector | use | country | Loan Theme Type | forkiva | Partner ID | Field Partner Name | . | 0 | 300 | 2014Q1 | nan | female | Fruits &amp; Vegetables | Food | To buy seasonal, fresh fruits to sell. | Pakistan | nan | nan | nan | nan | . | 1 | 575 | 2014Q1 | a1050000000sjEC | female_group | Rickshaw | Transportation | to repair and maintain the auto rickshaw used in their business. | Pakistan | Underserved | No | 247 | BRAC Pakistan | . | 2 | 150 | 2014Q1 | a1050000002VkWz | female | Transportation | Transportation | To repair their old cycle-van and buy another one to rent out as a source of income | India | Underserved | Yes | 334 | Belghoria Janakalyan Samity | . → 결합 후의 null값 파악 . kiva_themes_df.info() . &lt;class 'pandas.core.frame.DataFrame'&gt; Int64Index: 671205 entries, 0 to 671204 Data columns (total 11 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 funded_amount 671205 non-null float64 1 posted_quarter 671205 non-null object 2 Loan Theme ID 657692 non-null object 3 borrower_type 671205 non-null object 4 activity 671205 non-null object 5 sector 671205 non-null object 6 country 671205 non-null object 7 Loan Theme Type 629021 non-null object 8 forkiva 629021 non-null object 9 Partner ID 629021 non-null float64 10 Field Partner Name 629021 non-null object dtypes: float64(2), object(9) memory usage: 61.5+ MB . | kiva loan 중 kiva만을 위한 loan이 어느 정도인지 파악 . | forkiva=Yes면 kiva만을 위해 구축한 loan이라는 의미 | . sns.countplot(data=kiva_themes_df, x='forkiva', palette='pink'); . | kiva만을 위한 loan과 아닌 loan의 평균 funded amount 비교 . sns.barplot(data=kiva_themes_df, x='forkiva', y='funded_amount', palette='pink'); . | forkiva=No인 loan이 압도적으로 많지만, 평균 funded amount 자체는 별 차이가 없음 | . | kiva만을 위한 loan과 아닌 loan의 분기별 총 funded amount 추이 . temp = kiva_themes_df.groupby(['posted_quarter', 'forkiva'])[['funded_amount']].sum().reset_index() plt.figure(figsize=(16, 6)) sns.lineplot(data = temp[temp['posted_quarter'] != '2017Q3'], x='posted_quarter', y='funded_amount', hue='forkiva', palette='pink'); . | kiva만을 위한 loan과 그렇지 않은 loan 모두 funded amount가 증가 추세에 있다 | . | . country, sector별 forkiva 비중 . | kiva만을 위한 loan 비중이 많은 country 확인 . # country별 forkiva=yes인 비중: 총 funded amount를 기준으로 계산 country_forkiva = pd.pivot_table(df, index='country', columns='forkiva', values='funded_amount', fill_value=0, aggfunc='sum') country_forkiva['forkiva_ratio(%)'] = (country_forkiva['Yes'] / (country_forkiva['No'] + country_forkiva['Yes'])) * 100 country_forkiva.sort_values(by='forkiva_ratio(%)', ascending=False, inplace=True) country_forkiva.head(10) . | country | No | Yes | forkiva_ratio(%) | . | Belize | 0 | 63150 | 100.00 | . | Chile | 0 | 38525 | 100.00 | . | Georgia | 105875 | 3263625 | 96.86 | . | Lao People’s Democratic Republic | 59000 | 1089050 | 94.86 | . | Moldova | 58825 | 628025 | 91.44 | . | Myanmar (Burma) | 292925 | 2742925 | 90.35 | . | Egypt | 428750 | 656175 | 60.48 | . | Mongolia | 532425 | 676950 | 55.98 | . | Zimbabwe | 1690075 | 1365500 | 44.69 | . | India | 2932575 | 2312850 | 44.09 | . → forkiva=yes인 비중이 0보다 큰 국가만 시각화 . plt.figure(figsize=(9, 9)) sns.barplot(data=country_forkiva[country_forkiva['forkiva_ratio(%)'] &gt; 0].reset_index(), x='forkiva_ratio(%)', y='country', palette='pink'); . | Belize, Chile는 지원받은 금액의 100%가 kiva만을 위해 구축된 loan | Georgia, Lao People’s Democratic Republic, Moldova, Myanmar는 90% 이상의 금액이 kiva만을 위한 loan으로 지원받은 것 | . | kiva만을 위한 loan 비중이 많은 sector 확인 . sector_forkiva = pd.pivot_table(kiva_themes_df, index='sector', columns='forkiva', values='funded_amount', fill_value=0, aggfunc='sum') sector_forkiva['forkiva_ratio(%)'] = (sector_forkiva['Yes'] / (sector_forkiva['No'] + sector_forkiva['Yes'])) * 100 sector_forkiva.sort_values(by='forkiva_ratio(%)', ascending=False, inplace=True) # Sector별 forkiva 비중을 시각화 plt.figure(figsize=(9, 6)) sns.barplot(data=sector_forkiva.reset_index(), x='forkiva_ratio(%)', y='sector', palette='pink'); . | ‘Personal Use’ sector는 funded amount의 약 43%가 kiva만을 위한 단독 loan으로 지원받은 것 | . | kiva만을 위한 loan 중, ‘Personal Use’ sector의 구체적인 use를 파악 . temp = kiva_themes_df[(kiva_themes_df['forkiva'] == 'Yes') &amp; (kiva_themes_df['sector'] == 'Personal Use')] temp.groupby('use')[['funded_amount']].sum().sort_values(by='funded_amount', ascending=False).head() . | use | funded_amount | . | to buy a water filter to provide safe drinking water for their family. | 805125 | . | to buy a water filter to provide safe drinking water for her family. | 628150 | . | to purchase TerraClear water filters so they can have access to safe drinking water. | 566900 | . | To buy a water filter to provide safe drinking water for their family. | 253300 | . | to buy a water filter to provide safe drinking water for his family. | 200375 | . → use에 등장하는 단어를 wordcloud로 시각화 . # Text Cleaning &amp; Case Conversion &amp; Tokenization words = [] for content in kiva_themes_df[(kiva_themes_df['forkiva'] == 'Yes') &amp; (kiva_themes_df['sector'] == 'Personal Use')]['use']: content_cleaned = str(content).replace('.', '').replace(',','').strip().lower() # ., 정도만 제거, strip, lower words.extend(content_cleaned.split()) # 간단하게 공백 기준으로만 쪼개줌 # POS Tagging tokens_pos = nltk.pos_tag(words) # Noun, Verb만 추출 &amp; Lemmatization wlem = nltk.WordNetLemmatizer() lemm_words = [] for word, pos in tokens_pos: if ('NN' in pos) | ('VB' in pos): lemm_words.append(wlem.lemmatize(word)) # Counter로 빈도 계산 c = Counter(lemm_words) generate_wordcloud(c) # 위에서 만들어둔 워드클라우드 만드는 함수 . +) Top 10 단어 bar graph로 시각화 . visualize_top10_words(c) # 위에서 만들어둔 top 10 단어 bargraph 만드는 함수 . | drinking water 관련 loan이 많다고 판단됨 | 식수도 관련 타 loan / 사업을 끌어오면 kiva 100% loan을 구축하지 않고도 다른 방법으로 금전적 지원이 가능할 수 있다고 생각됨 | . | . Field Partner와의 협업 관계 확인 . # funded amount순으로 Field Partner Name 확인 loans_partner = kiva_themes_df.groupby('Field Partner Name')[['funded_amount']].sum().reset_index() loans_partner.sort_values(by='funded_amount', ascending=False, inplace=True) plt.figure(figsize=(9, 6)) sns.barplot(data=loans_partner.head(10), x='funded_amount', y='Field Partner Name', palette='pink') plt.xticks(rotation=60); . | Negros Women for Tomorrow Foundation (NWTF)가 가장 많은 funded amount의 자금 융통을 담당하는 field partner | . +) NWTF가 주로 담당하는 지역 확인: . temp = kiva_themes_df[kiva_themes_df['Field Partner Name'] == 'Negros Women for Tomorrow Foundation (NWTF)'] temp.groupby('country')[['funded_amount']].sum().sort_values(by='funded_amount', ascending=False).head(3) . | country | funded_amount | . | Philippines | 33831500.0 | . | Kenya | 25.0 | . | Bolivia | 0.0 | . | NWTF는 Philippines에서 주로 활동하는 field partner인 것으로 확인됨 | . 국가별 funded amount와 MPI . | 국가별 총 funded amount와 평균 MPI 지수를 비교 | . | region 기준으로 merge가 가능할지 탐색 . kiva_df_region_set = set(kiva_df['region']) location_df_region_set = set(location_df['region']) print('kiva_df: ', len(kiva_df_region_set)) print('location_df: ', len(location_df_region_set)) print('겹치는 region: ', len(kiva_df_region_set &amp; location_df_region_set)) . kiva_df: 12696 location_df: 838 겹치는 region: 129 . | 겹치는 region이 너무 적어서 단순 merge는 불가… | . | country 기준으로 총 funded amount, 평균 MPI 지수를 각각 groupby 후 merge . kiva_df_groupby = kiva_df.groupby('country')[['funded_amount', 'loan_amount']].sum().reset_index() location_df_groupby = location_df.groupby(['country', 'world_region'])[['MPI']].mean().reset_index() groupby_merged = pd.merge(kiva_df_groupby, location_df_groupby, how='inner', on='country') print(len(groupby_merged)) groupby_merged.head() . |   | country | funded_amount | loan_amount | world_region | MPI | . | 0 | Afghanistan | 14000 | 14000 | South Asia | 0.309853 | . | 1 | Belize | 114025 | 114025 | Latin America and Caribbean | 0.0201429 | . | 2 | Benin | 516825 | 518950 | Sub-Saharan Africa | 0.320333 | . | 3 | Bhutan | 15625 | 20000 | South Asia | 0.123474 | . | 4 | Brazil | 661025 | 662200 | Latin America and Caribbean | 0.0272593 | . → 국가별 총 funded_amount와 평균 MPI 간의 관계 시각화 . plt.figure(figsize=(12, 8)) sns.scatterplot(data=groupby_merged, x='funded_amount', y='MPI', hue='world_region', style='world_region', palette='Set2', s=100); . | 평균 MPI가 높은 국가일수록 funded amount가 많을 거라고 예상했지만, 그런 관계는 발견되지 않음 | 오히려 평균 MPI가 높은 (= poverty level이 높은) Sub-Shararan Africa의 국가들 중 상위 8개 국가는 funded amount가 많지 않은 편 | . | . ",
    "url": "https://chaelist.github.io/docs/kaggle/kiva_crowdfunding2/#%EB%8D%B0%EC%9D%B4%ED%84%B0-%EA%B2%B0%ED%95%A9%ED%95%B4-%EB%B6%84%EC%84%9D",
    "relUrl": "/docs/kaggle/kiva_crowdfunding2/#데이터-결합해-분석"
  },"95": {
    "doc": "빈도 분석 (한글)",
    "title": "빈도 분석 (한글)",
    "content": ". | 사용할 text 수집 | 전처리 (Preprocessing) . | Text Cleaning | Tokenization + Lemmatization + POS tagging | Stopwords 제거 | . | 빈도 분석 . | Counter | WordCloud | . | . ",
    "url": "https://chaelist.github.io/docs/text_analysis/korean_text/",
    "relUrl": "/docs/text_analysis/korean_text/"
  },"96": {
    "doc": "빈도 분석 (한글)",
    "title": "사용할 text 수집",
    "content": "import requests from bs4 import BeautifulSoup url = 'https://news.joins.com/article/23904235' r = requests.get(url) soup = BeautifulSoup(r.text, 'lxml') title = soup.select_one('#article_title').text.strip() content = soup.select_one('#article_body').text.strip() print(title, '\\n') print(content) . [강찬수의 에코사이언스] 해양 플라스틱 쓰레기에 붙은 엄청난 세균이 건강 위협한다 강찬수 환경전문기자 택배 노동자들이 잇따라 숨지고 있다. 전국 택배연대 노조 등에 따르면 이달에만 CJ대한통운 서울 강북지점, 한진택배 서울 동대문지사, 경북 칠곡 쿠팡 물류센터에서 일했던 노동자가 연달아 숨졌다. 배달 물량 급증으로 인한 과로사라는 지적이 쏟아진다. 실제로 국토교통부가 국회에 제출한 자료에 따르면, 2020년 6월 생활물류 택배 물동량은 2억 9340만 개로 지난해 6월보다 36.3% 늘었다. (생략) . ",
    "url": "https://chaelist.github.io/docs/text_analysis/korean_text/#%EC%82%AC%EC%9A%A9%ED%95%A0-text-%EC%88%98%EC%A7%91",
    "relUrl": "/docs/text_analysis/korean_text/#사용할-text-수집"
  },"97": {
    "doc": "빈도 분석 (한글)",
    "title": "전처리 (Preprocessing)",
    "content": "  . *전처리 과정 (한글): (영어와 비교) . | Text Cleaning . | case conversion은 필요 없다 | . | Tokenization + Lemmatization + POS tagging . | 한글의 경우, KoNLPy의 형태소 분석기를 통해 3가지 작업을 한번에 수행 (.pos() 함수 이용) | . | 원하는 품사의 단어들만 선택 | Stopwords Removal | . Text Cleaning . # 불필요한 기호 없애기 - 정규식 사용 import re cleaned_content = re.sub('[^,.?!\\w\\s]','', content) ## ,.?!와 문자+숫자+_(\\w)와 공백(\\s)만 남김 # 추가로 .replace를 활용해 불필요한 단어/기호를 없애준다 cleaned_content = cleaned_content.replace('\\xa0', '').replace('강찬수', '').replace('연합뉴스', '').replace('환경전문기자', '') cleaned_content . 택배 노동자들이 잇따라 숨지고 있다. 전국 택배연대 노조 등에 따르면 이달에만 CJ대한통운 서울 강북지점, 한진택배 서울 동대문지사, 경북 칠곡 쿠팡 물류센터에서 일했던 노동자가 연달아 숨졌다. 배달 물량 급증으로 인한 과로사라는 지적이 쏟아진다. 실제로 국토교통부가 국회에 제출한 자료에 따르면, 2020년 6월 생활물류 택배 물동량은 2억 9340만 개로 지난해 6월보다 36.3 늘었다. 신종 코로나바이러스 감염증코로나19 탓에 시민들이 외출과 매장 쇼핑을 꺼린 탓이다. 1회용 마스크 재질이 플라스틱먼지 치솟으면 음식 배달도 늘어낙동강 물고기에도 미세플라스틱다회용기 사용 등 대안모델 필요 배달음식 주문도 늘었다. (생략) . Tokenization + Lemmatization + POS tagging . *‘KoNLPy’의 ‘Kkma’ Class 활용 . | KoNLPy: 한국어 NLP를 위한 Python 패키지. | 설치해야 사용 가능: https://konlpy-ko.readthedocs.io/ko/v0.4.3/install/ | . | Kkma: KoNLPy에서 제공하는 한글 형태소분석기 중 하나. | 다른 Class와의 비교: https://konlpy.org/ko/v0.5.2/morph/ | . | . from konlpy.tag import Kkma # import해서 사용 kkma = Kkma() NN_words = [] kkma_pos = kkma.pos(cleaned_content) ## 하나의 string을 input으로 받는다 for word, pos in kkma_pos: if 'NN' in pos: ## 명사만 골라냄 NN_words.append(word) print(NN_words) . ['택배', '노동자', '전국', '택배', '연대', '노조', '등', '이달', '통운', '서울', '강북', '지점', '한진', '택배', '서울', '동대문', '지사', '경북', '곡', '물류', '센터', '노동자', '배달', '물량', '급증', '과로', '사', '지적', '국토', '교통부', '국회', '제출', (생략)] . Stopwords 제거 . # 직접 만든 불용어 사전 활용 customized_stopwords = ['것', '등', '탓', '바', '용', '년', '개', '당', '면', '말'] unique_NN_words = set(NN_words) for word in unique_NN_words: if word in customized_stopwords: while word in NN_words: NN_words.remove(word) . ",
    "url": "https://chaelist.github.io/docs/text_analysis/korean_text/#%EC%A0%84%EC%B2%98%EB%A6%AC-preprocessing",
    "relUrl": "/docs/text_analysis/korean_text/#전처리-preprocessing"
  },"98": {
    "doc": "빈도 분석 (한글)",
    "title": "빈도 분석",
    "content": "Counter . from collections import Counter c = Counter(NN_words) ## 단어 개수를 세어준다 print(c) . Counter({'플라스틱': 28, '미세': 13, '쓰레기': 12, '배달': 8, '연구': 8, '음식': 7, '톤': 7, '택배': 6, '팀': 6, '코로나': 5, '생산': 5, '발표': 5, '논문': 5, '노동자': 4, '마스크': 4, '먼지': 4, '물고기': 4, '최근': 4, '중': 4, '바다': 4, '지난해': 3, '국내': 3, '상황': 3, '환경': 3, (생략)}) . → 가장 많이 나온 단어 10개 추출 . print(c.most_common(10)) . [('플라스틱', 28), ('미세', 13), ('쓰레기', 12), ('배달', 8), ('연구', 8), ('음식', 7), ('톤', 7), ('택배', 6), ('팀', 6), ('코로나', 5)] . WordCloud . | 한글로 워드클라우드를 그릴 때는 font path를 지정해줘야 한다. | . from wordcloud import WordCloud import matplotlib.pyplot as plt total_words = '' for word in NN_words: total_words = total_words+' '+word wordcloud = WordCloud(font_path='c:/Windows/Fonts/malgun.ttf', # 한글을 출력하려면 font_path를 지정해줘야 한다 max_words=200, relative_scaling = 0.5, background_color='white', colormap='summer').generate(total_words) plt.imshow(wordcloud) plt.axis('off') plt.show() . +) 원하는 이미지로 워드클라우드 만들기 . | mask로 사용할 이미지 불러오기 from PIL import Image import matplotlib.pyplot as plt import numpy as np icon = Image.open('green.png') # mask로 사용할 이미지 불러오기 plt.imshow(icon) mask = Image.new(\"RGB\", icon.size, (255,255,255)) mask.paste(icon,icon) mask = np.array(mask) . | mask=mask 옵션 추가해서 워드클라우드 그리기 from wordcloud import WordCloud, ImageColorGenerator import matplotlib.pyplot as plt total_words = '' for word in NN_words: total_words = total_words+' '+word wordcloud = WordCloud(font_path='c:/Windows/Fonts/malgun.ttf', max_words=200, background_color='white', mask=mask).generate(total_words) plt.figure(figsize=(7,7)) #액자사이즈설정 plt.axis('off') #테두리 선 없애기 ## mask의 색으로 단어들의 색을 칠해줌 - 이 부분을 생략하면 마스크 모양대로 워드클라우드를 만들어주되, 색은 기본색으로 나온다. image_colors = ImageColorGenerator(mask) plt.imshow(wordcloud.recolor(color_func=image_colors),interpolation=\"bilinear\") . | . ",
    "url": "https://chaelist.github.io/docs/text_analysis/korean_text/#%EB%B9%88%EB%8F%84-%EB%B6%84%EC%84%9D",
    "relUrl": "/docs/text_analysis/korean_text/#빈도-분석"
  },"99": {
    "doc": "기초 & Linear Regression",
    "title": "기초 &amp; Linear Regression",
    "content": ". | Machine Learning이란? . | 관련 용어들 | Maching Learning의 종류 | . | Linear Regression (선형회귀) . | 기본 개념 | scikit-learn으로 구현하기 | . | Polynomial Regression (다항회귀) . | 기본 개념 | scikit-learn으로 구현하기 | . | . ",
    "url": "https://chaelist.github.io/docs/ml_basics/linear_regression/#%EA%B8%B0%EC%B4%88--linear-regression",
    "relUrl": "/docs/ml_basics/linear_regression/#기초--linear-regression"
  },"100": {
    "doc": "기초 & Linear Regression",
    "title": "Machine Learning이란?",
    "content": ": 데이터셋으로 모델을 학습시켜 이를 바탕으로 새로운 데이터셋을 예측할 수 있게 하는 방식. | 인공지능(AI)의 한 분야이며, 명시적으로 프로그래밍하지 않아도 기계 스스로의 학습을 통한 예측이 가능. | . 관련 용어들 . | 종속변수와 독립변수 . | y = f(x)에서의 y가 종속변수, x가 독립변수. | Independent variable (독립변수): ML에서 학습의 단서가 되는 변수들. (ex. 귀의 모양, 털의 색, 수염의 길이) . | feature 또는 입력변수라고도 함. | . | Dependent variable (종속변수): ML에서 예측해야 하는 정답 (ex. 개 / 고양이) . | 목표변수라고도 함. | . | IV와 DV 간의 관계는 ‘파라미터’의 값에 따라 달라진다 . | y = a + bx에서의 a, b처럼 모델의 모양을 결정짓는 값들을 Parameter(파라미터)라고 한다 | . | .   . | 연속변수와 이산변수 . | Continuous variable (연속변수): 무한한 값을 취할 수 있는 변수 (ex. 체중, 수입) | Discrete variable (이산변수): 취할 수 있는 값이 유한한 변수. (ex. 성별, 자녀 수) . | Categorical varialbe (범주형변수): 취하는 값이 숫자의 크기가 아닌 그룹의 이름을 지칭하는 변수. (ex. 성별) | . | . | . Maching Learning의 종류 . | Supervised Learning (지도 학습) . | 정답(=DV)이 있는 training data로 학습 → 새로운 데이터(test data)에 적용해서 정답을 예측한다. | Machine Learning의 대부분은 Supervised Learning. | . *예시: . | Regression problems (회귀): DV가 연속 변수인 경우. (ex. 아파트 가격 예측, 연봉 예측) | Classifiaction problems (분류): DV가 범주형 변수인 경우. (ex. 고양이인지 강아지인지 분류) | .   . | Unsupervised Learning (비지도 학습) . | training data가 필요 없음 (=정답이 따로 없음). 비교를 통해 비슷한 data point끼리 묶어준다. | 비지도 학습은 대부분 탐색적 데이터 분석의 일부로 수행된다. | . *예시: . | Clustering (군집화): 거리를 계산해 유사한 data point끼리 같은 Cluster로 묶어준다. | Dimension reduction (차원 축소): 많은 feature를 갖고 있는 다차원 데이터셋의 차원을 축소해 새로운 데이터셋을 생성해준다. | .   . | Reinforcement learning (강화학습) . | action &amp; feedback 프로세스를 통해 문제를 풀어나가는 방식. | 현재 상태에서 높은 reward를 받는 방법을 찾아가며 action을 취하는 것을 반복하다보면 높은 reward를 받을 수 있는 전략을 터득하게 됨 | . | . +. Deep Learning:    머신러닝 중 인공신경망 모델을 집중 연구하는 분야로, 지도학습, 비지도학습, 강화학습과 모두 연결이 가능하다 . ",
    "url": "https://chaelist.github.io/docs/ml_basics/linear_regression/#machine-learning%EC%9D%B4%EB%9E%80",
    "relUrl": "/docs/ml_basics/linear_regression/#machine-learning이란"
  },"101": {
    "doc": "기초 & Linear Regression",
    "title": "Linear Regression (선형회귀)",
    "content": "기본 개념 . | 가설 함수: 일차 함수 (학습 = 데이터에 가장 잘 맞는 일차 함수를 찾는 것!) . | feature가 하나인 경우: hθ(x) = θ0 + θ1x1 | feature가 여러 개인 경우: hθ(x) = θ0 + θ1x1 + θ2x2 + … + θnxn . | feature가 여러 개인 경우도 항이 많긴 하지만 일차함수. | . | . | 손실함수: MSE(평균제곱오차) . | 손실 함수(loss function): 가설 함수를 평가하기 위한 함수. 손실 함수의 결과값이 작을수록 손실이 작은 것. = 좋은 가설 함수. | MSE: 각 데이터포인트들이 가설 함수로부터 얼마나 떨어져 있는지, 그 오차를 모두 제곱해서 합하고 평균을 낸 것. | 손실 함수의 최저점, 즉 MSE가 가장 최소가 되는 지점을 찾으면 데이터에 가장 잘 맞는 가설함수를 찾을 수 있다. | . | 손실함수의 최저점을 찾는 방법: . | Normal Equation: derivative(미분값)이 0이 되는 지점을 직접 바로 계산해서 찾는다 | Gradient Descent (경사하강법): 특정 지점에서 시작, 점점 값을 update해가면서 최저점을 찾는다 | . | . ※ sklearn의 Linear Regression 모델은 scipy.linalg.lstsq를 사용해서 최저점을 계산. (Normal Equation 방식) . scikit-learn으로 구현하기 . ※ Anaconda를 사용하고 있다면 scikit-learn은 이미 내장되어 있음. 그 외 경우에는 pip install scikit-learn로 설치. | sklearn에서 기본 제공하는 학습용 dataset 중 ‘boston 집값 데이터’를 활용 | IV이 2개 이상인 ‘Multiple Linear Regression (다중선형회귀)’ 문제. | . from sklearn.datasets import load_boston # boston 집값 데이터 불러오기 from sklearn.model_selection import train_test_split # 데이터셋을 training set / test set 나누기 위한 함수 from sklearn.linear_model import LinearRegression # 선형회귀를 구현해주는 함수 from sklearn.metrics import mean_squared_error # mean squared error(평균제곱오차)를 구해주는 함수 import pandas as pd . 1. 데이터 준비 . boston_dataset = load_boston() # 데이터셋을 가져와준다 . +) print(boston_dataset.DESCR)를 해주면 데이터셋에 대한 정보를 살펴볼 수 있음 . | Number of Instances: 506 (506개의 집값 데이터가 들어있음) | Number of Attributes: 13 (입력변수 13개) | 14번째 속성이 대체로 목표변수인 ‘집값’ | .   . → 데이터 정리 . # boston dataset의 입력변수들을 dataframe으로 정리 X = pd.DataFrame(boston_dataset.data, columns=boston_dataset.feature_names) # 목표변수도 dataframe으로 정리 y = pd.DataFrame(boston_dataset.target, columns=['MEDV']) .   . X.head() . |   | CRIM | ZN | INDUS | CHAS | NOX | RM | AGE | DIS | RAD | TAX | PTRATIO | B | LSTAT | . | 0 | 0.00632 | 18 | 2.31 | 0 | 0.538 | 6.575 | 65.2 | 4.09 | 1 | 296 | 15.3 | 396.9 | 4.98 | . | 1 | 0.02731 | 0 | 7.07 | 0 | 0.469 | 6.421 | 78.9 | 4.9671 | 2 | 242 | 17.8 | 396.9 | 9.14 | . | 2 | 0.02729 | 0 | 7.07 | 0 | 0.469 | 7.185 | 61.1 | 4.9671 | 2 | 242 | 17.8 | 392.83 | 4.03 | . | 3 | 0.03237 | 0 | 2.18 | 0 | 0.458 | 6.998 | 45.8 | 6.0622 | 3 | 222 | 18.7 | 394.63 | 2.94 | . | 4 | 0.06905 | 0 | 2.18 | 0 | 0.458 | 7.147 | 54.2 | 6.0622 | 3 | 222 | 18.7 | 396.9 | 5.33 | .   . y.head() . |   | MEDV | . | 0 | 24.0 | . | 1 | 21.6 | . | 2 | 34.7 | . | 3 | 33.4 | . | 4 | 36.2 | . 2. train_test_split . # training set / test set 분리 X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=5) . # 약 4:1로 잘 나뉘었음을 확인할 수 있음 print(X_train.shape) print(X_test.shape) print(y_train.shape) print(y_test.shape) . (404, 13) (102, 13) (404, 1) (102, 1) . 3. 모델 학습시키기 . model = LinearRegression() model.fit(X_train, y_train) # 학습 . LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False) . *theta값 확인 (변수별 가중치를 확인할 수 있다) . # theta_1부터의 theta값들. (각 변수별 가중치) model.coef_ . array([[ -0.13, 0.05, 0. , 2.71, -15.96, 3.41, 0. , -1.49, 0.36, -0.01, -0.95, 0.01, -0.59]]) . # theta_0. (절편) model.intercept_ . array([37.91248701]) . 4. test data로 성능 체크 . | 평균제곱근오차 확인 . # X_test로 예측 y_test_prediction = model.predict(X_test) ## 평균제곱근오차 (RMSE: Root Mean Square Error) mean_squared_error(y_test, y_test_prediction) ** 0.5 . 4.568292042303217 . | R 스퀘어 값 확인 . | model.score(): R square (R2) 값을 계산해주는 함수 | R square: 0 ~ 1 사이의 값을 가지며, 1에 가까울수록 모델의 성능이 좋은 것. (설명력이 좋은 것) | . print(model.score(X_train, y_train)) print(model.score(X_test, y_test)) . 0.738339392059052 0.7334492147453064 . training data는 약 74%, test data는 약 73%를 잘 예측한다는 뜻 . | . ",
    "url": "https://chaelist.github.io/docs/ml_basics/linear_regression/#linear-regression-%EC%84%A0%ED%98%95%ED%9A%8C%EA%B7%80",
    "relUrl": "/docs/ml_basics/linear_regression/#linear-regression-선형회귀"
  },"102": {
    "doc": "기초 & Linear Regression",
    "title": "Polynomial Regression (다항회귀)",
    "content": ". | IV와 DV 간의 관계를 가장 잘 표현하는 게 일차함수가 아니라 다항식(곡선 모양)인 경우, 다항 회귀를 사용 | . 기본 개념 . | 가설 함수: 다항식 (원하는 모양에 따라 2차 함수 ~ n차 함수) . | ex) feature가 하나이고, 가설함수가 2차 함수: hθ(x) = θ0 + θ1x1 + θ2x2 | ex) feature가 하나이고, 가설함수가 3차 함수: hθ(x) = θ0 + θ1x1 + θ2x2 + + θ3x3 | ex) feature가 2개이고, 가설함수가 2차 함수: hθ(x) = θ0 + θ1x1 + θ2x2 + θ3x1x2 + θ4x12 + θ5x22 | . | 선형회귀와 동일한 방식으로 문제를 해결 . | ex) hθ(x) = θ0 + θ1x1 + θ2x2: 입력변수가 2개인 선형회귀로 취급 | ex) hθ(x) = θ0 + θ1x1 + θ2x2 + θ3x1x2 + θ4x12 + θ5x22: 입력변수가 5개인 선형회귀로 취급 | . | 다항회귀의 장점: 속성 간에 있을 수 있는 복잡한 관계들이 학습에 반영된다 . | ex) 집 값 예측 문제에서 IV가 ‘너비’, ‘높이’ 2개일 때, 다항회귀를 사용하면 ‘너비x높이’라는 새로운 변수가 입력변수로 들어가는 셈이기에, 예측력이 커짐 (단순선형회귀의 경우 두 변수가 독립적이라, ‘높이와 너비가 모두 커야 집 값이 높다’는 관계는 학습 불가) | . | 다항회귀의 단점: 너무 차수를 높여서 training data에 꼭 맞는 가설 함수를 만드는 경우, training data는 잘 설명하지만 새로운 데이터는 잘 설명하지 못하는 overfitting(과적합) 문제가 발생할 수 있다 | . scikit-learn으로 구현하기 . | sklearn에서 기본 제공하는 학습용 dataset 중 ‘boston 집값 데이터’를 활용해 2차 함수를 만들어줄 예정. | IV이 2개 이상인 ‘다중 다항 회귀’ 문제. | LinearRegression 모델을 사용하되, 다항 속성을 만들어서 IV로 넣어준다 | . from sklearn.datasets import load_boston from sklearn.preprocessing import PolynomialFeatures # 다항속성을 만들어주는 툴 from sklearn.model_selection import train_test_split from sklearn.linear_model import LinearRegression # 똑같이 LinearRegression 모델을 사용 from sklearn.metrics import mean_squared_error import pandas as pd . 1. 다항 속성 만들어주기 . # boston dataset 가져옴 boston_dataset = load_boston() . polynomial_transformer = PolynomialFeatures(2) # () 안에 몇차 함수로 할 건지 써줌 - 여기선 2차함수로 지정. polynomial_data = polynomial_transformer.fit_transform(boston_dataset.data) polynomial_data.shape # 열이 13개에서 105개로 늘어났음을 확인 가능. (506, 105) . +) 어떤 feature들이 105개 안에 들어갔나 확인 . polynomial_feature_names = polynomial_transformer.get_feature_names(boston_dataset.feature_names) print(polynomial_feature_names) # 가능한 모든 2차 조합이 다 들어있음을 확인 가능 . ['1', 'CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'CRIM^2', 'CRIM ZN', 'CRIM INDUS', 'CRIM CHAS', 'CRIM NOX', 'CRIM RM', 'CRIM AGE', 'CRIM DIS', 'CRIM RAD', 'CRIM TAX', 'CRIM PTRATIO', 'CRIM B', 'CRIM LSTAT', 'ZN^2', 'ZN INDUS', 'ZN CHAS', 'ZN NOX', 'ZN RM', 'ZN AGE', 'ZN DIS', 'ZN RAD', 'ZN TAX', 'ZN PTRATIO', 'ZN B', 'ZN LSTAT', 'INDUS^2', 'INDUS CHAS', 'INDUS NOX', 'INDUS RM', 'INDUS AGE', 'INDUS DIS', 'INDUS RAD', 'INDUS TAX', 'INDUS PTRATIO', 'INDUS B', 'INDUS LSTAT', 'CHAS^2', 'CHAS NOX', 'CHAS RM', 'CHAS AGE', 'CHAS DIS', 'CHAS RAD', 'CHAS TAX', 'CHAS PTRATIO', 'CHAS B', 'CHAS LSTAT', 'NOX^2', 'NOX RM', 'NOX AGE', 'NOX DIS', 'NOX RAD', 'NOX TAX', 'NOX PTRATIO', 'NOX B', 'NOX LSTAT', 'RM^2', 'RM AGE', 'RM DIS', 'RM RAD', 'RM TAX', 'RM PTRATIO', 'RM B', 'RM LSTAT', 'AGE^2', 'AGE DIS', 'AGE RAD', 'AGE TAX', 'AGE PTRATIO', 'AGE B', 'AGE LSTAT', 'DIS^2', 'DIS RAD', 'DIS TAX', 'DIS PTRATIO', 'DIS B', 'DIS LSTAT', 'RAD^2', 'RAD TAX', 'RAD PTRATIO', 'RAD B', 'RAD LSTAT', 'TAX^2', 'TAX PTRATIO', 'TAX B', 'TAX LSTAT', 'PTRATIO^2', 'PTRATIO B', 'PTRATIO LSTAT', 'B^2', 'B LSTAT', 'LSTAT^2'] . 2. X, y값 정리 . X = pd.DataFrame(polynomial_data, columns=polynomial_feature_names) X.head() . |   | 1 | CRIM | ZN | INDUS | CHAS | NOX | RM | AGE | DIS | RAD | TAX | PTRATIO | B | LSTAT | CRIM^2 | CRIM ZN | CRIM INDUS | CRIM CHAS | CRIM NOX | CRIM RM | CRIM AGE | CRIM DIS | CRIM RAD | CRIM TAX | CRIM PTRATIO | CRIM B | CRIM LSTAT | ZN^2 | ZN INDUS | ZN CHAS | ZN NOX | ZN RM | ZN AGE | ZN DIS | ZN RAD | ZN TAX | ZN PTRATIO | ZN B | ZN LSTAT | INDUS^2 | INDUS CHAS | INDUS NOX | INDUS RM | INDUS AGE | INDUS DIS | INDUS RAD | INDUS TAX | INDUS PTRATIO | INDUS B | INDUS LSTAT | CHAS^2 | CHAS NOX | CHAS RM | CHAS AGE | CHAS DIS | CHAS RAD | CHAS TAX | CHAS PTRATIO | CHAS B | CHAS LSTAT | NOX^2 | NOX RM | NOX AGE | NOX DIS | NOX RAD | NOX TAX | NOX PTRATIO | NOX B | NOX LSTAT | RM^2 | RM AGE | RM DIS | RM RAD | RM TAX | RM PTRATIO | RM B | RM LSTAT | AGE^2 | AGE DIS | AGE RAD | AGE TAX | AGE PTRATIO | AGE B | AGE LSTAT | DIS^2 | DIS RAD | DIS TAX | DIS PTRATIO | DIS B | DIS LSTAT | RAD^2 | RAD TAX | RAD PTRATIO | RAD B | RAD LSTAT | TAX^2 | TAX PTRATIO | TAX B | TAX LSTAT | PTRATIO^2 | PTRATIO B | PTRATIO LSTAT | B^2 | B LSTAT | LSTAT^2 | . | 0 | 1 | 0.00632 | 18 | 2.31 | 0 | 0.538 | 6.575 | 65.2 | 4.09 | 1 | 296 | 15.3 | 396.9 | 4.98 | 3.99424e-05 | 0.11376 | 0.0145992 | 0 | 0.00340016 | 0.041554 | 0.412064 | 0.0258488 | 0.00632 | 1.87072 | 0.096696 | 2.50841 | 0.0314736 | 324 | 41.58 | 0 | 9.684 | 118.35 | 1173.6 | 73.62 | 18 | 5328 | 275.4 | 7144.2 | 89.64 | 5.3361 | 0 | 1.24278 | 15.1883 | 150.612 | 9.4479 | 2.31 | 683.76 | 35.343 | 916.839 | 11.5038 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0.289444 | 3.53735 | 35.0776 | 2.20042 | 0.538 | 159.248 | 8.2314 | 213.532 | 2.67924 | 43.2306 | 428.69 | 26.8917 | 6.575 | 1946.2 | 100.598 | 2609.62 | 32.7435 | 4251.04 | 266.668 | 65.2 | 19299.2 | 997.56 | 25877.9 | 324.696 | 16.7281 | 4.09 | 1210.64 | 62.577 | 1623.32 | 20.3682 | 1 | 296 | 15.3 | 396.9 | 4.98 | 87616 | 4528.8 | 117482 | 1474.08 | 234.09 | 6072.57 | 76.194 | 157530 | 1976.56 | 24.8004 | . | 1 | 1 | 0.02731 | 0 | 7.07 | 0 | 0.469 | 6.421 | 78.9 | 4.9671 | 2 | 242 | 17.8 | 396.9 | 9.14 | 0.000745836 | 0 | 0.193082 | 0 | 0.0128084 | 0.175358 | 2.15476 | 0.135652 | 0.05462 | 6.60902 | 0.486118 | 10.8393 | 0.249613 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 49.9849 | 0 | 3.31583 | 45.3965 | 557.823 | 35.1174 | 14.14 | 1710.94 | 125.846 | 2806.08 | 64.6198 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0.219961 | 3.01145 | 37.0041 | 2.32957 | 0.938 | 113.498 | 8.3482 | 186.146 | 4.28666 | 41.2292 | 506.617 | 31.8937 | 12.842 | 1553.88 | 114.294 | 2548.49 | 58.6879 | 6225.21 | 391.904 | 157.8 | 19093.8 | 1404.42 | 31315.4 | 721.146 | 24.6721 | 9.9342 | 1202.04 | 88.4144 | 1971.44 | 45.3993 | 4 | 484 | 35.6 | 793.8 | 18.28 | 58564 | 4307.6 | 96049.8 | 2211.88 | 316.84 | 7064.82 | 162.692 | 157530 | 3627.67 | 83.5396 | . | 2 | 1 | 0.02729 | 0 | 7.07 | 0 | 0.469 | 7.185 | 61.1 | 4.9671 | 2 | 242 | 17.8 | 392.83 | 4.03 | 0.000744744 | 0 | 0.19294 | 0 | 0.012799 | 0.196079 | 1.66742 | 0.135552 | 0.05458 | 6.60418 | 0.485762 | 10.7203 | 0.109979 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 49.9849 | 0 | 3.31583 | 50.798 | 431.977 | 35.1174 | 14.14 | 1710.94 | 125.846 | 2777.31 | 28.4921 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0.219961 | 3.36976 | 28.6559 | 2.32957 | 0.938 | 113.498 | 8.3482 | 184.237 | 1.89007 | 51.6242 | 439.003 | 35.6886 | 14.37 | 1738.77 | 127.893 | 2822.48 | 28.9555 | 3733.21 | 303.49 | 122.2 | 14786.2 | 1087.58 | 24001.9 | 246.233 | 24.6721 | 9.9342 | 1202.04 | 88.4144 | 1951.23 | 20.0174 | 4 | 484 | 35.6 | 785.66 | 8.06 | 58564 | 4307.6 | 95064.9 | 975.26 | 316.84 | 6992.37 | 71.734 | 154315 | 1583.1 | 16.2409 | . | 3 | 1 | 0.03237 | 0 | 2.18 | 0 | 0.458 | 6.998 | 45.8 | 6.0622 | 3 | 222 | 18.7 | 394.63 | 2.94 | 0.00104782 | 0 | 0.0705666 | 0 | 0.0148255 | 0.226525 | 1.48255 | 0.196233 | 0.09711 | 7.18614 | 0.605319 | 12.7742 | 0.0951678 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 4.7524 | 0 | 0.99844 | 15.2556 | 99.844 | 13.2156 | 6.54 | 483.96 | 40.766 | 860.293 | 6.4092 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0.209764 | 3.20508 | 20.9764 | 2.77649 | 1.374 | 101.676 | 8.5646 | 180.741 | 1.34652 | 48.972 | 320.508 | 42.4233 | 20.994 | 1553.56 | 130.863 | 2761.62 | 20.5741 | 2097.64 | 277.649 | 137.4 | 10167.6 | 856.46 | 18074.1 | 134.652 | 36.7503 | 18.1866 | 1345.81 | 113.363 | 2392.33 | 17.8229 | 9 | 666 | 56.1 | 1183.89 | 8.82 | 49284 | 4151.4 | 87607.9 | 652.68 | 349.69 | 7379.58 | 54.978 | 155733 | 1160.21 | 8.6436 | . | 4 | 1 | 0.06905 | 0 | 2.18 | 0 | 0.458 | 7.147 | 54.2 | 6.0622 | 3 | 222 | 18.7 | 396.9 | 5.33 | 0.0047679 | 0 | 0.150529 | 0 | 0.0316249 | 0.4935 | 3.74251 | 0.418595 | 0.20715 | 15.3291 | 1.29123 | 27.4059 | 0.368036 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 4.7524 | 0 | 0.99844 | 15.5805 | 118.156 | 13.2156 | 6.54 | 483.96 | 40.766 | 865.242 | 11.6194 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0.209764 | 3.27333 | 24.8236 | 2.77649 | 1.374 | 101.676 | 8.5646 | 181.78 | 2.44114 | 51.0796 | 387.367 | 43.3265 | 21.441 | 1586.63 | 133.649 | 2836.64 | 38.0935 | 2937.64 | 328.571 | 162.6 | 12032.4 | 1013.54 | 21512 | 288.886 | 36.7503 | 18.1866 | 1345.81 | 113.363 | 2406.09 | 32.3115 | 9 | 666 | 56.1 | 1190.7 | 15.99 | 49284 | 4151.4 | 88111.8 | 1183.26 | 349.69 | 7422.03 | 99.671 | 157530 | 2115.48 | 28.4089 | .   . y = pd.DataFrame(boston_dataset.target, columns=['MEDV']) y.head() . |   | MEDV | . | 0 | 24.0 | . | 1 | 21.6 | . | 2 | 34.7 | . | 3 | 33.4 | . | 4 | 36.2 | . 3. train_test_split &amp; 학습 . X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=5) . model = LinearRegression() model.fit(X_train, y_train) #학습 . LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False) . *theta값 확인 . model.coef_ # 변수별 가중치 . array([[ -0. , -5.09, -0.17, -5.97, 24.32, 165.18, 21.99, 1.03, -5.67, 3.22, -0.01, 5.35, -0.05, 0.75, 0. , 0.27, 0.59, 2.42, -0.03, 0.09, -0.01, -0.06, 0.36, -0.04, 0.54, -0. , 0.02, -0. , -0.01, -0.11, -1.28, 0.03, 0. , -0.01, -0. , 0. , -0.01, 0. , -0. , 0.03, -0.21, 1.3 , 0.31, 0. , 0.08, -0.01, 0. , -0.01, 0.01, -0.01, 24.32, -18.48, -6.89, 0.04, 3.05, -0.41, 0.02, -0.85, 0.03, -0.47, -46.79, 3.65, -0.6 , 15.93, -0.99, 0.13, -11.92, -0.04, 1.5 , 0.16, -0.06, -0.02, -0.15, -0.01, -0.54, -0. , -0.22, 0. , -0.01, 0.02, -0. , 0. , -0. , -0.01, 0.51, -0.1 , -0.01, -0.19, -0.01, 0.1 , -0.14, 0.01, -0.12, -0. , -0.05, -0. , 0.01, -0. , -0. , -0.01, 0.01, -0. , -0. , -0. , 0.01]]) . model.intercept_ # 절편 . array([-141.9]) . 4. test data로 성능 체크 . | 평균제곱근오차 확인 y_test_prediction = model.predict(X_test) ## 평균제곱근오차 (RMSE: Root Mean Square Error) mean_squared_error(y_test, y_test_prediction) ** 0.5 . 3.1965276513308156 . | R 스퀘어 값 확인 print(model.score(X_train, y_train)) print(model.score(X_test, y_test)) . 0.9315569004651908 0.8694943908787136 . training data는 약 93%, test data는 약 87%를 잘 예측한다는 뜻 . | . ",
    "url": "https://chaelist.github.io/docs/ml_basics/linear_regression/#polynomial-regression-%EB%8B%A4%ED%95%AD%ED%9A%8C%EA%B7%80",
    "relUrl": "/docs/ml_basics/linear_regression/#polynomial-regression-다항회귀"
  },"103": {
    "doc": "기초 & Linear Regression",
    "title": "기초 & Linear Regression",
    "content": " ",
    "url": "https://chaelist.github.io/docs/ml_basics/linear_regression/",
    "relUrl": "/docs/ml_basics/linear_regression/"
  },"104": {
    "doc": "물류 최적화",
    "title": "물류 최적화",
    "content": ". | 운송 경로 최적화 . | 운송 비용 최소화 모델 계산 | 최적 운송 경로 가시화 | 제약 조건 만족 여부 확인 | . | 생산 계획 최적화 . | 이익 극대화 모델 계산 | 제약 조건 만족 여부 확인 | . | 물류 네트워크 최적화 . | 물류 네트워크 최적화 계산 | 수요 만족 여부 확인 | 최적 운송 경로 확인 | 최적 생산량 확인 | . | . ",
    "url": "https://chaelist.github.io/docs/ml_application/logistics/",
    "relUrl": "/docs/ml_application/logistics/"
  },"105": {
    "doc": "물류 최적화",
    "title": "운송 경로 최적화",
    "content": "# 필요한 library를 import import numpy as np import pandas as pd from itertools import product from pulp import LpVariable, lpSum, value from ortoolpy import model_min, addvars, addvals . | itertools: 효율적인 반복을 위한 함수. - ex) itertools.product(AB, xy) → Ax, Ay, Bx, By | PuLP: 최적화 모델을 작성하는 역할. | ortoolpy: 목적 함수를 생성해서 최적화 문제를 푸는 역할. | pupl, ortoolpy는 별도로 설치해야 사용 가능 (pip install로 설치) | . # 데이터 불러오기 ## 데이터 출처: https://github.com/wikibook/pyda100 df_tc = pd.read_csv('data/trans_cost.csv', index_col='공장') df_tr = pd.read_csv('data/trans_route.csv', index_col=\"공장\") df_demand = pd.read_csv('data/demand.csv') df_supply = pd.read_csv('data/supply.csv') . | trans_cost.csv: 창고와 공장 간의 운송 비용 | trans_route.csv: 운송 경로 | demand.csv: 공장의 제품 생산량에 대한 수요 | supply.csv: 창고가 공급 가능한 최대 부품 수 | . print(df_tc, '\\n') print(df_tr, '\\n') print(df_demand, '\\n') print(df_supply) . F1 F2 F3 F4 공장 W1 10 10 11 27 W2 18 21 12 14 W3 15 12 14 12 F1 F2 F3 F4 공장 W1 15 15 0 5 W2 5 0 30 5 W3 10 15 2 15 F1 F2 F3 F4 0 28 29 31 25 W1 W2 W3 0 35 41 42 . 운송 비용 최소화 모델 계산 . | 현재의 운송 비용 계산 . total_cost = 0 for i in df_tc.index: for c in df_tc.columns: total_cost += df_tr.loc[i, c] * df_tc.loc[i, c] print(f\"현재 총 운송 비용: {total_cost}만원\") . 현재 총 운송 비용: 1493만원 . | 운송 비용을 최소화하는 운송 경로를 계산 . warehouses = len(df_tc.index) # 창고 수: 3 factories = len(df_tc.columns) # 공장 수: 4 iter_pr = list(product(range(warehouses), range(factories))) # [(0, 0), (0, 1), (0, 2), (0, 3), (1, 0), (1, 1), (1, 2), (1, 3), (2, 0), (2, 1), (2, 2), (2, 3)] ## 수리 모델 작성 model = model_min() # 목적 함수를 제약 조건 하에서 '최소화'하는 모델을 정의 v1 = {(i,j): LpVariable(f'v{i}_{j}', lowBound=0) for i,j in pr} # pr의 i,j를 dict 형식으로 저장 = ex) 0, 0 → (0, 0): v0_0 # 목적 함수 추가 model += lpSum(df_tc.iloc[i, j] * v1[i, j] for i, j in iter_pr) # 제약 조건 추가 for i in range(nw): model += lpSum(v1[i, j] for j in range(factories)) &lt;= df_supply.iloc[0, i] # 각 창고가 공급할 부품이 최대 공급 가능한 한계를 넘지 않도록 for j in range(nf): model += lpSum(v1[i, j] for i in range(warehouses)) &gt;= df_demand.iloc[0, j] # 각 공장이 제조할 제품 수요량을 만족시키도록 model.solve() # 생성한 수리 모델을 계산 . 1 . +) 작성된 수리 모델 확인해보기 . model . NoName: MINIMIZE 10*v0_0 + 10*v0_1 + 11*v0_2 + 27*v0_3 + 18*v1_0 + 21*v1_1 + 12*v1_2 + 14*v1_3 + 15*v2_0 + 12*v2_1 + 14*v2_2 + 12*v2_3 + 0 SUBJECT TO _C1: v0_0 + v0_1 + v0_2 + v0_3 &lt;= 35 _C2: v1_0 + v1_1 + v1_2 + v1_3 &lt;= 41 _C3: v2_0 + v2_1 + v2_2 + v2_3 &lt;= 42 _C4: v0_0 + v1_0 + v2_0 &gt;= 28 _C5: v0_1 + v1_1 + v2_1 &gt;= 29 _C6: v0_2 + v1_2 + v2_2 &gt;= 31 _C7: v0_3 + v1_3 + v2_3 &gt;= 25 VARIABLES v0_0 Continuous v0_1 Continuous v0_2 Continuous v0_3 Continuous v1_0 Continuous v1_1 Continuous v1_2 Continuous v1_3 Continuous v2_0 Continuous v2_1 Continuous v2_2 Continuous v2_3 Continuous . | 최적화된 운송 경로 확인 및 운송 비용 계산 . df_tr_solved = df_tr.copy() # df_tr과 같은 index, column을 갖는 df를 생성하기 위해 복제 for k, x in v1.items(): # k, x 예시: ((0, 0), v0_0) i, j = k[0], k[1] df_tr_solved.iloc[i, j] = value(x) # pulp.value: 계산된 LpVariable의 값을 돌려줌 print(df_tr_solved) print(f\"\\n최적화된 총 운송 비용: {value(model.objective) :.0f}만원\") # pulp.value(model.objective): 최적화된 결과값을 확인 가능 . F1 F2 F3 F4 공장 W1 28 7 0 0 W2 0 0 31 5 W3 0 22 0 20 최적화된 총 운송 비용: 1296만원 . | 위에서 계산했던 1493만원보다 비용이 크게 절감됨! | . | . 최적 운송 경로 가시화 . import matplotlib.pyplot as plt import networkx as nx import seaborn as sns . | 창고 - 공장 간 경로를 네트워크로 만들어 시각화 . # 객체 생성 G = nx.Graph() # 노드 설정 G.add_nodes_from(df_tr_solved.columns) G.add_nodes_from(df_tr_solved.index) # edge 설정 for index in df_tr_solved.index: for column in df_tr_solved.columns: if df_tr_solved.loc[index, column] != 0: # 0이 아닌 경우만 추가 G.add_edge(index, column) G[index][column]['weight'] = df_tr_solved.loc[index, column] # edge 및 weight 확인 G.edges(data=True) . EdgeDataView([('F1', 'W1', {'weight': 28}), ('F2', 'W1', {'weight': 7}), ('F2', 'W3', {'weight': 22}), ('F3', 'W2', {'weight': 31}), ('F4', 'W2', {'weight': 5}), ('F4', 'W3', {'weight': 20})]) . # 좌표 설정: 잘 보이도록 위치를 임의로 설정 pos = {'W1':(0,1), 'W2':(0,2), 'W3':(0,3), 'F1':(4, 0.5), 'F2':(4,1.5), 'F3':(4,2.5), 'F4':(4,3.5)} # 그리기 ## node &amp; 네트워크 그리기 weights = [G[i][c]['weight']*0.1 for i,c in G.edges] nx.draw_networkx(G, pos, font_size=14, node_size = 500, node_color='k', font_color='w', width=weights) # weight에 따라 선의 굵기를 다르게 표시 ## weight 그려넣기 labels = nx.get_edge_attributes(G, 'weight') nx.draw_networkx_edge_labels(G, pos, font_size=12, edge_labels=labels) plt.axis('off') # turn off axis plt.show() . | heatmap으로도 시각화해보기 . df_tr_copy = df_tr_solved.copy() df_tr_copy.index.name = None # 그래프에서 index name을 안보이게 하려고 지움 sns.heatmap(df_tr_copy, annot=True, cmap=\"YlGnBu\"); . | W1 → F1, W2 → F3, W3 → F4 &amp; F2로의 공급이 대부분이며, 그 외의 공급은 최소한으로 제한됨 | 운송 경로가 어느 정도 집중되어야 비용이 절감되는 것… | . | . 제약 조건 만족 여부 확인 . : 위에서 계산한 최적 운송 경로가 제약 조건을 잘 만족하고 있는지 확인해보기 . print(df_demand, '\\n') print(df_supply, '\\n') print(df_tr_solved) . F1 F2 F3 F4 0 28 29 31 25 W1 W2 W3 0 35 41 42 F1 F2 F3 F4 공장 W1 28 7 0 0 W2 0 0 31 5 W3 0 22 0 20 . # 공급측 flag = np.zeros(len(df_supply.columns)) for i in range(len(df_supply.columns)): supply_limit = df_supply.iloc[0, i] supply = df_tr_sol.iloc[i, :].sum() print(f'{df_supply.columns[i]} 공급 가능 한계: {supply_limit}, 수요: {supply}') if supply &lt;= supply_limit: flag[i] = 1 print(f'&gt;&gt; 공급 조건 계산 결과: {flag}\\n') # 수요측 flag = np.zeros(len(df_demand.columns)) for i in range(len(df_demand.columns)): demand_limit = df_demand.iloc[0, i] demand = df_tr_sol.iloc[:, i].sum() print(f'{df_demand.columns[i]} 최소 필요량: {demand_limit}, 공급: {demand}') if demand &gt;= demand_limit: flag[i] = 1 print(f'&gt;&gt; 수요 조건 계산 결과: {flag}') . W1 공급 가능 한계: 35, 수요: 35 W2 공급 가능 한계: 41, 수요: 36 W3 공급 가능 한계: 42, 수요: 42 &gt;&gt; 공급 조건 계산 결과: [1. 1. 1.] F1 최소 필요량: 28, 공급: 28 F2 최소 필요량: 29, 공급: 29 F3 최소 필요량: 31, 공급: 31 F4 최소 필요량: 25, 공급: 25 &gt;&gt; 수요 조건 계산 결과: [1. 1. 1. 1.] . ",
    "url": "https://chaelist.github.io/docs/ml_application/logistics/#%EC%9A%B4%EC%86%A1-%EA%B2%BD%EB%A1%9C-%EC%B5%9C%EC%A0%81%ED%99%94",
    "relUrl": "/docs/ml_application/logistics/#운송-경로-최적화"
  },"106": {
    "doc": "물류 최적화",
    "title": "생산 계획 최적화",
    "content": ": 어떤 제품을 얼마나 생산하는 것이 이익을 극대화하는지 계산 . ## 데이터 출처: https://github.com/wikibook/pyda100 df_material = pd.read_csv('data/product_plan_material.csv', index_col=\"제품\") df_profit = pd.read_csv('data/product_plan_profit.csv', index_col=\"제품\") df_stock = pd.read_csv('data/product_plan_stock.csv', index_col=\"항목\") df_plan = pd.read_csv('data/product_plan.csv', index_col=\"제품\") . | product_plan_material.csv: 제품 제조에 필요한 원료 비율 | product_plan_profit.csv: 제품 이익 | product_plan_stock.csv: 원료 재고 | product_plan.csv: 제품 생산량 | . print(df_material, '\\n') print(df_profit, '\\n') print(df_stock, '\\n') print(df_plan) . 원료1 원료2 원료3 제품 제품1 1 4 3 제품2 2 4 1 이익 제품 제품1 5.0 제품2 4.0 원료1 원료2 원료3 항목 재고 40 80 50 생산량 제품 제품1 16 제품2 0 . | 현재는 더 이익이 높은 제품1만 생산되고 있음. (원료가 효율적으로 사용되고 있지 않음) | . 이익 극대화 모델 계산 . | 현재의 총 이익 계산 . profit = 0 for i in range(len(df_profit.index)): for j in range(len(df_plan.columns)): profit += df_profit.iloc[i, j] * df_plan.iloc[i, j] # 각 제품의 이익 * 생산량 print(f'현재 총 이익: {profit :.0f}만원') . 현재 총 이익: 80만원 . | 이익을 극대화하는 생산량을 계산 . from pulp import LpVariable, lpSum, value from ortoolpy import model_max, addvars, addvals # 운송 비용 최소화 문제와 다르게 model_max를 사용 ## 수리 모델 작성 model = model_max() # 목적 함수를 제약조건 하에서 '최대화'하는 모델을 정의 v1 = {i: LpVariable(f'v{i}', lowBound=0) for i in range(len(df_profit))} # {0: v0, 1: v1} # 목적 함수 추가 model += lpSum(df_profit.iloc[i] * v1[i] for i in range(len(df_profit))) # 제품별 이익 * 생산량 # 제약 조건 추가 for i in range(len(df_material.columns)): model += lpSum(df_material.iloc[j, i] * v1[j] for j in range(len(df_profit)) ) &lt;= df_stock.iloc[:, i] # 제품에 사용되는 원료의 총 합이 재고 수를 넘지 않도록 model.solve() # 생성한 수리 모델을 계산 . 1 . +) 작성된 수리 모델 확인해보기 . model . NoName: MAXIMIZE 5.0*v0 + 4.0*v1 + 0.0 SUBJECT TO _C1: v0 + 2 v1 &lt;= 40 _C2: 4 v0 + 4 v1 &lt;= 80 _C3: 3 v0 + v1 &lt;= 50 VARIABLES v0 Continuous v1 Continuous . | 최적화된 생산 계획 및 이익 계산 . df_plan_solved = df_plan.copy() for k, x in v1.items(): # k, x 예시: (0, v0) df_plan_solved.iloc[k] = value(x) print(df_plan_solved) print(f'\\n최적화된 총 이익: {value(m.objective) :.0f}만원') . 생산량 제품 제품1 15 제품2 5 총 이익: 95만원 . | 위에서 계산했던 이익 80만원보다 증가함! (효율적으로 사용되지 않고 있던 원료들을 활용해 제품2 생산을 늘리는 것이 이익 증가로 이어짐) | . | . 제약 조건 만족 여부 확인 . print(df_plan_solved, '\\n') print(df_material, '\\n') print(df_stock) . 생산량 제품 제품1 15 제품2 5 원료1 원료2 원료3 제품 제품1 1 4 3 제품2 2 4 1 원료1 원료2 원료3 항목 재고 40 80 50 . # 제약 조건 계산 flag = np.zeros(len(df_material.columns)) for i in range(len(df_material.columns)): usage = 0 for j in range(len(df_material.index)): usage += df_plan_solved.iloc[j, 0] * df_material.iloc[j, i] stock = df_stock.iloc[0, i] print(f'{df_material.columns[i]} 사용량: {usage}, 재고: {stock}') if usage &lt;= stock: flag[i] = 1 print(f'&gt;&gt; 제약 조건 계산 결과: {flag}') . 원료1 사용량: 25, 재고: 40 원료2 사용량: 80, 재고: 80 원료3 사용량: 50, 재고: 50 &gt;&gt; 제약 조건 계산 결과: [1. 1. 1.] . | 원료2와 원료3은 재고를 모두 사용. 원료1은 조금 남지만, 최적화 계산 전보다 사용 효율이 크게 개선됨. | . ",
    "url": "https://chaelist.github.io/docs/ml_application/logistics/#%EC%83%9D%EC%82%B0-%EA%B3%84%ED%9A%8D-%EC%B5%9C%EC%A0%81%ED%99%94",
    "relUrl": "/docs/ml_application/logistics/#생산-계획-최적화"
  },"107": {
    "doc": "물류 최적화",
    "title": "물류 네트워크 최적화",
    "content": ": 운송 경로 &amp; 생산 계획 최적화를 동시에 (실제 물류 네트워크 설계에서는 두 가지를 모두 고려해야 한다) . | 수요를 만족시키는 선에서 운송 비용과 제조 비용이 최소가 되어야 함 | 목적 함수: 운송 비용 + 제조 비용 | 제약 조건: 각 대리점의 판매 수 &gt;= 수요 수 (수요를 충족하고 남을 만큼 상품이 준비되어야 함) | . 물류 네트워크 최적화 계산 . # 데이터 직접 생성 ## 참고: https://github.com/wikibook/pyda100 item = ['A', 'B'] # 제품: A, B 2개 store = ['P', 'Q'] # 대리점: P, Q 2개 factory = ['X', 'Y'] # 공장: X, Y 2개 prod_line = [0, 1] # 각 공장마다 생산라인 2개 (0, 1) # 대리점별 제품별 수요 df_demand = pd.DataFrame(list(product(store, item)), columns=['대리점', '제품']) df_demand['수요'] = [10, 10, 20, 20] print(df_demand, '\\n') # 대리점 → 공장 운송비 df_trans_fee = pd.DataFrame(list(product(store, factory)), columns=['대리점', '공장']) df_trans_fee['운송비'] = [1, 2, 3, 1] print(df_trans_fee, '\\n') # 공장 라인별 제품별 생산가능량, 생산비 df_prod = pd.DataFrame(list(product(factory, prod_line, item)), columns=['공장', '생산라인', '제품']) df_prod['하한'] = 0 df_prod['상한'] = np.inf df_prod.loc[4,'상한'] = 10 df_prod['생산비'] = [1, np.nan, np.nan, 1, 3, np.nan, 5, 3] # 몇 개 조합은 일부로 null로 만들어서 삭제 (해당 생산라인에서 생산X) df_prod.dropna(inplace=True) print(df_prod) . 대리점 제품 수요 0 P A 10 1 P B 10 2 Q A 20 3 Q B 20 대리점 공장 운송비 0 P X 1 1 P Y 2 2 Q X 3 3 Q Y 1 공장 생산라인 제품 하한 상한 생산비 0 X 0 A 0 inf 1.0 3 X 1 B 0 inf 1.0 4 Y 0 A 0 10.0 3.0 6 Y 1 A 0 inf 5.0 7 Y 1 B 0 inf 3.0 . → 최적 생산량 &amp; 최적 운송량 계산 (운송비, 생산비를 최소화하는 제품별 생산량 &amp; 운송 경로 배치 계산) . from ortoolpy import logistics_network _, df_trans, _ = logistics_network(df_demand, df_trans_fee, df_prod, dep = '대리점', dem = '수요', fac = '공장', prd = '제품', tcs = '운송비', pcs = '생산비', lwb = '하한', upb = '상한') print(df_prod, '\\n') # ValY: 계산된 최적 생산량 print(df_trans) # ValX: 계산된 최적 운송량 . 공장 생산라인 제품 하한 상한 생산비 VarY ValY 0 X 0 A 0 inf 1.0 v000009 20.0 3 X 1 B 0 inf 1.0 v000010 10.0 4 Y 0 A 0 10.0 3.0 v000011 10.0 6 Y 1 A 0 inf 5.0 v000012 0.0 7 Y 1 B 0 inf 3.0 v000013 20.0 대리점 공장 운송비 제품 VarX ValX 0 P X 1 A v000001 10.0 1 P X 1 B v000002 10.0 2 Q X 3 A v000003 10.0 3 Q X 3 B v000004 0.0 4 P Y 2 A v000005 0.0 5 P Y 2 B v000006 0.0 6 Q Y 1 A v000007 10.0 7 Q Y 1 B v000008 20.0 . +) ortoolpy.logistics_network 설명: . | input: . | 순서대로 df1, df2, df3가 필요: . | df1: 수요지(제품을 운송받을 판매처), 제품, 수요 3개의 정보를 담은 dataframe | df2: 수요지, 공장, 운송비 3개의 정보를 담은 dataframe | df3: 공장, 제품, 생산비 3개의 정보를 담은 dataframe (+ 하한, 상한 정보는 optional) | . | dep, dem, fac, prd, tcs, pcs, lwb, upb 변수에 각각 해당되는 컬럼명을 대응시켜줌 | . | output: . | 해의 유무 (계산이 잘 되었는지), 운송표 (운송량이 계산된 dataframe), 생산표(생산량이 계산된 dataframe) | 해의 유무: 계산이 가능하면 True를 return | 운송표: df1, df2를 바탕으로 계산된 새로운 dataframe | 생산표: df3에 계산값이 추가된 것 (생산표는 df3 자체가 변형되므로 별도로 받아서 저장할 필요X) | . | . 수요 만족 여부 확인 . calculated = df_trans.groupby(['대리점', '제품'])[['ValX']].sum().reset_index() calculated.rename(columns={'ValX':'계산된 최적 운송량'}, inplace=True) calculated = pd.merge(tbde, calculated, on=['대리점', '제품']) calculated['수요 만족 여부'] = np.where(calculated['수요'] &lt;= calculated['계산된 최적 운송량'], 'O', 'X') # 수요 &lt;= 계산된 최적 운송량이면 O calculated . |   | 대리점 | 제품 | 수요 | 계산된 최적 운송량 | 수요 만족 여부 | . | 0 | P | A | 10 | 10 | O | . | 1 | P | B | 10 | 10 | O | . | 2 | Q | A | 20 | 20 | O | . | 3 | Q | B | 20 | 20 | O | . | 운송량이 모두 대리점별로 필요한 제품 수요를 완벽히 만족하고 있음을 확인 가능 | . 최적 운송 경로 확인 . | 보기 편하게 칼럼 순서 정렬 . # 어느 공장에서 어느 대리점으로 운송하는지, 보기 편하게 칼럼 순서 정렬 df_trans = df_trans[['공장', '대리점', '운송비', '제품', 'VarX', 'ValX']] df_trans . |   | 공장 | 대리점 | 운송비 | 제품 | VarX | ValX | . | 0 | X | P | 1 | A | v000001 | 10 | . | 1 | X | P | 1 | B | v000002 | 10 | . | 2 | X | Q | 3 | A | v000003 | 10 | . | 3 | X | Q | 3 | B | v000004 | 0 | . | 4 | Y | P | 2 | A | v000005 | 0 | . | 5 | Y | P | 2 | B | v000006 | 0 | . | 6 | Y | Q | 1 | A | v000007 | 10 | . | 7 | Y | Q | 1 | B | v000008 | 20 | . | 총 운송비 계산 . trans_cost = sum(df_trans['운송비'] * df_trans['ValX']) print(f'총 운송비: {trans_cost :.0f}만원') . 총 운송비: 80만원 . | 운송 경로 heatmap으로 시각화 . plt.rcParams['font.family'] = 'Malgun Gothic' # 한글 폰트 설정 (깨짐 방지) plt.rc('font', size=12) # 글자 크기 조정 plt.figure(figsize=(6, 4)) trans_pivot = pd.pivot_table(tbdi2, index=['공장', '제품'], columns='대리점', values='ValX', aggfunc='sum') sns.heatmap(trans_pivot, annot=True, cmap=\"YlGnBu\"); . | 운송비가 적은 ‘공장 X → 대리점 P’, ‘공장 Y → 대리점 Q’의 경로를 주로 이용하고, 대리점 Q의 상품 A 수요를 충족시키기 위해 공장 X에서 대리점 Q로 A만 10만큼 추가로 운송 (공장 Y 0 생산라인의 생산 한계를 고려) | . | . 최적 생산량 확인 . | 최적 생산량 계산값 확인 . df_prod . |   | 공장 | 레인 | 제품 | 하한 | 상한 | 생산비 | VarY | ValY | . | 0 | X | 0 | A | 0 | inf | 1 | v000009 | 20 | . | 3 | X | 1 | B | 0 | inf | 1 | v000010 | 10 | . | 4 | Y | 0 | A | 0 | 10 | 3 | v000011 | 10 | . | 6 | Y | 1 | A | 0 | inf | 5 | v000012 | 0 | . | 7 | Y | 1 | B | 0 | inf | 3 | v000013 | 20 | . | 총 생산비 계산 . ## +) (내가 덧붙임) prod_cost = sum(df_prod['생산비'] * df_prod['ValY']) print(f'총 생산비: {prod_cost :.0f}만원') . 총 생산비: 120만원 . | 제품별 생산량 heatmap으로 시각화 . prod_pivot = pd.pivot_table(tbfa, index=['공장', '레인'], columns='제품', values='ValY', aggfunc='sum') sns.heatmap(prod_pivot, annot=True, cmap=\"YlGnBu\"); . | 생산 비용이 낮은 공장 X에서의 생산량을 늘리기 위해 제품 A는 20만큼, 제품 B는 10만큼 생산 | 생산 비용만을 생각하면 공장 X에서 모든 제품을 제조해도 되지만, 운송 비용까지 고려했을 때, 수요가 많은 대리점 Q까지의 운송 비용이 적은 공장 Y에서도 일정량을 생산하는 것이 유리 | 운송비와 생산비의 균형을 생각하면 대체로 타당한 계산이라고 생각됨. | . | . ",
    "url": "https://chaelist.github.io/docs/ml_application/logistics/#%EB%AC%BC%EB%A5%98-%EB%84%A4%ED%8A%B8%EC%9B%8C%ED%81%AC-%EC%B5%9C%EC%A0%81%ED%99%94",
    "relUrl": "/docs/ml_application/logistics/#물류-네트워크-최적화"
  },"108": {
    "doc": "Matplotlib",
    "title": "Matplotlib",
    "content": ". | plt.plot(): 선 그리기 | plt.scatter(): 점 찍기 | data labeling . | plt.annotate() | plt.text() | . | . *Matplotlib: 기초적인 데이터 시각화 library → 서브모듈 matplotlib.pyplot으로 다양한 그래프를 직접 그려볼 수 있다 . ",
    "url": "https://chaelist.github.io/docs/visualization/matplotlib/",
    "relUrl": "/docs/visualization/matplotlib/"
  },"109": {
    "doc": "Matplotlib",
    "title": "plt.plot(): 선 그리기",
    "content": "import matplotlib.pyplot as plt ## 보통 pyplot만 별도로 plt로 줄여서 import %matplotlib inline ## jupyter notebook에서 시각화 결과가 표시되도록 하는 설정 . | 직접 점을 찍어서 선 그리기 plt.plot([1,2,3,4,5,6,7,8,9,8,7,6,5,4,3,2,1]) # x값은 자동으로 0, 1, 2, ..., 16으로 그려짐 . | x값, y값을 가지고 선 그리기 import numpy as np x = np.arange(0, 12, 0.01) # 0에서 12까지 0.01 간격으로 데이터를 만듦 y = np.sin(x) # x의 sin값 (사인함수) plt.figure(figsize=(6,4)) # figsize로 얼마나 크게 그래프를 그릴 지 지정 가능 plt.plot(x, y) . | grid, label, title 추가 plt.plot(x, y) plt.grid() # 격자무늬 생성 plt.xlabel('time') # x축 라벨 plt.ylabel('Amplitude') # y축 라벨 plt.title('Example of sinewave'); # 제목 . | 한 화면에 그래프 두 개 그리기 ## plot 메소드를 2번 써서 한 화면에 그래프를 두 개 그리기 plt.plot(x, np.sin(x), label='sin') ## plt.legend()를 하려면 label을 붙여줘야 한다 plt.plot(x, np.cos(x), label='cos') plt.grid() # 격자무늬 생성 plt.legend() # legend(범례) 생성 plt.xlabel('time') # x축 라벨 plt.ylabel('Amplitude') # y축 라벨 plt.title('Example of sinewave'); # 제목 . | 선 굵기, 색상 조정 plt.plot(x, np.sin(x), linewidth=3) # linewidth (혹은 lw): 선 굵기를 조정 plt.plot(x, np.cos(x), color='r') # color (혹은 c): 색을 조정 ('r'은 red로 하겠다는 뜻) plt.grid() # 격자무늬 생성 plt.xlabel('time') # x축 라벨 plt.ylabel('Amplitude') # y축 라벨 plt.title('Example of sinewave'); # 제목 . | 선 스타일 조정 x = [0, 1, 2, 3, 4, 5, 6] y = [1, 4, 5, 8, 9, 5, 3] plt.plot(x, y, color='green', linestyle='dashed'); # 선 스타일을 점선으로 지정 . | marker 생성 . | marker 종류는 'o', '.', 'v', '^', '&lt;', '&gt;', '1', 's' 등… 다양 | 다양한 스타일은 https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.plot.html 참조 | . plt.plot(x, y, color='green', linestyle='dashed', marker='*', # 별 모양 마커 markerfacecolor='blue', markersize=12); # markerfacecolor로 마커 색 지정, markersize로 마커의 크기 지정 . | . ",
    "url": "https://chaelist.github.io/docs/visualization/matplotlib/#pltplot-%EC%84%A0-%EA%B7%B8%EB%A6%AC%EA%B8%B0",
    "relUrl": "/docs/visualization/matplotlib/#pltplot-선-그리기"
  },"110": {
    "doc": "Matplotlib",
    "title": "plt.scatter(): 점 찍기",
    "content": ". | 기본 scatter plot x = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) y = np.array([9, 8, 7, 9, 8, 3, 2, 4, 3, 4]) plt.scatter(x, y) . | colormap 옵션 지정 plt.scatter(x, y, s = 50, c = x, marker='&gt;') # c = x라고 하면 x값에 따라 색상을 바꾸는 colormap으로 표현하겠다는 뜻. # marker로 마커의 모양을 지정, s로 마커 크기를 지정 plt.colorbar(); # 오른쪽에 colorbar 표시 . | . ",
    "url": "https://chaelist.github.io/docs/visualization/matplotlib/#pltscatter-%EC%A0%90-%EC%B0%8D%EA%B8%B0",
    "relUrl": "/docs/visualization/matplotlib/#pltscatter-점-찍기"
  },"111": {
    "doc": "Matplotlib",
    "title": "data labeling",
    "content": "plt.annotate() . import pandas as pd score_df = pd.read_excel('data/math_reading_scores.xlsx') score_df . |   | Name | Math | Reading | . | 0 | Annie | 75 | 95 | . | 1 | Bella | 78 | 85 | . | 2 | Chloe | 98 | 75 | . | 3 | Diana | 65 | 65 | . | 4 | Emily | 78 | 75 | . | 5 | George | 84 | 85 | . | 6 | Serene | 82 | 95 | . | 7 | Jakob | 95 | 85 | . | 8 | Kim | 75 | 75 | . → 각 점에 ‘Name’ 라벨 붙이기 . plt.figure(figsize=(9, 6)) # 그래프 크기 결정 x = score_df['Math'] y = score_df['Reading'] plt.scatter(x = x, y = y) # 제목, x축 이름, y축 이름 붙이기 plt.title('&lt;Score&gt;', pad=20, size=20) plt.xlabel('Math', size=16) plt.ylabel('Reading', size=16) # 각 점에 label 붙여주기: annotate() 메소드 사용 for i in range(len(score_df)): label = score_df['Name'][i] plt.annotate(label, xy=(x[i],y[i]), xytext=(x[i]+0.3,y[i]+0.3), fontsize=10) ## xy는 (x,y)로 라벨이 붙을 점을 알려주는 역할 ## xytext는 text가 어느 위치에 들어갈지를 결정하는 역할. (x[i]+0.3,y[i]+0.3)는 각 (x,y) 점에서 0.3씩 떨어진 곳에 글을 쓰겠다는 뜻 plt.grid() # 격자무늬 만들기 . cf) pandas.plot()에 plt.annotate를 조합해서 아래와 같이 표현하는 것도 가능 . score_df.plot(kind='scatter', x='Math', y='Reading', figsize=(9, 6), grid=True) # x축, y축 이름도 자동으로 붙음 x = score_df['Math'] y = score_df['Reading'] for i in range(len(score_df)): list1 = score_df.loc[i].values.tolist() label = score_df['Name'][i] plt.annotate(label, xy=(x[i],y[i]), xytext=(x[i]+0.3,y[i]+0.3), fontsize=10) . plt.text() . | annotate와 거의 동일하게 표현 가능 | . plt.figure(figsize=(9, 6)) x = score_df['Math'] y = score_df['Reading'] plt.scatter(x = x, y = y) # 제목, x축 이름, y축 이름 붙이기 plt.title('&lt;Score&gt;', pad=20, size=20) plt.xlabel('Math', size=16) plt.ylabel('Reading', size=16) # 각 점에 이름 붙여주기: plt.text() 메소드 사용 for i in range(len(score_df)): plt.text(score_df['Math'][i]*1.01, score_df['Reading'][i], score_df['Name'][i], fontsize=10) ## plt.text(x값, y값, label붙일 값) 이렇게 값을 assign. ## score_df['Math'][i]*1.01이라고 한 건 조금 오른쪽로 떨어져서 label이 보이도록 한 것. plt.grid() . ",
    "url": "https://chaelist.github.io/docs/visualization/matplotlib/#data-labeling",
    "relUrl": "/docs/visualization/matplotlib/#data-labeling"
  },"112": {
    "doc": "Machine Learning 심화",
    "title": "Machine Learning 심화",
    "content": " ",
    "url": "https://chaelist.github.io/docs/ml_advanced",
    "relUrl": "/docs/ml_advanced"
  },"113": {
    "doc": "Machine Learning 응용",
    "title": "Machine Learning 응용",
    "content": " ",
    "url": "https://chaelist.github.io/docs/ml_application",
    "relUrl": "/docs/ml_application"
  },"114": {
    "doc": "Machine Learning 기초",
    "title": "Machine Learning 기초",
    "content": " ",
    "url": "https://chaelist.github.io/docs/ml_basics",
    "relUrl": "/docs/ml_basics"
  },"115": {
    "doc": "Model Selection",
    "title": "Model Selection",
    "content": ". | k-겹 교차 검증 (k-fold cross validation) . | k-겹 교차 검증 방법 | scikit-learn으로 구현 | . | 그리드 서치 (Grid Search) . | scikit-learn으로 구현 | Randomized Search | . | . ",
    "url": "https://chaelist.github.io/docs/ml_advanced/model_selection/",
    "relUrl": "/docs/ml_advanced/model_selection/"
  },"116": {
    "doc": "Model Selection",
    "title": "k-겹 교차 검증 (k-fold cross validation)",
    "content": ": 머신 러닝 모델의 성능을 보다 정확하게 평가할 수 있는 방법. | test set과 training set을 임의로 나눌 때, 어떻게 나뉘는지에 따라 딱 이 test set에서만 성능이 좋게 나올 수도 있고, 안좋게 나올 수도 있다. 이런 문제를 해결해주는 것이 교차 검증! | k값은 데이터를 몇 세트로 나눌지를 의미. 데이터 수에 따라 다르지만, 가장 일반적으로 사용하는 숫자는 5 | +) 데이터가 많을수록 우연히 test set에서만 성능이 다르게 나올 확률이 적기 때문에 작은 k를 사용해도 된다 | . k-겹 교차 검증 방법 . | 먼저 전체 데이터를 k 개의 같은 사이즈로 나눈다. | ex) k=5, 데이터가 총 1000개가 있다면 이 데이터를 200개씩 5개의 셋으로 나눈다 | . | 맨 처음에는 첫 번째 데이터 셋을 test test으로 사용하고, 나머지를 training set으로 사용 | 그 다음에는 두 번째 데이터 셋을 test set으로 사용하고, 나머지 4개를 training set으로 사용해서 다시 모델을 학습시키고, 성능을 파악 | 이 과정을 모든 데이터 셋에 반복 → 5개의 테스트 셋에 대한 각 성능 5개의 평균을 모델 성능으로 본다 (모델의 성능을 여러 번 다른 데이터로 검증하기 때문에 평가에 대한 신뢰가 올라가는 것!) | . scikit-learn으로 구현 . | sklearn.model_selection.cross_val_score를 사용 | . from sklearn import datasets from sklearn.model_selection import cross_val_score # 교차 검증을 해주는 함수 from sklearn.linear_model import LogisticRegression import numpy as np import pandas as pd # 경고 메시지 출력을 막는 코드 추가 import warnings warnings.simplefilter(action='ignore', category=FutureWarning) . *iris dataset 준비 . iris_data = datasets.load_iris() X = pd.DataFrame(iris_data.data, columns=iris_data.feature_names) y = pd.DataFrame(iris_data.target, columns=['Class']) X.head() . |   | sepal length (cm) | sepal width (cm) | petal length (cm) | petal width (cm) | . | 0 | 5.1 | 3.5 | 1.4 | 0.2 | . | 1 | 4.9 | 3 | 1.4 | 0.2 | . | 2 | 4.7 | 3.2 | 1.3 | 0.2 | . | 3 | 4.6 | 3.1 | 1.5 | 0.2 | . | 4 | 5 | 3.6 | 1.4 | 0.2 | .   . y.head() . |   | Class | . | 0 | 0 | . | 1 | 0 | . | 2 | 0 | . | 3 | 0 | . | 4 | 0 | . *LogisticRegression 모델로 교차 검증 . logistic_model = LogisticRegression(max_iter=2000) cross_val_score(logistic_model, X, y.values.ravel(), cv=5) # ravel(): 다차원 array를 1차원 array로 평평하게 펴주는 함수. array([0.96666667, 1. , 0.93333333, 0.96666667, 1. ]) . | cross_val_score: 알아서 모델을 학습시켜서 k겹 교차검증을 해주고, 각 k개의 성능이 return된다 . | 파라미터: k-겹 교차검증을 해줄 모델명, X값, y값, cv = k값을 결정. | 여기서는 cv=5니까 5겹 교차검증을 한다는 뜻. | . | . → cross_val_score의 결과로 return된 값들을 평균내준다 . # 이렇게 평균낸 값을 해당 모델의 성능으로 생각 np.average(cross_val_score(logistic_model, X, y.values.ravel(), cv=5)) . 0.9733333333333334 . ",
    "url": "https://chaelist.github.io/docs/ml_advanced/model_selection/#k-%EA%B2%B9-%EA%B5%90%EC%B0%A8-%EA%B2%80%EC%A6%9D-k-fold-cross-validation",
    "relUrl": "/docs/ml_advanced/model_selection/#k-겹-교차-검증-k-fold-cross-validation"
  },"117": {
    "doc": "Model Selection",
    "title": "그리드 서치 (Grid Search)",
    "content": ": 좋은 하이퍼파라미터를 고르는 방법 중 하나 . | 우선 정해줘야 하는 각 하이퍼파라미터에 넣어보고 싶은 후보 값을 몇 개씩 정해준다 | 선택한 후보 값들의 모든 조합을 써서 학습을 시키고, 그중 성능이 가장 좋게 나오는 하이퍼파라미터 조합을 고른다 | . scikit-learn으로 구현 . | sklearn.model_selection.GridSearchCV를 사용 | . from sklearn import datasets from sklearn.linear_model import Lasso from sklearn.preprocessing import PolynomialFeatures from sklearn.model_selection import GridSearchCV # Grid Search를 쉽게 해주는 함수 import numpy as np import pandas as pd . 1. boston housing 데이터 준비 . boston_dataset = datasets.load_boston() X = pd.DataFrame(boston_dataset.data, columns=boston_dataset.feature_names) # 입력변수를 6차항으로 바꾸기 polynomial_transformer = PolynomialFeatures(2) # 2차항 변환기 준비 polynomial_features = polynomial_transformer.fit_transform(X.values) features = polynomial_transformer.get_feature_names(X.columns) # 바꾼 값들을 다시 X로 저장 X = pd.DataFrame(polynomial_features, columns=features) X.head() . |   | 1 | CRIM | ZN | INDUS | CHAS | NOX | RM | AGE | DIS | RAD | TAX | PTRATIO | B | LSTAT | CRIM^2 | CRIM ZN | CRIM INDUS | CRIM CHAS | CRIM NOX | CRIM RM | CRIM AGE | CRIM DIS | CRIM RAD | CRIM TAX | CRIM PTRATIO | CRIM B | CRIM LSTAT | ZN^2 | ZN INDUS | ZN CHAS | ZN NOX | ZN RM | ZN AGE | ZN DIS | ZN RAD | ZN TAX | ZN PTRATIO | ZN B | ZN LSTAT | INDUS^2 | INDUS CHAS | INDUS NOX | INDUS RM | INDUS AGE | INDUS DIS | INDUS RAD | INDUS TAX | INDUS PTRATIO | INDUS B | INDUS LSTAT | CHAS^2 | CHAS NOX | CHAS RM | CHAS AGE | CHAS DIS | CHAS RAD | CHAS TAX | CHAS PTRATIO | CHAS B | CHAS LSTAT | NOX^2 | NOX RM | NOX AGE | NOX DIS | NOX RAD | NOX TAX | NOX PTRATIO | NOX B | NOX LSTAT | RM^2 | RM AGE | RM DIS | RM RAD | RM TAX | RM PTRATIO | RM B | RM LSTAT | AGE^2 | AGE DIS | AGE RAD | AGE TAX | AGE PTRATIO | AGE B | AGE LSTAT | DIS^2 | DIS RAD | DIS TAX | DIS PTRATIO | DIS B | DIS LSTAT | RAD^2 | RAD TAX | RAD PTRATIO | RAD B | RAD LSTAT | TAX^2 | TAX PTRATIO | TAX B | TAX LSTAT | PTRATIO^2 | PTRATIO B | PTRATIO LSTAT | B^2 | B LSTAT | LSTAT^2 | . | 0 | 1 | 0.00632 | 18 | 2.31 | 0 | 0.538 | 6.575 | 65.2 | 4.09 | 1 | 296 | 15.3 | 396.9 | 4.98 | 3.99424e-05 | 0.11376 | 0.0145992 | 0 | 0.00340016 | 0.041554 | 0.412064 | 0.0258488 | 0.00632 | 1.87072 | 0.096696 | 2.50841 | 0.0314736 | 324 | 41.58 | 0 | 9.684 | 118.35 | 1173.6 | 73.62 | 18 | 5328 | 275.4 | 7144.2 | 89.64 | 5.3361 | 0 | 1.24278 | 15.1883 | 150.612 | 9.4479 | 2.31 | 683.76 | 35.343 | 916.839 | 11.5038 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0.289444 | 3.53735 | 35.0776 | 2.20042 | 0.538 | 159.248 | 8.2314 | 213.532 | 2.67924 | 43.2306 | 428.69 | 26.8917 | 6.575 | 1946.2 | 100.598 | 2609.62 | 32.7435 | 4251.04 | 266.668 | 65.2 | 19299.2 | 997.56 | 25877.9 | 324.696 | 16.7281 | 4.09 | 1210.64 | 62.577 | 1623.32 | 20.3682 | 1 | 296 | 15.3 | 396.9 | 4.98 | 87616 | 4528.8 | 117482 | 1474.08 | 234.09 | 6072.57 | 76.194 | 157530 | 1976.56 | 24.8004 | . | 1 | 1 | 0.02731 | 0 | 7.07 | 0 | 0.469 | 6.421 | 78.9 | 4.9671 | 2 | 242 | 17.8 | 396.9 | 9.14 | 0.000745836 | 0 | 0.193082 | 0 | 0.0128084 | 0.175358 | 2.15476 | 0.135652 | 0.05462 | 6.60902 | 0.486118 | 10.8393 | 0.249613 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 49.9849 | 0 | 3.31583 | 45.3965 | 557.823 | 35.1174 | 14.14 | 1710.94 | 125.846 | 2806.08 | 64.6198 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0.219961 | 3.01145 | 37.0041 | 2.32957 | 0.938 | 113.498 | 8.3482 | 186.146 | 4.28666 | 41.2292 | 506.617 | 31.8937 | 12.842 | 1553.88 | 114.294 | 2548.49 | 58.6879 | 6225.21 | 391.904 | 157.8 | 19093.8 | 1404.42 | 31315.4 | 721.146 | 24.6721 | 9.9342 | 1202.04 | 88.4144 | 1971.44 | 45.3993 | 4 | 484 | 35.6 | 793.8 | 18.28 | 58564 | 4307.6 | 96049.8 | 2211.88 | 316.84 | 7064.82 | 162.692 | 157530 | 3627.67 | 83.5396 | . | 2 | 1 | 0.02729 | 0 | 7.07 | 0 | 0.469 | 7.185 | 61.1 | 4.9671 | 2 | 242 | 17.8 | 392.83 | 4.03 | 0.000744744 | 0 | 0.19294 | 0 | 0.012799 | 0.196079 | 1.66742 | 0.135552 | 0.05458 | 6.60418 | 0.485762 | 10.7203 | 0.109979 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 49.9849 | 0 | 3.31583 | 50.798 | 431.977 | 35.1174 | 14.14 | 1710.94 | 125.846 | 2777.31 | 28.4921 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0.219961 | 3.36976 | 28.6559 | 2.32957 | 0.938 | 113.498 | 8.3482 | 184.237 | 1.89007 | 51.6242 | 439.003 | 35.6886 | 14.37 | 1738.77 | 127.893 | 2822.48 | 28.9555 | 3733.21 | 303.49 | 122.2 | 14786.2 | 1087.58 | 24001.9 | 246.233 | 24.6721 | 9.9342 | 1202.04 | 88.4144 | 1951.23 | 20.0174 | 4 | 484 | 35.6 | 785.66 | 8.06 | 58564 | 4307.6 | 95064.9 | 975.26 | 316.84 | 6992.37 | 71.734 | 154315 | 1583.1 | 16.2409 | . | 3 | 1 | 0.03237 | 0 | 2.18 | 0 | 0.458 | 6.998 | 45.8 | 6.0622 | 3 | 222 | 18.7 | 394.63 | 2.94 | 0.00104782 | 0 | 0.0705666 | 0 | 0.0148255 | 0.226525 | 1.48255 | 0.196233 | 0.09711 | 7.18614 | 0.605319 | 12.7742 | 0.0951678 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 4.7524 | 0 | 0.99844 | 15.2556 | 99.844 | 13.2156 | 6.54 | 483.96 | 40.766 | 860.293 | 6.4092 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0.209764 | 3.20508 | 20.9764 | 2.77649 | 1.374 | 101.676 | 8.5646 | 180.741 | 1.34652 | 48.972 | 320.508 | 42.4233 | 20.994 | 1553.56 | 130.863 | 2761.62 | 20.5741 | 2097.64 | 277.649 | 137.4 | 10167.6 | 856.46 | 18074.1 | 134.652 | 36.7503 | 18.1866 | 1345.81 | 113.363 | 2392.33 | 17.8229 | 9 | 666 | 56.1 | 1183.89 | 8.82 | 49284 | 4151.4 | 87607.9 | 652.68 | 349.69 | 7379.58 | 54.978 | 155733 | 1160.21 | 8.6436 | . | 4 | 1 | 0.06905 | 0 | 2.18 | 0 | 0.458 | 7.147 | 54.2 | 6.0622 | 3 | 222 | 18.7 | 396.9 | 5.33 | 0.0047679 | 0 | 0.150529 | 0 | 0.0316249 | 0.4935 | 3.74251 | 0.418595 | 0.20715 | 15.3291 | 1.29123 | 27.4059 | 0.368036 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 4.7524 | 0 | 0.99844 | 15.5805 | 118.156 | 13.2156 | 6.54 | 483.96 | 40.766 | 865.242 | 11.6194 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0.209764 | 3.27333 | 24.8236 | 2.77649 | 1.374 | 101.676 | 8.5646 | 181.78 | 2.44114 | 51.0796 | 387.367 | 43.3265 | 21.441 | 1586.63 | 133.649 | 2836.64 | 38.0935 | 2937.64 | 328.571 | 162.6 | 12032.4 | 1013.54 | 21512 | 288.886 | 36.7503 | 18.1866 | 1345.81 | 113.363 | 2406.09 | 32.3115 | 9 | 666 | 56.1 | 1190.7 | 15.99 | 49284 | 4151.4 | 88111.8 | 1183.26 | 349.69 | 7422.03 | 99.671 | 157530 | 2115.48 | 28.4089 | .   . y = pd.DataFrame(boston_dataset.target, columns=['MEDV']) y.head() . |   | MEDV | . | 0 | 24.0 | . | 1 | 21.6 | . | 2 | 34.7 | . | 3 | 33.4 | . | 4 | 36.2 | .   . +) 데이터 셔플 . # boston housing 데이터는 미리 셔플하지 않으면 교차검증할 때 score값이 매우 낮게 나옴 # 이처럼, 데이터가 특정 순서에 따라 정렬되어 있는 경우 셔플해서 사용하는 게 좋다 joined_data = X.join(y) shuffled_data = joined_data.sample(frac=1) shuffled_X = shuffled_data.iloc[:, :-1] shuffled_y = shuffled_data.iloc[:, -1] . 2. hyperparameter 후보 정리 . # Grid Search를 통해 성능을 테스트해보고 싶은 hyperparameter를 dictionary 형태로 적어준다 # key: parameter명, value: 테스트해볼 값들의 리스트 hyper_parameter = { 'alpha': [0.001, 0.001, 0.01, 0.1, 1], 'max_iter': [500, 1000, 1500, 2000, 3000] } . 3. Grid Search . lasso_model = Lasso() ## hyperparameter를 우선 하나도 넣지 않고 모델을 만든다 grid_search = GridSearchCV(lasso_model, hyper_parameter, cv=5) # 검증할 모델, 검증할 hyperparameter들을 적어준 dictionary를 적어줌 # cv는 성능을 몇 겹 교차검증으로 평가할지 정하는 파라미터. cv=5면 5겹 교차검증을 하겠다는 의미. (cv=5가 default) grid_search.fit(shuffled_X, shuffled_y) # 일반적인 모델을 학습시키듯이 fit()에 X, y를 써주면 된다 . GridSearchCV(cv=5, error_score=nan, estimator=Lasso(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=1000, normalize=False, positive=False, precompute=False, random_state=None, selection='cyclic', tol=0.0001, warm_start=False), iid='deprecated', n_jobs=None, param_grid={'alpha': [0.001, 0.001, 0.01, 0.1, 1], 'max_iter': [500, 1000, 1500, 2000, 3000]}, pre_dispatch='2*n_jobs', refit=True, return_train_score=False, scoring=None, verbose=0) . | refit=True가 default setting이라 여러 hyperparameter 조합을 모두 학습시켜본 후 모델을 최적의 조합으로 다시 학습시켜준다. | . 4. 최적의 hyperparameter 조합 확인 . grid_search.best_params_ ## 최적의 hyperparameter를 찾아줌 . {'alpha': 0.1, 'max_iter': 3000} . +) best 조합의 score를 확인 . grid_search.best_score_ . 0.8425712188051439 . +) score가 높은 Top 5 조합과 그 평균점수 확인 . | cv_results_로 테스트된 각 조합별로, 각 교차검증 회별 점수와 평균점수 등을 모두 확인할 수 있다 | . scores_df = pd.DataFrame(grid_search.cv_results_) scores_df[scores_df['rank_test_score'] &lt; 6][['params', 'mean_test_score', 'rank_test_score']] . |   | params | mean_test_score | rank_test_score | . | 16 | {‘alpha’: 0.1, ‘max_iter’: 1000} | 0.836727 | 4 | . | 17 | {‘alpha’: 0.1, ‘max_iter’: 1500} | 0.838901 | 3 | . | 18 | {‘alpha’: 0.1, ‘max_iter’: 2000} | 0.840577 | 2 | . | 19 | {‘alpha’: 0.1, ‘max_iter’: 3000} | 0.842571 | 1 | . | 24 | {‘alpha’: 1, ‘max_iter’: 3000} | 0.834124 | 5 | . Randomized Search . | hyperparameter grid가 커질수록 Grid Search는 느려진다 | 그렇기에, 가능한 모든 조합을 시도하는 대신, random하게 grid 안 여러 조합을 적당히 건너뛰면서 시도하면 시간을 크게 단축할 수 있다 | Randomized Search를 활용하면 최적의 조합을 건너뛸 확률도 약간 존재하지만, 대체로 빠른 시간 내에 최적에 가까운 조합을 찾아낼 수 있다. (같은 시간 내에 더 많은 hyperparameter를 조정 가능) | . # Import RandomizedSearchCV from sklearn.model_selection import RandomizedSearchCV # GridSearch에서 활용했던 lasso_model와 hyper_parameter를 그대로 활용 lasso_model = Lasso() hyper_parameter = { 'alpha': [0.001, 0.001, 0.01, 0.1, 1], 'max_iter': [500, 1000, 1500, 2000, 3000] } # Randomized Search 모델을 만들고 fit random_search = RandomizedSearchCV(lasso_model, hyper_parameter, cv=5) random_search.fit(shuffled_X, shuffled_y) . RandomizedSearchCV(cv=5, error_score=nan, estimator=Lasso(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=1000, normalize=False, positive=False, precompute=False, random_state=None, selection='cyclic', tol=0.0001, warm_start=False), iid='deprecated', n_iter=10, n_jobs=None, param_distributions={'alpha': [0.001, 0.001, 0.01, 0.1, 1], 'max_iter': [500, 1000, 1500, 2000, 3000]}, pre_dispatch='2*n_jobs', random_state=None, refit=True, return_train_score=False, scoring=None, verbose=0) . *최적의 hyperparameter 조합 확인 . print(random_search.best_params_) . {'max_iter': 3000, 'alpha': 0.1} . +) best 조합의 score를 확인 . random_search.best_score_ . 0.8425712188051439 . +) score가 높은 Top 5 조합과 그 평균점수 확인 . scores_df = pd.DataFrame(random_search.cv_results_) scores_df[scores_df['rank_test_score'] &lt; 6][['params', 'mean_test_score', 'rank_test_score']] . |   | params | mean_test_score | rank_test_score | . | 0 | {‘max_iter’: 1000, ‘alpha’: 0.001} | 0.820117 | 5 | . | 1 | {‘max_iter’: 500, ‘alpha’: 0.01} | 0.824949 | 2 | . | 2 | {‘max_iter’: 3000, ‘alpha’: 0.1} | 0.842571 | 1 | . | 6 | {‘max_iter’: 1000, ‘alpha’: 0.001} | 0.820117 | 5 | . | 7 | {‘max_iter’: 3000, ‘alpha’: 0.001} | 0.820963 | 3 | . | 8 | {‘max_iter’: 3000, ‘alpha’: 0.001} | 0.820963 | 3 | . ",
    "url": "https://chaelist.github.io/docs/ml_advanced/model_selection/#%EA%B7%B8%EB%A6%AC%EB%93%9C-%EC%84%9C%EC%B9%98-grid-search",
    "relUrl": "/docs/ml_advanced/model_selection/#그리드-서치-grid-search"
  },"118": {
    "doc": "테이블 가공",
    "title": "테이블 가공",
    "content": ". | 컬럼 추가/변경/삭제 . | 컬럼 관련 기타 작업들 | . | 컬럼에 속성 추가 . | NOT NULL, DEFAULT, UNIQUE | CURRENT_TIMESTAMP 속성 | CONSTRAINT 속성 | . | 테이블 변경/복사/삭제 | . ",
    "url": "https://chaelist.github.io/docs/sql/modify_table/",
    "relUrl": "/docs/sql/modify_table/"
  },"119": {
    "doc": "테이블 가공",
    "title": "컬럼 추가/변경/삭제",
    "content": ". | 컬럼 추가 . | ALTER TABLE 테이블명 ADD 칼럼 속성;의 구조로 작성 | . -- student라는 테이블에 CHAR(1) 타입 &amp; NULL이 가능한 gender 칼럼을 추가 ALTER TABLE student ADD gender CHAR(1) NULL; . | 컬럼명 변경 . | ALTER TABLE 테이블명 RENAME COLUMN 칼럼명1 TO 칼럼명2;의 구조로 작성 | . -- student 테이블의 student_number 컬럼을 registration_number라는 이름으로 변경 ALTER TABLE student RENAME COLUMN student_number TO registration_number; . | 컬럼 삭제 . | ALTER TABLE 테이블명 DROP COLUMN 컬럼;의 구조로 작성 | . -- student 테이블에서 admission_date 컬럼을 삭제 ALTER TABLE student DROP COLUMN admission_date; . | 컬럼의 데이터 타입 변경 . | ALTER TABLE 테이블명 MODIFY 컬럼 속성;의 구조로 작성 | ※ 유의: 데이터 타입을 변경하려면 컬럼 내 값들이 해당 타입이 수용할 수 있는 형태가 되어야 한다 (ex. INT 타입으로 변경하려면 우선 값들을 다 숫자로 바꿔줘야 한다) | . -- 전공명이 저장된 VARCHAR(15) 타입의 컬럼 'major'를 전공코드를 저장하는 INT 타입으로 변경: --- UPDATE문으로 각 전공명을 전공코드로 변경해준 후, major 컬럼의 데이터 타입을 INT로 바꿔줌 UPDATE student SET major = 10 WHERE major = '언론홍보영상학과'; UPDATE student SET major = 12 WHERE major = '경영학과'; ALTER TABLE student MODIFY major INT; . | . 컬럼 관련 기타 작업들 . | 컬럼을 가장 앞으로 보내기 . | 속성으로 ‘FIRST’를 추가: ALTER TABLE 테이블명 MODIFY 컬럼 속성 FIRST;의 구조 | . -- id 컬럼을 가장 앞으로 보내주기 (주로 primary key를 가장 앞에 둔다) ALTER TABLE student MODIFY id INT NOT NULL AUTO_INCREMENT FIRST; . | 컬럼 간 순서 바꾸기 . | ‘AFTER’를 사용: ALTER TABLE 테이블명 MODIFY 컬럼 속성 AFTER 컬럼2;의 구조 | . -- gender 컬럼을 name 컬럼 뒤로 보내기 ALTER TABLE student MODIFY gender CHAR(1) NULL AFTER name; . | 컬럼의 이름과 데이터 타입 및 속성 동시에 수정하기 . | ‘CHANGE’절을 이용하면 컬럼명과 데이터 타입, 속성을 동시에 수정할 수 있다 | ALTER TABLE 테이블명 CHANGE 컬럼명1 컬럼명2 속성의 구조 | . -- role 컬럼의 이름을 'position'으로 바꾸고, 데이터 타입은 VARCHAR(5), 속성은 NOT NULL로 바꿔줌 ALTER TABLE player CHANGE role position VARCHAR(5) NOT NULL; . | 하나의 ALTER TABLE문으로 여러 개의 작업을 하는 것도 가능 . | ex) 아래 4가지 작업을 동시에 수행: 1) id 컬럼의 이름을 number로 수정 2) name 컬럼의 데이터 타입을 VARCHAR(20)로, 속성을 NOT NULL로 수정 3) position 컬럼을 테이블에서 삭제 4) 새로운 컬럼 height를 추가 | . ALTER TABLE player RENAME COLUMN id TO number, MODIFY name VARCHAR(20) NOT NULL, DROP COLUMN position, ADD height DOUBLE NOT NULL; . | . ",
    "url": "https://chaelist.github.io/docs/sql/modify_table/#%EC%BB%AC%EB%9F%BC-%EC%B6%94%EA%B0%80%EB%B3%80%EA%B2%BD%EC%82%AD%EC%A0%9C",
    "relUrl": "/docs/sql/modify_table/#컬럼-추가변경삭제"
  },"120": {
    "doc": "테이블 가공",
    "title": "컬럼에 속성 추가",
    "content": "NOT NULL, DEFAULT, UNIQUE . | NOT NULL 속성 추가 . | ALTER ~ MODIFY 문을 사용해서 NOT NULL 속성 추가: ALTER TABLE 테이블명 MODIFY 컬럼 속성 NOT NULL의 구조 | MODIFY 뒤에는 컬럼의 기존 데이터 속성도 함께 써준 후에 NOT NULL을 추가해야 한다. 혹은, 변경하려고 하는 데이터 속성을 써줘도 된다 (데이터 타입 변경 &amp; NOT NULL 추가 동시에) | NOT NULL 속성을 추가한 컬럼에 값이 없는 row를 추가하려고 하면 error가 난다 | . -- student 테이블에 NOT NULL 속성 추가 ALTER TABLE student MODIFY name VARCHAR(20) NOT NULL; . | DEFAULT 속성 추가 . | ALTER ~ MODIFY 문을 사용해서 DEFAULT 속성 추가 | DEFAULT 값을 추가해두면, 해당 컬럼에 값이 없는 row를 추가할 때 자동으로 DEFAULT 값이 입력됨 | NOT NULL 속성을 갖고 있지 않은 컬럼들은 Default값이 자동으로 ‘NULL’로 설정되어 있음: 해당 컬럼에 값이 없는 row를 추가하면, 자동으로 NULL값이 들어간다 | . -- major 컬럼의 default값을 101로 설정 -- 앞으로 major 값이 없는 row를 추가하면 자동으로 101이라는 값이 저장된다는 의미 ALTER TABLE student MODIFY major INT NOT NULL DEFAULT 101; . | UNIQUE 속성 추가 . | ALTER ~ MODIFY 문을 사용해서 UNIQUE 속성 추가 | UNIQUE 속성을 추가해두면, 컬럼에 이미 존재하는 값과 중복되는 값을 가진 새 row를 입력하려고 하면 error가 난다 | ‘학번’처럼, 각 값이 반드시 고유한 값을 가져야 하는 컬럼의 경우 UNIQUE 속성을 주면 좋다 | cf) UNIQUE 속성이 있는 컬럼이라도, NULL값은 허용될 수 있다. (Primary Key의 경우 반드시 NOT NULL이여야 하는 것과 조금 다른 개념…) | . -- student_number 컬럼에 UNIQUE 속성 추가 ALTER TABLE student MODIRY student_number INT NOT NULL UNIQUE; . | . CURRENT_TIMESTAMP 속성 . : DATETIME, TIMESTAMP 타입의 컬럼에 추가할 수 있는 속성. row가 추가/갱신될 때마다 현재 시각을 저장해줘야 하는 경우 유용하다. (ex. 게시글 업로드 시각, 마지막 수정 시각) . | DEFAULT CURRENT_TIMESTAMP 속성 . | 테이블에 새 row를 추가할 때 별도로 NOW()값을 주지 않아도 현재 시간이 자동 저장되도록 하는 속성 | . | ON UPDATE CURRENT_TIMESTAMP 속성 . | 기존 row가 단 하나의 컬럼이라도 수정되면, 업데이트될 때의 시간이 자동으로 저장되도록 하는 속성. | . | . -- upload_time 컬럼에는 DEFAULT CURRENT_TIMESTAMP 속성을, -- recent_modified_time 컬럼에는 DEFAULT CURRENT_TIMESTAMP 속성과 ON UPDATE CURRENT_TIMESTAMP 속성을 모두 추가 ALTER TABLE post MODIFY upload_time DATETIME DEFAULT CURRENT_TIMESTAMP, MODIFY recent_modified_time DATETIME DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP; . | 이렇게 해두면 1) 새로운 게시글을 추가할 때 다른 컬럼들에만 값을 적어줘도 upload_time, recent_modified_time 컬럼에 자동으로 현재 시간이 들어가고, 2) 게시글 내용을 수정하면 recent_modified_time의 값이 자동으로 해당 시각으로 업데이트된다 | .   . +) NOW() 함수: 현재 시각을 추가해주는 함수 . | NOW() 함수를 사용해도 위와 같은 결과를 낼 수 있음 (다만, 수동으로 NOW()를 매법 입력해줘야 한다) . INSERT INTO post (title, content, upload_time, recent_modified_time) VALUES (\"제목\", \"본문\", NOW(), NOW()); -- upload_time, recent_modified_time 모두 현재 시각이 추가됨 . UPDATE post SET content = '본문 수정', recent_modified_time = NOW() WHERE id = 1; -- recent_modified_time이 현재 시각으로 변경됨 . | . CONSTRAINT 속성 . : 컬럼에 적절한 constraint(제약 사항)을 걸어두면 잘못된 데이터가 입력되는 것을 막아서 데이터 퀄리티를 관리할 수 있다 . | ADD CONSTRAINT . | 컬럼에 제약 사항을 추가해주는 것 | ALTER TABLE 테이블명 ADD CONSTRAINT 제약이름 CHECK (조건);의 구조로 작성 | 제약 조건에 위배되는 데이터를 넣으려고 하면 error가 발생한다 | . -- student 테이블에 'student_number는 3천만보다 작아야 한다'는 제약 걸기 ALTER TABLE student ADD CONSTRAINT st_rule CHECK (student_number &lt; 30000000); -- (여기서 st_rule은 이 제약 사항의 이름) -- 위와 같은 제약을 건 후에 student_number에 30000000이 들어가는 row를 삽입하려 하면 error가 발생 . | DROP CONSTRAINT . | 걸어둔 CONSTRAINT 삭제하기 | ALTER TABLE 테이블명 DROP CONSTRAINT 제약이름;의 구조 | . ALTER TABLE student DROP CONSTRAINT st_rule; . | 두 개 이상의 조건이 담긴 CONSTRAINT 만들기 . | 여러 조건을 AND로 연결해주면 된다 | . -- email 컬럼에 들어갈 값에는 '@'가 들어가야 하고, gender 컬럼에 들어갈 값은 'm'이나 'f' 둘 중 하나여야 한다는 조건 걸기: ALTER TABLE student ADD CONSTRAINT st_rule CHECK (email LIKE '%@%' AND gender IN ('m', 'f')); . | . ",
    "url": "https://chaelist.github.io/docs/sql/modify_table/#%EC%BB%AC%EB%9F%BC%EC%97%90-%EC%86%8D%EC%84%B1-%EC%B6%94%EA%B0%80",
    "relUrl": "/docs/sql/modify_table/#컬럼에-속성-추가"
  },"121": {
    "doc": "테이블 가공",
    "title": "테이블 변경/복사/삭제",
    "content": ". | 테이블 이름 변경 . | RENAME TABLE 테이블명 TO 테이블명2의 구조로 작성 | . RENAME TABLE student TO undergraduate; . | 테이블 복사 . | ‘AS’를 사용해 SELECT문으로 가져온 데이터를 복사한 테이블을 생성 | CREATE TABLE 테이블명 AS SELECT문;의 구조로 작성 | . CREATE TABLE undergraduate_copy AS SELECT * FROM undergraduate; . +) ‘WHERE’절을 사용해 특정 조건의 데이터만 복사한 테이블을 생성하는 것도 가능: . -- gender가 'u'인 item 테이블의 row들만 새로운 테이블로 복사 CREATE TABLE item_copy AS SELECT * FROM item WHERE gender = 'u'; . | 테이블 삭제 . | DROP TABLE 테이블명;의 구조로 작성 | . DROP TABLE undergraduate_copy; . | 테이블의 컬럼 구조 복사 . | 테이블 내의 데이터를 제외하고, 컬럼 구조만 복사해오는 것 | CREATE TABLE 테이블명 LIKE 기존_테이블;의 구조로 작성 | . CREATE TABLE undergraduate_copy LIKE undergraduate; . +) 컬럼 구조만 복사해둔 테이블에, 다시 데이터까지 복사해서 붙여넣으려면?: . -- SELECT문으로 가져온 데이터를 INSERT INTO문으로 넣어주기 INSERT INTO undergraduate_copy SELECT * FROM undergraduate; . | 테이블 내 데이터만 삭제 . | 테이블의 구조는 남기고, 데이터만 모두 삭제하기 (cf. DROP TABLE은 테이블 자체를 삭제) | TRUNCATE 테이블명;의 구조로 작성 | . TRUNCATE exam_result; . +) DELETE FROM을 사용해도 데이터를 모두 삭제하는 결과를 낼 수 있다: . DELETE FROM exam_result; . | 다만, DELETE FROM으로 삭제하면 데이터가 한줄한줄 제거되는 과정을 거치게 되며, 데이터는 지워지되 사용하던 공간은 그대로 남아 있음 (TRUNCATE와 DELETE는 내부적으로 구현되는 방식이 다름) | . | . ",
    "url": "https://chaelist.github.io/docs/sql/modify_table/#%ED%85%8C%EC%9D%B4%EB%B8%94-%EB%B3%80%EA%B2%BD%EB%B3%B5%EC%82%AC%EC%82%AD%EC%A0%9C",
    "relUrl": "/docs/sql/modify_table/#테이블-변경복사삭제"
  },"122": {
    "doc": "Network Analysis",
    "title": "Network Analysis",
    "content": " ",
    "url": "https://chaelist.github.io/docs/network_analysis",
    "relUrl": "/docs/network_analysis"
  },"123": {
    "doc": "Network Analysis 기초",
    "title": "Network Analysis 기초",
    "content": ". | Network 기본 개념 . | Node와 Edge | Tie(관계)의 종류 | . | 간단한 네트워크 그려보기 . | 그래프 생성 | Basic Calculations | 네트워크 시각화 | node, edge에 attribute 더하기 | attribute 포함해서 시각화 | 특정 조건의 node, edge 찾기 | DiGraph 그리기 &amp; self-loop | . | Network 구조 파악하기 . | Neighbors &amp; Degree | Shortest path | Centrality | Cliques | Subgraphs | . | Visualization with nxviz . | nxviz example 1 | nxviz example 2: complex network | . | . ",
    "url": "https://chaelist.github.io/docs/network_analysis/network_basics/",
    "relUrl": "/docs/network_analysis/network_basics/"
  },"124": {
    "doc": "Network Analysis 기초",
    "title": "Network 기본 개념",
    "content": "Node와 Edge . | Network: Node와 Edge로 이루어진 자료 구조. | Node: vertex라고도 부르며, network를 이루는 각 점을 의미한다. ex) twitter user network를 만든다면, user들이 각각의 node. | Edge: link, 또는 tie라고도 부르며, 각 node 간의 관계를 의미한다. ex) twitter user network를 만든다면, user(=node)간 follow 관계를 edge로 표현할 수 있다. | . Tie(관계)의 종류 . | 비대칭 관계(asymmetric tie): 관계에서 두 node가 갖는 특성이 다른 경우. | 관계에 방향성이 있으므로, directed tie라고도 한다. | ex) SNS에서의 follower - followee 관계 (A가 B를 follow하면 A → B와 같이 관계의 방향이 존재) | directed tie로 구성된 네트워크를 directed network라고 함 | . | 대칭 관계 (symmetric tie): 관계에서 두 node가 갖는 특성이 비슷한 경우 . | 관계에 방향성이 없으므로, undirexted tie라고도 한다. | ex) 특정 집단 내 사람들 간의 친분 관계. (서로 아는 사이인지) | undirected tie로 구성된 네트워크를 undirected network라고 함 | . | . ",
    "url": "https://chaelist.github.io/docs/network_analysis/network_basics/#network-%EA%B8%B0%EB%B3%B8-%EA%B0%9C%EB%85%90",
    "relUrl": "/docs/network_analysis/network_basics/#network-기본-개념"
  },"125": {
    "doc": "Network Analysis 기초",
    "title": "간단한 네트워크 그려보기",
    "content": ". | network 분석에 특화된 python library인 networkx를 사용 | . import networkx as nx # import해야 사용 가능; 보통 nx로 줄여서 import import matplotlib.pyplot as plt # 시각화를 위해 미리 함께 import . 그래프 생성 . | 먼저, 비어있는 graph를 생성 # Create an empy undirected graph g = nx.Graph() . | directed graph를 그리려면: nx.DiGraph() | +) nx.MultiGraph(), nx.MultiDiGraph() 옵션도 존재 (multi-edge graph) . | 같은 node 2개를 연결하는 edge가 여러 개일 수 있는 그래프. | ex) 정류장 간 trip을 형상화 → A 정류장에서 B 정류장을 거쳐 가는 route가 3종류면 edge가 3개 | 하지만 multi edge를 그리려면 리소스가 많이 들어서, 보통 그냥 edge 하나로 collapse해주고 ‘weight’ metadata로 edge의 강도를 표현해준다. (Graph나 DiGraph를 그리고, weight를 지정) | . | . +) type 확인: ‘Graph’ type . type(g) . networkx.classes.graph.Graph . | directed graph의 type은 networkx.classes.digraph.DiGraph | . | node 추가하기 g.add_nodes_from([1,2,3,4,5,6]) # Add nodes from a list . | cf) node를 하나씩 추가하려면: g.add_node(1) 이렇게 하나씩 써주면 된다 | . | edge 추가하기 g.add_edges_from([(1,3), (2,4), (2,5), (2,6), (3,4), (4,6), (5,6)]) # Add edges from a list . | cf) edge를 하나씩 추가하려면: g.add_edge(1,3) 이렇게 하나씩 써주면 된다 | 만약 모든 node가 edge가 연결되어 있다면, edge만 추가해도 그 안에 포함된 node도 자동으로 추가된다 | . | . Basic Calculations . print(g.nodes()) # nodes print(g.edges()) # edges print(g.number_of_nodes()) # number of nodes print(g.number_of_edges()) # number of edges . [1, 2, 3, 4, 5, 6] [(1, 3), (2, 4), (2, 5), (2, 6), (3, 4), (4, 6), (5, 6)] 6 7 . 네트워크 시각화 . | nx.draw_networkx() 함수를 이용해 시각화 | draw_networkx는 다른 것과 연결이 많은 노드를 중심에 오게 자동으로 위치를 결정해서 가시화해주며, node의 label도 자동으로 함께 보여준다 | nx.draw() 함수를 사용해도 되지만, draw_networkx가 부가 기능이 더 많음 | . nx.draw_networkx(g) plt.axis('off') # turn off axis plt.show() . +) graphML 파일로 저장하기 . | graphML 파일로 내보낸 후, gephi 등의 툴로 시각화할 수도 있다 | . nx.write_graphml(g, 'graph_test.graphml') . node, edge에 attribute 더하기 . | Add node attributes g.nodes[1]['gender']='male' # {'gender':'male'}이라는 dictionary의 느낌. key-value pair. g.nodes[2]['gender']='female' g.nodes[3]['gender']='male' g.nodes[4]['gender']='female' g.nodes[5]['gender']='male' g.nodes[6]['gender']='male' print(nx.get_node_attributes(g, 'gender')) . {1: 'male', 2: 'female', 3: 'male', 4: 'female', 5: 'male', 6: 'male'} . | Add edge attributes g[1][3]['weight'] = 3 ## 이렇게 접근해도 됨: g.edges[1, 3]['weight'] = 3 g[2][4]['weight'] = 1 g[2][5]['weight'] = 4 g[2][6]['weight'] = 3 g[3][4]['weight'] = 2 g[4][6]['weight'] = 3 g[5][6]['weight'] = 4 print(nx.get_edge_attributes(g, 'weight')) . {(1, 3): 3, (2, 4): 1, (2, 5): 4, (2, 6): 3, (3, 4): 2, (4, 6): 3, (5, 6): 4} . | 1-3 사이의 edge를 접근하는 방법: g[1][3] = g.edges[1, 3] (어떻게 접근하든 상관없음) | . | . +) 속성까지 포함해서 node, edge 보기 . | data=True 옵션을 넣어주면 됨 | . g.nodes(data=True) ## node들의 속성까지 함께 볼 수 있음 . NodeDataView({1: {'gender': 'male'}, 2: {'gender': 'female'}, 3: {'gender': 'male'}, 4: {'gender': 'female'}, 5: {'gender': 'male'}, 6: {'gender': 'male'}}) . g.edges(data=True) ## edge들의 속성까지 함께 볼 수 있음 . EdgeDataView([(1, 3, {'weight': 3}), (2, 4, {'weight': 1}), (2, 5, {'weight': 4}), (2, 6, {'weight': 3}), (3, 4, {'weight': 2}), (4, 6, {'weight': 3}), (5, 6, {'weight': 4})]) . attribute 포함해서 시각화 . | edge의 ‘weight’를 포함해서 시각화 pos=nx.spring_layout(g) # 각 node의 position을 정해서 그려줘야 edge_label를 맞춰서 넣을 수 있음 nx.draw_networkx(g, pos) labels = nx.get_edge_attributes(g,'weight') nx.draw_networkx_edge_labels(g, pos, edge_labels=labels) plt.axis('off') # turn off axis plt.show() . | node의 ‘gender’ attribute에 따라 색 다르게 표현 color_map = [] for n, d in g.nodes(data=True): if d['gender'] == 'female': color_map.append('pink') # 여성: pink else: color_map.append('skyblue') # 남성: skyblue pos=nx.spring_layout(g) nx.draw_networkx(g, pos, node_color=color_map) labels = nx.get_edge_attributes(g,'weight') plt.axis('off') # turn off axis plt.show() . | . 특정 조건의 node, edge 찾기 . | ‘gender’가 ‘female’인 node만 찾기 female_nodes = [n for n, d in g.nodes(data=True) if d['gender'] == 'female'] print(female_nodes) . [2, 4] . | ‘weight’가 3보다 큰 edge만 찾기 strong_edges = [(u, v) for u, v, d in g.edges(data=True) if d['weight'] &gt; 3] print(strong_edges) . [(2, 5), (5, 6)] . | . DiGraph 그리기 &amp; self-loop . diG = nx.DiGraph() diG.add_edges_from([(1, 2), (2, 4), (4, 2), (3, 3), (1, 3), (5, 1)]) # edge만 추가해도, 자동으로 이에 포함된 node도 함께 추가됨 . → 시각화해보기 . nx.draw_networkx(diG) plt.axis('off') # turn off axis plt.show() . +) self-loop 확인하기 . | self-loop: 자기 자신으로 돌아오는 루프 (edge that begin and end on the same node) | ex) 버스 노선 중, A 플랫폼에서 떠나서 A 플랫폼으로 돌아오는 순환선이 self-loop로 표현될 수 있음. | 이번 diG에서 추가한 (3, 3) edge가 바로 self-loop | self-loop는 그래프를 시각화했을 때 잘 눈에 띄지 않기에, nx.number_of_selfloops()함수로 self-loop의 개수를 파악할 수 있다. | . nx.number_of_selfloops(diG) # diG에는 1개의 self-loop가 포함되어 있음 . 1 . ",
    "url": "https://chaelist.github.io/docs/network_analysis/network_basics/#%EA%B0%84%EB%8B%A8%ED%95%9C-%EB%84%A4%ED%8A%B8%EC%9B%8C%ED%81%AC-%EA%B7%B8%EB%A0%A4%EB%B3%B4%EA%B8%B0",
    "relUrl": "/docs/network_analysis/network_basics/#간단한-네트워크-그려보기"
  },"126": {
    "doc": "Network Analysis 기초",
    "title": "Network 구조 파악하기",
    "content": "# 위에서 생성한 g 네트워크를 이어서 사용 nx.draw_networkx(g) plt.axis('off') # turn off axis plt.show() . Neighbors &amp; Degree . | g.neighbors(node): 그래프 g 내에서 특정 node와 연결된 neighbor node들을 파악할 수 있음 | g.degree(node): 그래프 g 내에서 특정 node의 neighbor 수를 파악할 수 있음. (연결된 node의 수) print(list(g.neighbors(4))) # 4's neighbors print(g.degree(4)) # 4's degree, i.e., number of neighbors . [2, 3, 6] 3 . | . Shortest path . | nx.shortest_path(g, node1, node2)를 활용하면 네트워크 g의 node1에서 node2까지의 shortest path를 찾을 수 있다. | +) nx.shortest_path(g, node1, node2, weight='weight')라고 하면 edge의 weight을 고려해서 찾아준다 | . nx.shortest_path(g, 3, 5) . [3, 4, 2, 5] . +) 그냥 nx.shortest_path(g)라고만 하면 g의 모든 node 간의 shortest path를 모두 찾아준다 . Centrality . : node의 중요성을 판별할 때 활용. | Degree Centrality: tie가 얼마나 많은지의 정도 . | 특정 node의 neighbor 수 / 최대로 가질 수 있는 neighbor 수 = degree / (n-1) | 최대로 가질 수 있는 neighbor 수는 n-1. (self-loop 고려X) | . # 각 node별 degree centrality 값을 dictionary 형태로 보여줌 nx.degree_centrality(g) # key: node, value: degree centrality score for that node . {1: 0.2, 2: 0.6000000000000001, 3: 0.4, 4: 0.6000000000000001, 5: 0.4, 6: 0.6000000000000001} . → 2, 4, 6이 가장 중요한 node로 판별됨 . | Betweenness Centrality: 얼마나 bridge 역할을 하는지의 정도 . | 특정 node를 지나가는 shortest path의 수 / 모든 가능한 shortest path의 수 | 여러 집단을 이어주는 역할을 하는 node를 파악할 때 용이 (ex. 정치 관심 그룹과 예술 관심 그룹을 이어주는 역할을 하는 node 파악) | . # 각 node별 betweenness centrality 값을 dictionary 형태로 보여줌 nx.betweenness_centrality(g) . {1: 0.0, 2: 0.15000000000000002, 3: 0.4, 4: 0.6000000000000001, 5: 0.0, 6: 0.15000000000000002} . → 4가 가장 중요한 node로 판별됨 . +) nx.betweenness_centrality(g, weight='weight')라고 weight 옵션을 적어주면, 각 edge의 weight를 고려해서 centrality를 계산 . | Closeness Centrality: 다른 node들과 얼마나 close하게 연결되어 있나 . | 특정 node에서의 다른 node까지의 shortest path를 고려 | (n-1) / 특정 node에서 다른 모든 node까지의 shortest path distance의 합 | n-1은 sum of minimum possible distances. (모든 다른 node까지 1로 가는 게 minimum이니까) | 분모(특정 node에서 다른 모든 node까지의 shortest path distance의 합)가 작을수록 central한 것이므로, closeness centraliy 값은 높을수록 더 중요도가 높은 것! | . # 각 node별 closeness centrality 값을 dictionary 형태로 보여줌 nx.closeness_centrality(g) . {1: 0.38461538461538464, 2: 0.625, 3: 0.5555555555555556, 4: 0.7142857142857143, 5: 0.45454545454545453, 6: 0.625} . → 4가 가장 중요한 node로 판별됨 . | Eigenvector centrality: 얼마나 central한 node들과 연결되어 있나 . | 특정 node의 neighbor들의 centrality를 고려 | ex) 단순히 많은 follower가 있는 사람보다 많은 follower가 있는 사람들에 의해 많이 follow되는 사람을 더 중요한 influencer라고 간주 | Ax = &lambda;x → eigenvector x의 n번째 값이 n번째 node의 eigenvector centrality. | A: adjacency matrix of the graph, λ: eigenvalue, x: eigenvector | . # 각 node별 eigenvector centrality 값을 dictionary 형태로 보여줌 nx.eigenvector_centrality(g) . {1: 0.07902199743319213, 2: 0.5299719499101774, 3: 0.20983546432528058, 4: 0.47818048045123035, 5: 0.39915848301808887, 6: 0.5299719499101774} . → 2, 6이 가장 중요한 node로 판별됨 . +) nx.eigenvector_centrality(g, weight='weight')라고 weight 옵션을 적어주면, 각 edge의 weight를 고려해서 centrality를 계산 . | . Cliques . : completely connected network . # 예시로 barbell graph를 만들어 사용 barbell_g = nx.barbell_graph(m1=5, m2=1) nx.draw_networkx(barbell_g) plt.axis('off') # turn off axis plt.show() . +) networkX로 만들 수 있는 기본 모양들: https://networkx.org/documentation/stable//reference/generators.html . | triangles: 3개의 node가 모두 서로 연결된 모양 . | simplest complex clique: a triangle. | 친구 추천 시스템에서 triangle 개념 활용 가능: ex) A와 B가 친구이고 A와 C가 친구 → A와 C도 서로 알 가능성이 높음 | . ## 각각의 node가 몇 개의 triangle에 속해 있는지 출력 nx.triangles(barbell_g) . {0: 6, 1: 6, 2: 6, 3: 6, 4: 6, 5: 0, 6: 6, 7: 6, 8: 6, 9: 6, 10: 6} . ## barbell_g 그래프의 6 node가 몇 개의 triangle에 속해 있는지만 출력 nx.triangles(barbell_g, 6) . 6 . | Maximal Cliques: 찾아지는 최대 크기의 clique. | a clique that, when extended by one more node is no longer a clique. | 네트워크 안의 특정 community를 찾는 데에 응용될 수 있다: Cliques form a good starting point for finding communities, as they are fully connected subgraphs within a larger graph. | . # 각각 maximal clique을 이루고 있는 node들의 list를 출력 list(nx.find_cliques(barbell_g)) . [[4, 0, 1, 2, 3], [4, 5], [6, 5], [6, 7, 8, 9, 10]] . ## 특정 node 6이 속한 clique들을 모두 출력 nx.cliques_containing_node(barbell_g, 6) . [[6, 5], [6, 7, 8, 9, 10]] . | . Subgraphs . : 큰 group에서 일부를 떼어서 Subgraph로 그려보는 것이 유용할 때가 있다 (특정 node 사이의 path, communties / cliques, degree of seperation 등을 파악하기 용이) . # 예시로 Erdős-Rényi graph를 만들어 사용 G = nx.erdos_renyi_graph(n=20, p=0.2) plt.axis('off') # turn off axis plt.show() . | node 8과 이와 연결된 neighbor 노드들을 추출 nodes = list(G.neighbors(8)) nodes.append(8) nodes . [2, 4, 15, 16, 8] . | node 8과 이와 연결된 node들로 subgraph를 구성 G_eight = G.subgraph(nodes) # subgraph를 구성할 node의 list를 G.subgraph() 함수에 넣어준다 G_eight.edges() # 넣어준 list 속 node들 사이의 edge가 반영됨 . EdgeView([(2, 4), (2, 8), (4, 8), (8, 15), (8, 16)]) . → subgraph 시각화 . nx.draw_networkx(G_eight) plt.axis('off') # turn off axis plt.show() . | 이렇게 특정 node와 이에 연결된 node들만으로 구성한 network를 ‘Ego Network’라고 한다 | . *전체 네트워크와 Ego Network . | 전체 네트워크(Whole Network): 네트워크를 구성하고 있는 모든 node와 그 사이의 관계를 모두 포함 (ex. 중세 플로랑스 지역 주요 가문들 간의 혼인 관계 네트워크) | ego network: 특정 node의 personal network를 의미 (ex. Medici 가문의 ego network: Medici 가문과 연결된 tie만 표현) | . | 특정 graph와 이의 subgraph는 동일한 type을 갖게 됨. print(type(G)) print(type(G_eight)) . &lt;class 'networkx.classes.graph.Graph'&gt; &lt;class 'networkx.classes.graph.Graph'&gt; . | . ",
    "url": "https://chaelist.github.io/docs/network_analysis/network_basics/#network-%EA%B5%AC%EC%A1%B0-%ED%8C%8C%EC%95%85%ED%95%98%EA%B8%B0",
    "relUrl": "/docs/network_analysis/network_basics/#network-구조-파악하기"
  },"127": {
    "doc": "Network Analysis 기초",
    "title": "Visualization with nxviz",
    "content": "*nxvis: a graph visualization package for NetworkX . # 먼저 설치해줘야 사용할 수 있다 $ conda install -c conda-forge nxviz ## conda install이 추천되지만, $ pip install nxviz로 설치해도 괜찮음 . nxviz example 1 . : 가장 위에서 만들어뒀던 ‘g’ 네트워크를 사용 . color_map = [] for n, d in g.nodes(data=True): if d['gender'] == 'female': color_map.append('pink') # 여성: pink else: color_map.append('skyblue') # 남성: skyblue pos=nx.spring_layout(g) nx.draw_networkx(g, pos, node_color=color_map) labels = nx.get_edge_attributes(g,'weight') nx.draw_networkx_edge_labels(g, pos,edge_labels=labels) plt.axis('off') # turn off axis plt.show() . | Arc Plot: 한 줄로 늘어선 node 간의 연결 관계를 반원 모양 라인으로 표현 . import nxviz as nv # import해서 사용 import matplotlib.pyplot as plt ap = nv.ArcPlot(g, node_color='gender', # 성별에 따라 색을 다르게 표현 edge_width='weight') # edge의 weight에 따라 선의 굵기를 다르게 표현 ap.draw() . | Circos Plot: Arc Plot의 양 끝을 circle로 결합한 모양. 동그라미 형태로 node들을 그려주고, 서로의 연결관계가 곡선으로 표현됨. c = nv.CircosPlot(g, node_labels=True, # node label을 함께 보여줌 node_color='gender', # 성별에 따라 색을 다르게 표현 node_order='gender') # 성별에 따라 묶어서 보여줌 (순서를 조정) c.draw() . | Matrix Plot: 각 node가 행/열에 들어가고, 서로 연결이 있는 node라면 해당 cell이 진하게 표현됨 . | weight에 따라 색의 강도가 다르게 표현됨 | Directed Network라면, A→B 연결은 행:A, 열:B에 해당되는 위치에 칠해서 표현 | . m = nv.MatrixPlot(g) m.draw() . | . nxviz example 2: complex network . | network에 node와 edge가 많으면 많을수록, 시각화하면 hairball 같은 형태가 되어 알아보기 어렵기 때문에, 복잡한 network일수록 nxviz로 시각화하면 더 깔끔하게 보여줄 수 있다. | . # 예시로 node가 30개인 Erdős-Rényi graph를 만들어 사용 . from random import choice G = nx.erdos_renyi_graph(n=30, p=0.2) for n, d in G.nodes(data=True): G.nodes[n][\"class\"] = choice([\"one\", \"two\", \"three\"]) . → 평범한 시각화 . color_map = [] for n, d in G.nodes(data=True): if d['class'] == 'one': color_map.append('pink') elif d['class'] == 'two': color_map.append('skyblue') else: color_map.append('lightgrey') nx.draw_networkx(G, node_color=color_map) plt.axis('off') # turn off axis plt.show() . | Arc Plot ap = nv.ArcPlot(G, node_color=\"class\", node_order='class') ap.draw() . | Circos Plot c = nv.CircosPlot(G, node_labels=True, node_color=\"class\", node_order=\"class\") c.draw() . | Matrix Plot . # 예시로 Lollipop Graph를 그려서 사용 . import numpy.random as npr lp_G = nx.lollipop_graph(m=10, n=4) nx.draw_networkx(lp_G) plt.axis('off') # turn off axis plt.show() . → Matrix Plot으로 표현 . m = nv.MatrixPlot(G) m.cmap = plt.cm.get_cmap(\"Greens\") # colormap을 Green으로 지정 m.draw() . | . ",
    "url": "https://chaelist.github.io/docs/network_analysis/network_basics/#visualization-with-nxviz",
    "relUrl": "/docs/network_analysis/network_basics/#visualization-with-nxviz"
  },"128": {
    "doc": "뉴스 기사 Clustering",
    "title": "뉴스 기사 Clustering",
    "content": ". | 뉴스 기사 Clustering . | 뉴스 기사 준비 | tokenize &amp; 명사만 저장 | Text Vector화 &amp; 학습 | 학습 결과 확인 | . | . ",
    "url": "https://chaelist.github.io/docs/ml_application/news_clustering/",
    "relUrl": "/docs/ml_application/news_clustering/"
  },"129": {
    "doc": "뉴스 기사 Clustering",
    "title": "뉴스 기사 Clustering",
    "content": ". | 뉴스 기사의 내용을 바탕으로 유사한 기사끼리 묶어보려고 함 | . # 사용할 library를 먼저 모두 import import pandas as pd import konlpy from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.cluster import AgglomerativeClustering . 뉴스 기사 준비 . | 우선, Clustering할 기사 데이터를 수집 | 다음 뉴스에서, ‘국회/정당’, ‘금융’, ‘건강’, ‘사건/사고’ 4개 토픽의 기사를 조금씩 수집해 옴 | . ## '다음 뉴스'에서 직접 수집해 온 데이터를 dataframe으로 정리해 둠 news_df.head() . |   | topic | article_title | news_text | . | 0 | 국회/정당 | [포토]재보선 D-1, ‘시민들의 선택은?’ | [국회사진취재단] 6일 오후 서대문구 홍제역에서 열린 박영선 더불어민주당 서울시장… | . | 1 | 국회/정당 | \"약속 지킨다\"..김종인, 8일 국민의힘 떠난다 | 김종인 국민의힘 비상대책위원장이 5일 오후 서울 관악구 서울대입구역에서 오세훈 서울… | . | 2 | 국회/정당 | \"위선·무능 정부 심판해야\"..빨강 운동화에 ‘골목’ 찾은 오세훈 | 오세훈 국민의힘 서울시장 후보가 6일 서울 성북구 정릉시장에서 만난 한 시민에게 허… | . | 3 | 국회/정당 | 김종인, 강남 대치역 찾아 오세훈 후보 지지호소 | (서울=뉴스1) 국회사진취재단 = 김종인 국민의힘 비상대책위원장이 6일 오후 서울… | . | 4 | 국회/정당 | 이낙연 \"특권층 득세하고 차별하는 서울로 퇴보할 텐가\" | [이데일리 이정현 기자] 이낙연 더불어민주당 상임선대위원장이 6일 오세훈 국민의힘.. | .   . news_df.groupby('topic')[['article_title']].count() . | topic | article_title | . | 건강 | 16 | . | 국회/정당 | 13 | . | 금융 | 15 | . | 사건/사고 | 15 | . tokenize &amp; 명사만 저장 . | 기사의 주제를 판단하는 데에 명사가 가장 중요하다고 생각해 명사만 사용 | . documents_processed = [] for text in news_df['news_text']: okt = konlpy.tag.Okt() okt_pos = okt.pos(text) words = [] for word, pos in okt_pos: if 'Noun' in pos: words.append(word) documents_processed.append(' '.join(words)) print(len(documents_processed)) documents_processed[0] . 59 '국회 진취 재단 오후 서대문구 홍제역 박영선 민주당 서울시장 후보 집중 유세 시민 휴대폰 관심 표명 노진환' . Text Vector화 &amp; 학습 . tfidf_vectorizer = TfidfVectorizer(min_df=1, ngram_range=(1,1)) tfidf_vector = tfidf_vectorizer.fit_transform(documents_processed) tfidf_dense = tfidf_vector.todense() ## sparse array로는 Agg.Clustering 학습이 안되어서, dense array로 바꿔줘야 함 . *TfidfVectorizer: 각 단어의 Term Frequency - Inverse Document Frequency를 기반으로 text를 vector화. | ngram_range: (1, 1) means only unigrams, (1, 2) means unigrams and bigrams, and (2, 2) means only bigrams. (default: (1,1)) | min_df: ignore terms that have a document frequency strictly lower than the given threshold. (default: 1) | . → AgglomerativeClustering으로 학습 . model = AgglomerativeClustering(linkage='complete', affinity='cosine', n_clusters=4) model.fit(tfidf_dense) model.labels_ . array([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 2, 0, 1, 2, 0, 2, 2, 2, 0, 2, 0, 2, 0, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 3], dtype=int64) . 학습 결과 확인 . | 원래 daum에서 분류되어 있던 주제와 비교해서, 유사한 기사끼리 잘 묶였나 확인 | . # topic, article_title에 cluster를 붙여서 확인 cluster_df = pd.DataFrame(zip(news_df['topic'], news_df['article_title'], model.labels_), columns=['topic', 'title', 'cluster']) cluster_df.head() . |   | topic | title | cluster | . | 0 | 국회/정당 | [포토]재보선 D-1, ‘시민들의 선택은?’ | 3 | . | 1 | 국회/정당 | \"약속 지킨다\"..김종인, 8일 국민의힘 떠난다 | 3 | . | 2 | 국회/정당 | \"위선·무능 정부 심판해야\"..빨강 운동화에 ‘골목’ 찾은 오세훈 | 3 | . | 3 | 국회/정당 | 김종인, 강남 대치역 찾아 오세훈 후보 지지호소 | 3 | . | 4 | 국회/정당 | 이낙연 \"특권층 득세하고 차별하는 서울로 퇴보할 텐가\" | 3 | . → 각 topic별 cluster 배정 현황을 확인: . cluster_df.groupby(['topic', 'cluster'])[['cluster']].count() . | topic | cluster | count | . | 건강 | 0 | 5 | . | ^^ | 1 | 1 | . | ^^ | 2 | 10 | . | 국회/정당 | 1 | 2 | . | ^^ | 3 | 11 | . | 금융 | 0 | 2 | . | ^^ | 1 | 13 | . | 사건/사고 | 0 | 13 | . | ^^ | 3 | 2 | . | 주로 ‘건강’: 2, ‘국회/정당’: 3, ‘금융’: 1, ‘사건/사고’: 0으로 많이 배정되었음을 확인 | . → daum의 분류와 다른 부분을 확인: . cluster_df[cluster_df['topic'] == '국회/정당'] . |   | topic | title | cluster | . | 0 | 국회/정당 | [포토]재보선 D-1, ‘시민들의 선택은?’ | 3 | . | 1 | 국회/정당 | \"약속 지킨다\"..김종인, 8일 국민의힘 떠난다 | 3 | . | 2 | 국회/정당 | \"위선·무능 정부 심판해야\"..빨강 운동화에 ‘골목’ 찾은 오세훈 | 3 | . | 3 | 국회/정당 | 김종인, 강남 대치역 찾아 오세훈 후보 지지호소 | 3 | . | 4 | 국회/정당 | 이낙연 \"특권층 득세하고 차별하는 서울로 퇴보할 텐가\" | 3 | . | 5 | 국회/정당 | 시민들과 기념사진 찍는 김종인 비대위원장 | 3 | . | 6 | 국회/정당 | 강남 찾아 오세훈 지지호소하는 김종인 | 3 | . | 7 | 국회/정당 | 김종인, 오세훈 후보 지원유세 | 3 | . | 8 | 국회/정당 | 지원유세 마치고 차량에 오른 김종인 | 3 | . | 9 | 국회/정당 | 오세훈 후보 지원유세하는 김종인 | 3 | . | 10 | 국회/정당 | 내 선거’처럼 뛴 안철수는 국민의힘에서 무엇이 될까 | 3 | . | 11 | 국회/정당 | 웹툰에서도 부딪히는 네이버 vs 카카오..’원조 국밥집’ 논란도? | 1 | . | 12 | 국회/정당 | 토스, 매출 230% 급증..\"출범 후 처음으로 매출 이익 동시 개선\" | 1 | . | topic 간 경계가 모호한 기사의 경우 daum에서 배정한 topic과 다른 cluster로 묶인 것… | . ",
    "url": "https://chaelist.github.io/docs/ml_application/news_clustering/",
    "relUrl": "/docs/ml_application/news_clustering/"
  },"130": {
    "doc": "Numbers, List, String",
    "title": "Numbers, List, String",
    "content": ". | Numbers . | type() 함수 | 산술 연산자 (Arithmetic Operators) | . | List . | Indexing | Slicing | List 변경하기 | Main List Fuctions | Other Common List Operators / Functions | . | String . | String 기초 | Main String Fuctions | String - Number Conversion | 문자열 포맷팅 (string formatting) | . | . ",
    "url": "https://chaelist.github.io/docs/python_basics/numbers_list_string/",
    "relUrl": "/docs/python_basics/numbers_list_string/"
  },"131": {
    "doc": "Numbers, List, String",
    "title": "Numbers",
    "content": ". | Integer(정수) - ex. -2, 0, 1 | Float(소수) - ex. 1.1, 3.14 | Bolean - True or False | . type() 함수 . : 각각 variable의 type을 확인하는 방법 . # type() 함수로 각각의 type을 체크 a = 1.1 b = 5 c = True print('a:', type(a)) print('b:', type(b)) print('c:', type(c)) . a: &lt;class 'float'&gt; b: &lt;class 'int'&gt; c: &lt;class 'bool'&gt; . 산술 연산자 (Arithmetic Operators) . # 연산 예시 a = 5 b = 3 print(a + b) # addition print(a - b) # subtraction print(a * b) # multiplication print(a / b) # division (나누어진 결과를 소수로 표시) print(a // b) # quotient. 나눈 몫을 반환 (ex. 5 // 3 = 1) print(a % b) # modulus. 나머지 값을 반환. (ex. 5 % 3 = 2) print(a ** b) # exponentiation (지수. 제곱) (ex. 3**5는 3의 5제곱을 의미) . 8 2 15 1.6666666666666667 1 2 125 . ",
    "url": "https://chaelist.github.io/docs/python_basics/numbers_list_string/#numbers",
    "relUrl": "/docs/python_basics/numbers_list_string/#numbers"
  },"132": {
    "doc": "Numbers, List, String",
    "title": "List",
    "content": "a = [‘python’, 1, 5] 와 같이, [ ]로 표현됨 . # 빈 리스트를 생성하는 방법 a = [] b = list() print(a, b) # 두 방법 모두 동일 . [] [] . Indexing . index number를 이용해 각 element에 접근할 수 있다. | index number는 0부터 시작 . | 첫번째 요소의 index number: 0 | 두번째 요소의 index number: 1 | . | 역순 indexing도 가능 . | 마지막 요소의 index number: -1 | 마지막에서 두번째 요소의 index number: -2 | . | [ ]를 사용해 indexing . | a[0]: a의 첫번째 요소의 값을 반환 | . | . # Indexing a = ['python', 1, 5] print(a[0]) # 첫번째 element를 의미 print(a[2]) # 세번째 element를 의미 print(a[-1]) # 마지막 element를 의미 (역순 indexing) . python 5 5 . Slicing . list_name[index1:index2]와 같은 방식으로, 특정 구간의 index들에 모두 접근 . | index1 &lt;= index &lt; index2 (index1 이상, index2 미만) | index number가 index1보다 크거나 같고 index2보다 작은 모든 요소를 반환 | . # Slicing a = ['python', 1, 5] print(a[0:2]) # index0, index1 (index2는 미포함) print(a[1:]) # index1이상 ~ 끝까지 print(a[:2]) # 처음 ~ index1까지 print(a[:]) # 그냥 a itself (처음 ~ 끝) . ['python', 1] [1, 5] ['python', 1] ['python', 1, 5] . +) list_name[index1:index2:step] . | index1 이상, index2 미만의 요소 중, 1번째, 1+step번째, 1+2step번째,…의 요소를 반환 | . numbers = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] numbers[1:5:2] # index number가 1, 3인 요소 반환 print(numbers[:6:3]) # index number가 0, 3인 요소 반환 print(numbers[::2]) # index number가 0, 2, 4, 6, 8인 요소 반환 . [2, 4] [1, 4] [1, 3, 5, 7, 9] . List 변경하기 . List is mutable; List can be modified . # Modifying a list x = [1, 2, 3, 4] x[1] = 'python' print(x) # index 1번 자리가 'python'으로 변경됨 . [1, 'python', 3, 4] . Main List Fuctions . | append(): 한 개의 element ‘추가’ # append x = [1, 2, 3, 4] x.append(5) x . [1, 2, 3, 4, 5] . | extend(): 새로운 list를 추가해 ‘확장’ # extend x = [1, 2, 3, 4] x.extend([6,7]) x . [1, 2, 3, 4, 6, 7] . cf) append에 list를 넣으면? . x = [1, 2, 3, 4] x.append([6,7]) x # extend와 다르게, 아예 list 자체가 하나의 element로 간주되어 들어감. [1, 2, 3, 4, [6, 7]] . +) list끼리 +로 더해줘도 extend의 효과: . x = [1, 2, 3, 4] x = x + [6,7] x . [1, 2, 3, 4, 6, 7] . | insert(): list의 ‘특정 index에’ element를 추가 x = [1, 2, 3, 4] x.insert(2, 5) # index number = 2인 위치에 5라는 element를 추가 x . [1, 2, 5, 3, 4] . | remove(): 한 개의 element를 제거 # remove x = [1, 2, 3, 4] x.remove(1) x . [2, 3, 4] . cf) 같은 값의 element가 두 개인 상황에서 remove()를 사용하면? . x = [1,2,3,4,1] x.remove(1) x ## 같은 값이 두 개일 경우, 더 앞에 있는 element만 지워진다. [2, 3, 4, 1] . | index(): 해당 element의 index number를 가져옴 # index x = [1, 2, 3, 4] x.index(3) # 3은 3번째 값, 즉 index number가 2인 값이므로 '2'가 반환됨 . 2 . cf) 같은 element가 2개 이상일 때 index() 함수를 사용하면? . x = [1, 2, 3, 4, 3] x.index(3) # 맨 처음 나오는 '3'의 index number인 2만 return. 2 . | count(): 해당 element의 개수를 셈 # count x = [1, 2, 3, 4, 3] x.count(3) # 3이 몇 개인지 세기 . 2 . | . Other Common List Operators / Functions . | ‘in’ operator: 특정 element가 list 안에 있는지 없는지, boolean값을 반환 # 'in' x = [1, 2, 3, 4] print(1 in x) # True print(5 in x) # False . True False . | min(x), max(x): 최소값, 최대값을 반환 # min, max x = [1, 2, 3, 4] print(min(x)) # 최소값 print(max(x)) # 최대값 . 1 4 . cf) string element들로 이루어진 list의 min(x), max(x) . y = ['c', 'b', 'a'] print(min(y)) print(max(y)) # string의 경우에도, 각각 number로 된 unicode를 갖기에, 해당 unicode를 비교해서 max, min을 return해줌 ## 하지만 integar와 string처럼 서로 다른 type들이 섞인 list는 min, max 계산 불가. a c . | sum(x): list에 있는 모든 값을 다 더해줌 (*값이 모두 integer 혹은 float여야 함) # sum x = [1, 2, 3, 4] sum(x) . 10 . cf) integer와 float이 공존하는 list도 sum() 가능 . x = [1, 2, 3.5, 4] sum(x) . 10.5 . | len(x): list의 element 수를 알려줌 # len(x) x = [1, 2, 3, 4] len(x) # element가 4개니까 4 . 4 . | del x[index]: 해당 index number를 갖는 element를 삭제 # del x[index] x = [1, 2, 3, 4] del x[0] # index number가 0인 '1'을 삭제 x . [2, 3, 4] . cf) slicing을 통한 del도 가능 . x = [1, 2, 3, 4] del x[0:2] # index number가 0, 1인 값을 모두 delete x . [3, 4] . *주의: remove와 del의 차이 . x = [1, 2, 3, 4] x.remove(3) print('remove의 결과:', x) # 실제 '3'이란 값을 갖는 element가 삭제되는 것. x = [1, 2, 3, 4] del x[3] print('del의 결과:', x) # index number가 3인 element, 즉 '4'가 삭제되는 것. remove의 결과: [1, 2, 4] del의 결과: [1, 2, 3] . | sort(): list 안의 값들을 순서대로 정렬해준다 (숫자는 오름차순, string은 첫 글자 알파벳순) # x.sort() x = [1, 4, 3, 2, 5] x.sort() print(x) friends = ['Joseph', 'Glenn', 'Sally'] friends.sort() print(friends) . [1, 2, 3, 4, 5] ['Glenn', 'Joseph', 'Sally'] . cf) 대문자와 소문자가 공존할 경우: 대문자가 먼저 알파벳순으로 정렬되고, 그 다음 소문자가 정렬됨 . fruits = ['apple', 'Banana', 'carrot', 'Dragonfruit'] fruits.sort() print(fruits) . ['Banana', 'Dragonfruit', 'apple', 'carrot'] . +) sorted(x): sort()와 마찬가지로, list 내용물을 알파벳순으로 정렬. | x.sort()는 x 자체를 변경 / sorted(x)는 x는 변형하지 않은 채 알파벳 순으로 정렬된 리스트를 반환 | . x = [1, 4, 3, 2, 5] sorted(x) . [1, 2, 3, 4, 5] . | reverse(): list 안의 element 순서를 반대로 뒤집어준다 x = [1, 4, 3, 2, 5] x.reverse() print(x) . [5, 2, 3, 4, 1] . +) reversed(x): x는 변형하지 않고, x의 element를 반대 순서로 정렬하는 iterator를 반환해준다 . | sorted(x)와 달리, 변형된 리스트가 return되는 것이 아니라 iterator가 반환된다 | . x = [1, 4, 3, 2, 5] [i for i in reversed(x)] # iterator가 반환되므로 이런 식으로 사용해야 list로 반환시킬 수 있음 . [5, 2, 3, 4, 1] . +) indexing을 통한 list 뒤집기: . x = [1, 4, 3, 2, 5] x[::-1] . [5, 2, 3, 4, 1] . | . ",
    "url": "https://chaelist.github.io/docs/python_basics/numbers_list_string/#list",
    "relUrl": "/docs/python_basics/numbers_list_string/#list"
  },"133": {
    "doc": "Numbers, List, String",
    "title": "String",
    "content": "문자열. sequence of characters. ‘ ‘이나 “ “를 활용해 표현 . String 기초 . | Indexing &amp; Slicing: list처럼, indexing &amp; slicing 가능 s = 'python' print(s[0]) print(s[-1]) print(s[0:3]) . p n pyt . | len(): string의 글자수를 세는 개념 # len() s = 'python' len(s) . 6 . | string간 덧셈(+) h = 'Hello' w = 'World' s = h + w # 두 개를 더하면 단순히 앞뒤로 이어붙여진다 s . 'HelloWorld' . | ‘나 “를 중간에 삽입하는 법 print('Tom\\\\s Book') # \\를 이용 print(\"Tom's Book\") # string을 감싸는 따옴표를 다른 종류로 사용 . Tom's Book Tom's Book . | white space characters (공백) . | \\t: tab | \\n: enter (newline) | . z1 = 'Tom is busy studying. \\nI am not busy. \\t\\tYou?' print(z1) . Tom is busy studying. I am not busy. You? . | . Main String Fuctions . | split(): whitespace(공백)을 기준으로 split . | split(‘a’): ‘a’를 기준으로 split | split 결과는 list로 나타남 | . # split() s = 'Today is a good day' print(s.split()) print(s.split('a')) print(s.split('good')) . ['Today', 'is', 'a', 'good', 'day'] ['Tod', 'y is ', ' good d', 'y'] ['Today is a ', ' day'] . +) join(): split된 문자열을 다시 모아주기 . split_list = ['Today', 'is', 'a', 'good', 'day'] print(' '.join(split_list)) # 중간에 공백을 두고 list의 element들을 합쳐줌 print('/'.join(split_list)) # 중간에 /를 두고 합쳐줌 . Today is a good day Today/is/a/good/day . | strip(): 양쪽 끝의 whitespace를 제거 . | strip(‘n’): 양쪽 끝의 ‘n’ 문자 제거 | lstrip()은 왼쪽 끝 element만, rstrip()은 오늘쪽 끝 element만 제거 | . # strip() t = '\\tpyth\\ton\\n' t.strip() ## 양 끝의 whitespace만 제거해주고, 중간의 \\t는 제거되지 않는다 . 'pyth\\ton' . | replace(‘a’, ‘b’): 모든 ‘a’를 ‘b’로 대체 # replace() s = 'python is important' print(s.replace('o', 'a')) print(s) #유의사항: string은 immutable! replace를 해도 원본 s가 바뀌는 것은 아니다. ## 바뀐 결과를 저장하고 싶으면 새로운 variable로 따로 저장해둬야 함 . pythan is impartant python is important . *무언가를 없애고 싶을 때에도 replace()를 사용 . ex) replace(‘a’, ‘‘)라고 하면 ‘a’를 다 없애주는 기능. (두번째 ‘’ 안을 비워두면 됨) . s = 'python, is, important,' print(s.replace(',', '')) . python is important . | find(): 해당 단어가 존재하면 첫번째 character의 index number를 출력 / 존재하지 않으면 -1을 출력 s = 'Data science is important' print(s.find('science')) # 첫글자 's'의 index number 출력 print(s.find('python')) # 존재하지 않으므로 -1 출력 . 5 -1 . cf) 존재 유무만을 확인하고 싶다면 find 대신 in을 사용해도 된다 . s = 'Data science is important' print('science' in s) print('python' in s) . True False . +) find를 사용하는 상황 예시 . data = 'From stephen.marquard@uct.ac.za Sat Jan 5 09:14:16 2008' # 이 데이터에서 보낸 사람의 메일 도메인만을 추출하고 싶음 atpos = data.find('@') # '@'의 위치를 알아냄 print(atpos) sppos = data.find(' ', atpos) # ' '(빈칸)이 몇 번째에 있나 atpos, 즉 21번째 문자 뒤에서부터 탐색 (atpost 앞의 빈칸은 무시) print(sppos) host = data[atpos+1 : sppos] # '@' 뒤부터 그 다음 나오는 ' '(빈칸)까지를 slicing해서 메일 도메인만 추출 print(host) . 21 31 uct.ac.za . | upper(), lower(), capitalize() . | upper(): 모든 문자를 uppercase로 바꿔줌 | lower(): 모든 문자를 lowercase로 바꿔줌 | capitalize(): 문자열의 가장 첫번째 문자는 uppercase로, 나머지는 lowercase로 맞춰줌 | . # 숫자/특수기호에는 반영되지 않고, 알파벳에만 적용됨 a = 'hAve a Nice daY 12#' print(a.upper()) print(a.lower()) print(a.capitalize()) . HAVE A NICE DAY 12# have a nice day 12# Have a nice day 12# . | startswith(), endswith() . | startswith(): 문자열이 ()안의 특정 문자로 시작되는지 확인, 결과는 boolean 값으로 반환 – if X.startswith('Y'): 이런 식으로 if문에서 주로 사용 | endswith(): 문자열이 특정 문자로 끝나는지 여부를 판별 | . line = 'Have a nice day' print(line.startswith('Have')) # True print(line.startswith('H')) # True print(line.startswith('h')) # 대문자 H와 소문자 h는 다르기 때문에, False가 반환됨 print(line.endswith('day')) . True True False True . | isalpha(), isalnum(), isdigit() . | isalpha(): 문자열이 모두 문자로만 이루어져 있는지 판별 (영어/한글 등 언어는 상관X) | isalnum(): 문자열이 모두 문자/숫자로만 이루어져 있는지 판별. 특수문자/공백 등이 함께 들어 있으면 False를 return | isdigit(): 문자열이 모두 숫자로만 이루어져 있는지 판별. | . line1 = 'Python코딩은즐거워' line2 = 'start123' line3 = '12345' line4 = 'Python 코딩' print(line1.isalpha()) print(line2.isalpha()) print(line4.isalpha()) # 공백만 하나 있어도 False가 됨 print(line1.isalnum()) print(line2.isalnum()) print(line3.isalnum()) print(line2.isdigit()) print(line3.isdigit()) . True False False True True True False True . | count(): list의 element를 세는 것과 동일하게 적용됨 . x = 'python python' x.count('python') . 2 . | . String - Number Conversion . | int(x): from string/float to integer # string -&gt; integer x = '123' # 이렇게 '' 안이 integer여야만 int(x)로 변환 가능 print(x, type(x)) y = int(x) print(y, type(y)) . 123 &lt;class 'str'&gt; 123 &lt;class 'int'&gt; . | float(x): from string to float # string -&gt; float -&gt; integer x = '123.123' # 이렇게 '' 안이 float이면, int(x)를 바로 할 수 없음. 먼저 float(x)를 해줘야 함. print(x, type(x)) y1 = float(x) print(y1, type(y1)) y2 = int(y1) # 이제 y1은 float이기에, 여기에 int(x)를 해주면 integer가 됨. print(y2, type(y2)) . 123.123 &lt;class 'str'&gt; 123.123 &lt;class 'float'&gt; 123 &lt;class 'int'&gt; . | str(number): from number to string # integer -&gt; string z = 123 print(z, type(z)) s = str(z) print(s, type(s)) . 123 &lt;class 'int'&gt; 123 &lt;class 'str'&gt; . | . 문자열 포맷팅 (string formatting) . | ‘format’ method print(\"오늘은 {}월 {}일입니다\".format(11, 12)) . 오늘은 11월 12일입니다 . *{ }의 순서 지정하기 . # 'format' method - {}의 순서 정하기 print(\"저는 {1}, {0}, {2}를 좋아합니다\".format(\"트와이스\", \"유재석\", \"비틀즈\")) . 저는 유재석, 트와이스, 비틀즈를 좋아합니다 . *소수점 제한 지정하기 . # 'format' method - 소수점 제한 지정 print(\"{0} 나누기 {1}은 {2:.2f}입니다\".format(1, 3, 1/3)) # :.2f라고 하면 floating point(소수) 둘째자리까지 출력하라는 뜻 . 1 나누기 3은 0.33입니다 . | :.4f는 소수점 넷째짜리까지 출력하라는 뜻 | :.0f는 정수로 출력하라는 뜻 (cf. 정수로 하려면 :d라고 해도 됨) | :f라고 하면 그냥 소수점 제한 없이 floating point로 출력하라는 뜻 | . | % 기호 . | 오래된 방식. C/자바 등 언어의 문자열 포맷팅 방식과 유사 | python에서는 권장되지는 않는다 (f-string 등 더 효율적인 방법이 있기 때문) | %s, %d와 같은 ‘포맷 스트링’을 사용 | . name = '최지아' age = 25 print(\"제 이름은 %s이고 %d살입니다.\" % (name, age)) . 제 이름은 최지아이고 25살입니다. | %s: str형 | %d: int형 | %f: float형 | %.[숫자]f: float형 (숫자를 통해 소수점을 지정) | . | f-string (파이썬 3.6부터 나온 방식) name = '최지아' age = 25 print(f\"제 이름은 {name}이고 {age}살입니다.\") . 제 이름은 최지아이고 25살입니다. | . ",
    "url": "https://chaelist.github.io/docs/python_basics/numbers_list_string/#string",
    "relUrl": "/docs/python_basics/numbers_list_string/#string"
  },"134": {
    "doc": "Numpy",
    "title": "Numpy",
    "content": " ",
    "url": "https://chaelist.github.io/docs/numpy",
    "relUrl": "/docs/numpy"
  },"135": {
    "doc": "Numpy 연산과 통계",
    "title": "Numpy 연산과 통계",
    "content": ". | Numpy 기본 연산과 통계 . | Numpy 기본 연산 | Numpy Boolean 연산 | Numpy 기본 통계 | nan을 포함한 array 통계량 계산 | . | Numpy를 활용한 행렬 연산 . | 행렬 ‘요소별 곱하기’ | 행렬 간 덧셈 &amp; 곱셈 | 전치 행렬, 단위 행렬, 역행렬 | . | . ",
    "url": "https://chaelist.github.io/docs/numpy/numpy_arithmetics/",
    "relUrl": "/docs/numpy/numpy_arithmetics/"
  },"136": {
    "doc": "Numpy 연산과 통계",
    "title": "Numpy 기본 연산과 통계",
    "content": "Numpy 기본 연산 . : Numpy는 수학적 연산이 매우 간단하다는 점에서 list와 차별화된다 . | 각 원소에 동일한 사칙연산 적용: import numpy as np # import해야 사용 가능 array1 = np.arange(10) print(array1) . [0 1 2 3 4 5 6 7 8 9] . # 각 원소에 2씩 더하기 . array1 + 2 . array([ 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]) . # 각 원소를 2씩 나누기 . array1 / 2 . array([0. , 0.5, 1. , 1.5, 2. , 2.5, 3. , 3.5, 4. , 4.5]) . # 각 원소에 2씩 곱하기 . array1 * 2 . array([ 0, 2, 4, 6, 8, 10, 12, 14, 16, 18]) . # 각 원소 제곱하기 . array1 ** 2 . array([ 0, 1, 4, 9, 16, 25, 36, 49, 64, 81]) . cf) list로 동일한 사칙연산을 해주려면 for문을 사용해서 요소별로 연산을 해줘야 한다 . # 아래와 같이 계산해야 array1 + 2와 같은 결과가 나온다 list1 = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] new_list = [] for a in list1: new_list.append(a + 2) new_list . [2, 3, 4, 5, 6, 7, 8, 9, 10, 11] . | Numpy Array 간 사칙연산: array1 = np.arange(10) array2 = np.arange(10, 20) print(array1) print(array2) . [0 1 2 3 4 5 6 7 8 9] [10 11 12 13 14 15 16 17 18 19] . # numpy array 간 덧셈 . array1 + array2 # 각 요소별로 차례대로 더해진다: 0+10, 1+11, 2+12, ... , 9+19 . array([10, 12, 14, 16, 18, 20, 22, 24, 26, 28]) . # numpy array 간 나눗셈 . array1 / array2 # 각 요소별로 차례대로 나눠진다: 0/10, 1/11, 2/12, ... , 9/19 . array([0. , 0.09090909, 0.16666667, 0.23076923, 0.28571429, 0.33333333, 0.375 , 0.41176471, 0.44444444, 0.47368421]) . # numpy array 간 곱셈 . array1 * array2 # 각 요소별로 차례대로 곱해진다: 0*10, 1*11, 2*12, ... , 9*19 . array([ 0, 11, 24, 39, 56, 75, 96, 119, 144, 171]) . cf) list 두 개를 numpy array 간 덧셈과 동일하게 요소별로 더해주려면 for문을 사용해서 계산해야 한다 . ## 아래와 같이 계산해야 array1 + array2와 같은 결과가 나온다 list1 = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] ist2 = [10, 11, 12, 13, 14, 15, 16, 17, 18, 19] added_list = [] for a in list1: added_list.append(a + list2[list1.index(a)]) added_list . [10, 12, 14, 16, 18, 20, 22, 24, 26, 28] . +) 단순히 list1 + list2 해버리면? . list1 + list2 # 아래와 같이 list1.extend(list2)의 결과가 나온다 . [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19] . | . Numpy Boolean 연산 . : 이 원리가 boolean indexing에 사용되는 것. array1 = np.array([2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31]) array1 &gt; 4 # 각 원소별로 4보다 큰지 여부를 판단해 Boolean 값을 반환 # 결과값도 array 형태. Boolean 값들을 element로 하는 numpy array . array([False, False, True, True, True, True, True, True, True, True, True]) . &gt;, &lt;, &gt;=, &lt;=, == 등 사용 가능 . array1 % 2 == 0 . array([ True, False, False, False, False, False, False, False, False, False, False]) . *np.where() 함수: 해당 Boolean 조건을 만족하는 값들의 index값을 반환 . | np.where( ) 안에는 boolean값들로 이루어진 numpy array가 들어간다 | 결과로 True인 element의 index값만 반환 (array형태로 반환) | . np.where(array1 &gt; 4) # '4보다 크다'는 조건을 만족하는 값들의 index값 반환 . (array([ 2, 3, 4, 5, 6, 7, 8, 9, 10]),) . &gt;&gt; Boolean 연산을 활용한 Boolean Indexing: 2가지 방법 . # 1번째 방법: 직접 indexing print(array1[array1 &gt; 4]) # 2번째 방법: np.where() 이용 filter = np.where(array1 &gt; 4) print(array1[filter]) . [ 5 7 11 13 17 19 23 29 31] [ 5 7 11 13 17 19 23 29 31] . Numpy 기본 통계 . | 최댓값, 최솟값, 평균값 array1 = np.array([14, 6, 13, 21, 23, 31, 9, 5]) print(array1.max()) # 최댓값 print(array1.min()) # 최솟값 print(array1.mean()) # 평균값 . 31 5 15.25 . cf) list도 max(x), min(x) 기능은 있지만, 평균 구하기 등의 기능은 없음 . | 중앙값 array1 = np.array([8, 12, 9, 15, 16]) array2 = np.array([14, 6, 13, 21, 23, 31, 9, 5]) print(np.median(array1)) # 중앙값 print(np.median(array2)) # 중앙값 - 짝수 개의 값이 있는 array일 경우, 중앙값 두 개를 평균 낸 값을 return ## array2의 경우, 중앙값이 13과 14 두 개. &gt;&gt; 두 개를 평균내면 13.5 . 12.0 13.5 . | 표준편차, 분산 array1 = np.array([14, 6, 13, 21, 23, 31, 9, 5]) print(array1.std()) # 표준 편차 print(array1.var()) # 분산 . 8.496322733983215 72.1875 . | . nan을 포함한 array 통계량 계산 . | 하나라도 np.nan(=Null값)이 포함된 array는 sum, mean 등을 계산하면 nan으로밖에 안나온다: array1 = np.array([14, 6, 13, 21, 23, np.nan, 9, 5]) print(array1.sum()) # np.sum(array1)과 동일 print(array1.mean()) # np.mean(array1)과 동일 . nan nan . | np.nansum(), np.nanmean()을 활용하면 nan을 제외한 통계량을 확인 가능: print(np.nansum(array1)) print(np.nanmean(array1)) . 91.0 13.0 . | +) np.nanstd(), np.nanmin(), np.nanmedian() 등… | . ",
    "url": "https://chaelist.github.io/docs/numpy/numpy_arithmetics/#numpy-%EA%B8%B0%EB%B3%B8-%EC%97%B0%EC%82%B0%EA%B3%BC-%ED%86%B5%EA%B3%84",
    "relUrl": "/docs/numpy/numpy_arithmetics/#numpy-기본-연산과-통계"
  },"137": {
    "doc": "Numpy 연산과 통계",
    "title": "Numpy를 활용한 행렬 연산",
    "content": "행렬 ‘요소별 곱하기’ . | 요소별 곱하기(Element-wise Multiplication): 같은 행/열에 있는 요소끼리 곱해서 새로운 행렬을 만드는 연산 | 행렬 덧셈과 마찬가지로 같은 차원을 갖는 행렬 사이에만 연산이 가능하다 | A ∘ B라고 표기 | . # numpy를 이용한 '요소별 곱하기' -- A * B와 같이 별표(*)를 사용하면 된다 A = np.array([ [1, 2, 3], [4, 5, 6], [7, 8, 9] ]) B = np.array([ [0, 1, 2], [2, 0, 1], [1, 2, 0] ]) A * B . array([[ 0, 2, 6], [ 8, 0, 6], [ 7, 16, 0]]) . 행렬 간 덧셈 &amp; 곱셈 . | 행렬 간 덧셈 A = np.array([ [1, 2, 3], [4, 5, 6], [7, 8, 9] ]) B = np.array([ [0, 1, 2], [2, 0, 1], [1, 2, 0] ]) A + B . array([[ 1, 3, 5], [ 6, 5, 7], [ 8, 10, 9]]) . | 스칼라곱 5 * A . array([[ 5, 10, 15], [20, 25, 30], [35, 40, 45]]) . | 행렬간 곱셉 (내적곱) . | 전제: A의 열 수 = B의 행 수 | A * B는 ‘요소별 곱하기’가 되니까 주의 | . *두 가지 방법: . | np.dot(A, B) np.dot(A, B) . array([[ 7, 7, 4], [16, 16, 13], [25, 25, 22]]) . | A @ B A @ B ## np.dot이랑 동일한 결과를 낸다. 더 간결한 방법. array([[ 7, 7, 4], [16, 16, 13], [25, 25, 22]]) . | . | 연산 섞어서 계산하기 . | 일반 연산과 마찬가지로, ()가 먼저 계산되고, 그 안에서도 덧셈보다 곱셈이 먼저 계산됨. | . A @ B + (A + 2*B) . array([[ 8, 11, 11], [24, 21, 21], [34, 37, 31]]) . | . 전치 행렬, 단위 행렬, 역행렬 . A = np.array([ [1, -1, 2], [3, 2, 2], [4, 1, 2] ]) A . array([[ 1, -1, 2], [ 3, 2, 2], [ 4, 1, 2]]) . | 전치 행렬 . | 기존 행렬 A에서 행과 열을 바꾼 행렬 | AT라고 표기 (A의 전치 행렬) | . *두 가지 방법: . | np.transpose(A) A_transpose = np.transpose(A) A_transpose . array([[ 1, 3, 4], [-1, 2, 1], [ 2, 2, 2]]) . | A.T A_transpose = A.T # .T만 붙여주면 됨. 더 간결한 방법. A_transpose . array([[ 1, 3, 4], [-1, 2, 1], [ 2, 2, 2]]) . | . | 단위 행렬(identity matrix) . | 정사각형 모양이며, 대각선으로는 원소가 쭉 1이고, 그 외에는 모두 0인 행렬. | 어떤 행렬이든 간에 단위 행렬을 곱하면 기존 행렬이 그대로 유지됨. | 보통 I라고 표기 | . *np.identity(숫자)로 생성 . I = np.identity(3) # 3x3 단위행렬 I . array([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]]) . *단위행렬 I를 A에 곱하면 그대로 A가 결과로 나옴 . A @ I . array([[ 1., -1., 2.], [ 3., 2., 2.], [ 4., 1., 2.]]) . | 역행렬(inverse matrix) . | 특정 행렬 A에 곱했을 때 단위 행렬 I가 나오도록 하는 행렬 | A-1라고 표기 (A의 역행렬) | 하지만 모든 행렬에 역행렬이 있는 것은 아니다! 무엇을 곱해든 I가 안나오는 행렬도 있음 | . *numpy의 linalg 모듈의 pinv 함수를 사용 . | ※이 함수는 역행렬이 없는 경우에도 가장 비슷한 효과를 내는 행렬을 return해준다) | cf) np.linalg.inv(A)도 역행렬을 return해주는 함수이지만, 역행렬이 없는 경우에는 작동X | . A_inverse = np.linalg.pinv(A) A_inverse . array([[-0.2, -0.4, 0.6], [-0.2, 0.6, -0.4], [ 0.5, 0.5, -0.5]]) . *A와 A의 역행렬을 곱하면 I가 나온다 . A @ A_inverse . array([[ 1.00000000e+00, 7.77156117e-16, -8.88178420e-16], [ 0.00000000e+00, 1.00000000e+00, -8.88178420e-16], [ 0.00000000e+00, 4.44089210e-16, 1.00000000e+00]]) . | 대각선은 다 1이 맞는데, 다른 부분들에 나오는 숫자들은 0에 가깝긴 하지만(e-16은 0.00000~가 16개 있는 것) 완전히 0이라고 나오지 않는다. » 이렇게 조금씩 이상한 이유는 무수히 많은 소수0은 컴퓨터가 다 표현하기 어렵기 때문. » 소수0을 사용할 때는 여기 보이는 것처럼 약간의 오차가 발생할 수 밖에 없다. | . | . +) 행렬식 (determinant) . | 역행렬 존재 여부를 테스트하는 식. 행렬식이 0이면 역행렬이 존재하지 않음. np.linalg.det(A) . -9.999999999999998 . | . ",
    "url": "https://chaelist.github.io/docs/numpy/numpy_arithmetics/#numpy%EB%A5%BC-%ED%99%9C%EC%9A%A9%ED%95%9C-%ED%96%89%EB%A0%AC-%EC%97%B0%EC%82%B0",
    "relUrl": "/docs/numpy/numpy_arithmetics/#numpy를-활용한-행렬-연산"
  },"138": {
    "doc": "Numpy 기초",
    "title": "Numpy 기초",
    "content": ". | Numpy Array 생성 . | list를 받아서 numpy array 생성 | np.full(), np.zeros(), np.ones() | random 숫자로 채워진 array 생성 | np.arange() | . | Numpy Array 변경 . | Data Type 확인 및 변경 | tolist() | reshape() | Numpy Array 정렬 | np.argsort() | . | Indexing &amp; Slicing . | Indexing | Slicing | Boolean Indexing | . | . *Numpy: 대규모 다차원 배열을 쉽게 수학적으로 연산할 수 있게 지원하는 라이브러리 . ",
    "url": "https://chaelist.github.io/docs/numpy/numpy_basics/",
    "relUrl": "/docs/numpy/numpy_basics/"
  },"139": {
    "doc": "Numpy 기초",
    "title": "Numpy Array 생성",
    "content": "list를 받아서 numpy array 생성 . import numpy as np # import해야 사용 가능; 보통 np로 줄여서 import list1 = [1,2,3] print(type(list1)) array1 = np.array(list1) ## 이렇게 하면 list가 numpy array 형태로 바뀜 # 물론, array1 = np.array([1, 2, 3]) 이렇게 직접 assign하는 것도 가능 print(type(array1)) print(array1) . &lt;class 'list'&gt; &lt;class 'numpy.ndarray'&gt; [1 2 3] . +) 다차원 list로 행렬 생성하기 . A = np.array([[1,2,3], # 다차원 list를 assign해줄 수 있다 [2,3,4]]) print('type:', type(A)) print('array 형태:', A.shape) # (2, 3)의 행렬 형태 print(A) . type: &lt;class 'numpy.ndarray'&gt; array 형태: (2, 3) [[1 2 3] [2 3 4]] . np.full(), np.zeros(), np.ones() . | np.full(): 모든 값이 같은 numpy array 생성 array1 = np.full(6, 7) # 7이라는 값으로 채워진 6개짜리 array를 만들라는 뜻 print(array1) . [7 7 7 7 7 7] . | np.zeros(): 모든 값이 0인 numpy array 생성 array2 = np.zeros(6, dtype=int) # dtype=int라고 쓰지 않으면 'float' 타입으로, 모든 값이 '0.' ## np.full(6, 0)이라고 해도 동일한 결과를 낼 수 있음 print(array2) . [0 0 0 0 0 0] . +) 0으로 채워진 (2, 4) 행렬 만들기 . C = np.zeros((2, 4)) # ()안에 ()를 또 넣어서 차원을 써줘야 함 C . array([[0., 0., 0., 0.], [0., 0., 0., 0.]]) . | np.ones(): 모든 값이 1인 numpy array 생성 array3 = np.ones(6, dtype=int) print(array3) . [1 1 1 1 1 1] . | . random 숫자로 채워진 array 생성 . : np.random.random(), np.random.randint(), np.random.rand(), np.random.randn() . | np.random.random(): 0~1 사이의 랜덤한 floating number로 구성된 array 생성 . | 원소 개수만을 input으로 받는다. | . array1 = np.random.random(6) # 6개짜리 array를 만들되, 0~1 사이의 랜덤한 floating number로 채우라는 뜻 array2 = np.random.random(6) ## 똑같은 코드로 두 번 생성해도 전혀 다른 숫자들로 구성된 array가 된다 print(array1) print(array2) . [0.88406249 0.39299664 0.92697636 0.85366744 0.07601589 0.33080729] [0.61871553 0.55877544 0.21509023 0.93186876 0.95658264 0.25799333] . | np.random.randint(): 정수 형태의 랜덤 숫자로 구성된 array 생성 . | np.random.random()과 달리 2개의 변수를 input으로 받는다 . np.random.randint(5, size=8) # 0에서 4까지의 정수로 랜덤하게 원소가 8개인 array 생성 . array([3, 4, 0, 3, 4, 4, 4, 3]) . | +) 변수 3개를 input으로 넣는 것도 가능 . np.random.randint(2, 10, size=(3, 5)) # 2에서 9까지의 정수로 랜덤하게 (3, 5) 사이즈의 array 생성 . array([[4, 3, 5, 8, 8], [5, 6, 9, 2, 5], [3, 4, 6, 8, 5]]) . | . | np.random.rand(): random한 값들을 넣은 ‘행렬’ 만들기 (0~1 사이의 균일분포에서 random한 floating number로 구성) . | () 안에 차원을 적으면 됨 | . np.random.rand(3, 5) # 3x5 행렬을 만들겠다는 뜻 . array([[0.20518818, 0.83743088, 0.58408188, 0.24389283, 0.28567188], [0.50262559, 0.50765122, 0.5385608 , 0.861137 , 0.69626678], [0.80102188, 0.44550328, 0.91234899, 0.91133805, 0.90298844]]) . | np.random.randn(): rand()와 마찬가지로 random한 값들을 넣은 ‘행렬’ 만들되, 평균 0, 표준편차 1의 가우시안 표준정규분포에서 random한 floating number를 추출해 구성 . | () 안에 차원을 적으면 됨 | . np.random.randn(3, 5) # 3x5 행렬 . array([[-0.13089314, 1.12283706, 0.50684862, -1.41899365, 0.07714081], [-0.20499738, 0.24543592, 0.19121621, -1.12409184, -0.8251468 ], [ 1.03761123, -0.99567926, 0.78983874, 1.2103671 , -0.84700707]]) . | . np.arange() . : 특정 숫자 범위만큼을 range로 하는 numpy array 생성 . | range() 함수와 작동 방식이 거의 유사 (참고) | . | np.arange(a) array1 = np.arange(6) # 0부터 5까지를 원소로 하는 array 생성 print(array1) . [0 1 2 3 4 5] . | np.arange(a, b) array1 = np.arange(2, 7) # 2부터 6까지를 원소로 하는 array 생성 print(array1) . [2 3 4 5 6] . | np.arange(a, b, k) array1 = np.arange(3, 17, 3) # 3부터 16까지, 3의 간격으로 증가하는 원소들을 갖는 array 생성 print(array1) . [ 3 6 9 12 15] . | . ",
    "url": "https://chaelist.github.io/docs/numpy/numpy_basics/#numpy-array-%EC%83%9D%EC%84%B1",
    "relUrl": "/docs/numpy/numpy_basics/#numpy-array-생성"
  },"140": {
    "doc": "Numpy 기초",
    "title": "Numpy Array 변경",
    "content": "Data Type 확인 및 변경 . | data type 확인하기 array1 = np.array([1,2,3]) print(array1.dtype) # 3개의 element가 모두 int이므로, dtype을 하면 int라고 나옴 . int64 . | 데이터 유형 혼재된 리스트를 Array로 변경 . | 서로 다른 데이터 유형이 섞여 있는 리스트를 array로 변경할 경우, 데이터 크기가 더 큰 데이터 타입으로 형 변환이 일괄 적용된다. | . list2 = [1, 2, 'test'] array2 = np.array(list2) print(array2) print(array2.dtype) ## Unicode 문자열로 일괄 변환됨 list3 = [1, 2, 3.0] array3 = np.array(list3) print(array3) print(array3.dtype) ## float 타입으로 일괄 변환됨 . ['1' '2' 'test'] &lt;U21 [1. 2. 3.] float64 . | astype( ) : array 내 element들을 원하는 타입으로 변경 array_int = np.array([1,2,3]) array_float = array_int.astype('float64') # int -&gt; float print(array_float, array_float.dtype) array_float1 = np.array([1.1, 2.1, 3.1]) array_int1 = array_float1.astype('int32') # float -&gt; int print(array_int1, array_int1.dtype) . [1. 2. 3.] float64 [1 2 3] int32 . | . tolist() . : numpy array를 list로 바꾸기 . array1 = np.array([2, 3, 5, 7]) print(type(array1), array1) list1 = array1.tolist() # tolist() 기능을 사용하면 list로 타입이 바뀜 print(type(list1), list1) . &lt;class 'numpy.ndarray'&gt; [2 3 5 7] &lt;class 'list'&gt; [2, 3, 5, 7] . reshape() . : numpy array의 shape 변경. | reshape(m, n): m행 n열짜리 array로 변환하라는 뜻. array1 = np.arange(10) print('array1:\\n', array1) array2 = array1.reshape(2, 5) # 2행 5열짜리 array로 변환 print('array2:\\n', array2) array3 = array1.reshape(5, 2) # 5행 2열짜리 array로 변환 print('array3:\\n', array3) . array1: [0 1 2 3 4 5 6 7 8 9] array2: [[0 1 2 3 4] [5 6 7 8 9]] array3: [[0 1] [2 3] [4 5] [6 7] [8 9]] . | reshape(m, -1): m행, 그리고 열 수은 자동으로 맞춰서 변환하라는 뜻. | ※ 인자로 -1를 적용하면, 고정된 행/열(숫자가 assign된 행/열)에 맞게 자동으로 shape 변환. | 반대로, reshape(-1, n)이라고 하면 n열, 그리고 자동으로 맞춘 행 수로 변환 | reshape(1, -1): 원본 ndarray가 어떤 형태라도 (ex. 3차원이라도), 반드시 1개의 행을 갖는 2차원 array로 변환됨 | . array1 = np.arange(10) print(array1) array2 = array1.reshape(-1, 5) # 고정된 5개 열에 맞게, 행은 2개로 변환됨 print('array2 shape:', array2.shape) array3 = array1.reshape(5, -1) # 고정된 5개 행에 맞게, 열은 2개로 변환됨 print('array3 shape:', array3.shape) . [0 1 2 3 4 5 6 7 8 9] array2 shape: (2, 5) array3 shape: (5, 2) . | 3차원 이상으로도 변경 가능 . | ex) reshape((m, n, o))라고 하면 3차원 array로 변경 가능 | . array1 = np.arange(8) array3d = array1.reshape((2,2,2)) # 3차원 print('array3d:\\n', array3d) print('array3d: %d차원' %array3d.ndim) . array3d: [[[0 1] [2 3]] [[4 5] [6 7]]] array3d: 3차원 . | . Numpy Array 정렬 . : np.sort()와 ndarray.sort() . | np.sort(): 원본 행렬을 바꾸지 않은 채, 순서대로 정렬된 새로운 행렬을 return . org_array = np.array([3,1,9,5]) # np.sort( )로 정렬 print(np.sort(org_array)) # sort된 새로운 array가 반환됨. print(org_array) # 원래의 행렬은 변하지 않음. ## 행렬 자체가 변하는 것이 아니므로, sort_array1 = np.sort(org_array) 이렇게 저장해두고 쓰는 것이 유용하다. [1 3 5 9] [3 1 9 5] . | .sort(): 원본 행렬 자체를 정렬해줌. # .sort( )로 정렬 print(org_array.sort()) # 결과로 아무것도 반환되지 않음. 그저 원래 행렬이 바뀐 것 뿐 print(org_array) # .sort()를 하고 나니, 원래의 행렬이 변함 . None [1 3 5 9] . | 내림차순으로 정렬하기 위해서는 [::-1]을 적용 sort_array1_desc = np.sort(org_array)[::-1] print(sort_array1_desc) org_array[::-1].sort() print(org_array) . [9 5 3 1] [9 5 3 1] . | 2차원 이상의 행렬: axis 축 값 설정을 통해 row 방향 / column 방향으로 각각 정렬 가능 . array2d = np.array([[8,12], [7,1]]) sort_array2d_axis0 = np.sort(array2d, axis=0) ## axis=0이면 row 방향 print('row 방향 정렬:\\n', sort_array2d_axis0) sort_array2d_axis1 = np.sort(array2d, axis=1) ## axis=1이면 column 방향 print('column 방향 정렬:\\n', sort_array2d_axis1) . row 방향 정렬: [[ 7 1] [ 8 12]] column 방향 정렬: [[ 8 12] [ 1 7]] . | . np.argsort() . : 행렬을 직접 정렬하는 대신, 연결된 값의 순서대로 index를 정렬해준다. org_array = np.array([3,1,9,5]) sort_indices = np.argsort(org_array) # 오름차순 정렬할 때의 인덱스 순서를 sort_indices에 저장 print(type(sort_indices)) # argsort의 결과물도 numpy array 타입. print('정렬된 인덱스:', sort_indices) print('원본 행렬:', org_array) # 원본 행렬에는 아무런 영향이 없음 . &lt;class 'numpy.ndarray'&gt; 정렬된 인덱스: [1 0 3 2] 원본 행렬: [3 1 9 5] . cf) 내림차순 . org_array = np.array([3,1,9,5]) sort_indices = np.argsort(org_array)[::-1] print(sort_indices) . [2 3 0 1] . *활용 예시: argsort( )를 이용한 indexing을 활용해 성적순으로 이름 출력하기 . name_array = np.array(['John', 'Mike', 'Sarah', 'Kate', 'Samuel']) score_array = np.array([78, 95, 94, 98, 88]) sort_score_indices = np.argsort(score_array) # 성적이 낮은 순서대로 index 정렬 name_array[sort_score_indices] # indexing을 통해 성적 낮은 순대로 이름 출력 . array(['John', 'Samuel', 'Sarah', 'Mike', 'Kate'], dtype='&lt;U6') . ",
    "url": "https://chaelist.github.io/docs/numpy/numpy_basics/#numpy-array-%EB%B3%80%EA%B2%BD",
    "relUrl": "/docs/numpy/numpy_basics/#numpy-array-변경"
  },"141": {
    "doc": "Numpy 기초",
    "title": "Indexing &amp; Slicing",
    "content": "Indexing . | 평범한 indexing array1 = np.array([2, 3, 5, 7, 11]) array1[2] ## index 2, 즉 3번째 element가 반환됨 . 5 . | indexing을 통한 array 내 값 수정 array1[0] = 9 array1[2] = 0 print(array1) . [ 9 3 0 7 11] . | list로 인덱싱 array1 = np.array([2, 3, 5, 7, 11]) array1[[1, 3, 4]] ## 1번, 3번, 4번 index의 값들이 추출됨 . array([ 3, 7, 11]) . | numpy array로 numpy array 인덱싱 array1 = np.array([2, 3, 5, 7, 11]) array2 = np.array([2, 1, 3]) array1[array2] . array([5, 3, 7]) . | 2차원 array (행렬) 인덱싱 A = np.arange(1, 10).reshape(3, 3) ## 1~10까지 원소로 갖는 array를 만들고, 3x3 행렬로 reshape print(A) print(A[0, 2]) # 0행(첫번째), 2열(세번째)에 위치한 값인 3를 받아오는 것 print(A[0][2]) # A[0, 2]와 동일한 결과 . [[1 2 3] [4 5 6] [7 8 9]] 3 3 . +) 추가: 2차원 array에서 1차원으로만 indexing 하면 1차원 ndarray로 변환됨 . print(A[0]) # 첫번째 row를 1차원 ndarray로 반환 print(A[0].shape) # 원소 3개짜리 1차원 array가 됨 . [1 2 3] (3,) . | . Slicing . | 기본 Slicing array1 = np.array([2, 3, 5, 7, 9, 11, 13]) print(array1[:3] print(array1[3:]) print(array1[:]) . [2, 3, 5] [ 7, 9, 11, 13] [ 2, 3, 5, 7, 9, 11, 13] . +) 추가: . array1 = np.array([2, 3, 5, 7, 9, 11, 13]) array1[2:6:2] # 3번째 element (index2) 부터 7번째 element (index6) 까지, index를 2씩 건너뛰어서. # 결과적으로 index 2, 4, 6에 해당하는 element들이 출력됨. [ 5, 9, 13] . | 2차원 array (행렬) slicing A = np.arange(1, 10).reshape(3, 3) print(A) print('\\nA[0:2, 0:2] \\n', A[0:2, 0:2]) print('\\nA[1:3, :] \\n', A[1:3, :]) print('\\nA[:2, 1:] \\n', A[:2, 1:]) . [[1 2 3] [4 5 6] [7 8 9]] A[0:2, 0:2] [[1 2] [4 5]] A[1:3, :] [[4 5 6] [7 8 9]] A[:2, 1:] [[2 3] [5 6]] . +) 추가: row나 column 중 한 쪽은 slicing, 다른 쪽은 단일 값 indexing을 적용해도 됨 . print(A[:2, 0]) # 1번쨰, 2번째 행의 1번째 element를 각각 추출 print(A[:2, 0].shape) # 원소 2개짜리 1차원 array가 됨 . [1 4] (2,) . | . Boolean Indexing . : 조건 필터링과 검색을 동시에 할 수 있는 인덱싱 방식. 자주 이용된다. array1 = np.arange(1, 10) print(array1) print(array1[array1 &gt; 5]) # 5보다 큰 숫자들만 indexing됨 . [1 2 3 4 5 6 7 8 9] [6 7 8 9] . ",
    "url": "https://chaelist.github.io/docs/numpy/numpy_basics/#indexing--slicing",
    "relUrl": "/docs/numpy/numpy_basics/#indexing--slicing"
  },"142": {
    "doc": "Pandas",
    "title": "Pandas",
    "content": " ",
    "url": "https://chaelist.github.io/docs/pandas",
    "relUrl": "/docs/pandas"
  },"143": {
    "doc": "Pandas 기초",
    "title": "Pandas 기초",
    "content": ". | Pandas DataFrame 만들기 . | From lists of lists, array of arrays, list of series | From dict of lists, dict of arrays, dict of series | From list of dicts | . | DataFrame 복사본 만들기 | CSV, Excel 데이터 취급 . | CSV 파일 읽고 쓰기 | Excel 파일 읽고 쓰기 | . | Indexing &amp; Slicing . | Indexing | Slicing | 조건으로 Indexing | 숫자 위치 기반 Indexing &amp; Slicing | . | 조건 indexing: Query 함수 | pd.set_option() | . *Pandas: 데이터 분석 목적으로 널리 사용되는 라이브러리 . ",
    "url": "https://chaelist.github.io/docs/pandas/pandas_basics/",
    "relUrl": "/docs/pandas/pandas_basics/"
  },"144": {
    "doc": "Pandas 기초",
    "title": "Pandas DataFrame 만들기",
    "content": "From lists of lists, array of arrays, list of series . : 2차원 리스트, 2차원 numpy array, pandas Series를 담고 있는 리스트로 DataFrame을 만들기 . import pandas as pd # import해야 사용 가능; 보통 pd로 줄여서 import # 2차원 리스트 two_dimensional_list = [['Emily', 50, 86], ['Abby', 89, 31], ['Cornelia', 68, 91], ['Kai', 88, 75]] # 2차원 numpy array two_dimensional_array = np.array(two_dimensional_list) # pandas series를 담고 있는 리스트 list_of_series = [ pd.Series(['Emily', 50, 86]), pd.Series(['Abby', 89, 31]), pd.Series(['Cornelia', 68, 91]), pd.Series(['Kai', 88, 75]) ] # 아래 셋은 모두 동일한 결과 반환 df1 = pd.DataFrame(two_dimensional_list) df2 = pd.DataFrame(two_dimensional_array) df3 = pd.DataFrame(list_of_series) df1 ## 따로 column과 row(index)에 대한 설정이 없으면 그냥 0, 1, 2, ... 순서로 값이 매겨짐 . |   | 0 | 1 | 2 | . | 0 | Emily | 50 | 86 | . | 1 | Abby | 89 | 31 | . | 2 | Cornelia | 68 | 91 | . | 3 | Kai | 88 | 75 | . +) 추가: 칼럼명과 index명을 지어주기 . two_dimensional_list = [['Emily', 50, 86], ['Abby', 89, 31], ['Cornelia', 68, 91], ['Kai', 88, 75]] my_df = pd.DataFrame(two_dimensional_list, columns=['name', 'french_score', 'math_score'], index=['a', 'b', 'c', 'd']) my_df . |   | name | french_score | math_score | . | a | Emily | 50 | 86 | . | b | Abby | 89 | 31 | . | c | Cornelia | 68 | 91 | . | d | Kai | 88 | 75 | . From dict of lists, dict of arrays, dict of series . : 파이썬 사전(dictionary)으로 DataFrame 만들기 . | 사전의 key로는 column 이름을 쓰고, 그 column에 해당하는 리스트, numpy array, 혹은 pandas Series를 사전의 value로 넣어주면 된다 | . names = ['Emily', 'Abby', 'Cornelia', 'Kai'] french_scores = [50, 89, 68, 88] math_scores = [86, 31, 91, 75] dict1 = { 'name': names, 'french_score': french_scores, 'math_score': math_scores } dict2 = { 'name': np.array(names), 'french_score': np.array(french_scores), 'math_score': np.array(math_scores) } dict3 = { 'name': pd.Series(names), 'french_score': pd.Series(french_scores), 'math_score': pd.Series(math_scores) } # 아래 셋은 모두 동일한 결과 반환 df1 = pd.DataFrame(dict1) df2 = pd.DataFrame(dict2) df3 = pd.DataFrame(dict3) df1 . |   | name | french_score | math_score | . | 0 | Emily | 50 | 86 | . | 1 | Abby | 89 | 31 | . | 2 | Cornelia | 68 | 91 | . | 3 | Kai | 88 | 75 | .   . +) DataFrame.from_dict() : from_dict()를 이용하면 dictionary 형태의 데이터를 row 방향이나 column 방향 중 어떤식으로든 DataFrame으로 만들 수 있다. | (default) dictionary의 key를 column으로: data = { 'name': ['Emily', 'Abby', 'Cornelia', 'Kai'], 'french_score': [50, 89, 68, 88], 'math_score': [86, 31, 91, 75] } pd.DataFrame.from_dict(data) # pd.DataFrame(data)과 동일한 결과 . |   | name | french_score | math_score | . | 0 | Emily | 50 | 86 | . | 1 | Abby | 89 | 31 | . | 2 | Cornelia | 68 | 91 | . | 3 | Kai | 88 | 75 | . | dictionary의 key를 row로: data = { 'row1': ['Emily', 50, 86], 'row2': ['Abby', 89, 31], 'row3': ['Cornelia', 68, 91], 'row4': ['Kai', 88, 75] } pd.DataFrame.from_dict(data, orient='index', columns=['name', 'french_score', 'math_score']) . |   | name | french_score | math_score | . | row1 | Emily | 50 | 86 | . | row2 | Abby | 89 | 31 | . | row3 | Cornelia | 68 | 91 | . | row4 | Kai | 88 | 75 | . | . From list of dicts . : 사전이 담긴 리스트로 DataFrame 만들기 . my_list = [ {'name': 'Emily', 'french_score': 50, 'math_score': 86}, {'name': 'Abby', 'french_score': 89, 'math_score': 31}, {'name': 'Cornelia', 'french_score': 68, 'math_score': 91}, {'name': 'Kai', 'french_score': 88, 'math_score': 75} ] df = pd.DataFrame(my_list) df . |   | name | french_score | math_score | . | 0 | Emily | 50 | 86 | . | 1 | Abby | 89 | 31 | . | 2 | Cornelia | 68 | 91 | . | 3 | Kai | 88 | 75 | . ※ 알아둘 사실: . | Dictionary는 data의 순서가 없기 때문에 입력 순서대로 column이 만들어지는 것은 아니다. | 반면, lists, arrays, series는 순서가 있는 자료구조이기 때문에, 입력한 순서대로 column을 만들어 준다. | . ",
    "url": "https://chaelist.github.io/docs/pandas/pandas_basics/#pandas-dataframe-%EB%A7%8C%EB%93%A4%EA%B8%B0",
    "relUrl": "/docs/pandas/pandas_basics/#pandas-dataframe-만들기"
  },"145": {
    "doc": "Pandas 기초",
    "title": "DataFrame 복사본 만들기",
    "content": ". | a = b 방식: 원본 데이터가 변하면 똑같이 변하는 ‘깊은 복사’ | df.copy() 방식: 복사 당시의 상태만 복사되는 ‘얕은 복사’ | . a = pd.DataFrame([['Kim', 23], ['Lee', 12], ['Chung', 28]], columns = ['Name', 'Age']) a . |   | Name | Age | . | 0 | Kim | 23 | . | 1 | Lee | 12 | . | 2 | Chung | 28 | . # 두 가지 방식으로 각각 복사하기 . just_copy = a ## a = b 방식의 '깊은 복사' pandas_copy = a.copy() ## 복사 당시의 상태만 복사하는 '얕은 복사' . # 원본 데이터 변경해보기 . print(a, '\\n') print(just_copy, '\\n') # 단순히 a = b 방식으로 한 '깊은 복사'의 경우, 원데이터가 변하면 함께 따라 변한다 print(pandas_copy) # a를 바꾸기 전 상태를 보존하려면, 이렇게 df.copy() 방식을 써야 한다 . Name Age 0 변경됨! 23 1 변경됨! 12 2 변경됨! 28 Name Age 0 변경됨! 23 1 변경됨! 12 2 변경됨! 28 Name Age 0 Kim 23 1 Lee 12 2 Chung 28 . ",
    "url": "https://chaelist.github.io/docs/pandas/pandas_basics/#dataframe-%EB%B3%B5%EC%82%AC%EB%B3%B8-%EB%A7%8C%EB%93%A4%EA%B8%B0",
    "relUrl": "/docs/pandas/pandas_basics/#dataframe-복사본-만들기"
  },"146": {
    "doc": "Pandas 기초",
    "title": "CSV, Excel 데이터 취급",
    "content": "CSV 파일 읽고 쓰기 . | CSV 파일을 DataFrame으로 읽어오기 df = pd.read_csv('경로/파일명.csv', header=1, index_col=0, encoding='utf-8') # 같은 폴더 안에 있는 파일은 '파일명.csv'라고만 써줘도 됨 . | header=1이라고 하면 파일의 두번째 행(index_num=1)이 header로 설정됨 (첫번째 행은 읽어오지 않음) . | header=None이라고 하면 첫 행이 header가 되는 대신 0부터 시작하는 숫자로 header가 채워짐 (default: csv 파일의 첫 행이 header로 들어감) | . | index_col=0이라고 하면 첫 번째 열이 index로 설정됨 (default: None) . | index_col='열이름' 이런 식으로 열의 이름을 직접 str으로 넣어줘도 된다 | . | 한글 등 글자가 깨질 때는 encoding='utf-8'과 같이 인코딩 방식을 설정해줄 수도 있다 | . | DataFrame을 CSV 파일로 내보내기 df.to_csv('경로/파일명.cvs', index=False) # 같은 폴더 안으로 저장하려면 '파일명.csv'라고만 써줘도 됨 . | 읽어올 때와 마찬가지로, encoding='utf-8'과 같은 옵션도 지정 가능 | index=False라고 하면 index는 저장되지 않음 (보통 index는 0부터 시작하는 숫자로 채워지기에, 굳이 저장하지 않아도 되는 경우가 많음) | . | . Excel 파일 읽고 쓰기 . | Excel 파일을 DataFrame으로 읽어오기 df = pd.read_excel('파일명.xlsx', sheet_name='Sheet2', usecols=\"C:X\", header=2, encoding='utf-8') # index_col=1이나 header=2 같은 방식으로 어느 열을 index로 가져갈건지, 어느 행을 header로 삼을건지 지정 가능. # usecols='C:X' 혹은 usecols='A,C,D' 이런 식으로해서 사용할 열 지정 가능 . | sheet_name='Sheet2'는 파일에서 Sheet2만 가져오겠다는 의미. | sheet_name에는 int/str/list를 넣어줄 수 있다 (ex. sheet_name=0, sheet_name=[0, 1, \"Sheet5\"]) | . | usecols='C:X는 C~X까지의 열을 가져오겠다는 의미. (C, X도 포함) . | usecols='A,C,D'이렇게 지정하는 것도 가능. (A, C, D 열만 가져오겠다는 의미) | . | index_col이나 header, encoding은 read_csv에서와 같은 기능 | . | DataFrame을 Excel 파일로 내보내기 df.to_excel('파일명.xlsx') . | encoding='utf-8'이나 index=False와 같은 옵션 추가 가능 | . | 여러 개의 DataFrame을 서로 다른 시트로 해서 Excel 파일로 저장 writer = pd.ExcelWriter('파일명.xlsx', engine='xlsxwriter') # Write each dataframe to a different worksheet. df1.to_excel(writer, sheet_name='df1', encoding='utf-8') # encoding='utf-8'은 옵션. df2.to_excel(writer, sheet_name='df2', encoding='utf-8') df3.to_excel(writer, sheet_name='df3', encoding='utf-8') # Close the Pandas Excel writer and output the Excel file. writer.save() . | . ",
    "url": "https://chaelist.github.io/docs/pandas/pandas_basics/#csv-excel-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%B7%A8%EA%B8%89",
    "relUrl": "/docs/pandas/pandas_basics/#csv-excel-데이터-취급"
  },"147": {
    "doc": "Pandas 기초",
    "title": "Indexing &amp; Slicing",
    "content": "iphone_df = pd.read_csv('data/iphone.csv', index_col=0) ## 데이터 출처: codeit iphone_df . |   | 출시일 | 디스플레이 | 메모리 | 출시 버전 | Face ID | . | iPhone 7 | 2016-09-16 | 4.7 | 2GB | iOS 10.0 | No | . | iPhone 7 Plus | 2016-09-16 | 5.5 | 3GB | iOS 10.0 | No | . | iPhone 8 | 2017-09-22 | 4.7 | 2GB | iOS 11.0 | No | . | iPhone 8 Plus | 2017-09-22 | 5.5 | 3GB | iOS 11.0 | No | . | iPhone X | 2017-11-03 | 5.8 | 3GB | iOS 11.1 | Yes | . | iPhone XS | 2018-09-21 | 5.8 | 4GB | iOS 12.0 | Yes | . | iPhone XS Max | 2018-09-21 | 6.5 | 4GB | iOS 12.0 | Yes | . Indexing . : .loc[행 방향, 열 방향] . | 행 방향 &amp; 열 방향 ## iPhone X의 메모리 가져오기 iphone_df.loc['iPhone X', '메모리'] . '3GB' . | 행 방향 ## iPhone X에 대한 모든 정보 가져오기 iphone_df.loc['iPhone X', :] ## iphone_df.loc['iPhone X'] 이렇게 써도 동일한 결과. 출시일 2017-11-03 디스플레이 5.8 메모리 3GB 출시 버전 iOS 11.1 Face ID Yes Name: iPhone X, dtype: object . ※ 하나의 행/열만 추출하면 ‘Pandas Series’ 형태로 가져와짐 . type(iphone_df.loc['iPhone X']) . pandas.core.series.Series . | 행 방향 - 2개 이상 # iPhone X와 iPhone 8에 대한 정보 가져오기 iphone_df.loc[['iPhone X', 'iPhone 8']] . |   | 출시일 | 디스플레이 | 메모리 | 출시 버전 | Face ID | . | iPhone X | 2017-11-03 | 5.8 | 3GB | iOS 11.1 | Yes | . | iPhone 8 | 2017-09-22 | 4.7 | 2GB | iOS 11.0 | No | . ※ 두 개 이상의 행/열을 추출하면 ‘Pandas DataFrame’ 형태로 가져와짐 . type(iphone_df.loc[['iPhone X', 'iPhone 8']]) . pandas.core.frame.DataFrame . | 열 방향 # 각 iPhone 모델에 대한 출시일 정보만 다 가져오기 iphone_df.loc[:,'출시일'] ## iphone_df['출시일'] 이렇게 써도 동일한 결과. (loc 사용X) . iPhone 7 2016-09-16 iPhone 7 Plus 2016-09-16 iPhone 8 2017-09-22 iPhone 8 Plus 2017-09-22 iPhone X 2017-11-03 iPhone XS 2018-09-21 iPhone XS Max 2018-09-21 Name: 출시일, dtype: object . *사실 열 방향 indexing은 iphone_df['출시일']과 같이 loc을 사용하지 않는 게 더 간단하다 . | 열 방향 - 2개 이상 # 출시일과 디스플레이 정보 가져오기 iphone_df[['출시일', '디스플레이']] ## iphone_df.loc[:,['출시일', '디스플레이']] 이렇게 써도 동일한 결과. |   | 출시일 | 디스플레이 | . | iPhone 7 | 2016-09-16 | 4.7 | . | iPhone 7 Plus | 2016-09-16 | 5.5 | . | iPhone 8 | 2017-09-22 | 4.7 | . | iPhone 8 Plus | 2017-09-22 | 5.5 | . | iPhone X | 2017-11-03 | 5.8 | . | iPhone XS | 2018-09-21 | 5.8 | . | iPhone XS Max | 2018-09-21 | 6.5 | . | . Slicing . iphone_df . |   | 출시일 | 디스플레이 | 메모리 | 출시 버전 | Face ID | . | iPhone 7 | 2016-09-16 | 4.7 | 2GB | iOS 10.0 | No | . | iPhone 7 Plus | 2016-09-16 | 5.5 | 3GB | iOS 10.0 | No | . | iPhone 8 | 2017-09-22 | 4.7 | 2GB | iOS 11.0 | No | . | iPhone 8 Plus | 2017-09-22 | 5.5 | 3GB | iOS 11.0 | No | . | iPhone X | 2017-11-03 | 5.8 | 3GB | iOS 11.1 | Yes | . | iPhone XS | 2018-09-21 | 5.8 | 4GB | iOS 12.0 | Yes | . | iPhone XS Max | 2018-09-21 | 6.5 | 4GB | iOS 12.0 | Yes | . | 행 방향 # iPhone 8부터 iPhone XS까지의 모든 row 반환 iphone_df.loc['iPhone 8':'iPhone XS'] . |   | 출시일 | 디스플레이 | 메모리 | 출시 버전 | Face ID | . | iPhone 8 | 2017-09-22 | 4.7 | 2GB | iOS 11.0 | No | . | iPhone 8 Plus | 2017-09-22 | 5.5 | 3GB | iOS 11.0 | No | . | iPhone X | 2017-11-03 | 5.8 | 3GB | iOS 11.1 | Yes | . | iPhone XS | 2018-09-21 | 5.8 | 4GB | iOS 12.0 | Yes | . | 열 방향 # 메모리부터 Face ID까지의 column 반환 iphone_df.loc[:, '메모리':'Face ID'] . |   | 메모리 | 출시 버전 | Face ID | . | iPhone 7 | 2GB | iOS 10.0 | No | . | iPhone 7 Plus | 3GB | iOS 10.0 | No | . | iPhone 8 | 2GB | iOS 11.0 | No | . | iPhone 8 Plus | 3GB | iOS 11.0 | No | . | iPhone X | 3GB | iOS 11.1 | Yes | . | iPhone XS | 4GB | iOS 12.0 | Yes | . | iPhone XS Max | 4GB | iOS 12.0 | Yes | . ※ 주의! indexing과 다르게, slicing은 loc을 안쓰고 직접할 수는 없음! . # loc 안쓰고 column을 slicing하려고 하면 아래와 같이 이상하게 뜬다 iphone_df['메모리':'Face ID'] . | 출시일 | 디스플레이 | 메모리 | 출시 버전 | Face ID | . | 행 방향 &amp; 열 방향 ## row, column 모두 slicing하기 iphone_df.loc['iPhone 7':'iPhone X', '메모리':'Face ID'] #순서는 무조건 row 다음 column . |   | 메모리 | 출시 버전 | Face ID | . | iPhone 7 | 2GB | iOS 10.0 | No | . | iPhone 7 Plus | 3GB | iOS 10.0 | No | . | iPhone 8 | 2GB | iOS 11.0 | No | . | iPhone 8 Plus | 3GB | iOS 11.0 | No | . | iPhone X | 3GB | iOS 11.1 | Yes | . | . 조건으로 Indexing . iphone_df . |   | 출시일 | 디스플레이 | 메모리 | 출시 버전 | Face ID | . | iPhone 7 | 2016-09-16 | 4.7 | 2GB | iOS 10.0 | No | . | iPhone 7 Plus | 2016-09-16 | 5.5 | 3GB | iOS 10.0 | No | . | iPhone 8 | 2017-09-22 | 4.7 | 2GB | iOS 11.0 | No | . | iPhone 8 Plus | 2017-09-22 | 5.5 | 3GB | iOS 11.0 | No | . | iPhone X | 2017-11-03 | 5.8 | 3GB | iOS 11.1 | Yes | . | iPhone XS | 2018-09-21 | 5.8 | 4GB | iOS 12.0 | Yes | . | iPhone XS Max | 2018-09-21 | 6.5 | 4GB | iOS 12.0 | Yes | . | 디스플레이가 5인치 이상인 스마트폰 정보만 추출하기 iphone_df.loc[iphone_df['디스플레이'] &gt; 5] . |   | 출시일 | 디스플레이 | 메모리 | 출시 버전 | Face ID | . | iPhone 7 Plus | 2016-09-16 | 5.5 | 3GB | iOS 10.0 | No | . | iPhone 8 Plus | 2017-09-22 | 5.5 | 3GB | iOS 11.0 | No | . | iPhone X | 2017-11-03 | 5.8 | 3GB | iOS 11.1 | Yes | . | iPhone XS | 2018-09-21 | 5.8 | 4GB | iOS 12.0 | Yes | . | iPhone XS Max | 2018-09-21 | 6.5 | 4GB | iOS 12.0 | Yes | . | FaceID가 가능한 스마트폰 정보만 추출 iphone_df.loc[iphone_df['Face ID'] == 'Yes'] . |   | 출시일 | 디스플레이 | 메모리 | 출시 버전 | Face ID | . | iPhone X | 2017-11-03 | 5.8 | 3GB | iOS 11.1 | Yes | . | iPhone XS | 2018-09-21 | 5.8 | 4GB | iOS 12.0 | Yes | . | iPhone XS Max | 2018-09-21 | 6.5 | 4GB | iOS 12.0 | Yes | . | 디스플레이가 5인치 이상 “AND” FaceID가 가능한 스마트폰 정보 추출 condition = (iphone_df['디스플레이'] &gt; 5) &amp; (iphone_df['Face ID'] == 'Yes') iphone_df[condition] # iphone_df.loc[condition]과 동일한 결과 . |   | 출시일 | 디스플레이 | 메모리 | 출시 버전 | Face ID | . | iPhone X | 2017-11-03 | 5.8 | 3GB | iOS 11.1 | Yes | . | iPhone XS | 2018-09-21 | 5.8 | 4GB | iOS 12.0 | Yes | . | iPhone XS Max | 2018-09-21 | 6.5 | 4GB | iOS 12.0 | Yes | . | 디스플레이가 5인치 이상 “OR” FaceID가 가능한 스마트폰 정보 추출 condition = (iphone_df['디스플레이'] &gt; 5) | (iphone_df['Face ID'] == 'Yes') iphone_df[condition] . |   | 출시일 | 디스플레이 | 메모리 | 출시 버전 | Face ID | . | iPhone 7 Plus | 2016-09-16 | 5.5 | 3GB | iOS 10.0 | No | . | iPhone 8 Plus | 2017-09-22 | 5.5 | 3GB | iOS 11.0 | No | . | iPhone X | 2017-11-03 | 5.8 | 3GB | iOS 11.1 | Yes | . | iPhone XS | 2018-09-21 | 5.8 | 4GB | iOS 12.0 | Yes | . | iPhone XS Max | 2018-09-21 | 6.5 | 4GB | iOS 12.0 | Yes | . | . 숫자 위치 기반 Indexing &amp; Slicing . : .iloc[행 방향, 열 방향] . ※ iloc: integer + location. 숫자 기반으로 위치에 접근해 indexing할 때는 iloc 사용 . iphone_df . |   | 출시일 | 디스플레이 | 메모리 | 출시 버전 | Face ID | . | iPhone 7 | 2016-09-16 | 4.7 | 2GB | iOS 10.0 | No | . | iPhone 7 Plus | 2016-09-16 | 5.5 | 3GB | iOS 10.0 | No | . | iPhone 8 | 2017-09-22 | 4.7 | 2GB | iOS 11.0 | No | . | iPhone 8 Plus | 2017-09-22 | 5.5 | 3GB | iOS 11.0 | No | . | iPhone X | 2017-11-03 | 5.8 | 3GB | iOS 11.1 | Yes | . | iPhone XS | 2018-09-21 | 5.8 | 4GB | iOS 12.0 | Yes | . | iPhone XS Max | 2018-09-21 | 6.5 | 4GB | iOS 12.0 | Yes | . | 3번째 행 &amp; 4번째 열의 값 추출 iphone_df.iloc[2, 4] . 'No' . | 2번째,4번째 행 &amp; 2번째,5번째 열 추출 iphone_df.iloc[[1,3], [1,4]] . |   | 디스플레이 | Face ID | . | iPhone 7 Plus | 5.5 | No | . | iPhone 8 Plus | 5.5 | No | . | iloc으로 slicing iphone_df.iloc[3:, 1:4] . |   | 디스플레이 | 메모리 | 출시 버전 | . | iPhone 8 Plus | 5.5 | 3GB | iOS 11.0 | . | iPhone X | 5.8 | 3GB | iOS 11.1 | . | iPhone XS | 5.8 | 4GB | iOS 12.0 | . | iPhone XS Max | 6.5 | 4GB | iOS 12.0 | . | 행: 4번째 행(index_num=3)부터 끝까지 | 열: 2번째 열(index_num=1)부터 4번째 열(index_num=3)까지 . | 1:4로 slicing했으니 5번째 열(index_num=4)는 포함되지 않는다 (list에서의 slicing과 동일) | cf) iphone_df.loc['iPhone 8':'iPhone XS']에서는 iPhone8에서 iPhone XS까지 양 끝 모두 포함 | . | . | . ",
    "url": "https://chaelist.github.io/docs/pandas/pandas_basics/#indexing--slicing",
    "relUrl": "/docs/pandas/pandas_basics/#indexing--slicing"
  },"148": {
    "doc": "Pandas 기초",
    "title": "조건 indexing: Query 함수",
    "content": ". | df.loc[]을 활용해 조건으로 indexing하는 것과 동일하지만, 더 간결한 문법으로 작성할 수 있다 (df.query() 함수도 결국은 df.loc[]의 형태로 구현됨) | . data = { 'name': ['Emily', 'Abby', 'Cornelia', 'Kai'], 'group': ['A', 'B', 'A', 'C'], 'english_score': [50, 89, 68, 88], 'math_score': [86, 31, 91, 75] } df = pd.DataFrame(data) df . |   | name | group | english_score | math_score | . | 0 | Emily | A | 50 | 86 | . | 1 | Abby | B | 89 | 31 | . | 2 | Cornelia | A | 68 | 91 | . | 3 | Kai | C | 88 | 75 | . | 비교 연산자(==, &gt;, &lt;, != 등) . df.query('english_score &gt; 80') . |   | name | group | english_score | math_score | . | 1 | Abby | B | 89 | 31 | . | 3 | Kai | C | 88 | 75 | . | in, not in . df.query('group in [\"B\", \"C\"]') # df.query('group == [\"B\", \"C\"]') 이렇게 작성해도 동일 . |   | name | group | english_score | math_score | . | 1 | Abby | B | 89 | 31 | . | 3 | Kai | C | 88 | 75 | . | and(&amp;), or(|) . | 각 조건을 괄호로 명확히 구분해주는 것이 좋다 | . df.query('(group == \"A\") &amp; (math_score &gt; 90)') . |   | name | group | english_score | math_score | . | 2 | Cornelia | A | 68 | 91 | . | index를 지칭 . | index에 이름이 있다면 그 이름(df.index.name)을 기록해줘야 함 | 만약 칼럼 중에도 ‘index’라는 칼럼이 있으면 그 칼럼으로 연산됨 | . df.query('index &gt;= 2') . |   | name | group | english_score | math_score | . | 2 | Cornelia | A | 68 | 91 | . | 3 | Kai | C | 88 | 75 | . | f-string 사용 . | f-string을 사용하면 외부 변수를 기준으로도 indexing할 수 있다 | . english_score_mean = df['english_score'].mean() # 73.75 df.query(f'english_score &gt;= {english_score_mean}') . |   | name | group | english_score | math_score | . | 1 | Abby | B | 89 | 31 | . | 3 | Kai | C | 88 | 75 | . | . ",
    "url": "https://chaelist.github.io/docs/pandas/pandas_basics/#%EC%A1%B0%EA%B1%B4-indexing-query-%ED%95%A8%EC%88%98",
    "relUrl": "/docs/pandas/pandas_basics/#조건-indexing-query-함수"
  },"149": {
    "doc": "Pandas 기초",
    "title": "pd.set_option()",
    "content": "num_df = pd.DataFrame({'Num': np.random.randn(3)*1000000000}) num_df . |   | Num | . | 0 | -4.69191e+08 | . | 1 | -2.64508e+08 | . | 2 | 9.69347e+08 | . → 위와 같이 숫자가 너무 커서 scientific notation으로 나오는 경우 / 혹은 소수점 밑 자리가 너무 길게 표시되어서 복잡한 경우, set_option으로 float display format을 변경해두면 유용하다: . pd.set_option('display.float_format', lambda x: '%.2f' % x) # float type 숫자는 소수점 두번째자리까지만 표시하겠다는 의미 . num_df . |   | Num | . | 0 | 1139992755.31 | . | 1 | 1196549705.47 | . | 2 | 777867250.54 | . +) reset_option을 활용하면 설정해둔 option을 초기화할 수 있다: . pd.reset_option('display.float_format') . ",
    "url": "https://chaelist.github.io/docs/pandas/pandas_basics/#pdset_option",
    "relUrl": "/docs/pandas/pandas_basics/#pdset_option"
  },"150": {
    "doc": "Pandas 데이터 분석",
    "title": "Pandas 데이터 분석",
    "content": ". | 큰 DataFrame 살펴보기 . | 데이터 분포 파악 | Type Conversion | 데이터 정렬 | Aggregation 함수로 데이터 속성 파악 | Series별로 추출해서 살펴보기 | . | 결손 데이터 (NaN) 처리 . | isna() | notna() | fillna() | dropna() | . | . ",
    "url": "https://chaelist.github.io/docs/pandas/pandas_data_analysis/",
    "relUrl": "/docs/pandas/pandas_data_analysis/"
  },"151": {
    "doc": "Pandas 데이터 분석",
    "title": "큰 DataFrame 살펴보기",
    "content": "데이터 분포 파악 . | df.head(): 맨 위의 행 5개만 보여줌. | 데이터가 큰 경우, 이렇게 위의 행만 일부 가져와서 데이터 구조를 살펴보면 도움이 된다. | . laptops_df = pd.read_csv('data/laptops.csv') ## 데이터 출처: codeit laptops_df.head() . |   | brand | model | ram | hd_type | hd_size | screen_size | price | processor_brand | processor_model | clock_speed | graphic_card_brand | graphic_card_size | os | weight | comments | . | 0 | Dell | Inspiron 15-3567 | 4 | hdd | 1024 | 15.6 | 40000 | intel | i5 | 2.5 | intel | nan | linux | 2.5 | nan | . | 1 | Apple | MacBook Air | 8 | ssd | 128 | 13.3 | 55499 | intel | i5 | 1.8 | intel | 2 | mac | 1.35 | nan | . | 2 | Apple | MacBook Air | 8 | ssd | 256 | 13.3 | 71500 | intel | i5 | 1.8 | intel | 2 | mac | 1.35 | nan | . | 3 | Apple | MacBook Pro | 8 | ssd | 128 | 13.3 | 96890 | intel | i5 | 2.3 | intel | 2 | mac | 3.02 | nan | . | 4 | Apple | MacBook Pro | 8 | ssd | 256 | 13.3 | 112666 | intel | i5 | 2.3 | intel | 2 | mac | 3.02 | nan | . +) 원하는 수만큼의 행을 불러올 수도 있음 . laptops_df.head(7) ## 위의 7행만 불러온다는 뜻 . |   | brand | model | ram | hd_type | hd_size | screen_size | price | processor_brand | processor_model | clock_speed | graphic_card_brand | graphic_card_size | os | weight | comments | . | 0 | Dell | Inspiron 15-3567 | 4 | hdd | 1024 | 15.6 | 40000 | intel | i5 | 2.5 | intel | nan | linux | 2.5 | nan | . | 1 | Apple | MacBook Air | 8 | ssd | 128 | 13.3 | 55499 | intel | i5 | 1.8 | intel | 2 | mac | 1.35 | nan | . | 2 | Apple | MacBook Air | 8 | ssd | 256 | 13.3 | 71500 | intel | i5 | 1.8 | intel | 2 | mac | 1.35 | nan | . | 3 | Apple | MacBook Pro | 8 | ssd | 128 | 13.3 | 96890 | intel | i5 | 2.3 | intel | 2 | mac | 3.02 | nan | . | 4 | Apple | MacBook Pro | 8 | ssd | 256 | 13.3 | 112666 | intel | i5 | 2.3 | intel | 2 | mac | 3.02 | nan | . | 5 | Apple | MacBook Pro (TouchBar) | 16 | ssd | 512 | 15 | 226000 | intel | i7 | 2.7 | intel | 2 | mac | 2.5 | nan | . | 6 | Apple | MacBook Pro (TouchBar) | 16 | ssd | 512 | 13.3 | 158000 | intel | i5 | 2.9 | intel | 2 | mac | 1.37 | nan | . | df.tail(): 맨 뒤의 5개 행만 보여줌. | head()와 마찬가지로, tail(10)과 같이 수를 지정하는 것도 가능. | . laptops_df.tail() . |   | brand | model | ram | hd_type | hd_size | screen_size | price | processor_brand | processor_model | clock_speed | graphic_card_brand | graphic_card_size | os | weight | comments | . | 162 | Asus | A555LF | 8 | hdd | 1024 | 15.6 | 39961 | intel | i3 4th gen | 1.7 | nvidia | 2 | windows | 2.3 | nan | . | 163 | Asus | X555LA-XX172D | 4 | hdd | 500 | 15.6 | 28489 | intel | i3 4th gen | 1.9 | intel | nan | linux | 2.3 | nan | . | 164 | Asus | X554LD | 2 | hdd | 500 | 15.6 | 29199 | intel | i3 4th gen | 1.9 | intel | 1 | linux | 2.3 | nan | . | 165 | Asus | X550LAV-XX771D | 2 | hdd | 500 | 15.6 | 29990 | intel | i3 4th gen | 1.7 | intel | nan | linux | 2.5 | nan | . | 166 | Asus | X540LA-XX538T | 4 | hdd | 1024 | 15.6 | 30899 | intel | i3 5th gen | 2 | intel | nan | windows | 2.3 | nan | . | df.shape: 행과 열의 개수를 알려줌. - 데이터의 분포를 한 눈에 확인할 수 있다. laptops_df.shape # 167개의 행과 15개의 열로 구성된 데이터라는 뜻 . (167, 15) . | df.columns: column명을 모두 추출. - 어떤 column들이 있는지 한 눈에 확인할 수 있다. laptops_df.columns . Index(['brand', 'model', 'ram', 'hd_type', 'hd_size', 'screen_size', 'price', 'processor_brand', 'processor_model', 'clock_speed', 'graphic_card_brand', 'graphic_card_size', 'os', 'weight', 'comments'], dtype='object') . | df.info(): 총 데이터 건수와 데이터 타입, Null 개수를 알 수 있다 laptops_df.info() # 예를 들어, weight 칼럼에서 non_null 값이 160개라는 것은, 167개 데이터 중 7개는 Null이라는 의미 . &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 167 entries, 0 to 166 Data columns (total 15 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 brand 167 non-null object 1 model 167 non-null object 2 ram 167 non-null int64 3 hd_type 167 non-null object 4 hd_size 167 non-null int64 5 screen_size 167 non-null float64 6 price 167 non-null int64 7 processor_brand 167 non-null object 8 processor_model 167 non-null object 9 clock_speed 166 non-null float64 10 graphic_card_brand 163 non-null object 11 graphic_card_size 81 non-null float64 12 os 167 non-null object 13 weight 160 non-null float64 14 comments 55 non-null object dtypes: float64(4), int64(3), object(8) memory usage: 19.7+ KB . | df.describe(): 데이터의 분포도 파악 . | 숫자형(int, float 등) 칼럼의 분포도만 조사하며, object 타입의 칼럼은 출력에서 제외됨 | count는 Not Null인 데이터 건수 | mean은 전체 데이터의 평균값, std는 표준편차, min은 최솟값, max는 최댓값 | . laptops_df.describe() . |   | ram | hd_size | screen_size | price | clock_speed | graphic_card_size | weight | . | count | 167 | 167 | 167 | 167 | 166 | 81 | 160 | . | mean | 6.8982 | 768.91 | 14.7752 | 64132.9 | 2.32108 | 52.1605 | 2.25081 | . | std | 3.78748 | 392.991 | 1.37653 | 42797.7 | 0.554187 | 444.134 | 0.648446 | . | min | 2 | 32 | 10.1 | 13872 | 1.1 | 1 | 0.78 | . | 25% | 4 | 500 | 14 | 35457.5 | 1.9 | 2 | 1.9 | . | 50% | 8 | 1024 | 15.6 | 47990 | 2.3 | 2 | 2.2 | . | 75% | 8 | 1024 | 15.6 | 77494.5 | 2.6 | 4 | 2.6 | . | max | 16 | 2048 | 17.6 | 226000 | 3.8 | 4000 | 4.2 | . ※ 데이터의 분포도를 아는 것은 머신러닝 알고리즘의 성능을 향상시키는 중요한 요소이다. (ex. 회귀에서 결정 값이 정규분포를 따르지 않고 특정 값으로 왜곡되어 있다면 예측 성능이 저하됨) . | . Type Conversion . | df.astype()을 활용하면 특정 column의 데이터 타입을 원하는 대로 변경할 수 있다. | ex1) 숫자형 변수인데 string으로 저장되어 있어서 df.describe()로 분포 파악이 안되는 경우, 숫자형 변수로 바꿔주면 좋다 | ex2) 데이터를 merge할 때, 통합 기준이 되는 column의 데이터 타입이 두 df에서 모두 같아야 한다 → 미리 type을 체크하고 통일해줘야 데이터 누락 없이 merge된다 | . # 'ram' column의 type 변경해보기 print(laptops_df['ram'].dtypes) # 이전 type 체크 laptops_df['ram'] = laptops_df['ram'].astype('str') # 바꾸고자 하는 type으로 변경 print(laptops_df['ram'].dtypes) # 바뀐 type 체크 laptops_df['ram'] = laptops_df['ram'].astype('int') # int type으로 원상 복구 print(laptops_df['ram'].dtypes) # 다시 바뀐 type 체크 . int64 object int64 . +) 여러 컬럼을 한 번에 각각 타입 변경하기: . data = {'col1': [1, 2], 'col2': [3, 4]} df = pd.DataFrame(data) print(df, '\\n') print(df.dtypes, '\\n') # col1은 int32로, col2는 str 타입으로 변경 df = df.astype({'col1': 'int32', 'col2':'str'}) print(df.dtypes) . col1 col2 0 1 3 1 2 4 col1 int64 col2 int64 dtype: object col1 int32 col2 object dtype: object . 데이터 정렬 . : df.sort_values()로 특정 열 기준으로 정렬하기 . | inplace=True를 써주면 dataframe 자체가 바뀌고, 써주지 않으면 그냥 정렬된 결과가 return되고 원본 dataframe은 바뀌지 않는다. | . *오름차순 정렬: . ## 가격 기준으로 오름차순 정렬 (ascending=True가 default라, 안써줘도 됨.) laptops_df.sort_values(by='price').head() ##다 확인하면 너무 많으니, 가격이 낮은 순으로 top5만 확인 . |   | brand | model | ram | hd_type | hd_size | screen_size | price | processor_brand | processor_model | clock_speed | graphic_card_brand | graphic_card_size | os | weight | comments | . | 148 | Acer | Aspire SW3-016 | 2 | ssd | 32 | 10.1 | 13872 | intel | Atom Z8300 | 1.44 | intel | nan | windows | 1.2 | nan | . | 83 | Acer | A315-31CDC UN.GNTSI.001 | 2 | ssd | 500 | 15.6 | 17990 | intel | Celeron | 1.1 | intel | nan | windows | 2.1 | nan | . | 108 | Acer | Aspire ES-15 NX.GKYSI.010 | 4 | hdd | 500 | 15.6 | 17990 | amd | A4-7210 | 1.8 | amd | nan | windows | 2.4 | nan | . | 100 | Acer | A315-31-P4CRUN.GNTSI.002 | 4 | hdd | 500 | 15.6 | 18990 | intel | pentium | 1.1 | intel | nan | windows | nan | nan | . | 73 | Acer | Aspire ES1-523 | 4 | hdd | 1024 | 15.6 | 19465 | amd | A4-7210 | 1.8 | amd | nan | linux | 2.4 | nan | . *내림차순 정렬: . # 가격 기준으로 내림차순 정렬 (가격 높은 것부터 순서대로.) laptops_df.sort_values(by='price', ascending=False).head() ##가격 높은 top5만 확인 . |   | brand | model | ram | hd_type | hd_size | screen_size | price | processor_brand | processor_model | clock_speed | graphic_card_brand | graphic_card_size | os | weight | comments | . | 5 | Apple | MacBook Pro (TouchBar) | 16 | ssd | 512 | 15 | 226000 | intel | i7 | 2.7 | intel | 2 | mac | 2.5 | nan | . | 90 | Alienware | 15 Notebook | 16 | hdd | 1024 | 15.6 | 199000 | intel | i7 | 2.6 | nvidia | 8 | windows | 3.5 | Maximum Display Resolution : 1920 x 1080 pixel | . | 96 | Alienware | AW13R3-7000SLV-PUS | 8 | ssd | 256 | 13.3 | 190256 | intel | i7 | 3 | nvidia | 6 | windows | 2.6 | 13.3 inch FHD (1920 x 1080) IPS Anti-Glare 300-nits Display 1 Lithium ion batteries required. (included) | . | 31 | Acer | Predator 17 | 16 | ssd | 256 | 17.3 | 178912 | intel | i7 | 2.6 | nvidia | nan | windows | 4.2 | Integrated Graphics | . | 154 | Microsoft | Surface Book CR9-00013 | 8 | ssd | 128 | 13.5 | 178799 | intel | i5 | 1.8 | intel | nan | windows | 1.5 | nan | . Aggregation 함수로 데이터 속성 파악 . : min(), max(), sum(), count() 등 사용 가능. | 전체 dataframe의 속성 일괄 파악 laptops_df.count() # 모든 칼럼의 count를 반환한다 (NaN이 아닌 값만 셈) . brand 167 model 167 ram 167 hd_type 167 hd_size 167 screen_size 167 price 167 processor_brand 167 processor_model 167 clock_speed 166 graphic_card_brand 163 graphic_card_size 81 os 167 weight 160 comments 55 model_len 167 Expensive_Affordable 167 price_cat 167 dtype: int64 . | 특정 칼럼들의 속성만 확인 laptops_df[['screen_size', 'price']].mean() # 특정 칼럼들만 추출해서 함수 적용 . screen_size 14.775210 price 64132.898204 dtype: float64 . | . Series별로 추출해서 살펴보기 . | srs.unique(): 중복을 제외하고 어떤 값이 있나 파악 . | dataframe에는 적용되지 않고 series에만 적용되는 함수 | . laptops_df['brand'].unique() # 겹치는 걸 제외하고 총 몇 개의 브랜드가 있나 살펴봄 . array(['Dell', 'Apple', 'Acer', 'HP', 'Lenovo', 'Alienware', 'Microsoft', 'Asus'], dtype=object) . +) unique한 값의 수 구하기 . laptops_df['brand'].nunique() ## len(laptops_df['brand'].unique())와 동일한 결과를 냄 . 8 . | srs.value_counts(): 각 값이 몇 번씩 나오는지 파악 laptops_df['brand'].value_counts() # 각 브랜드가 몇 번 나오는지 살펴봄 . HP 55 Acer 35 Dell 31 Lenovo 18 Asus 9 Apple 7 Alienware 6 Microsoft 6 Name: brand, dtype: int64 . | srs.describe(): df.describe()를 할 때와 같은 효과. (데이터 분포 요약) laptops_df['brand'].describe() ## 'freq'는 'top' 빈도로 등장하는 'HP'가 55번 나온다는 뜻 . count 167 unique 8 top HP freq 55 Name: brand, dtype: object . | . ",
    "url": "https://chaelist.github.io/docs/pandas/pandas_data_analysis/#%ED%81%B0-dataframe-%EC%82%B4%ED%8E%B4%EB%B3%B4%EA%B8%B0",
    "relUrl": "/docs/pandas/pandas_data_analysis/#큰-dataframe-살펴보기"
  },"152": {
    "doc": "Pandas 데이터 분석",
    "title": "결손 데이터 (NaN) 처리",
    "content": ". | 결측치가 있으면 머신러닝을 할 수 없기에, 결측치를 삭제하거나 다른 방법으로 채워줘야 한다 | . isna() . : 각 값이 NaN인지 아닌지를 True나 False로 알려준다 (NaN이면 True, 값이 존재하면 False) . laptops_df.isna().head(3) . |   | brand | model | ram | hd_type | hd_size | screen_size | price | processor_brand | processor_model | clock_speed | graphic_card_brand | graphic_card_size | os | weight | comments | . | 0 | False | False | False | False | False | False | False | False | False | False | False | True | False | False | True | . | 1 | False | False | False | False | False | False | False | False | False | False | False | False | False | False | True | . | 2 | False | False | False | False | False | False | False | False | False | False | False | False | False | False | True | . +) 칼럼별 결손 데이터 수 구하기 . | df.isna().sum()하면 True는 1, False는 0으로 계산되므로 각 칼럼별 결손 데이터 수를 구할 수 있다 | . laptops_df.isna().sum() . brand 0 model 0 ram 0 hd_type 0 hd_size 0 screen_size 0 price 0 processor_brand 0 processor_model 0 clock_speed 1 graphic_card_brand 4 graphic_card_size 86 os 0 weight 7 comments 112 dtype: int64 . notna() . : isna()와 반대로, 값이 존재하면 True, NaN(결측치)면 False를 반환 . laptops_df.notna().head(3) . |   | brand | model | ram | hd_type | hd_size | screen_size | price | processor_brand | processor_model | clock_speed | graphic_card_brand | graphic_card_size | os | weight | comments | . | 0 | True | True | True | True | True | True | True | True | True | True | True | False | True | True | False | . | 1 | True | True | True | True | True | True | True | True | True | True | True | True | True | True | False | . | 2 | True | True | True | True | True | True | True | True | True | True | True | True | True | True | False | . fillna() . : 결손 데이터 대체 . | fillna(대체할 값)으로 적어주면 NaN(결측치)가 모두 ‘대체할 값’으로 바뀐다 | inplace=True를 해주면 dataframe 자체가 변경됨 | . # graphic_card_brand 칼럼의 NaN 값을 모두 'N/A'로 대체 laptops_df['graphic_card_brand'].fillna('N/A', inplace=True) laptops_df.isna().sum() ## graphic_card_brand 칼럼의 NaN 값이 모두 없어짐 . brand 0 model 0 ram 0 hd_type 0 hd_size 0 screen_size 0 price 0 processor_brand 0 processor_model 0 clock_speed 1 graphic_card_brand 0 graphic_card_size 86 os 0 weight 7 comments 112 dtype: int64 . +) NaN 값을 해당 칼럼의 평균값으로 대체하기 . # graphic_card_size 칼럼의 NaN 값을 평균값으로 대체해보기 laptops_df['graphic_card_size'].fillna(laptops_df['graphic_card_size'].mean(), inplace=True) laptops_df.head(3) ## index=0 행의 graphic_card_size 값이 원래는 NaN이었는데, 평균값인 '52.1605'로 바뀐 것을 확인할 수 있다 . |   | brand | model | ram | hd_type | hd_size | screen_size | price | processor_brand | processor_model | clock_speed | graphic_card_brand | graphic_card_size | os | weight | comments | . | 0 | Dell | Inspiron 15-3567 | 4 | hdd | 1024 | 15.6 | 40000 | intel | i5 | 2.5 | intel | 52.1605 | linux | 2.5 | nan | . | 1 | Apple | MacBook Air | 8 | ssd | 128 | 13.3 | 55499 | intel | i5 | 1.8 | intel | 2 | mac | 1.35 | nan | . | 2 | Apple | MacBook Air | 8 | ssd | 256 | 13.3 | 71500 | intel | i5 | 1.8 | intel | 2 | mac | 1.35 | nan | . dropna() . : 결측치가 있는 행 삭제 . | NaN 값을 적절한 값으로 대체해줘도 좋지만, 적절하게 대체되지 않는 경우 머신러닝의 성능에 악영향을 줄 수 있다. 그렇기에 아예 결측치가 있는 행을 삭제해버리는 것도 좋은 방법이다. | df.dropna()라고만 하면 하나의 칼럼에라도 결측치가 있는 모든 행이 삭제됨 | df.dropna(subset=['칼럼명'])이라고 제한을 걸면, 해당 칼럼에 결측치가 있는 데이터만 삭제된다 | . laptops_df.dropna(subset=['weight'], inplace=True) ## 'weight' 칼럼에 결측치가 있는 행만 삭제 # 이 경우, 'comment' 칼럼은 결측치가 많은 것이 당연하기에, dropna()라고만 하면 대부분의 데이터가 삭제됨 laptops_df.isna().sum() . brand 0 model 0 ram 0 hd_type 0 hd_size 0 screen_size 0 price 0 processor_brand 0 processor_model 0 clock_speed 1 graphic_card_brand 3 graphic_card_size 0 os 0 weight 0 comments 105 dtype: int64 . ",
    "url": "https://chaelist.github.io/docs/pandas/pandas_data_analysis/#%EA%B2%B0%EC%86%90-%EB%8D%B0%EC%9D%B4%ED%84%B0-nan-%EC%B2%98%EB%A6%AC",
    "relUrl": "/docs/pandas/pandas_data_analysis/#결손-데이터-nan-처리"
  },"153": {
    "doc": "Pandas 데이터 가공",
    "title": "Pandas 데이터 가공",
    "content": ". | Pandas DataFrame 변경하기 . | 데이터 수정하기 | df.replace()로 일괄 수정 | 새로운 행/열 추가 | df.drop() | df.drop_duplicates() | df.insert() | df.transpose() | . | Index, 칼럼 변경하기 . | df.rename() | Index 이름 붙이기 | df.reset_index() | df.set_index() | 칼럼 순서 변경하기 | . | . ",
    "url": "https://chaelist.github.io/docs/pandas/pandas_data_modifying/",
    "relUrl": "/docs/pandas/pandas_data_modifying/"
  },"154": {
    "doc": "Pandas 데이터 가공",
    "title": "Pandas DataFrame 변경하기",
    "content": "iphone_df = pd.read_csv('data/iphone.csv', index_col=0) ## 데이터 출처: codeit iphone_df . |   | 출시일 | 디스플레이 | 메모리 | 출시 버전 | Face ID | . | iPhone 7 | 2016-09-16 | 4.7 | 2GB | iOS 10.0 | No | . | iPhone 7 Plus | 2016-09-16 | 5.5 | 3GB | iOS 10.0 | No | . | iPhone 8 | 2017-09-22 | 4.7 | 2GB | iOS 11.0 | No | . | iPhone 8 Plus | 2017-09-22 | 5.5 | 3GB | iOS 11.0 | No | . | iPhone X | 2017-11-03 | 5.8 | 3GB | iOS 11.1 | Yes | . | iPhone XS | 2018-09-21 | 5.8 | 4GB | iOS 12.0 | Yes | . | iPhone XS Max | 2018-09-21 | 6.5 | 4GB | iOS 12.0 | Yes | . 데이터 수정하기 . iphone_df.loc['iPhone 8', '메모리'] = '2.5GB' iphone_df . |   | 출시일 | 디스플레이 | 메모리 | 출시 버전 | Face ID | . | iPhone 7 | 2016-09-16 | 4.7 | 2GB | iOS 10.0 | No | . | iPhone 7 Plus | 2016-09-16 | 5.5 | 3GB | iOS 10.0 | No | . | iPhone 8 | 2017-09-22 | 4.7 | 2.5GB | iOS 11.0 | No | . | iPhone 8 Plus | 2017-09-22 | 5.5 | 3GB | iOS 11.0 | No | . | iPhone X | 2017-11-03 | 5.8 | 3GB | iOS 11.1 | Yes | . | iPhone XS | 2018-09-21 | 5.8 | 4GB | iOS 12.0 | Yes | . | iPhone XS Max | 2018-09-21 | 6.5 | 4GB | iOS 12.0 | Yes | . df.replace()로 일괄 수정 . # '메모리' 칼럼의 '4GB' 값들을 다 '5GB'로 replace. iphone_df.replace({'메모리': '4GB'}, '5GB') ## iphone_df에 다시 저장해주거나 inplace=True하면 df 자체를 바꿀 수 있음 . |   | 출시일 | 디스플레이 | 메모리 | 출시 버전 | Face ID | . | iPhone 7 | 2016-09-16 | 4.7 | 2GB | iOS 10.0 | No | . | iPhone 7 Plus | 2016-09-16 | 5.5 | 3GB | iOS 10.0 | No | . | iPhone 8 | 2017-09-22 | 4.7 | 2.5GB | iOS 11.0 | No | . | iPhone 8 Plus | 2017-09-22 | 5.5 | 3GB | iOS 11.0 | No | . | iPhone X | 2017-11-03 | 5.8 | 3GB | iOS 11.1 | Yes | . | iPhone XS | 2018-09-21 | 5.8 | 5GB | iOS 12.0 | Yes | . | iPhone XS Max | 2018-09-21 | 6.5 | 5GB | iOS 12.0 | Yes | .   . +) 다양한 df.replace() 활용법: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.replace.html . | df 전체에서 0은 10으로, 1은 100으로 바꾸기: df.replace({0: 10, 1: 100}) . | A 칼럼의 0과 B 칼럼의 5를 100으로 바꾸기: df.replace({'A': 0, 'B': 5}, 100) . | A 칼럼에서만, 0은 100으로, 4는 400으로 바꾸기: df.replace({'A': {0: 100, 4: 400}}) . | .   . 새로운 행/열 추가 . | 행 추가 iphone_df.loc['iPhone XR'] = ['2018-10-26', 6.1, '3GB', 'iOS 12.0.1', 'Yes'] iphone_df . |   | 출시일 | 디스플레이 | 메모리 | 출시 버전 | Face ID | . | iPhone 7 | 2016-09-16 | 4.7 | 2GB | iOS 10.0 | No | . | iPhone 7 Plus | 2016-09-16 | 5.5 | 3GB | iOS 10.0 | No | . | iPhone 8 | 2017-09-22 | 4.7 | 2.5GB | iOS 11.0 | No | . | iPhone 8 Plus | 2017-09-22 | 5.5 | 3GB | iOS 11.0 | No | . | iPhone X | 2017-11-03 | 5.8 | 3GB | iOS 11.1 | Yes | . | iPhone XS | 2018-09-21 | 5.8 | 4GB | iOS 12.0 | Yes | . | iPhone XS Max | 2018-09-21 | 6.5 | 4GB | iOS 12.0 | Yes | . | iPhone XR | 2018-10-26 | 6.1 | 3GB | iOS 12.0.1 | Yes | . | 열 추가 iphone_df['제조사'] = 'Apple' iphone_df . |   | 출시일 | 디스플레이 | 메모리 | 출시 버전 | Face ID | 제조사 | . | iPhone 7 | 2016-09-16 | 4.7 | 2GB | iOS 10.0 | No | Apple | . | iPhone 7 Plus | 2016-09-16 | 5.5 | 3GB | iOS 10.0 | No | Apple | . | iPhone 8 | 2017-09-22 | 4.7 | 2.5GB | iOS 11.0 | No | Apple | . | iPhone 8 Plus | 2017-09-22 | 5.5 | 3GB | iOS 11.0 | No | Apple | . | iPhone X | 2017-11-03 | 5.8 | 3GB | iOS 11.1 | Yes | Apple | . | iPhone XS | 2018-09-21 | 5.8 | 4GB | iOS 12.0 | Yes | Apple | . | iPhone XS Max | 2018-09-21 | 6.5 | 4GB | iOS 12.0 | Yes | Apple | . | iPhone XR | 2018-10-26 | 6.1 | 3GB | iOS 12.0.1 | Yes | Apple | . | . df.drop() . : 행/열 삭제 . | 행 삭제: axis=0 혹은 axis='index' iphone_df.drop('iPhone XR', axis='index', inplace=True) # inplace=True라고 해야 원본 df 자체가 변경됨. # inplace=False라고 하면 원본 df는 그대로 있고, 행/열이 삭제된 새로운 df가 결과로 반환됨 iphone_df . |   | 출시일 | 디스플레이 | 메모리 | 출시 버전 | Face ID | 제조사 | . | iPhone 7 | 2016-09-16 | 4.7 | 2GB | iOS 10.0 | No | Apple | . | iPhone 7 Plus | 2016-09-16 | 5.5 | 3GB | iOS 10.0 | No | Apple | . | iPhone 8 | 2017-09-22 | 4.7 | 2.5GB | iOS 11.0 | No | Apple | . | iPhone 8 Plus | 2017-09-22 | 5.5 | 3GB | iOS 11.0 | No | Apple | . | iPhone X | 2017-11-03 | 5.8 | 3GB | iOS 11.1 | Yes | Apple | . | iPhone XS | 2018-09-21 | 5.8 | 4GB | iOS 12.0 | Yes | Apple | . | iPhone XS Max | 2018-09-21 | 6.5 | 4GB | iOS 12.0 | Yes | Apple | . | 열 삭제: axis=1 혹은 axis='columns' iphone_df.drop('제조사', axis='columns', inplace=True) iphone_df . |   | 출시일 | 디스플레이 | 메모리 | 출시 버전 | Face ID | . | iPhone 7 | 2016-09-16 | 4.7 | 2GB | iOS 10.0 | No | . | iPhone 7 Plus | 2016-09-16 | 5.5 | 3GB | iOS 10.0 | No | . | iPhone 8 | 2017-09-22 | 4.7 | 2.5GB | iOS 11.0 | No | . | iPhone 8 Plus | 2017-09-22 | 5.5 | 3GB | iOS 11.0 | No | . | iPhone X | 2017-11-03 | 5.8 | 3GB | iOS 11.1 | Yes | . | iPhone XS | 2018-09-21 | 5.8 | 4GB | iOS 12.0 | Yes | . | iPhone XS Max | 2018-09-21 | 6.5 | 4GB | iOS 12.0 | Yes | . | 여러 줄 한 번에 삭제 . iphone_df.drop(['iPhone 7', 'iPhone 8','iPhone X'], axis='index', inplace=True) iphone_df . |   | 출시일 | 디스플레이 | 메모리 | 출시 버전 | Face ID | . | iPhone 7 Plus | 2016-09-16 | 5.5 | 3GB | iOS 10.0 | No | . | iPhone 8 Plus | 2017-09-22 | 5.5 | 3GB | iOS 11.0 | No | . | iPhone XS | 2018-09-21 | 5.8 | 4GB | iOS 12.0 | Yes | . | iPhone XS Max | 2018-09-21 | 6.5 | 4GB | iOS 12.0 | Yes | . | 특정 조건의 줄만 삭제하기 . liverpool_df = pd.read_csv('data/liverpool.csv', index_col=0) ## 데이터 출처: codeit liverpool_df . |   | position | born | number | nationality | . | Roberto Firmino | FW | 1991 | no. 9 | Brazil | . | Sadio Mane | FW | 1992 | no. 10 | Senegal | . | Mohamed Salah | FW | 1992 | no. 11 | Egypt | . | Joe Gomez | DF | 1997 | no. 12 | England | . | Alisson Becker | GK | 1992 | no. 13 | Brazil | . → ‘nationality’가 ‘Brazil’ 출신인 선수만 삭제하기 . drop_index = liverpool_df.loc[liverpool_df['nationality'] == 'Brazil'].index liverpool_df.drop(drop_index, axis='index', inplace=True) liverpool_df . |   | position | born | number | nationality | . | Sadio Mane | FW | 1992 | no. 10 | Senegal | . | Mohamed Salah | FW | 1992 | no. 11 | Egypt | . | Joe Gomez | DF | 1997 | no. 12 | England | . | . df.drop_duplicates() . : 중복된 행을 삭제. 중복된 행들 중 가장 위에 있는 행만 남기고 다 삭제해준다. cake_df = pd.DataFrame({ 'brand': ['Yummmy', 'Yummmy', 'Sweet', 'Sweet', 'Sweet'], 'taste': ['Chocolate', 'Chocolate', 'Strawberry', 'Strawberry', 'Cheese'], 'rating': [4, 4, 3.5, 15, 5] }) cake_df . |   | brand | taste | rating | . | 0 | Yummmy | Chocolate | 4 | . | 1 | Yummmy | Chocolate | 4 | . | 2 | Sweet | Strawberry | 3.5 | . | 3 | Sweet | Strawberry | 15 | . | 4 | Sweet | Cheese | 5 | . | 기본 drop_duplicates(): 모든 열의 값이 다 일치하는 행만 중복데이터로 간주해 삭제 cake_df.drop_duplicates() # inplace=True를 하지 않으면 실제 df가 변형되지는 않음. |   | brand | taste | rating | . | 0 | Yummmy | Chocolate | 4 | . | 2 | Sweet | Strawberry | 3.5 | . | 3 | Sweet | Strawberry | 15 | . | 4 | Sweet | Cheese | 5 | . | subset=[] 조건: 지정한 열(들)에서만 값이 일치하면 중복데이터로 간주해 삭제 cake_df.drop_duplicates(subset=['brand', 'taste']) # inplace=True를 하지 않으면 실제 df가 변형되지는 않음. |   | brand | taste | rating | . | 0 | Yummmy | Chocolate | 4 | . | 2 | Sweet | Strawberry | 3.5 | . | 4 | Sweet | Cheese | 5 | . | . +) df.duplicated()를 사용하면 drop_duplicates를 하기 전에 미리 중복값이 어느 정도 있는지 확인할 수 있음 (마찬가지로 subset='칼럼명'도 사용 가능!) . df.insert() . : 특정 위치에 열을 삽입해주는 함수 . iphone_df.insert(loc=3, column='순위', value=np.arange(1, len(iphone_df)+1)) # loc=3이면 4번째 열에 삽입하겠다는 뜻 # column에는 새로 삽입할 열의 이름을 입력 # value에는 새로 삽입할 열에 들어가는 값들을 입력 (list나 np array 형태) ## 이 경우에는, 1부터 해당 df의 열 개수만큼 증가하게 1,2,3,... 이런 식의 열을 추가 iphone_df . |   | 출시일 | 디스플레이 | 메모리 | 순위 | 출시 버전 | Face ID | . | iPhone 7 Plus | 2016-09-16 | 5.5 | 3GB | 1 | iOS 10.0 | No | . | iPhone 8 Plus | 2017-09-22 | 5.5 | 3GB | 2 | iOS 11.0 | No | . | iPhone XS | 2018-09-21 | 5.8 | 4GB | 3 | iOS 12.0 | Yes | . | iPhone XS Max | 2018-09-21 | 6.5 | 4GB | 4 | iOS 12.0 | Yes | . df.transpose() . (혹은 df.T): 행과 열 바꾸기 (numpy에 transpose하는 것과 동일) . iphone_df.transpose() # iphone_df.T라고 해도 동일 . |   | iPhone 7 Plus | iPhone 8 Plus | iPhone XS | iPhone XS Max | . | 출시일 | 2016-09-16 | 2017-09-22 | 2018-09-21 | 2018-09-21 | . | 디스플레이 | 5.5 | 5.5 | 5.8 | 6.5 | . | 메모리 | 3GB | 3GB | 4GB | 4GB | . | 순위 | 1 | 2 | 3 | 4 | . | 출시 버전 | iOS 10.0 | iOS 11.0 | iOS 12.0 | iOS 12.0 | . | Face ID | No | No | Yes | Yes | . ",
    "url": "https://chaelist.github.io/docs/pandas/pandas_data_modifying/#pandas-dataframe-%EB%B3%80%EA%B2%BD%ED%95%98%EA%B8%B0",
    "relUrl": "/docs/pandas/pandas_data_modifying/#pandas-dataframe-변경하기"
  },"155": {
    "doc": "Pandas 데이터 가공",
    "title": "Index, 칼럼 변경하기",
    "content": "liverpool_df . |   | position | born | number | nationality | . | Roberto Firmino | FW | 1991 | no. 9 | Brazil | . | Sadio Mane | FW | 1992 | no. 10 | Senegal | . | Mohamed Salah | FW | 1992 | no. 11 | Egypt | . | Joe Gomez | DF | 1997 | no. 12 | England | . | Alisson Becker | GK | 1992 | no. 13 | Brazil | . df.rename() . : 칼럼명 바꾸기 . liverpool_df.rename(columns={'position':'Position'}, inplace=True) liverpool_df . |   | Position | born | number | nationality | . | Roberto Firmino | FW | 1991 | no. 9 | Brazil | . | Sadio Mane | FW | 1992 | no. 10 | Senegal | . | Mohamed Salah | FW | 1992 | no. 11 | Egypt | . | Joe Gomez | DF | 1997 | no. 12 | England | . | Alisson Becker | GK | 1992 | no. 13 | Brazil | . +) 칼럼명 여러 개 한번에 바꾸기 . liverpool_df.rename(columns={'born':'Born', 'number':'Number', 'nationality':'Nationality'}, inplace=True) liverpool_df . |   | Position | Born | Number | Nationality | . | Roberto Firmino | FW | 1991 | no. 9 | Brazil | . | Sadio Mane | FW | 1992 | no. 10 | Senegal | . | Mohamed Salah | FW | 1992 | no. 11 | Egypt | . | Joe Gomez | DF | 1997 | no. 12 | England | . | Alisson Becker | GK | 1992 | no. 13 | Brazil | . Index 이름 붙이기 . : df.index.name 활용 . liverpool_df.index.name = 'Player Name' liverpool_df . | Player Name | Position | Born | Number | Nationality | . | Roberto Firmino | FW | 1991 | no. 9 | Brazil | . | Sadio Mane | FW | 1992 | no. 10 | Senegal | . | Mohamed Salah | FW | 1992 | no. 11 | Egypt | . | Joe Gomez | DF | 1997 | no. 12 | England | . | Alisson Becker | GK | 1992 | no. 13 | Brazil | . **df.index.name은 Index 이름을 출력해주는 코드 . liverpool_df.index.name . 'Player Name' . df.reset_index() . : 새로운 숫자형 index를 생성함과 동시에 기존 index를 칼럼으로 추가 . | 인덱스가 연속된 int 숫자형 데이터가 아닐 경우에 다시 이를 연속 int 숫자형 데이터로 바꿔줄 때 주로 사용 | +) 참고: 만약 기존 index가 이름이 없었다면, ‘index’라는 이름의 칼럼명으로 추가됨 | . liverpool_df.reset_index() # inplace=True를 해주지 않았기에 df 자체는 바뀌지 않음 ## 기존 index였던 'Player Name'가 칼럼으로 들어가고, 0부터 연속 숫자형으로 새롭게 index가 할당됨 . |   | Player Name | Position | Born | Number | Nationality | . | 0 | Roberto Firmino | FW | 1991 | no. 9 | Brazil | . | 1 | Sadio Mane | FW | 1992 | no. 10 | Senegal | . | 2 | Mohamed Salah | FW | 1992 | no. 11 | Egypt | . | 3 | Joe Gomez | DF | 1997 | no. 12 | England | . | 4 | Alisson Becker | GK | 1992 | no. 13 | Brazil | . +) drop=True로 하면 이전 index는 없애고 새로운 index만 남는다 . liverpool_df.reset_index(drop=True) # inplace=True를 해주지 않았기에 df 자체는 바뀌지 않음 ## 기존 index였던 'Player Name'는 없어지고, 0부터 연속 숫자형으로 새롭게 index가 할당됨 . |   | Position | Born | Number | Nationality | . | 0 | FW | 1991 | no. 9 | Brazil | . | 1 | FW | 1992 | no. 10 | Senegal | . | 2 | FW | 1992 | no. 11 | Egypt | . | 3 | DF | 1997 | no. 12 | England | . | 4 | GK | 1992 | no. 13 | Brazil | . df.set_index() . : 다른 칼럼을 index로 설정, 기존의 index는 없어진다 . liverpool_df.set_index('Number') # inplace=True를 해주지 않았기에 df 자체는 바뀌지 않음 ## 기존 index였던 'Player Name'는 없어지고, 'Number' 칼럼이 index가 됨 . | Number | Position | Born | Nationality | . | no. 9 | FW | 1991 | Brazil | . | no. 10 | FW | 1992 | Senegal | . | no. 11 | FW | 1992 | Egypt | . | no. 12 | DF | 1997 | England | . | no. 13 | GK | 1992 | Brazil | . +) 기존 index를 잃지 않고 set_index 하려면 미리 index를 별도의 열에 저장해주면 된다 . liverpool_df['Player Name'] = liverpool_df.index # 기존 index를 별도의 열에 저장 liverpool_df.set_index('Number', inplace=True) # 다음도 같은 효과: # liverpool_df = liverpool_df.reset_index().set_index('Number') liverpool_df . | Number | Position | Born | Nationality | Player Name | . | no. 9 | FW | 1991 | Brazil | Roberto Firmino | . | no. 10 | FW | 1992 | Senegal | Sadio Mane | . | no. 11 | FW | 1992 | Egypt | Mohamed Salah | . | no. 12 | DF | 1997 | England | Joe Gomez | . | no. 13 | GK | 1992 | Brazil | Alisson Becker | . 칼럼 순서 변경하기 . liverpool_df = pd.read_csv('data/liverpool.csv', index_col=0) ## 다시 불러옴 liverpool_df . |   | position | born | number | nationality | . | Roberto Firmino | FW | 1991 | no. 9 | Brazil | . | Sadio Mane | FW | 1992 | no. 10 | Senegal | . | Mohamed Salah | FW | 1992 | no. 11 | Egypt | . | Joe Gomez | DF | 1997 | no. 12 | England | . | Alisson Becker | GK | 1992 | no. 13 | Brazil | . | sorted()를 활용하여 alphabetical order로 column명 정렬 . columns = list(liverpool_df.columns) # column을 list로 받아옴 srt_col = sorted(columns) # column명 알파벳순 정렬 liverpool_df_srt = liverpool_df[srt_col] # 원하는 순서의 열 list를 이렇게 넣어주면 됨 liverpool_df_srt ## 열 순서: born, nationality, number, position . |   | born | nationality | number | position | . | Roberto Firmino | 1991 | Brazil | no. 9 | FW | . | Sadio Mane | 1992 | Senegal | no. 10 | FW | . | Mohamed Salah | 1992 | Egypt | no. 11 | FW | . | Joe Gomez | 1997 | England | no. 12 | DF | . | Alisson Becker | 1992 | Brazil | no. 13 | GK | . +) list(df.columns): 칼럼명들을 list로 출력해준다 . list(liverpool_df.columns) . ['position', 'born', 'number', 'nationality'] . | reversed()를 활용하여 원래 순서의 반대로 column명 정렬 . rvs_col = list(reversed(columns)) # 원래 column 순서의 반대로 정렬 liverpool_df_rvs = liverpool_df[rvs_col] liverpool_df_rsv ## 열 순서: nationality, number, born, position . |   | nationality | number | born | position | . | Roberto Firmino | Brazil | no. 9 | 1991 | FW | . | Sadio Mane | Senegal | no. 10 | 1992 | FW | . | Mohamed Salah | Egypt | no. 11 | 1992 | FW | . | Joe Gomez | England | no. 12 | 1997 | DF | . | Alisson Becker | Brazil | no. 13 | 1992 | GK | . | . ",
    "url": "https://chaelist.github.io/docs/pandas/pandas_data_modifying/#index-%EC%B9%BC%EB%9F%BC-%EB%B3%80%EA%B2%BD%ED%95%98%EA%B8%B0",
    "relUrl": "/docs/pandas/pandas_data_modifying/#index-칼럼-변경하기"
  },"156": {
    "doc": "Pandas 데이터 결합 & 요약",
    "title": "Pandas 데이터 결합 &amp; 요약",
    "content": ". | Merge, Join, Concatenate . | pd.merge(df1, df2) | df1.join(df2) | pd.concat([df1, df2]) | . | 데이터 요약 집계 . | df.groupby() | pd.pivot_table() | df.pivot() | . | Multi-Index, Multi-Header 다루기 . | Multi-Index 다루기 | Multi-Header 다루기 | . | unstack, stack으로 pivot . | df.unstack() | df.stack() | . | . ",
    "url": "https://chaelist.github.io/docs/pandas/pandas_merge_group/#pandas-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EA%B2%B0%ED%95%A9--%EC%9A%94%EC%95%BD",
    "relUrl": "/docs/pandas/pandas_merge_group/#pandas-데이터-결합--요약"
  },"157": {
    "doc": "Pandas 데이터 결합 & 요약",
    "title": "Merge, Join, Concatenate",
    "content": "pd.merge(df1, df2) . df1 = pd.DataFrame({'customer_id': [1001, 1002, 1003, 1004, 1005], 'name': ['Annie', 'Chloe', 'Jacob', 'David', 'Ellie'], 'sex': ['F', 'F', 'M', 'M', 'F']}) df2 = pd.DataFrame({'customer_id': [1001, 1001, 1005, 1004, 1002, 1008], 'amount_spent': [10000, 20000, 12000, 5000, 30000, 35000]}) print(df1, '\\n') print(df2) . customer_id name sex 0 1001 Annie F 1 1002 Chloe F 2 1003 Jacob M 3 1004 David M 4 1005 Ellie F customer_id amount_spent 0 1001 10000 1 1001 20000 2 1005 12000 3 1004 5000 4 1002 30000 5 1008 35000 . | inner join (default): 두 DataFrame에 모두 데이터가 존재하는 행만 남는다 pd.merge(df1, df2) # 보통 default로 'inner join' 방식이 사용되며, 자동으로 이름이 같은 column을 기준으로 merge된다 . |   | customer_id | name | sex | amount_spent | . | 0 | 1001 | Annie | F | 10000 | . | 1 | 1001 | Annie | F | 20000 | . | 2 | 1002 | Chloe | F | 30000 | . | 3 | 1004 | David | M | 5000 | . | 4 | 1005 | Ellie | F | 12000 | . | ‘on’으로 통합할 열 지정 pd.merge(df1, df2, on='customer_id') # on에 어떤 열을 기준으로 통합할 것인지 지정해줄 수 있다. (두 df에 모두 존재하는 열이여야 함) ## 이 예시에서는 상관 없지만, 기준 열이 아니면서 이름이 같은 열이 2개 이상일 경우에는 꼭 on을 지정해줘야 함 . |   | customer_id | name | sex | amount_spent | . | 0 | 1001 | Annie | F | 10000 | . | 1 | 1001 | Annie | F | 20000 | . | 2 | 1002 | Chloe | F | 30000 | . | 3 | 1004 | David | M | 5000 | . | 4 | 1005 | Ellie | F | 12000 | . | outer join: 어느 한 쪽의 DataFrame에만 있는 값이라도 다 보여주는 방식 pd.merge(df1, df2, how='outer') # 한 쪽에만 데이터가 있다면, 반대쪽 DataFrame의 해당 부분은 NaN으로 채워짐 . |   | customer_id | name | sex | amount_spent | . | 0 | 1001 | Annie | F | 10000 | . | 1 | 1001 | Annie | F | 20000 | . | 2 | 1002 | Chloe | F | 30000 | . | 3 | 1003 | Jacob | M | NaN | . | 4 | 1004 | David | M | 5000 | . | 5 | 1005 | Ellie | F | 12000 | . | 6 | 1008 | NaN | NaN | 35000 | . | left outer join: 왼쪽 DataFrame (df1)에 존재하는 값을 기준으로 merge pd.merge(df1, df2, how='left') . |   | customer_id | name | sex | amount_spent | . | 0 | 1001 | Annie | F | 10000 | . | 1 | 1001 | Annie | F | 20000 | . | 2 | 1002 | Chloe | F | 30000 | . | 3 | 1003 | Jacob | M | NaN | . | 4 | 1004 | David | M | 5000 | . | 5 | 1005 | Ellie | F | 12000 | . | right outer join: 오른쪽 DataFrame (df2)에 존재하는 값을 기준으로 merge pd.merge(df1, df2, how='right') . |   | customer_id | name | sex | amount_spent | . | 0 | 1001 | Annie | F | 10000 | . | 1 | 1001 | Annie | F | 20000 | . | 2 | 1005 | Ellie | F | 12000 | . | 3 | 1004 | David | M | 5000 | . | 4 | 1002 | Chloe | F | 30000 | . | 5 | 1008 | NaN | NaN | 35000 | . | 키가 되는 기준열의 이름이 두 데이터프레임에서 다른 경우 df1 = pd.DataFrame({'customer_id': [1001, 1002, 1003, 1004, 1005], 'name': ['Annie', 'Chloe', 'Jacob', 'David', 'Ellie'], 'sex': ['F', 'F', 'M', 'M', 'F']}) df3 = pd.DataFrame({'customer': ['Chloe', 'Jacob', 'Ellie', 'Haley', 'Serene'], 'VIP_type': ['VIP', 'Gold', 'VIP', 'Silver', 'Gold']}) print(df1, '\\n') print(df3) . customer_id name sex 0 1001 Annie F 1 1002 Chloe F 2 1003 Jacob M 3 1004 David M 4 1005 Ellie F customer VIP_type 0 Chloe VIP 1 Jacob Gold 2 Ellie VIP 3 Haley Silver 4 Serene Gold . # 키가 되는 기준열의 이름이 두 데이터프레임에서 다른 경우: left_on, right_on 인수를 사용하여 기준열을 명시 . pd.merge(df1, df3, left_on='name', right_on=\"customer\") # default: inner join . |   | customer_id | name | sex | customer | VIP_type | . | 0 | 1002 | Chloe | F | Chloe | VIP | . | 1 | 1003 | Jacob | M | Jacob | Gold | . | 2 | 1005 | Ellie | F | Ellie | VIP | . | . df1.join(df2) . | join 메소드를 사용해도 merge와 같은 결과를 낼 수 있다 | 하지만 join()은 index를 기준으로 결합하는 것이 기본이고, how=’left’가 default다 | . # join을 쓰기 위해, df1과 df2 모두 결합할 기준이 되는 열인 'customer_id'를 index로 지정해준다 df1.set_index('customer_id', inplace=True) df2.set_index('customer_id', inplace=True) print(df1, '\\n') print(df2) . name sex customer_id 1001 Annie F 1002 Chloe F 1003 Jacob M 1004 David M 1005 Ellie F amount_spent customer_id 1001 10000 1001 20000 1005 12000 1004 5000 1002 30000 1008 35000 . | left outer join (default) df1.join(df2) # how='left'가 default값 . | customer_id | name | sex | amount_spent | . | 1001 | Annie | F | 10000 | . | 1001 | Annie | F | 20000 | . | 1002 | Chloe | F | 30000 | . | 1003 | Jacob | M | NaN | . | 1004 | David | M | 5000 | . | 1005 | Ellie | F | 12000 | . | inner join df1.join(df2, how='inner') # how에 'right', 'outer', 'innner' 모두 사용 가능 . | customer_id | name | sex | amount_spent | . | 1001 | Annie | F | 10000 | . | 1001 | Annie | F | 20000 | . | 1002 | Chloe | F | 30000 | . | 1004 | David | M | 5000 | . | 1005 | Ellie | F | 12000 | . | . pd.concat([df1, df2]) . | 기준 열(key column)을 사용하지 않고 단순히 데이터를 연결(concatenate)한다 | 기본적으로는 위/아래로 데이터 행을 연결하며, axis=1로 인수를 설정하면 옆으로 연결해준다 | . import numpy as np df1 = pd.DataFrame( np.arange(6).reshape(3, 2), columns=['a', 'b']) df2 = pd.DataFrame( 5 + np.arange(6).reshape(2,3), columns=['a', 'b', 'c']) print(df1, '\\n') print(df2) . a b 0 0 1 1 2 3 2 4 5 a b c 0 5 6 7 1 8 9 10 . | 위 아래로 연결 (default) pd.concat([df1, df2]) ## 위 아래로 연결됨. (column명 기준) . |   | a | b | c | . | 0 | 0 | 1 | NaN | . | 1 | 2 | 3 | NaN | . | 2 | 4 | 5 | NaN | . | 0 | 5 | 6 | 7 | . | 1 | 8 | 9 | 10 | . +) 그냥 이어붙이면 행 인덱스번호도 그대로 가져오기에, ignore_index=True로 인덱스를 재배열할 수 있다 . pd.concat([df1, df2], ignore_index=True) . |   | a | b | c | . | 0 | 0 | 1 | NaN | . | 1 | 2 | 3 | NaN | . | 2 | 4 | 5 | NaN | . | 3 | 5 | 6 | 7 | . | 4 | 8 | 9 | 10 | . | 옆으로 연결: axis=1 설정 pd.concat([df1, df2], axis=1) # axis=1로 설정해주면, 옆으로 연결된다 (index 기준) . |   | a | b | a | b | c | . | 0 | 0 | 1 | 5 | 6 | 7 | . | 1 | 2 | 3 | 8 | 9 | 10 | . | 2 | 4 | 5 | NaN | NaN | NaN | . | Series 간 결합 sr1 = pd.Series(['e0','e1','e2','e3'], name = 'e') sr2 = pd.Series(['g0','g1','g2','g3'], name = 'g') result1 = pd.concat([sr1, sr2], axis = 1) #열방향 연결, type = 데이터프레임이 됨 print(result1, '\\n') result2 = pd.concat([sr1, sr2], axis = 0) #행방향 연결, type = 시리즈 print(result2, '\\n') ## 사실, pd.concat([sr1, sr2], axis = 0)는 sr1.append(sr2)와 동일한 결과 result3 = sr1.append(sr2) #이렇게 하면 행방향 concat과 동일 print(result3) . e g 0 e0 g0 1 e1 g1 2 e2 g2 3 e3 g3 0 e0 1 e1 2 e2 3 e3 0 g0 1 g1 2 g2 3 g3 dtype: object 0 e0 1 e1 2 e2 3 e3 0 g0 1 g1 2 g2 3 g3 dtype: object . +) 참고) pd.merge()는 Series간 결합에는 사용할 수 없다. (공통의 column이 있어야 하는 게 전제조건) . pd.merge(sr1, sr2) ## series끼리 merge하려고 하면 error. MergeError Traceback (most recent call last) &lt;ipython-input-43-176c5dc5c9a2&gt; in &lt;module&gt;() ----&gt; 1 pd.merge(sr1, sr2) . | . ",
    "url": "https://chaelist.github.io/docs/pandas/pandas_merge_group/#merge-join-concatenate",
    "relUrl": "/docs/pandas/pandas_merge_group/#merge-join-concatenate"
  },"158": {
    "doc": "Pandas 데이터 결합 & 요약",
    "title": "데이터 요약 집계",
    "content": "df.groupby() . : 그룹별로 요약해서 집계하기 . titanic_df = pd.read_csv('data/titanic.csv') ## 데이터 출처: kaggle titanic_df.head() . |   | PassengerId | Survived | Pclass | Name | Sex | Age | SibSp | Parch | Ticket | Fare | Cabin | Embarked | . | 0 | 1 | 0 | 3 | Braund, Mr. Owen Harris | male | 22 | 1 | 0 | A/5 21171 | 7.25 | nan | S | . | 1 | 2 | 1 | 1 | Cumings, Mrs. John Bradley (Florence Briggs Thayer) | female | 38 | 1 | 0 | PC 17599 | 71.2833 | C85 | C | . | 2 | 3 | 1 | 3 | Heikkinen, Miss. Laina | female | 26 | 0 | 0 | STON/O2. 3101282 | 7.925 | nan | S | . | 3 | 4 | 1 | 1 | Futrelle, Mrs. Jacques Heath (Lily May Peel) | female | 35 | 1 | 0 | 113803 | 53.1 | C123 | S | . | 4 | 5 | 0 | 3 | Allen, Mr. William Henry | male | 35 | 0 | 0 | 373450 | 8.05 | nan | S | . | df.groupby(‘칼럼명’).함수() . | DataFrame에 groupby()를 호출해 반환된 결과에 aggregation 함수를 호출하면, groupby() 대상 칼럼을 제외한 모든 칼럼에 aggregation 함수를 적용한 결과를 보여준다 | aggregation 함수 종류: count, sum, max, mean, nunique 등. (*nunique: unique count) | 대상 칼럼이 index로 들어감 | . # 'Pclass' 기준으로 데이터 수를 집계 titanic_df.groupby('Pclass').count() ## 3등석이 가장 사람이 많음을 유추 가능. | Pclass | PassengerId | Survived | Name | Sex | Age | SibSp | Parch | Ticket | Fare | Cabin | Embarked | . | 1 | 216 | 216 | 216 | 216 | 186 | 216 | 216 | 216 | 216 | 176 | 214 | . | 2 | 184 | 184 | 184 | 184 | 173 | 184 | 184 | 184 | 184 | 16 | 184 | . | 3 | 491 | 491 | 491 | 491 | 355 | 491 | 491 | 491 | 491 | 12 | 491 | . | as_index=False: 대상 칼럼이 index가 아닌 칼럼으로 들어감 # 이렇게 as_index=False를 넣어주면 index가 아닌 칼럼으로 들어감 titanic_df.groupby('Pclass', as_index=False).count() . |   | Pclass | PassengerId | Survived | Name | Sex | Age | SibSp | Parch | Ticket | Fare | Cabin | Embarked | . | 0 | 1 | 216 | 216 | 216 | 216 | 186 | 216 | 216 | 216 | 216 | 176 | 214 | . | 1 | 2 | 184 | 184 | 184 | 184 | 173 | 184 | 184 | 184 | 184 | 16 | 184 | . | 2 | 3 | 491 | 491 | 491 | 491 | 355 | 491 | 491 | 491 | 491 | 12 | 491 | . | 특정 칼럼만 필터링해서 함수 적용 . titanic_df.groupby('Pclass')[['PassengerId', 'Survived']].count() . | Pclass | PassengerId | Survived | . | 1 | 216 | 216 | . | 2 | 184 | 184 | . | 3 | 491 | 491 | . +) 이렇게 하는 것도 가능: . titanic_df.groupby('Pclass', as_index=False)[['PassengerId', 'Survived']].count() . |   | Pclass | PassengerId | Survived | . | 0 | 1 | 216 | 216 | . | 1 | 2 | 184 | 184 | . | 2 | 3 | 491 | 491 | . | 여러 개의 aggregation 함수를 적용 . | 적용하려는 함수명을 agg( ) 내에 인자로 입력해서 사용해야 함 | . titanic_df.groupby('Pclass')['Age'].agg([max, min]) . | Pclass | max | min | . | 1 | 80 | 0.92 | . | 2 | 70 | 0.67 | . | 3 | 74 | 0.42 | . | 여러 칼럼에 여러 aggregation 함수 적용 . titanic_df.groupby('Pclass')[['Age', 'Fare', 'SibSp']].agg([max, min]) . |   | : Age | | : Fare | | : SibSp | |   |   |   | . | Pclass | max | min | max | min | max | min | . | 1 | 80 | 0.92 | 512.329 | 0 | 3 | 0 | . | 2 | 70 | 0.67 | 73.5 | 0 | 3 | 0 | . | 3 | 74 | 0.42 | 69.55 | 0 | 8 | 0 | . | 여러 칼럼에 서로 다른 Aggregation 함수를 적용 . | dictionary 형태로 함수를 적용할 칼럼과 함수명을 입력 | ※ aggregation 방식 중 nunique의 경우, 'Age': pd.Series.nunique와 같은 방식으로 표기해줘야 한다 | . agg_format = {'Age':'max', 'SibSp':'sum', 'Fare':'mean'} titanic_df.groupby('Pclass').agg(agg_format) . | Pclass | Age | SibSp | Fare | . | 1 | 80 | 90 | 84.1547 | . | 2 | 70 | 74 | 20.6622 | . | 3 | 74 | 302 | 13.6756 | . | . +) string value groupby . # sample dataframe 생성: 이용자와 이용한 앱이 매칭되어 있는 데이터 users = ['A', 'B', 'A', 'B', 'C', 'D', 'E', 'D', 'E'] apps_used = ['Google', 'Naver', 'YouTube', 'Google', 'YouTube', 'Facebook', 'Instagram', 'Naver', 'Google'] sample_df = pd.DataFrame({'users':users, 'apps_used':apps_used}) sample_df . |   | users | apps_used | . | 0 | A | Google | . | 1 | B | Naver | . | 2 | A | YouTube | . | 3 | B | Google | . | 4 | C | YouTube | . | 5 | D | Facebook | . | 6 | E | Instagram | . | 7 | D | Naver | . | 8 | E | Google | . | list로 묶기: 각 이용자별 이용한 앱 조합 sample_df.groupby('users')['apps_used'].apply(list) . users A [Google, YouTube] B [Naver, Google] C [YouTube] D [Facebook, Naver] E [Instagram, Google] Name: apps_used, dtype: object . | set으로 묶기: 순서 고려X sample_df.groupby('users')['apps_used'].apply(set) . users A {Google, YouTube} B {Google, Naver} C {YouTube} D {Facebook, Naver} E {Google, Instagram} Name: apps_used, dtype: object . | apply lambda를 활용해 원하는 모양으로 결합 sample_df.groupby('users')['apps_used'].apply(lambda x: ', '.join(x)) . users A Google, YouTube B Naver, Google C YouTube D Facebook, Naver E Instagram, Google Name: apps_used, dtype: object . | .   . pd.pivot_table() . : 엑셀에서처럼 pivot 돌리기 (엑셀에서 pivot 돌리는 것의 동작 방식을 생각하면 쉬움) . | ex) pd.pivot_table(df, index='칼럼1', columns=['칼럼2','칼럼3'], values='칼럼4', fill_value=0, aggfunc='sum') . | index(열)나 columns(행)에 str 하나만 넣어도 되고, [ ] 리스트 형태로 여러 개 넣으면 multiindex / multiheader되는 것 | values가 실제 pivot을 채울 값 | aggfunc에는 집계 함수를 적어 줌. - ‘sum’이면 합산 / ‘mean’이면 평균 | fill_value=0은 NaN 값들을 0으로 채워서 return하겠다는 뜻 | . | . titanic_df.head() . |   | PassengerId | Survived | Pclass | Name | Sex | Age | SibSp | Parch | Ticket | Fare | Cabin | Embarked | . | 0 | 1 | 0 | 3 | Braund, Mr. Owen Harris | male | 22 | 1 | 0 | A/5 21171 | 7.25 | nan | S | . | 1 | 2 | 1 | 1 | Cumings, Mrs. John Bradley (Florence Briggs Thayer) | female | 38 | 1 | 0 | PC 17599 | 71.2833 | C85 | C | . | 2 | 3 | 1 | 3 | Heikkinen, Miss. Laina | female | 26 | 0 | 0 | STON/O2. 3101282 | 7.925 | nan | S | . | 3 | 4 | 1 | 1 | Futrelle, Mrs. Jacques Heath (Lily May Peel) | female | 35 | 1 | 0 | 113803 | 53.1 | C123 | S | . | 4 | 5 | 0 | 3 | Allen, Mr. William Henry | male | 35 | 0 | 0 | 373450 | 8.05 | nan | S | . | 기본 예시 . pd.pivot_table(titanic_df, # 피벗할 데이터프레임 index='Pclass', # 행 위치에 들어갈 열 columns='Sex', # 열 위치에 들어갈 열 values='Age', # 데이터로 사용할 열 aggfunc='mean') # 데이터 집계함수 . | Sex | female | male | . | Pclass |   |   | . | 1 | 34.6118 | 41.2814 | . | 2 | 28.723 | 30.7407 | . | 3 | 21.75 | 26.5076 | . | 여러 개의 데이터 집계함수 넣기 pd.pivot_table(titanic_df, index='Pclass', columns='Sex', values='Survived', aggfunc=['mean', 'sum']) . |   | : mean | | : sum| |   |   | . | Sex | female | male | female | male | . | Pclass |   |   |   |   | . | 1 | 0.968085 | 0.368852 | 91 | 45 | . | 2 | 0.921053 | 0.157407 | 70 | 17 | . | 3 | 0.5 | 0.135447 | 72 | 47 | . | 많은 칼럼을 동시에 인자로 입력 . | index=, columns=, values=에 모두 list 형태로 여러 개의 칼럼을 인자로 넣을 수 있다 | . pd.pivot_table(titanic_df, index=['Pclass', 'Sex'], columns='Survived', values=['Age', 'Fare'], aggfunc=['mean', 'max']) . |   |   | : mean ||| | : max ||| |   |   |   |   |   |   | . |   |   | : Age | | : Fare | | : Age | | : Fare | |   |   |   |   | . |   | Survived | : 0 | : 1 | : 0 | : 1 | : 0 | : 1 | : 0 | : 1 | . | Pclass | Sex |   |   |   |   |   |   |   |   | . | 1 | female | 25.6667 | 34.939 | 110.604 | 105.978 | 50 | 63 | 151.55 | 512.329 | . | ^^ | male | 44.582 | 36.248 | 62.8949 | 74.6373 | 71 | 80 | 263 | 512.329 | . | 2 | female | 36 | 28.0809 | 18.25 | 22.289 | 57 | 55 | 26 | 65 | . | ^^ | male | 33.369 | 16.022 | 19.489 | 21.0951 | 70 | 62 | 73.5 | 39 | . | 3 | female | 23.8182 | 19.3298 | 19.7731 | 12.4645 | 48 | 63 | 69.55 | 31.3875 | . | ^^ | male | 27.2558 | 22.2742 | 12.2045 | 15.5797 | 74 | 45 | 69.55 | 56.4958 | . | . df.pivot() . : 데이터 형태를 변경해준다. 원하는 형태로 데이터를 변형해준다는 점에서 pd.pivot_table과 유사하지만, pivot_table과 달리 데이터를 요약 집계해주는 기능은 없음 . pivot_sample = titanic_df.groupby(['Pclass', 'Sex'])[['Fare']].mean().reset_index() pivot_sample . |   | Pclass | Sex | Fare | . | 0 | 1 | female | 106.126 | . | 1 | 1 | male | 67.2261 | . | 2 | 2 | female | 21.9701 | . | 3 | 2 | male | 19.7418 | . | 4 | 3 | female | 16.1188 | . | 5 | 3 | male | 12.6616 | . → ‘Pclass’ 정보가 index로, ‘Sex’ 정보가 column으로, 그리고 ‘Fare’가 value로 들어가도록 reshape: . pivot_sample.pivot(index='Pclass', columns='Sex', values='Fare') . | Pclass | female | male | . | 1 | 106.126 | 67.2261 | . | 2 | 21.9701 | 19.7418 | . | 3 | 16.1188 | 12.6616 | . ",
    "url": "https://chaelist.github.io/docs/pandas/pandas_merge_group/#%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%9A%94%EC%95%BD-%EC%A7%91%EA%B3%84",
    "relUrl": "/docs/pandas/pandas_merge_group/#데이터-요약-집계"
  },"159": {
    "doc": "Pandas 데이터 결합 & 요약",
    "title": "Multi-Index, Multi-Header 다루기",
    "content": ": groupby나 pivot_table로 데이터를 요약 집계하다 보면, multi-index나 multi-header를 다루게 된다 . Multi-Index 다루기 . gdf = titanic_df.groupby(['Pclass', 'Sex'])[['Age', 'Fare', 'Survived']].mean() gdf . |   |   | : Age | : Fare | : Survived | . | Pclass | Sex |   |   |   | . | 1 | female | 34.6118 | 106.126 | 0.968085 | . | ^^ | male | 41.2814 | 67.2261 | 0.368852 | . | 2 | female | 28.723 | 21.9701 | 0.921053 | . | ^^ | male | 30.7407 | 19.7418 | 0.157407 | . | 3 | female | 21.75 | 16.1188 | 0.5 | . | ^^ | male | 26.5076 | 12.6616 | 0.135447 | . | 인덱싱: 1st level # [Pclass]열의 [1]행만 인덱싱 (index_number=1이 아니라, 인덱스 이름이 1) gdf.loc[1] ## 1은 int 타입이라서 '1'이라고 쓰지 않음 . | Sex | Age | Fare | Survived | . | female | 34.6118 | 106.126 | 0.968085 | . | male | 41.2814 | 67.2261 | 0.368852 | . | 인덱싱: 1st level - 2nd level (순서대로, 차근차근) # [Pclass]열의 [1]행 중, [Sex] 열의 [female]행만 인덱싱 (순서: 1, female) ## 이렇게 tuple 형태로 넣어주려면, 더 밖에 있는 index부터 차근차근 접근해야 한다 gdf.loc[(1, 'female')] ## pivot_df.loc[1].loc['female'] 이렇게 해도 동일한 결과. Age 34.611765 Fare 106.125798 Survived 0.968085 Name: (1, female), dtype: float64 . | 인덱싱: 2nd level에 바로 접근 . | 멀티인덱서 xs를 이용하면 그룹 범주와 상관 없이 수준(level)만 명시해주면 인덱싱이 가능 (cf.loc와 iloc를 이용하려면 큰 그룹부터 순차적으로 인덱싱을 해야 함) | . # Sex그룹의 male값을 갖는 행을 모두 추출, 즉 등급(class)별 male에 대한 자료를 인덱싱 gdf.xs('male', level='Sex') . | Pclass | Age | Fare | Survived | . | 1 | 41.2814 | 67.2261 | 0.368852 | . | 2 | 30.7407 | 19.7418 | 0.157407 | . | 3 | 26.5076 | 12.6616 | 0.135447 | . +) level=에는 인덱스명을 적어줘도 되고, 해당 인덱스의 레벨을 int로 넣어줘도 된다. (0부터 시작) . ## 'Sex' level은 두번째 level이므로, level=1이라고 넣어줘도 위와 같은 결과. gdf.xs('male', level=1) . | 멀티 인덱스 해제: reset_index() 활용 . gdf.reset_index() . |   | Pclass | Sex | Age | Fare | Survived | . | 0 | 1 | female | 34.6118 | 106.126 | 0.968085 | . | 1 | 1 | male | 41.2814 | 67.2261 | 0.368852 | . | 2 | 2 | female | 28.723 | 21.9701 | 0.921053 | . | 3 | 2 | male | 30.7407 | 19.7418 | 0.157407 | . | 4 | 3 | female | 21.75 | 16.1188 | 0.5 | . | 5 | 3 | male | 26.5076 | 12.6616 | 0.135447 | . | . Multi-Header 다루기 . gdf2 = titanic_df.groupby('Pclass')[['Age', 'Fare']].agg(['mean', 'max']) gdf2 . |   | : Age | | : Fare | |   |   | . |   | : mean | : max | : mean | : max | . | Pclass |   |   |   |   | . | 1 | 38.2334 | 80 | 84.1547 | 512.329 | . | 2 | 29.8776 | 70 | 20.6622 | 73.5 | . | 3 | 25.1406 | 74 | 13.6756 | 69.55 | . | 인덱싱: 1st level . # 'Age' 열을 indexing gdf2['Age'] . | Pclass | mean | max | . | 1 | 38.2334 | 80 | . | 2 | 29.8776 | 70 | . | 3 | 25.1406 | 74 | . | 인덱싱: 1st level - 2nd level (순서대로, 차근차근) . # 'Age' 열의 'mean' 열을 indexing gdf2['Age']['mean'] ## gdf2[('Age', 'mean')] 이렇게 넣어줘도 동일한 결과. Pclass 1 38.233441 2 29.877630 3 25.140620 Name: mean, dtype: float64 . | 인덱싱: 2nd level에 바로 접근 . | 열 방향 멀티인덱싱에도 xs를 사용 – axis=1이라고 하면 열 방향을 의미 | . # 'mean' 행을 모두 추출 -- 'Age'와 'Fare'의 'mean'을 모두 추출 gdf2.xs('mean', level=1, axis=1) . | Pclass | Age | Fare | . | 1 | 38.2334 | 84.1547 | . | 2 | 29.8776 | 20.6622 | . | 3 | 25.1406 | 13.6756 | . | 멀티 헤더 해제: column명을 아래와 같이 새로 넣어주면 된다 . # 이렇게 column명을 넣어주면 된다 (두 레벨의 칼럼명을 모두 담은 이름으로 적어야 좋음) gdf2.columns = ['Age_mean', 'Age_max', 'Fare_mean','Fare_max'] gdf2 . | Pclass | Age_mean | Age_max | Fare_mean | Fare_max | . | 1 | 38.2334 | 80 | 84.1547 | 512.329 | . | 2 | 29.8776 | 70 | 20.6622 | 73.5 | . | 3 | 25.1406 | 74 | 13.6756 | 69.55 | . +) map 사용해서 새로운 칼럼명 생성하기 . gdf2.columns = gdf2.columns.map('{0[0]}_{0[1]}'.format) # column1_column2의 형태로 새 column명을 생성해줌 gdf2 . | Pclass | Age_mean | Age_max | Fare_mean | Fare_max | . | 1 | 38.2334 | 80 | 84.1547 | 512.329 | . | 2 | 29.8776 | 70 | 20.6622 | 73.5 | . | 3 | 25.1406 | 74 | 13.6756 | 69.55 | . | . ",
    "url": "https://chaelist.github.io/docs/pandas/pandas_merge_group/#multi-index-multi-header-%EB%8B%A4%EB%A3%A8%EA%B8%B0",
    "relUrl": "/docs/pandas/pandas_merge_group/#multi-index-multi-header-다루기"
  },"160": {
    "doc": "Pandas 데이터 결합 & 요약",
    "title": "unstack, stack으로 pivot",
    "content": "df.unstack() . : index의 특정 level을 column 방향으로 pivot해주는 함수 (multi-index를 풀어준다) . gdf3 = titanic_df.groupby(['Pclass', 'Sex'])[['Age']].mean() gdf3 . | | | : Age |   | . | Pclass | Sex |   | . | 1 | female | 34.6118 | . | ^^ | male | 41.2814 | . | 2 | female | 28.723 | . | ^^ | male | 30.7407 | . | 3 | female | 21.75 | . | ^^ | male | 26.5076 | . | level=0: 1st level 인덱스를 column 방향으로 pivot gdf3.unstack(level=0) . |   | : Age || |   |   | . | Pclass | : 1 | : 2 | : 3 | . | Sex |   |   |   | . | female | 34.6118 | 28.723 | 21.75 | . | male | 41.2814 | 30.7407 | 26.5076 | . | level=1: 2nd level 인덱스를 column 방향으로 pivot . gdf3.unstack(level=1) . |   | :Age | |   | . | Sex | :female | :male | . | Pclass |   |   | . | 1 | 34.6118 | 41.2814 | . | 2 | 28.723 | 30.7407 | . | 3 | 21.75 | 26.5076 | . +) 이 경우, 2nd level이 마지막 level이므로, level=-1라고 적어도 동일한 결과 . | . df.stack() . : column의 특정 level을 index 방향으로 pivot해주는 함수 (unstack과 반대: multi-index를 만들어준다) . gdf4 = titanic_df.groupby('Pclass')[['Age']].agg(['mean', 'max']) gdf4 . |   | : Age | |   | . |   | : mean | : max | . | Pclass |   |   | . | 1 | 38.2334 | 80 | . | 2 | 29.8776 | 70 | . | 3 | 25.1406 | 74 | . → 2번째 level (마지막 level)을 index 방향으로 pivot . gdf4.stack(level=-1) . | | | : Age |   | . | Pclass |   |   | . | 1 | mean | 38.2334 | . | ^^ | max | 80 | . | 2 | mean | 29.8776 | . | ^^ | max | 70 | . | 3 | mean | 25.1406 | . | ^^ | max | 74 | . ",
    "url": "https://chaelist.github.io/docs/pandas/pandas_merge_group/#unstack-stack%EC%9C%BC%EB%A1%9C-pivot",
    "relUrl": "/docs/pandas/pandas_merge_group/#unstack-stack으로-pivot"
  },"161": {
    "doc": "Pandas 데이터 결합 & 요약",
    "title": "Pandas 데이터 결합 & 요약",
    "content": " ",
    "url": "https://chaelist.github.io/docs/pandas/pandas_merge_group/",
    "relUrl": "/docs/pandas/pandas_merge_group/"
  },"162": {
    "doc": "Pandas plot() 함수",
    "title": "Pandas plot() 함수",
    "content": ". | 선 그래프 | 막대 그래프 | 파이 그래프 | 박스 플롯 | 산점도 (Scatter Plot) | . *Pandas 내장 기능인 .plot() 함수를 사용하면 쉽게 그래프를 그릴 수 있다. (당연히 element가 ‘숫자형’일때만 그래프 그려짐) . ",
    "url": "https://chaelist.github.io/docs/visualization/pandas_plot/",
    "relUrl": "/docs/visualization/pandas_plot/"
  },"163": {
    "doc": "Pandas plot() 함수",
    "title": "선 그래프",
    "content": "import pandas as pd . %matplotlib inline # 그래프의 결과를 출력 세션에 나타나게 하는 설정 . df = pd.read_csv('data/broadcast.csv', index_col=0) ## 데이터 출처: codeit df . |   | KBS | MBC | SBS | TV CHOSUN | JTBC | Channel A | MBN | . | 2011 | 35.951 | 18.374 | 11.173 | 9.102 | 7.38 | 3.771 | 2.809 | . | 2012 | 36.163 | 16.022 | 11.408 | 8.785 | 7.878 | 5.874 | 3.31 | . | 2013 | 31.989 | 16.778 | 9.673 | 9.026 | 7.81 | 5.35 | 3.825 | . | 2014 | 31.21 | 15.663 | 9.108 | 9.44 | 7.49 | 5.776 | 4.572 | . | 2015 | 27.777 | 16.573 | 9.099 | 9.94 | 7.267 | 6.678 | 5.52 | . | 2016 | 27.583 | 14.982 | 8.669 | 9.829 | 7.727 | 6.624 | 5.477 | . | 2017 | 26.89 | 12.465 | 8.661 | 8.886 | 9.453 | 6.056 | 5.215 | . | 기본 그래프 그려보기 df.plot() # df.plot(kind='line') 이렇게 써도 동일. 선(line) 그래프가 default이기 때문. | 1개 값에 대해서만 그래프 그리기 df.plot(y='KBS') # KBS에 대해서만 그래프를 그림 # df['KBS'].plot() 이렇게 써도 거의 동일. | 2개 값에 대해 그래프 그리기 df.plot(y=['KBS', 'MBC']); # df[['KBS', 'MBC']].plot() 이렇게 써도 동일 . | . ",
    "url": "https://chaelist.github.io/docs/visualization/pandas_plot/#%EC%84%A0-%EA%B7%B8%EB%9E%98%ED%94%84",
    "relUrl": "/docs/visualization/pandas_plot/#선-그래프"
  },"164": {
    "doc": "Pandas plot() 함수",
    "title": "막대 그래프",
    "content": "df = pd.read_csv('data/sports.csv',index_col=0) ## 데이터 출처: codeit df . |   | Male | Female | . | Swimming | 103 | 178 | . | Baseball | 363 | 289 | . | Basketball | 151 | 97 | . | Golf | 154 | 232 | . | Soccer | 413 | 109 | . | Bowling | 88 | 129 | . | 기본 막대 그래프 df.plot(kind='bar') . | 가로 방향 막대 그래프 df.plot(kind='barh'); # 'h' is for 'horizontal' . | 누적 막대 그래프 df.plot(kind='bar', stacked=True); # 남+여 통틀어 어떤 운동이 가장 인기가 많은지 볼 수 있다 . | 1개 값에 대해서만 그래프 그리기 df['Female'].plot(kind='bar'); # 여성을 대상으로한 조사결과만 보고 싶을 때 . | . ",
    "url": "https://chaelist.github.io/docs/visualization/pandas_plot/#%EB%A7%89%EB%8C%80-%EA%B7%B8%EB%9E%98%ED%94%84",
    "relUrl": "/docs/visualization/pandas_plot/#막대-그래프"
  },"165": {
    "doc": "Pandas plot() 함수",
    "title": "파이 그래프",
    "content": "df = pd.read_csv('data/broadcast.csv', index_col=0) ## 데이터 출처: codeit df . |   | KBS | MBC | SBS | TV CHOSUN | JTBC | Channel A | MBN | . | 2011 | 35.951 | 18.374 | 11.173 | 9.102 | 7.38 | 3.771 | 2.809 | . | 2012 | 36.163 | 16.022 | 11.408 | 8.785 | 7.878 | 5.874 | 3.31 | . | 2013 | 31.989 | 16.778 | 9.673 | 9.026 | 7.81 | 5.35 | 3.825 | . | 2014 | 31.21 | 15.663 | 9.108 | 9.44 | 7.49 | 5.776 | 4.572 | . | 2015 | 27.777 | 16.573 | 9.099 | 9.94 | 7.267 | 6.678 | 5.52 | . | 2016 | 27.583 | 14.982 | 8.669 | 9.829 | 7.727 | 6.624 | 5.477 | . | 2017 | 26.89 | 12.465 | 8.661 | 8.886 | 9.453 | 6.056 | 5.215 | . → 2017년 데이터만 추출 . df.loc[2017] . KBS 26.890 MBC 12.465 SBS 8.661 TV CHOSUN 8.886 JTBC 9.453 Channel A 6.056 MBN 5.215 Name: 2017, dtype: float64 . → 2017년 데이터로 파이 그래프 그리기 . df.loc[2017].plot(kind='pie') . +) 추가 tip: pie가 동그랗게 그려지지 않는 경우, axis('equal')을 붙여주면 된다 . df.loc[2017].plot(kind='pie').axis('equal') . ###히스토그램 . df = pd.read_csv('data/body.csv', index_col=0) ## 데이터 출처: codeit df.head() . | Number | Height | Weight | . | 1 | 176 | 85.2 | . | 2 | 175.3 | 67.7 | . | 3 | 168.6 | 75.2 | . | 4 | 168.1 | 67.1 | . | 5 | 175.3 | 63 | . | 기본 히스토그램: 10개 구간으로 나눠서 그려짐 df.plot(kind='hist', y='Height'); # 10개 구간으로 나뉘는 게 default setting . | y축의 ‘Frequency’는 빈도수. 이 히스토그램의 경우, 키가 175-177.5인 학생이 250명 있다는 뜻 | . | 구간 수 조정 df.plot(kind='hist', y='Height', bins=15); # 15개 구간으로 나누기 . | bins는 숫자가 크다고 무조건 좋은 게 아니라, 인사이트를 가져오기 좋은 적당한 숫자를 잘 골라야한다 | . | . ",
    "url": "https://chaelist.github.io/docs/visualization/pandas_plot/#%ED%8C%8C%EC%9D%B4-%EA%B7%B8%EB%9E%98%ED%94%84",
    "relUrl": "/docs/visualization/pandas_plot/#파이-그래프"
  },"166": {
    "doc": "Pandas plot() 함수",
    "title": "박스 플롯",
    "content": "df = pd.read_csv('data/exam.csv') ## 데이터 출처: codeit df.head() . |   | gender | race/ethnicity | parental level of education | lunch | test preparation course | math score | reading score | writing score | . | 0 | female | group B | bachelor’s degree | standard | none | 72 | 72 | 74 | . | 1 | female | group C | some college | standard | completed | 69 | 90 | 88 | . | 2 | female | group B | master’s degree | standard | none | 90 | 95 | 93 | . | 3 | male | group A | associate’s degree | free/reduced | none | 47 | 57 | 44 | . | 4 | male | group C | some college | standard | none | 76 | 78 | 75 | . *‘math score’ 데이터 분포 확인해보기(최솟값, 1사분위값, 2사분위값, …) . df['math score'].describe() . count 1000.00000 mean 66.08900 std 15.16308 min 0.00000 25% 57.00000 50% 66.00000 75% 77.00000 max 100.00000 Name: math score, dtype: float64 . *‘math score’ 데이터 분포 박스플롯으로 확인하기 . df.plot(kind='box', y='math score') . ※ box plot 설명 . | 박스 위~아래가 IQR(Interquartile Range) | 박스 중앙의 선이 중앙값 (= Q2, 2사분위값, 50% 지점) | 박스 맨 윗부분: Q3 (3사분위값, 75% 지점) | 박스 맨 아랫부분: Q1 (1사분위값, 25% 지점) | 박스 위쪽 선(수염)의 끝 부분: Upperfence 내 최대값 (= Q3 + 1.5*IQR보다 작은 값 중 가장 큰 값) . | Upperfence(상위 경계): Q3 + 1.5*IQR | . | 박스 아래쪽 선(수염)의 끝 부분: Lowerfence 내 최소값 (= Q1 - 1.5*IQR보다 큰 값 중 가장 작은 값) . | Lowerfence(하위 경계): Q1 - 1.5*IQR | . | 박스 &amp; 수염 부분을 벗어난 동그란 점들은 outlier(이상점) | . ",
    "url": "https://chaelist.github.io/docs/visualization/pandas_plot/#%EB%B0%95%EC%8A%A4-%ED%94%8C%EB%A1%AF",
    "relUrl": "/docs/visualization/pandas_plot/#박스-플롯"
  },"167": {
    "doc": "Pandas plot() 함수",
    "title": "산점도 (Scatter Plot)",
    "content": "df = pd.read_csv('data/exam.csv') ## 데이터 출처: codeit df.head() . |   | gender | race/ethnicity | parental level of education | lunch | test preparation course | math score | reading score | writing score | . | 0 | female | group B | bachelor’s degree | standard | none | 72 | 72 | 74 | . | 1 | female | group C | some college | standard | completed | 69 | 90 | 88 | . | 2 | female | group B | master’s degree | standard | none | 90 | 95 | 93 | . | 3 | male | group A | associate’s degree | free/reduced | none | 47 | 57 | 44 | . | 4 | male | group C | some college | standard | none | 76 | 78 | 75 | . | 수학 점수와 읽기 점수 간의 연관성 확인 df.plot(kind='scatter', x='math score', y='reading score') . | 읽기 점수와 쓰기 점수 간의 연관성 확인 df.plot(kind='scatter', x='reading score', y='writing score') . | 읽기 점수와 쓰기 점수 간의 연관성이 수학 점수와 읽기 점수 간의 연관성보다 크다고 판단 | . | . ",
    "url": "https://chaelist.github.io/docs/visualization/pandas_plot/#%EC%82%B0%EC%A0%90%EB%8F%84-scatter-plot",
    "relUrl": "/docs/visualization/pandas_plot/#산점도-scatter-plot"
  },"168": {
    "doc": "Pandas str, dt, 조건문",
    "title": "Pandas str, dt, 조건문",
    "content": ". | 문자열 처리 함수 ‘str’ . | 문자열 인덱싱: str[ ] | 문자열 분할: str.split( ) | 특정 글자 기준 필터링 | 정규식 &amp; findall 활용 | 문자 대체: str.replace( ) | 문자열 패딩 | 양 끝 공백 제거: str.strip( ) | 대소문자 변경 | . | 날짜형 데이터 변환/가공 . | ‘Datetime’ 타입으로 변환/가공 | Datetime 데이터 가공하기 | 숫자로 읽혀온 날짜형 데이터 수정 | relativedelta로 기간 계산 | +) 더 간단한 기간 계산 | . | 조건문으로 데이터 가공 . | lambda 식 + apply() | lambda 식 + applymap() | pd.where() | np.where() 활용 | . | . ",
    "url": "https://chaelist.github.io/docs/pandas/pandas_str_dt_con/",
    "relUrl": "/docs/pandas/pandas_str_dt_con/"
  },"169": {
    "doc": "Pandas str, dt, 조건문",
    "title": "문자열 처리 함수 ‘str’",
    "content": ". | DataFrame에 직접 적용은 안되고, Series에 적용해야 한다 | . df = pd.read_csv('data/전국_법정동코드.txt', sep='\\t', encoding='cp949') ## 데이터 출처: 행정표준코드관리시스템 df.head() . |   | 법정동코드 | 법정동명 | 폐지여부 | . | 0 | 1100000000 | 서울특별시 | 존재 | . | 1 | 1111000000 | 서울특별시 종로구 | 존재 | . | 2 | 1111010100 | 서울특별시 종로구 청운동 | 존재 | . | 3 | 1111010200 | 서울특별시 종로구 신교동 | 존재 | . | 4 | 1111010300 | 서울특별시 종로구 궁정동 | 존재 | . 문자열 인덱싱: str[ ] . | 기존의 string indexing 방법과 동일 (참고) | . df['법정동명'].str[:5].head() # 앞 5자리까지 추출 . 0 서울특별시 1 서울특별시 2 서울특별시 3 서울특별시 4 서울특별시 Name: 법정동명, dtype: object . 문자열 분할: str.split( ) . | string에 split()을 적용하는 방법과 동일 (참고) | . df['법정동명'].str.split().head() # 공백 기준 분리 . 0 [서울특별시] 1 [서울특별시, 종로구] 2 [서울특별시, 종로구, 청운동] 3 [서울특별시, 종로구, 신교동] 4 [서울특별시, 종로구, 궁정동] Name: 법정동명, dtype: object . +) 분할된 개별 리스트를 바로 데이터 프레임으로 만드려면, expand=True옵션을 추가 . df['법정동명'].str.split(\" \", expand=True).head() ## df['법정동명'].str.split(expand=True).head()라고만 해도 됨 (공백 기준으로 분리하는 경우) . |   | 0 | 1 | 2 | 3 | 4 | . | 0 | 서울특별시 | None | None | None | None | . | 1 | 서울특별시 | 종로구 | None | None | None | . | 2 | 서울특별시 | 종로구 | 청운동 | None | None | . | 3 | 서울특별시 | 종로구 | 신교동 | None | None | . | 4 | 서울특별시 | 종로구 | 궁정동 | None | None | . 특정 글자 기준 필터링 . | str.startswith(): 특정 글자로 시작하는 데이터만 필터링 # '대전'으로 시작하는 데이터만 필터링 df[df['법정동명'].str.startswith(\"대전\")].head() . |   | 법정동코드 | 법정동명 | 폐지여부 | . | 3870 | 3000000000 | 대전광역시 | 존재 | . | 3871 | 3011000000 | 대전광역시 동구 | 존재 | . | 3872 | 3011010100 | 대전광역시 동구 원동 | 존재 | . | 3873 | 3011010200 | 대전광역시 동구 인동 | 존재 | . | 3874 | 3011010300 | 대전광역시 동구 효동 | 존재 | . | str.endswith(): 특정 글자로 끝나는 데이터만 필터링 # '구'으로 끝나는 데이터만 필터링 df[df['법정동명'].str.endswith(\"구\")].head() . |   | 법정동코드 | 법정동명 | 폐지여부 | . | 1 | 1111000000 | 서울특별시 종로구 | 존재 | . | 94 | 1114000000 | 서울특별시 중구 | 존재 | . | 179 | 1117000000 | 서울특별시 용산구 | 존재 | . | 229 | 1120000000 | 서울특별시 성동구 | 존재 | . | 332 | 1121500000 | 서울특별시 광진구 | 존재 | . | str.contains(): 특정 글자를 포함하는 데이터만 필터링 . # '서대문구'가 들어간 데이터만 필터링 df[df['법정동명'].str.contains(\"서대문구\")].head() . |   | 법정동코드 | 법정동명 | 폐지여부 | . | 597 | 1141000000 | 서울특별시 서대문구 | 존재 | . | 635 | 1141010100 | 서울특별시 서대문구 충정로2가 | 존재 | . | 636 | 1141010200 | 서울특별시 서대문구 충정로3가 | 존재 | . | 637 | 1141010300 | 서울특별시 서대문구 합동 | 존재 | . | 638 | 1141010400 | 서울특별시 서대문구 미근동 | 존재 | . | . 정규식 &amp; findall 활용 . | 특정 정규식에 매칭되는 값 추출하기 (참고: 정규식) | . df['법정동명'].str.findall('\\w+동').head() . 0 [] 1 [] 2 [청운동] 3 [신교동] 4 [궁정동] Name: 법정동명, dtype: object . 문자 대체: str.replace( ) . | string에 replace()을 적용하는 방법과 동일 (참고) | . df['법정동명'].str.replace(\" \", \"_\").head() # 공백을 \"_\"로 대체 . 0 서울특별시 1 서울특별시_종로구 2 서울특별시_종로구_청운동 3 서울특별시_종로구_신교동 4 서울특별시_종로구_궁정동 Name: 법정동명, dtype: object . 문자열 패딩 . | 고정된 길이로, 남는 부분 채우기 | . # 문자열 길이를 20자로 맞추려고 함 &amp; 남는 만큼 왼쪽을 \"_\"로 채우기 df['법정동명'].str.pad(width=20, side='left', fillchar='_').head() ## side='right'이라고 하면 오른쪽이 \"-\"로 채워짐 . 0 _______________서울특별시 1 ___________서울특별시 종로구 2 _______서울특별시 종로구 청운동 3 _______서울특별시 종로구 신교동 4 _______서울특별시 종로구 궁정동 Name: 법정동명, dtype: object . +) side=를 지정해주지 않으면 (=default) 좌우가 균일하게 채워진다. (글자가 가운데로.) . # 글자를 가운데에 두고 좌우로 \"_\"를 채워서 문자열 길이 20자를 맞춰줌 df['법정동명'].str.center(width=20, fillchar='_').head() . 0 _______서울특별시________ 1 _____서울특별시 종로구______ 2 ___서울특별시 종로구 청운동____ 3 ___서울특별시 종로구 신교동____ 4 ___서울특별시 종로구 궁정동____ Name: 법정동명, dtype: object . 양 끝 공백 제거: str.strip( ) . | string에 strip()을 적용하는 방법과 동일 (참고) | rstip()과 lstrip()도 적용 가능. | . df2 = pd.DataFrame({'col1':['abcde ',' FFFFghij ','abCCe '], 'col2':[' fgHAAij ',' fghij ','lmnop ']}) df2 . |   | col1 | col2 | . | 0 | abcde | fgHAAij | . | 1 | FFFFghij | fghij | . | 2 | abCCe | lmnop | .   . df2['col1'].str.strip() # 앞 뒤 공백 제거 ## 마찬가지로, rstip()과 lstrip()도 적용 가능! . 0 abcde 1 FFFFghij 2 abCCe Name: col1, dtype: object . → list로 변환해서 살펴보면 확실히 앞 뒤 공백이 제거되었음을 확인 가능 . df2['col1'].str.strip().to_list() . ['abcde', 'FFFFghij', 'abCCe'] . 대소문자 변경 . | str.lower(): 소문자로 변경 df2['col1'].str.lower() . 0 abcde 1 ffffghij 2 abcce Name: col1, dtype: object . | str.upper(): 대문자로 변경 df2['col1'].str.upper() . 0 ABCDE 1 FFFFGHIJ 2 ABCCE Name: col1, dtype: object . | str.swapcase(): 소문자 ↔ 대문자 서로 바꿔줌 df2['col1'].str.swapcase() # 소문자는 대문자로, 대문자는 소문자로 . 0 ABCDE 1 ffffGHIJ 2 ABccE Name: col1, dtype: object . | . ",
    "url": "https://chaelist.github.io/docs/pandas/pandas_str_dt_con/#%EB%AC%B8%EC%9E%90%EC%97%B4-%EC%B2%98%EB%A6%AC-%ED%95%A8%EC%88%98-str",
    "relUrl": "/docs/pandas/pandas_str_dt_con/#문자열-처리-함수-str"
  },"170": {
    "doc": "Pandas str, dt, 조건문",
    "title": "날짜형 데이터 변환/가공",
    "content": "‘Datetime’ 타입으로 변환/가공 . transaction = pd.read_csv('data/transaction_1.csv') ## 데이터 출처: https://github.com/wikibook/pyda100 transaction.head() . |   | transaction_id | price | payment_date | customer_id | . | 0 | T0000000113 | 210000 | 2019-02-01 01:36:57 | PL563502 | . | 1 | T0000000114 | 50000 | 2019-02-01 01:37:23 | HD678019 | . | 2 | T0000000115 | 120000 | 2019-02-01 02:34:19 | HD298120 | . | 3 | T0000000116 | 210000 | 2019-02-01 02:47:23 | IK452215 | . | 4 | T0000000117 | 170000 | 2019-02-01 04:33:46 | PL542865 | . *pd.to_datetime(): 데이터를 datetime type으로 변환해주는 함수 . | datetime 형태로 생긴 데이터: 그냥 pd.to_datetime() 안에 넣어주면 알아서 연/월/일 등이 식별된다 print(transaction['payment_date'].dtypes) # pd.to_datetime 함수를 활용해 datetime type으로 변환 transaction['payment_date'] = pd.to_datetime(transaction['payment_date']) print(transaction['payment_date'].dtypes) . object datetime64[ns] . | str, int 등 타입의 데이터 변환하기 (연/월/일의 구조가 모호한 경우) . | format=을 통해 어떤 형태로 데이터가 담겨 있는지 명시해주면 알맞게 datetime으로 변환 가능 | ex) 데이터가 ‘12/08/2021’의 형태라면, format='%d/%m/%Y이라고 명시 | . df = pd.DataFrame({'date_example': [201901, 201902, 201903, 201904, 201905]}) df['datetime'] = pd.to_datetime(df['date_example'], format='%Y%m') df . |   | date_example | datetime | . | 0 | 201901 | 2019-01-01 00:00:00 | . | 1 | 201902 | 2019-02-01 00:00:00 | . | 2 | 201903 | 2019-03-01 00:00:00 | . | 3 | 201904 | 2019-04-01 00:00:00 | . | 4 | 201905 | 2019-05-01 00:00:00 | . +) 데이터 타입 확인: . df.dtypes . date_example int64 datetime datetime64[ns] dtype: object . | . Datetime 데이터 가공하기 . | .dt.strftime(): 날짜와 시간 정보를 지정한 특정 문자열 형태로 바꿔준다 . | %Y: 앞의 빈자리를 0으로 채우는 4자리 연도 숫자 | %m: 앞의 빈자리를 0으로 채우는 2자리 월 숫자 | %d: 앞의 빈자리를 0으로 채우는 2자리 일 숫자 | .dt.strftime(\"%Y년 %m월 %d일\") 이런 식으로 (= ‘2020년 12월 25일’의 형식) 정리하는 것도 가능. | . # '201902'와 같은 형태로 날짜를 연월 단위로 정리 transaction['payment_month'] = transaction['payment_date'].dt.strftime('%Y%m') transaction[['payment_date', 'payment_month']].head() . |   | payment_date | payment_month | . | 0 | 2019-02-01 01:36:57 | 201902 | . | 1 | 2019-02-01 01:37:23 | 201902 | . | 2 | 2019-02-01 02:34:19 | 201902 | . | 3 | 2019-02-01 02:47:23 | 201902 | . | 4 | 2019-02-01 04:33:46 | 201902 | . → 이렇게 월 단위로 정리해놓으면, 월별 금액 비교 등이 용이하다. # month 기준으로 price의 합 집계 transaction.groupby('payment_month').sum()['price'] ## transaction.groupby('payment_month')['price'].sum()라고 해도 됨. payment_month 201902 160185000 201903 160370000 201904 160510000 201905 155420000 201906 74755000 Name: price, dtype: int64 . cf) strftime() 메소드로 정리한 데이터는 str 타입 (= object 타입) . transaction[['payment_date', 'payment_month']].dtypes . payment_date datetime64[ns] payment_month object dtype: object . | .dt.weekday: 해당 날짜의 요일을 계산해준다 . | 요일은 숫자로 표시 (월:0, 화:1, …., 토:5, 일:6) | . transaction['weekday'] = transaction['payment_date'].dt.weekday transaction.head() . |   | transaction_id | price | payment_date | customer_id | payment_month | weekday | . | 0 | T0000000113 | 210000 | 2019-02-01 01:36:57 | PL563502 | 201902 | 4 | . | 1 | T0000000114 | 50000 | 2019-02-01 01:37:23 | HD678019 | 201902 | 4 | . | 2 | T0000000115 | 120000 | 2019-02-01 02:34:19 | HD298120 | 201902 | 4 | . | 3 | T0000000116 | 210000 | 2019-02-01 02:47:23 | IK452215 | 201902 | 4 | . | 4 | T0000000117 | 170000 | 2019-02-01 04:33:46 | PL542865 | 201902 | 4 | . +) dt.year, dt.month, dt.day 등의 속성도 있음 . transaction['payment_date'].dt.year.head() # 연도별로 정리 . 0 2019 1 2019 2 2019 3 2019 4 2019 Name: payment_date, dtype: int64 . | . 숫자로 읽혀온 날짜형 데이터 수정 . : excel 날짜형 데이터가 숫자로 잘못 읽혀오는 경우, pd.to_timedelta()를 활용해 수정해줄 수 있다 . kokyaku_data = pd.read_excel('data/kokyaku_daicho.xlsx') ## 데이터 출처: https://github.com/wikibook/pyda10 kokyaku_data.head() . |   | 고객이름 | 지역 | 등록일 | . | 0 | 김 현성 | H시 | 2018-01-04 00:00:00 | . | 1 | 김 도윤 | E시 | 42782 | . | 2 | 김 지한 | A시 | 2018-01-07 00:00:00 | . | 3 | 김 하윤 | F시 | 42872 | . | 4 | 김 시온 | E시 | 43127 | . | ‘등록일’ 칼럼의 몇몇 데이터가 날짜가 아닌 숫자형으로 읽혀온 것을 볼 수 있다 | . # 숫자로 읽혀온 데이터를 판별해, flg_is_serial에 저장 (True/False로) flg_is_serial = kokyaku_data['등록일'].astype('str').str.isdigit() ## isdigit()을 통해 숫자인지를 판별. flg_is_serial.sum() ## 숫자로 된 날짜 정보가 22개임을 알 수 있음. 22 . → to_timedelta()를 활용해 숫자 데이터를 ‘~일’ 데이터로 바꿔주고, 1900/01/01에 더하기 . | 엑셀에서, 42782를 날짜 형태로 나타내면 ‘1900/01/01’을 기준으로 42782일을 더한, ‘2017/02/16’이 된다. (엑셀의 날짜 기억법) | pd.to_timedelta(이름.astype(‘float’), unit=’D’)는 ‘42782 days’와 같은 형태로 숫자 데이터를 ‘~일’ 데이터로 바꿔준다 . | unit=’D’는 ‘days’를 의미. ‘~일’로 바꾸라는 것. | . | . # to_timedelta 활용 from_serial = pd.to_timedelta(kokyaku_data.loc[flg_is_serial, '등록일'].astype('float'), unit='D') + pd.to_datetime('1900/01/01') from_serial.head() ## 날짜형으로 잘 변환된 것을 볼 수 있다 . 1 2017-02-18 3 2017-05-19 4 2018-01-29 21 2017-07-06 27 2017-06-17 Name: 등록일, dtype: datetime64[ns] . +) 예시로, to_timedelta, unit=’D’가 어떤 방식으로 데이터를 변환하는지 보여주기 위해 이렇게 출력해봄: . pd.to_timedelta(kokyaku_data.loc[flg_is_serial, '등록일'].astype('float'), unit='D').iloc[0] . Timedelta('42782 days 00:00:00') . +) 추가: 사실, ‘42782’가 엑셀에서는 ‘2017-02-16’라고 나온다. (위에서는 2017-02-18로 변환.) . | 1) 엑셀은 숫자가 0이 아닌 1부터 시작하고, 2) 1900년은 평년인데 1900/02/29일을 유효한 날짜로 계산하기 때문. (엑셀의 버그) | 그래서 엑셀의 날짜 형식 숫자를 단순히 파이썬으로 계산하면 이틀이 어긋나는 것.. | 그래서 원래는 다음과 같이 -2를 해줘서 계산해야 엑셀 날짜에 맞출 수 있다. | . from_serial = pd.to_timedelta(kokyaku_data.loc[flg_is_serial, '등록일'].astype('float') - 2, unit='D') + pd.to_datetime('1900/01/01') from_serial.head() . 1 2017-02-16 3 2017-05-17 4 2018-01-27 21 2017-07-04 27 2017-06-15 Name: 등록일, dtype: datetime64[ns] . relativedelta로 기간 계산 . (relative delta: 날짜 비교 함수) . customer = pd.read_csv('data/customer_master.csv') ## 데이터 출처: https://github.com/wikibook/pyda10 customer.head() # 스포츠센터 회원 데이터. start_date는 가입일, end_date는 탈퇴일. ## end_date가 NaN인 것은 아직 탈퇴하지 않은 회원 . |   | customer_id | name | class | gender | start_date | end_date | campaign_id | is_deleted | . | 0 | OA832399 | XXXX | C01 | F | 2015-05-01 00:00:00 | NaN | CA1 | 0 | . | 1 | PL270116 | XXXXX | C01 | M | 2015-05-01 00:00:00 | NaN | CA1 | 0 | . | 2 | OA974876 | XXXXX | C01 | M | 2015-05-01 00:00:00 | NaN | CA1 | 0 | . | 3 | HD024127 | XXXXX | C01 | F | 2015-05-01 00:00:00 | NaN | CA1 | 0 | . | 4 | HD661448 | XXXXX | C03 | F | 2015-05-01 00:00:00 | NaN | CA1 | 0 | . → start_date와 end_date 칼럼을 datetime 타입으로 변경 . customer['start_date'] = pd.to_datetime(customer['start_date']) customer['end_date'] = pd.to_datetime(customer['end_date']) customer.head() ## NaT는 datetime type의 NaN . |   | customer_id | name | class | gender | start_date | end_date | campaign_id | is_deleted | . | 0 | OA832399 | XXXX | C01 | F | 2015-05-01 | NaT | CA1 | 0 | . | 1 | PL270116 | XXXXX | C01 | M | 2015-05-01 | NaT | CA1 | 0 | . | 2 | OA974876 | XXXXX | C01 | M | 2015-05-01 | NaT | CA1 | 0 | . | 3 | HD024127 | XXXXX | C01 | F | 2015-05-01 | NaT | CA1 | 0 | . | 4 | HD661448 | XXXXX | C03 | F | 2015-05-01 | NaT | CA1 | 0 | . → 가입~탈퇴까지의 회원 기간 계산하기 . from dateutil.relativedelta import relativedelta # 날짜 비교 함수 relativedelta를 사용하기 위해 라이브러리 import customer['calc_date'] = customer['end_date'] # 날짜 계산용 칼럼을 따로 복제 customer['calc_date'] = customer['calc_date'].fillna(pd.to_datetime('20190430')) # 결측치에는 일괄적으로 2019.04.30을 대입 customer['membership_period'] = 0 for i in range(len(customer)): delta = relativedelta(customer['calc_date'].iloc[i], customer['start_date'].iloc[i]) customer['membership_period'].iloc[i] = delta.years*12 + delta.months # 회원 기간을 월 단위로 계산 (몇 달 후에 탈퇴했나) customer.head() . |   | customer_id | name | class | gender | start_date | end_date | campaign_id | is_deleted | calc_date | membership_period | . | 0 | OA832399 | XXXX | C01 | F | 2015-05-01 | NaT | CA1 | 0 | 2019-04-30 | 47 | . | 1 | PL270116 | XXXXX | C01 | M | 2015-05-01 | NaT | CA1 | 0 | 2019-04-30 | 47 | . | 2 | OA974876 | XXXXX | C01 | M | 2015-05-01 | NaT | CA1 | 0 | 2019-04-30 | 47 | . | 3 | HD024127 | XXXXX | C01 | F | 2015-05-01 | NaT | CA1 | 0 | 2019-04-30 | 47 | . | 4 | HD661448 | XXXXX | C03 | F | 2015-05-01 | NaT | CA1 | 0 | 2019-04-30 | 47 | . +) relativedelta 사용법 살펴보기 . delta = relativedelta(pd.to_datetime('20190430'), pd.to_datetime('20170301')) print(delta) ## 몇년 몇개월 며칠 차이 나는지 저장되어 있음 print(delta.years) print(delta.months) print(delta.days) . relativedelta(years=+2, months=+1, days=+29) 2 1 29 . +) 더 간단한 기간 계산 . | 두 column이 동일한 datetime 타입을 가지고 있다면, 간단히 빼기(-)로 기간을 계산할 수 있다. | 그 결과는 000 days와 같이 일수 기준으로 나오며, .dt.days를 붙여주면 int 형태로 일수만 추출된다 | . # 위에서 relativedelta로 계산한 것과 달리, 'days' 기준의 회원 기간 계산 customer['membership_period'] = (customer['calc_date'] - customer['start_date']).dt.days customer.head() . |   | customer_id | name | class | gender | start_date | end_date | campaign_id | is_deleted | calc_date | membership_period | . | 0 | OA832399 | XXXX | C01 | F | 2015-05-01 | NaT | CA1 | 0 | 2019-04-30 | 1460 | . | 1 | PL270116 | XXXXX | C01 | M | 2015-05-01 | NaT | CA1 | 0 | 2019-04-30 | 1460 | . | 2 | OA974876 | XXXXX | C01 | M | 2015-05-01 | NaT | CA1 | 0 | 2019-04-30 | 1460 | . | 3 | HD024127 | XXXXX | C01 | F | 2015-05-01 | NaT | CA1 | 0 | 2019-04-30 | 1460 | . | 4 | HD661448 | XXXXX | C03 | F | 2015-05-01 | NaT | CA1 | 0 | 2019-04-30 | 1460 | . ",
    "url": "https://chaelist.github.io/docs/pandas/pandas_str_dt_con/#%EB%82%A0%EC%A7%9C%ED%98%95-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EB%B3%80%ED%99%98%EA%B0%80%EA%B3%B5",
    "relUrl": "/docs/pandas/pandas_str_dt_con/#날짜형-데이터-변환가공"
  },"171": {
    "doc": "Pandas str, dt, 조건문",
    "title": "조건문으로 데이터 가공",
    "content": "lambda 식 + apply() . : series.apply(lambda x: 식)과 같은 형식으로 사용. laptops_df = pd.read_csv('data/laptops.csv') ## 데이터 출처: codeit laptops_df.head() . |   | brand | model | ram | hd_type | hd_size | screen_size | price | processor_brand | processor_model | clock_speed | graphic_card_brand | graphic_card_size | os | weight | comments | . | 0 | Dell | Inspiron 15-3567 | 4 | hdd | 1024 | 15.6 | 40000 | intel | i5 | 2.5 | intel | nan | linux | 2.5 | nan | . | 1 | Apple | MacBook Air | 8 | ssd | 128 | 13.3 | 55499 | intel | i5 | 1.8 | intel | 2 | mac | 1.35 | nan | . | 2 | Apple | MacBook Air | 8 | ssd | 256 | 13.3 | 71500 | intel | i5 | 1.8 | intel | 2 | mac | 1.35 | nan | . | 3 | Apple | MacBook Pro | 8 | ssd | 128 | 13.3 | 96890 | intel | i5 | 2.3 | intel | 2 | mac | 3.02 | nan | . | 4 | Apple | MacBook Pro | 8 | ssd | 256 | 13.3 | 112666 | intel | i5 | 2.3 | intel | 2 | mac | 3.02 | nan | . | 각 줄의 ‘model’명의 길이를 세서 model_len이라는 칼럼으로 저장하기 . laptops_df['model_len'] = laptops_df['model'].apply(lambda x: len(x)) laptops_df[['model', 'model_len']].head() . |   | model | model_len | . | 0 | Inspiron 15-3567 | 16 | . | 1 | MacBook Air | 11 | . | 2 | MacBook Air | 11 | . | 3 | MacBook Pro | 11 | . | 4 | MacBook Pro | 11 | . | lambda 식에서 if-else 절 사용: 각 줄의 가격이 비싼지 아닌지 정의하는 새로운 칼럼 생성하기 . # price 칼럼의 평균 -- 아래에서 Expensive 여부 판단 기준으로 사용 laptops_df['price'].mean() . 64132.89820359281 . → 각 행의 ‘price’를 보고 64133 이상이면 Expensive, 아니면 Affordable이라고 정의하는 새로운 열 생성 . laptops_df['Expensive_Affordable'] = laptops_df['price'].apply(lambda x: 'Expensive' if x &gt; 64133 else 'Affordable') laptops_df[['price','Expensive_Affordable']].head() . |   | price | Expensive_Affordable | . | 0 | 40000 | Affordable | . | 1 | 55499 | Affordable | . | 2 | 71500 | Expensive | . | 3 | 96890 | Expensive | . | 4 | 112666 | Expensive | . | lambda 식에 함수를 받아서 사용: 각 줄의 가격을 세분화해 분류하는 새로운 칼럼 생성하기 . # price 칼럼의 분포 -- 아래에서 카테고리 판단 기준으로 사용 laptops_df['price'].describe() . count 167.000000 mean 64132.898204 std 42797.674010 min 13872.000000 25% 35457.500000 50% 47990.000000 75% 77494.500000 max 226000.000000 Name: price, dtype: float64 . → 각 행의 ‘price’를 보고 아래 get_category() 함수에 따른 분류를 적어주는 새로운 열 생성 . def get_category(price): if price &lt;= 35457: cat = 'cheap' elif price &lt;= 47990: cat = 'affordable' elif price &lt;= 77494: cat = 'expensive' else: cat = 'above budget' return cat laptops_df['price_cat'] = laptops_df['price'].apply(lambda x: get_category(x)) laptops_df[['price', 'price_cat']].head(10) . |   | price | price_cat | . | 0 | 40000 | affordable | . | 1 | 55499 | expensive | . | 2 | 71500 | expensive | . | 3 | 96890 | above budget | . | 4 | 112666 | above budget | . | 5 | 226000 | above budget | . | 6 | 158000 | above budget | . | 7 | 96990 | above budget | . | 8 | 33225 | cheap | . | 9 | 21990 | cheap | . | . lambda 식 + applymap() . : series.applymap(lambda x: 식)과 같은 형식으로 사용. | df.apply()의 경우 row / column basis로 작용하고, df.applymap()의 경우 element-wise로 작용한다는 차이가 있기 때문에, 각 element를 하나 하나 조건식으로 검사해서 변경하고자 하는 경우에는 applymap()을 사용해야 한다 | . df = pd.DataFrame({ 'A': [0, 1, 2, 3], 'B': [1, 0, 0, 0], 'C': [3, 0, 2, 0], 'D': [0, 0, 1, 4]}) df . |   | A | B | C | D | . | 0 | 0 | 1 | 3 | 0 | . | 1 | 1 | 0 | 0 | 0 | . | 2 | 2 | 0 | 2 | 1 | . | 3 | 3 | 0 | 0 | 4 | . → 유무만을 나타내기 위해, 0보다 큰 숫자는 모두 ‘O’로, 그 외는 공백으로 변경: . df.applymap(lambda x: 'O' if x &gt; 0 else '') . |   | A | B | C | D | . | 0 |   | O | O |   | . | 1 | O |   |   |   | . | 2 | O |   | O | O | . | 3 | O |   |   | O | . pd.where() . : series.where(series객체에 대한 조건문, 거짓 값에 대한 대체 값)의 형태로 사용 . df = pd.DataFrame({'a': [1, 2, 3, 4, 5], 'b': [10, 20, 30, 40, 50]}) df . |   | a | b | . | 0 | 1 | 10 | . | 1 | 2 | 20 | . | 2 | 3 | 30 | . | 3 | 4 | 40 | . | 4 | 5 | 50 | . | seriesA.where(sereiesA에 대한 조건, 값) . df['c'] = df['a'].where(df['a'] &lt; 3, 10) df . |   | a | b | c | . | 0 | 1 | 10 | 1 | . | 1 | 2 | 20 | 2 | . | 2 | 3 | 30 | 10 | . | 3 | 4 | 40 | 10 | . | 4 | 5 | 50 | 10 | . | seriesB.where(sereiesA에 대한 조건, 값) . # a열 중 3보다 작은 값(1,2)에는 b열의 값, 3이상인 값에 대해서는 100을 넣는다 df['d'] = df['b'].where(df['a'] &lt; 3, 100) df . |   | a | b | c | d | . | 0 | 1 | 10 | 1 | 10 | . | 1 | 2 | 20 | 2 | 20 | . | 2 | 3 | 30 | 10 | 100 | . | 3 | 4 | 40 | 10 | 100 | . | 4 | 5 | 50 | 10 | 100 | . | . np.where() 활용 . : np.where(조건, 조건이 부합할 때의 값, 아닐 때의 값)의 형태로 사용 . import numpy as np df = pd.DataFrame({'a':[1, 2, 3, 4, 5], 'b':[5, 4, 3, 2, 1]}) df['flag'] = np.where(df['a'] &lt; df['b'], 'b is bigger', 'a is bigger') df . |   | a | b | flag | . | 0 | 1 | 5 | b is bigger | . | 1 | 2 | 4 | b is bigger | . | 2 | 3 | 3 | a is bigger | . | 3 | 4 | 2 | a is bigger | . | 4 | 5 | 1 | a is bigger | . ",
    "url": "https://chaelist.github.io/docs/pandas/pandas_str_dt_con/#%EC%A1%B0%EA%B1%B4%EB%AC%B8%EC%9C%BC%EB%A1%9C-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EA%B0%80%EA%B3%B5",
    "relUrl": "/docs/pandas/pandas_str_dt_con/#조건문으로-데이터-가공"
  },"172": {
    "doc": "Plotly",
    "title": "Plotly",
    "content": ". | Bar graph | Scatter Plot . | Dot Plot | Bubble Chart | . | Line Chart | Pie Graph . | Treemap | Sunburst Chart | . | Statistical Charts . | Box Plot | Strip Plot | Violin Plot | Histogram | . | . *Plotly : 세련된 opensource interactive graphing library. *Plotly Express : easy-to-use, high-level visualization library (Plotly와 Plotly Express의 관계는 Matplotlib과 Seaborn의 관계와 유사) . | pip install plotly로 설치해서 사용 | . *Chart Studio : plotly로 시각화한 차트를 chart studio에 upload하면 쉽게 web 상에 embed할 수 있다 . | pip install chart_studio로 설치 | username과 api_key를 활용해 연결: import chart_studio username = '' # 자신의 username (plotly account를 만들어야 함) api_key = '' # 자신의 api key (settings &gt; regenerate key) chart_studio.tools.set_credentials_file(username=username, api_key=api_key) . | 작성한 차트를 upload하는 코드: chart_studio.plotly.plot(fig, filename = '파일이름', auto_open=True) # fig: 작성한 차트를 저장한 변수 ## 위 코드를 실행하면 새로운 window로 해당 차트의 링크가 열리고, notebook에도 link를 아래에 return해줌 . | . ",
    "url": "https://chaelist.github.io/docs/visualization/plotly/",
    "relUrl": "/docs/visualization/plotly/"
  },"173": {
    "doc": "Plotly",
    "title": "Bar graph",
    "content": ". | https://plotly.com/python/bar-charts/ | . # 필요한 library를 import import plotly.express as px import plotly.io as pio import pandas as pd import numpy as np pio.templates.default = \"plotly_white\" # default template을 지정 (지정하지 않으면 기본은 'plotly') . bills_df = px.data.tips() # plotly express에서 제공되는 기본 data 사용 bills_df.head() . |   | total_bill | tip | sex | smoker | day | time | size | . | 0 | 16.99 | 1.01 | Female | No | Sun | Dinner | 2 | . | 1 | 10.34 | 1.66 | Male | No | Sun | Dinner | 3 | . | 2 | 21.01 | 3.5 | Male | No | Sun | Dinner | 3 | . | 3 | 23.68 | 3.31 | Male | No | Sun | Dinner | 2 | . | 4 | 24.59 | 3.61 | Female | No | Sun | Dinner | 4 | . | daily average bills per sex . fig = px.bar(bills_df.groupby(['day', 'sex'])[['total_bill']].mean().reset_index(), x='day', y='total_bill', color='sex', title='Average Bills per Day', category_orders={'day':['Thur', 'Fri', 'Sat', 'Sun']}, color_discrete_sequence=px.colors.qualitative.Pastel) fig.show() . | 연속적이지 않은 color palette는 color_discrete_sequence 옵션으로 지정 | 보통 discrete_sequence에는 px.colors.qualitative에 있는 색을 넣지만, px.colors.sequential / px.colors.diverging / px.colors.cyclical에 있는 색을 넣어도 괜찮다 | color로 구분을 넣으면 default로 누적 그래프 형태로 시각화됨 | . | daily average bills per sex (2) . fig = px.bar(bills_df.groupby(['day', 'sex'])[['total_bill']].mean().reset_index(), x='day', y='total_bill', color='sex', height=400, title='Average Bills per Day', category_orders={'day':['Thur', 'Fri', 'Sat', 'Sun']}, color_discrete_sequence=px.colors.qualitative.Pastel, barmode='group') # 양 옆으로 놓이는 구조 # barmode='stack'라고 하면 누적 (dafault setting) fig.show() . | barmode='group' 옵션을 추가해주면 그룹별 값이 위아래 대신 양옆으로 배치됨 | height 옵션으로 크기를 지정해주는 것도 가능 (+. width 옵션도 가능) | . | daily total bills per sex . fig = px.bar(bills_df, x='day', y='total_bill', color='sex', title='Total Bills per Day', category_orders={'day':['Thur', 'Fri', 'Sat', 'Sun']}, color_discrete_sequence=px.colors.qualitative.Pastel) fig.show() . | seaborn에서는 전체 data를 넣어서 bar graph를 그리면 default로 평균값을 시각화해주는 반면, plotly express는 전체 data의 그대로 합산해서 시각화해준다 | . | category별로 영역을 나누어 그리기 . fig = px.bar(bills_df, x='day', y='total_bill', color='smoker', barmode='group', facet_col='sex', category_orders={'day':['Thur', 'Fri', 'Sat', 'Sun']}, color_discrete_sequence=px.colors.qualitative.Safe, template='plotly') fig.show() . | facet_col 옵션을 통해 특정 그룹별로 영역을 나눠서 시각화할 수 있다 | template 옵션을 통해 각 그래프별 template을 별도로 지정할 수도 있다 (원래 default는 ‘plotly’지만, 위에서 default template을 ‘plotly_white’로 지정해두었으므로, 해당 그래프에서는 ‘plotly’ template을 사용하기 위해 옵션을 별도로 지정) | . | . ",
    "url": "https://chaelist.github.io/docs/visualization/plotly/#bar-graph",
    "relUrl": "/docs/visualization/plotly/#bar-graph"
  },"174": {
    "doc": "Plotly",
    "title": "Scatter Plot",
    "content": ". | https://plotly.com/python/line-and-scatter/ | . iris_df = px.data.iris() iris_df.head() . |   | sepal_length | sepal_width | petal_length | petal_width | species | species_id | . | 0 | 5.1 | 3.5 | 1.4 | 0.2 | setosa | 1 | . | 1 | 4.9 | 3 | 1.4 | 0.2 | setosa | 1 | . | 2 | 4.7 | 3.2 | 1.3 | 0.2 | setosa | 1 | . | 3 | 4.6 | 3.1 | 1.5 | 0.2 | setosa | 1 | . | 4 | 5 | 3.6 | 1.4 | 0.2 | setosa | 1 | . | 기본 scatter plot . fig = px.scatter(iris_df, x='sepal_width', y='sepal_length', color='species', color_discrete_sequence=px.colors.qualitative.Safe) fig.show() . | trendline을 포함한 scatter plot . fig = px.scatter(bills_df, x='total_bill', y='tip', trendline='ols', color_discrete_sequence=px.colors.qualitative.Pastel1, trendline_color_override='gold') fig.show() . | trendline='ols' 옵션을 통해 선형회귀선을 함께 시각화 | . | . Dot Plot . gapminder_df = px.data.gapminder() gapminder_df.head() . |   | country | continent | year | lifeExp | pop | gdpPercap | iso_alpha | iso_num | . | 0 | Afghanistan | Asia | 1952 | 28.801 | 8425333 | 779.445 | AFG | 4 | . | 1 | Afghanistan | Asia | 1957 | 30.332 | 9240934 | 820.853 | AFG | 4 | . | 2 | Afghanistan | Asia | 1962 | 31.997 | 10267083 | 853.101 | AFG | 4 | . | 3 | Afghanistan | Asia | 1967 | 34.02 | 11537966 | 836.197 | AFG | 4 | . | 4 | Afghanistan | Asia | 1972 | 36.088 | 13079460 | 739.981 | AFG | 4 | . → scatter plot에 categorical axis를 넣어서 시각화 (dot plot) . fig = px.scatter(gapminder_df.query(\"continent=='Americas'\"), x='lifeExp', y='country', color='year', color_continuous_scale='Burgyl') fig.show() . | 연속적인 color palette는 color_continuous_scale 옵션으로 지정 | . Bubble Chart . : scatter plot에서 size option을 지정해 시각화하면 된다 . | 2007년의 gdpPercap과 liefExp의 관계를 시각화 (bubble size: population) . fig = px.scatter(gapminder_df.query(\"year == 2007\"), x='gdpPercap', y='lifeExp', size='pop', color='continent', hover_name='country') fig.show() . | hover_name 옵션을 통해 마우스를 hover할 때 우선적으로 보여지는 정보를 지정 | . | x값을 log 변환해 시각화 . fig = px.scatter(gapminder_df.query(\"year == 2007\"), x='gdpPercap', y='lifeExp', size='pop', color='continent', hover_name='country', log_x=True, size_max=60) fig.show() . | x값과 y값의 더 명확한 관계를 시각화하기 위해 log_x=True 옵션을 지정 | 더 bubble의 size를 구분하기 쉽도록 size_max=60 옵션으로 bubble size를 조정 | . | . ",
    "url": "https://chaelist.github.io/docs/visualization/plotly/#scatter-plot",
    "relUrl": "/docs/visualization/plotly/#scatter-plot"
  },"175": {
    "doc": "Plotly",
    "title": "Line Chart",
    "content": ". | https://plotly.com/python/line-charts/ | . fig = px.line(gapminder_df.query(\"continent == 'Oceania'\"), x='year', y='lifeExp', color='country', symbol='country', color_discrete_sequence=px.colors.qualitative.Pastel1) fig.show() . | symbol='country' 옵션을 통해 각 국가별 marker의 모양을 다르게 지정 | 각 그룹별 marker 모양을 구분하지 않고 모두 동일한 marker로 표시하고 싶으면 symbol 옵션 대신 markers=True 옵션을 추가해주면 된다 | . ",
    "url": "https://chaelist.github.io/docs/visualization/plotly/#line-chart",
    "relUrl": "/docs/visualization/plotly/#line-chart"
  },"176": {
    "doc": "Plotly",
    "title": "Pie Graph",
    "content": ". | https://plotly.com/python/pie-charts/ | . # 2007년 gapminder 수치 중, Asia 지역의 국가별 population 비중을 시각화 temp_df = gapminder_df.query(\"year == 2007\").query(\"continent == 'Asia'\") # population이 가장 많은 top 15 국가를 제외하고는 다 'Other countries'로 처리 temp_df.sort_values(by='pop', ascending=False, inplace=True) temp_df.iloc[15:, 0] = 'Other countries' fig = px.pie(temp_df, values='pop', names='country', color_discrete_sequence=px.colors.qualitative.Antique) fig.show() . Treemap . | https://plotly.com/python/treemaps/ | . temp_df = gapminder_df.query(\"year == 2007\") fig = px.treemap(temp_df, path=[px.Constant('world'), 'continent', 'country'], values='pop', color='lifeExp', color_continuous_scale='RdBu', color_continuous_midpoint=np.average(temp_df['lifeExp'], weights=temp_df['pop'])) fig.update_layout(margin = dict(t=50, l=25, r=25, b=25)) fig.show() . Sunburst Chart . | https://plotly.com/python/sunburst-charts/ | . fig = px.sunburst(bills_df, path=['day', 'time', 'sex'], values='tip', color='time', color_discrete_sequence=px.colors.qualitative.Pastel) fig.show() . ",
    "url": "https://chaelist.github.io/docs/visualization/plotly/#pie-graph",
    "relUrl": "/docs/visualization/plotly/#pie-graph"
  },"177": {
    "doc": "Plotly",
    "title": "Statistical Charts",
    "content": "Box Plot . | https://plotly.com/python/box-plots/ | . | 기본 box plot . fig = px.box(bills_df, x='sex', y='total_bill', color='smoker', color_discrete_sequence=px.colors.qualitative.Pastel) fig.show() . | 각 point를 함께 시각화 . fig = px.box(bills_df, x='sex', y='total_bill', color='smoker', points='all', color_discrete_sequence=px.colors.qualitative.Pastel) fig.show() . | points='all' 옵션을 지정하면 box plot 옆에 모든 point를 함께 시각화해줌 (default: points='outliers') | . | . Strip Plot . | https://plotly.com/python/strip-charts/ | . | 기본 strip plot . fig = px.strip(bills_df, x='sex', y='total_bill', color='smoker', color_discrete_sequence=px.colors.qualitative.Pastel) fig.show() . | category별로 세분화해 시각화 . fig = px.strip(bills_df, x='total_bill', y='time', color='sex', facet_col='day', category_orders={'day':['Thur', 'Fri', 'Sat', 'Sun']}, color_discrete_sequence=px.colors.qualitative.Safe, template='plotly') fig.show() . | . Violin Plot . | https://plotly.com/python/violin/ | . | 기본 violin plot . fig = px.violin(bills_df, x='sex', y='total_bill', color='smoker', color_discrete_sequence=px.colors.qualitative.Pastel) fig.show() . | box, points 옵션 지정 . fig = px.violin(bills_df, y='total_bill', color='sex', box=True, points='all', color_discrete_sequence=px.colors.qualitative.Pastel) fig.show() . | points='all' 옵션을 지정하면 violin plot 옆에 모든 point를 함께 시각화해줌 | box=True 옵션을 지정하면 violin plot 안에 box plot를 함께 시각화해줌 | . | . Histogram . | https://plotly.com/python/histograms/ | . | 기본 histogram . fig = px.histogram(bills_df, x='total_bill', nbins=10) fig.show() . | nbins 옵션으로 number of bins를 조절 | . | categorical data를 넣으면 count plot처럼 작용 . fig = px.histogram(bills_df, x='day', category_orders={'day':['Thur', 'Fri', 'Sat', 'Sun']}) fig.show() . | x값에 categorical data를 넣으면 각 카테고리별 수를 세어서 시각화해줌 (count plot) | . | histogram과 각 값의 분포를 함께 시각화 . fig = px.histogram(bills_df, x='total_bill', y='tip', color='sex', marginal='box', color_discrete_sequence=px.colors.qualitative.Pastel) fig.show() . | marginal='box' 옵션을 추가하면 각 값의 분포를 box plot으로 함께 시각화해줌 (marginal: box, violin, rug 중에 선택 가능) | 참고: Combined statistical representations | . | . +) 참고용 링크들 . | color_discrete_sequence | color_continuous_scale | colors in plotly express – plotly.express.colors.sequential.swatches()와 같은 코드로 색 종류 확인 가능 | adjusting size | hovermode and hover labels | templates | image export / html export | plotly.express python api reference | . ",
    "url": "https://chaelist.github.io/docs/visualization/plotly/#statistical-charts",
    "relUrl": "/docs/visualization/plotly/#statistical-charts"
  },"178": {
    "doc": "데이터 전처리",
    "title": "데이터 전처리",
    "content": ". | Feature Scaling . | Min-Max Normalization (최소-최대 정규화) | Standardization (표준화) | . | 카테고리 변수 인코딩 . | One-hot Encoding | . | . ",
    "url": "https://chaelist.github.io/docs/ml_advanced/preprocessing/",
    "relUrl": "/docs/ml_advanced/preprocessing/"
  },"179": {
    "doc": "데이터 전처리",
    "title": "Feature Scaling",
    "content": ": 입력 변수(feature)의 크기를 조정(scale)해서 일정 범위 내에 떨어지도록 바꿔주는 것. | 서로 다른 단위와 범위를 갖는 입력변수들을 활용해서 machine learning을 할 때, 더 큰 값들을 갖는 변수가 갖는 영향이 과다하게 나타나는 것을 막으려면 Feature Scaling을 해줘야 한다 . | ex) 키와 몸무게 변수를 사용할 때, feature scaling을 하지 않으면 키 변수의 중요성이 과다하게 나타날 수 있다 | . | Feature Scaling은 경사하강법을 더 빨리 할 수 있도록 해준다 . | 더 빨리 최소점을 찾을 수 있기 때문에 Maching Learning에서의 모델 학습이 빨라진다 | . | . Min-Max Normalization (최소-최대 정규화) . : 가장 흔한 정규화 방법. 데이터의 크기를 0과 1사이로 바꿔주는 것. (최솟값 → 0, 최댓값 → 1) . | $ X_{new} = \\dfrac{X_{old} - X_{min}}{X_{max} - X_{min}} $ | 새로운 값 = (원래 값 - 최솟값) / (최댓값 - 최솟값) | . import pandas as pd import numpy as np from sklearn.datasets import load_boston # boston 집값 데이터 from sklearn import preprocessing . ⁣1. boston dataset을 불러옴 . boston_dataset = load_boston() boston_data = pd.DataFrame(boston_dataset.data, columns=boston_dataset.feature_names) boston_data.head() . |   | CRIM | ZN | INDUS | CHAS | NOX | RM | AGE | DIS | RAD | TAX | PTRATIO | B | LSTAT | . | 0 | 0.00632 | 18 | 2.31 | 0 | 0.538 | 6.575 | 65.2 | 4.09 | 1 | 296 | 15.3 | 396.9 | 4.98 | . | 1 | 0.02731 | 0 | 7.07 | 0 | 0.469 | 6.421 | 78.9 | 4.9671 | 2 | 242 | 17.8 | 396.9 | 9.14 | . | 2 | 0.02729 | 0 | 7.07 | 0 | 0.469 | 7.185 | 61.1 | 4.9671 | 2 | 242 | 17.8 | 392.83 | 4.03 | . | 3 | 0.03237 | 0 | 2.18 | 0 | 0.458 | 6.998 | 45.8 | 6.0622 | 3 | 222 | 18.7 | 394.63 | 2.94 | . | 4 | 0.06905 | 0 | 2.18 | 0 | 0.458 | 7.147 | 54.2 | 6.0622 | 3 | 222 | 18.7 | 396.9 | 5.33 | . ⁣2. describe()로 각 변수의 분포를 확인: max값과 min값이 제각각임을 확인 가능 . boston_data.describe() . |   | CRIM | ZN | INDUS | CHAS | NOX | RM | AGE | DIS | RAD | TAX | PTRATIO | B | LSTAT | . | count | 506 | 506 | 506 | 506 | 506 | 506 | 506 | 506 | 506 | 506 | 506 | 506 | 506 | . | mean | 3.61352 | 11.3636 | 11.1368 | 0.06917 | 0.554695 | 6.28463 | 68.5749 | 3.79504 | 9.54941 | 408.237 | 18.4555 | 356.674 | 12.6531 | . | std | 8.60155 | 23.3225 | 6.86035 | 0.253994 | 0.115878 | 0.702617 | 28.1489 | 2.10571 | 8.70726 | 168.537 | 2.16495 | 91.2949 | 7.14106 | . | min | 0.00632 | 0 | 0.46 | 0 | 0.385 | 3.561 | 2.9 | 1.1296 | 1 | 187 | 12.6 | 0.32 | 1.73 | . | 25% | 0.082045 | 0 | 5.19 | 0 | 0.449 | 5.8855 | 45.025 | 2.10018 | 4 | 279 | 17.4 | 375.377 | 6.95 | . | 50% | 0.25651 | 0 | 9.69 | 0 | 0.538 | 6.2085 | 77.5 | 3.20745 | 5 | 330 | 19.05 | 391.44 | 11.36 | . | 75% | 3.67708 | 12.5 | 18.1 | 0 | 0.624 | 6.6235 | 94.075 | 5.18843 | 24 | 666 | 20.2 | 396.225 | 16.955 | . | max | 88.9762 | 100 | 27.74 | 1 | 0.871 | 8.78 | 100 | 12.1265 | 24 | 711 | 22 | 396.9 | 37.97 | . ⁣3. MinMaxScaler()로 normalize: . # Min-Max Normalization scaler = preprocessing.MinMaxScaler() normalized_data = scaler.fit_transform(boston_data) # df로 정리해서 확인 normalized_df = pd.DataFrame(normalized_data, columns=boston_dataset.feature_names) normalized_df.head() ## 다 0~1 사이의 값으로 바뀌었음 . |   | CRIM | ZN | INDUS | CHAS | NOX | RM | AGE | DIS | RAD | TAX | PTRATIO | B | LSTAT | . | 0 | 0 | 0.18 | 0.0678152 | 0 | 0.314815 | 0.577505 | 0.641607 | 0.269203 | 0 | 0.208015 | 0.287234 | 1 | 0.0896799 | . | 1 | 0.000235923 | 0 | 0.242302 | 0 | 0.17284 | 0.547998 | 0.782698 | 0.348962 | 0.0434783 | 0.104962 | 0.553191 | 1 | 0.20447 | . | 2 | 0.000235698 | 0 | 0.242302 | 0 | 0.17284 | 0.694386 | 0.599382 | 0.348962 | 0.0434783 | 0.104962 | 0.553191 | 0.989737 | 0.0634658 | . | 3 | 0.000292796 | 0 | 0.0630499 | 0 | 0.150206 | 0.658555 | 0.441813 | 0.448545 | 0.0869565 | 0.0667939 | 0.648936 | 0.994276 | 0.0333885 | . | 4 | 0.00070507 | 0 | 0.0630499 | 0 | 0.150206 | 0.687105 | 0.528321 | 0.448545 | 0.0869565 | 0.0667939 | 0.648936 | 1 | 0.0993377 | . ⁣4. 잘 normalize되었는지 min, max값을 확인 . normalized_df.describe() ## 모든 열이 min=0, max=1로 잘 바뀜 . |   | CRIM | ZN | INDUS | CHAS | NOX | RM | AGE | DIS | RAD | TAX | PTRATIO | B | LSTAT | . | count | 506 | 506 | 506 | 506 | 506 | 506 | 506 | 506 | 506 | 506 | 506 | 506 | 506 | . | mean | 0.0405441 | 0.113636 | 0.391378 | 0.06917 | 0.349167 | 0.521869 | 0.676364 | 0.242381 | 0.371713 | 0.422208 | 0.622929 | 0.898568 | 0.301409 | . | std | 0.0966793 | 0.233225 | 0.251479 | 0.253994 | 0.238431 | 0.134627 | 0.289896 | 0.191482 | 0.378576 | 0.321636 | 0.230313 | 0.230205 | 0.197049 | . | min | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . | 25% | 0.000851131 | 0 | 0.173387 | 0 | 0.131687 | 0.445392 | 0.433831 | 0.088259 | 0.130435 | 0.175573 | 0.510638 | 0.94573 | 0.14404 | . | 50% | 0.00281208 | 0 | 0.338343 | 0 | 0.314815 | 0.507281 | 0.76828 | 0.188949 | 0.173913 | 0.272901 | 0.68617 | 0.986232 | 0.265728 | . | 75% | 0.0412585 | 0.125 | 0.646628 | 0 | 0.49177 | 0.586798 | 0.93898 | 0.369088 | 1 | 0.914122 | 0.808511 | 0.998298 | 0.420116 | . | max | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | . Standardization (표준화) . : 데이터의 평균을 0, 표준편차를 1로 맞춰주는 것 . | $ X_{new} = \\dfrac{X_{old} - \\bar{X}}{\\sigma} $ | 새로운 값 = (원래 값 - 평균) / 표준편차 | 표준화를 한 데이터를 보통 z-score라고 한다 - ex) z-score가 1.5면 ‘평균값보다 1.5 표준편차만큼 크다’는 뜻 | . # 소수점 2번째 자리까지만 출력되도록 설정 pd.set_option('display.float_format', lambda x: '%.2f' % x) # Standardization scaler = preprocessing.StandardScaler() # MinMaxScaler와 비교했을 때, 이 한 줄만 바뀜. standardized_data = scaler.fit_transform(boston_data) # 위에서 사용했던 boston_data df를 그대로 사용 standardized_df = pd.DataFrame(standardized_data, columns=boston_dataset.feature_names) standardized_df.describe() ## 모든 열이 평균은 0, 표준편차는 1이 되도록 잘 표준화됨 . |   | CRIM | ZN | INDUS | CHAS | NOX | RM | AGE | DIS | RAD | TAX | PTRATIO | B | LSTAT | . | count | 506.00 | 506.00 | 506.00 | 506.00 | 506.00 | 506.00 | 506.00 | 506.00 | 506.00 | 506.00 | 506.00 | 506.00 | 506.00 | . | mean | -0.00 | -0.00 | -0.00 | -0.00 | -0.00 | -0.00 | -0.00 | -0.00 | -0.00 | -0.00 | -0.00 | -0.00 | -0.00 | . | std | 1.00 | 1.00 | 1.00 | 1.00 | 1.00 | 1.00 | 1.00 | 1.00 | 1.00 | 1.00 | 1.00 | 1.00 | 1.00 | . | min | -0.42 | -0.49 | -1.56 | -0.27 | -1.46588 | -3.88 | -2.34 | -1.27 | -0.98 | -1.31 | -2.71 | -3.91 | -1.53 | . | 25% | -0.41 | -0.49 | -0.87 | -0.27 | -0.91 | -0.57 | -0.84 | -0.81 | -0.64 | -0.77 | -0.49 | 0.21 | -0.80 | . | 50% | -0.39 | -0.49 | -0.21 | -0.27 | -0.14 | -0.11 | 0.32 | -0.28 | -0.52 | -0.46 | 0.27 | 0.38 | -0.18 | . | 75% | 0.01 | 0.05 | 1.02 | -0.27 | 0.60 | 0.48 | 0.91 | 0.66 | 1.66 | 1.53 | 0.81 | 0.43 | 0.60 | . | max | 9.93 | 3.80 | 2.42 | 3.66 | 2.73 | 3.56 | 1.12 | 3.96 | 1.66 | 1.80 | 1.64 | 0.44 | 3.55 | . ",
    "url": "https://chaelist.github.io/docs/ml_advanced/preprocessing/#feature-scaling",
    "relUrl": "/docs/ml_advanced/preprocessing/#feature-scaling"
  },"180": {
    "doc": "데이터 전처리",
    "title": "카테고리 변수 인코딩",
    "content": ". | 문자열로 구성된 카테고리 변수는 숫자형으로 변환해서 표기해야 학습할 수 있다 | Label Encoding과 One-hot Encoding 방식이 가능 | *Label Encoding: 고양이 → 1, 강아지 → 2, 너구리 → 3 이런식으로 각 카테고리를 숫자 값으로 변환해주는 방식. | 하지만 이렇게 하면 실제로는 크고 작음이 없는 데이터인데 ML 알고리즘에서 1 &lt; 2 이런 식으로 숫자의 크고 작음에 따라 중요도가 존재하는 것으로 인식될 가능성이 있으므로 좋은 방법은 아니다. | . | . One-hot Encoding . | Label Encoding의 문제점을 해결해주는 인코딩 방식. | 0과 1로 이루어진 벡터로 각 변수를 표현해준다 | ex) 고양이 → 1 0 0, 강아지 → 0 1 0, 너구리 → 0 0 1 이런식으로 나타냄 | sklearn.preprocessing의 OneHotEncoder를 사용해도 되고, pandas의 get_dummies() 함수를 이용해도 쉽게 활용할 수 있다 | . (출처: towardsdatascience.com) .   . import pandas as pd titanic_df = pd.read_csv('data/titanic.csv') ## 데이터 출처: kaggle titanic_df.head() . |   | PassengerId | Survived | Pclass | Name | Sex | Age | SibSp | Parch | Ticket | Fare | Cabin | Embarked | . | 0 | 1 | 0 | 3 | Braund, Mr. Owen Harris | male | 22 | 1 | 0 | A/5 21171 | 7.25 | nan | S | . | 1 | 2 | 1 | 1 | Cumings, Mrs. John Bradley (Florence Briggs Thayer) | female | 38 | 1 | 0 | PC 17599 | 71.2833 | C85 | C | . | 2 | 3 | 1 | 3 | Heikkinen, Miss. Laina | female | 26 | 0 | 0 | STON/O2. 3101282 | 7.925 | nan | S | . | 3 | 4 | 1 | 1 | Futrelle, Mrs. Jacques Heath (Lily May Peel) | female | 35 | 1 | 0 | 113803 | 53.1 | C123 | S | . | 4 | 5 | 0 | 3 | Allen, Mr. William Henry | male | 35 | 0 | 0 | 373450 | 8.05 | nan | S | .   . ⁣1. 사용할 열만 따로 가져와서 더미변수화 . titanic_sex_embarked = titanic_df[['Sex', 'Embarked']] ## 사용할 열만 따로 가져옴 titanic_sex_embarked.head() . |   | Sex | Embarked | . | 0 | male | S | . | 1 | female | C | . | 2 | female | S | . | 3 | female | S | . | 4 | male | S | .   . ## pandas의 get_dummies 함수를 사용하면 쉽게 더미변수를 만들 수 있다 one_hot_encoded_df = pd.get_dummies(titanic_sex_embarked) one_hot_encoded_df.head() . |   | Sex_female | Sex_male | Embarked_C | Embarked_Q | Embarked_S | . | 0 | 0 | 1 | 0 | 0 | 1 | . | 1 | 1 | 0 | 1 | 0 | 0 | . | 2 | 1 | 0 | 0 | 0 | 1 | . | 3 | 1 | 0 | 0 | 0 | 1 | . | 4 | 0 | 1 | 0 | 0 | 1 | .   . ⁣2. one-hot encoding할 열을 따로 저장하지 않고, 기존 df에서 특정 부분만 encoding . # columns=[]로 더미변수화해줄 열을 정해주면 된다 one_hot_encoded_df = pd.get_dummies(data=titanic_df, columns=['Sex', 'Embarked']) one_hot_encoded_df.head() . |   | PassengerId | Survived | Pclass | Name | Age | SibSp | Parch | Ticket | Fare | Cabin | Sex_female | Sex_male | Embarked_C | Embarked_Q | Embarked_S | . | 0 | 1 | 0 | 3 | Braund, Mr. Owen Harris | 22 | 1 | 0 | A/5 21171 | 7.25 | nan | 0 | 1 | 0 | 0 | 1 | . | 1 | 2 | 1 | 1 | Cumings, Mrs. John Bradley (Florence Briggs Thayer) | 38 | 1 | 0 | PC 17599 | 71.2833 | C85 | 1 | 0 | 1 | 0 | 0 | . | 2 | 3 | 1 | 3 | Heikkinen, Miss. Laina | 26 | 0 | 0 | STON/O2. 3101282 | 7.925 | nan | 1 | 0 | 0 | 0 | 1 | . | 3 | 4 | 1 | 1 | Futrelle, Mrs. Jacques Heath (Lily May Peel) | 35 | 1 | 0 | 113803 | 53.1 | C123 | 1 | 0 | 0 | 0 | 1 | . | 4 | 5 | 0 | 3 | Allen, Mr. William Henry | 35 | 0 | 0 | 373450 | 8.05 | nan | 0 | 1 | 0 | 0 | 1 | . ",
    "url": "https://chaelist.github.io/docs/ml_advanced/preprocessing/#%EC%B9%B4%ED%85%8C%EA%B3%A0%EB%A6%AC-%EB%B3%80%EC%88%98-%EC%9D%B8%EC%BD%94%EB%94%A9",
    "relUrl": "/docs/ml_advanced/preprocessing/#카테고리-변수-인코딩"
  },"181": {
    "doc": "Python 기초",
    "title": "Python 기초",
    "content": " ",
    "url": "https://chaelist.github.io/docs/python_basics",
    "relUrl": "/docs/python_basics"
  },"182": {
    "doc": "Regular Expressions",
    "title": "Regular Expressions",
    "content": ". | 정규 표현식(Regular Expressions) . | [ ]: 문자 클래스(character class) | 자주 사용하는 문자 클래스 | 각 표현식의 의미 | . | re 모듈 . | re.search() | re.findall() | re.sub() | re.compile() | . | 추가 활용 Tip | . ",
    "url": "https://chaelist.github.io/docs/data_handling/regular_expressions/",
    "relUrl": "/docs/data_handling/regular_expressions/"
  },"183": {
    "doc": "Regular Expressions",
    "title": "정규 표현식(Regular Expressions)",
    "content": ": 메타 문자(meta characters)를 적절히 활용해 복잡한 문자열을 처리하는 기법 . | 메타 문자: . ^ $ * + ? { } [ ] \\ | ( ) | . [ ]: 문자 클래스(character class) . : ”[ ] 사이의 문자들과 매치”라는 의미 . | [abc]: “a, b, c 중 한 개의 문자와 매치”라는 의미 . | “a”는 정규식과 일치하는 문자인 “a”가 있으므로 매치되는 부분이 존재 | “before”는 정규식과 일치하는 문자인 “b”가 있으므로 매치되는 부분이 존재 | “dude”는 정규식과 일치하는 문자 “a”, “b”, “c” 중 어느 하나도 포함하지 않으므로 매치되지 않음 | . | [ ] 안의 두 문자 사이에 하이픈(-)을 이용하면 두 문자 사이의 범위(from - to)를 의미 . | [a-z]: a~z의 모든 알파벳 | [0-5]: 012345 | [a-zA-Z]: 알파벳 모두 | [0-9]: 숫자 모두 | . | [ ] 안에 ^를 넣으면, 반대(not)의 의미가 됨 . | [^0-9]: 숫자가 아닌 문자만 매치됨 | . | . 자주 사용하는 문자 클래스 . : 아래와 같이 자주 사용하는 정규식은 별도의 표기법으로 표현이 가능하다 . | \\d - 숫자와 매치. [0-9]와 동일 | \\D - 숫자가 아닌 것과 매치. [^0-9]와 동일 | \\s - whitespace character와 매치, [ \\t\\n\\r\\f\\v]와 동일 (맨 앞의 빈 칸은 공백(space)를 의미) | \\S - non-whitespace character과 매치, [^ \\t\\n\\r\\f\\v]와 동일 | \\w - alphanumeric(문자+숫자) &amp; underscore(_)와 매치, [a-zA-Z0-9_]와 동일 | \\W - alphanumeric(문자+숫자) &amp; underscore(_)가 아닌 것과 매치, [^a-zA-Z0-9_]와 동일 | . (※ 각각, 대문자로 사용된 것은 소문자의 반대) . 각 표현식의 의미 . | Dot(.): 줄바꿈 문자인 \\n을 제외한 모든 문자와 매치됨을 의미 . | a.b: “a + 모든 문자 + b”라는 뜻 . | “aab”, “a0b” 모두 a.b와 매치된다 | “abc”는 a와 b 사이에 어떤 문자도 없으므로 매치되지 않는다 | . | cf) a[.]b: “a + . + b”라는 뜻. “a.b”와만 매치된다 | . | 반복(*): 0번 이상 반복된다는 의미 . | ca*t: * 바로 앞의 문자 ‘a’가 0번 이상 반복된다는 뜻 (a는 없어도 됨) . | “ct”, “cat”, “caaat” 모두 ca*t와 매치된다 (0번도 가능) | . | . | 반복(+): 1번 이상 반복된다는 의미 . | ca+t: + 바로 앞의 문자 ‘a’가 1번 이상 반복된다는 뜻 (무조건 a가 있어야 함) . | “cat”, “caaat”은 ca*t와 매치, 하지만 “ct”처럼 a가 한 번도 없는 것은 매치되지 않는다 | . | . | {m,n}: m~n번 반복된다는 의미 (반복 횟수를 제한) . | ca{2,5}t: {m,n} 바로 앞의 문자 a가 2번~5번 반복된다는 뜻 . | “caat”, “caaat”, “caaaat”, “caaaaat” 모두 ca{2,5}t와 매칭된다 | . | . | ?: 있어도 되고 없어도 된다는 의미 (0번 or 1번 있으면 됨) . | ab?c: “a + b(있어도 되고 없어도 됨) + c”라는 뜻 . | “abc”, “ac” 모두 매치된다 | “abbc” 등 b가 2번 이상이면 매치되지 않는다 | . | . | ^: Matches the beginning of a line . | ^X라고 하면 X로 시작하는 아이들만 매치됨 . | “Xylophone”은 ^X와 매칭된다 (case sensitive하므로 반드시 대문자 X여야 매칭) | . | . | $: $: Matches the end of a line . | X$라고 하면 X로 끝나는 아이들만 매치됨 . | “AdEX”는 X$와 매칭된다 | . | . | ( ): Indicates where string extraction is to start &amp; to end . | ^From (\\S+@\\S+)라고 하면, ‘From’으로 시작하는 \\S+@\\S+들을 찾아주되, ( ) 안의 부분만 extract. | “From chaelist@github.com”이라는 string이 있다면, 이 중 “chaelist@github.com”만 extract. | . | . | . ",
    "url": "https://chaelist.github.io/docs/data_handling/regular_expressions/#%EC%A0%95%EA%B7%9C-%ED%91%9C%ED%98%84%EC%8B%9Dregular-expressions",
    "relUrl": "/docs/data_handling/regular_expressions/#정규-표현식regular-expressions"
  },"184": {
    "doc": "Regular Expressions",
    "title": "re 모듈",
    "content": ": 파이썬에서 정규 표현식을 지원하는 re(regular expression) 모듈 . re.search() . : 특정 string이 해당 regular expression과 매칭되는지 여부에 따라 True/False를 return . | if문과 함께 사용해서 원하는 string들만 읽어올 때 사용하면 유용함 | . import re # import해줘야 사용 가능 lines = ['From chaelist@github.com', 'hi', 'From me', 'nice to meet you', 'From CYC'] for line in lines: if re.search('^From', line): ## if line.starswith('From'): 과 동일 print(line) . From chaelist@github.com From me From CYC . re.findall() . : 해당 regular expression에 매칭되는 portions of string을 extract. | 매치되는 부분을 모두 리스트로 저장해 return | . (※ re.search() returns a True/False, re.findall() returns the matching strings) . x = 'My 2 favorite numbers are 19 and 42' y = re.findall('[0-9]+', x) # 숫자(0-9)가 1번 이상 반복되는 부분을 모두 찾으므로, [2, 19, 42]가 return됨 print(y) z = re.findall('[AEIOU]+', x) # A, E, I, O, U 중 하나가 1번 이상 반복되는 부분을 찾는데, 없으므로 빈 리스트 return print(z) . ['2', '19', '42'] [] . re.sub() . : 패턴에 일치되는 문자열을 다른 문자열로 바꿔준다 . | re.sub(pattern, repl, string)의 형태로 사용 | . # re.sub() 간단한 버전 print(re.sub('\\d{4}', 'XXXX', '010-1256-9999')) . 010-XXXX-XXXX . +) count 추가: . # re.sub(pattern, repl, string, count) print(re.sub(pattern='Hello', repl='Bye', count=2, string='Hello, Hello, Hello Everybody.')) ## 일치하는 문자열이 3개이지만 count=2로 한정되어 있으면 딱 2개까지만 변환됨 . Bye, Bye, Hello Everybody. +) re.subn(): re.sub()와 매우 유사하지만, 치환된 문자열과 함께 치환된 개수도 return해준다 . print(re.subn('\\d{4}', 'XXXX', '010-1234-5678')) ## 치환 결과와 치환된 개수를 element로 하는 tuple 반환 . ('010-XXXX-XXXX', 2) . re.compile() . : 특정 정규표현식을 컴파일해두고 사용하는 방식 . x = 'My 2 favorite numbers are 19 and 42' y = re.findall('[0-9]+', x) print(y) p = re.compile('[0-9]+') # 이렇게 re.compile을 통해 특정 정규표현식을 저장해두고, m = p.findall(x) # 여기에 .findall('문자열')을 해줘도 re.findall('정규표현식', '문자열')과 동일. print(m) . ['2', '19', '42'] ['2', '19', '42'] . ※ 결론: 아래 1번과 2번은 완전히 동일한 결과를 낸다 . # 1번 result = re.findall('정규표현식', '문자열') # 2번 p = re.compile('정규표현식') result = p.findall('문자열') . ",
    "url": "https://chaelist.github.io/docs/data_handling/regular_expressions/#re-%EB%AA%A8%EB%93%88",
    "relUrl": "/docs/data_handling/regular_expressions/#re-모듈"
  },"185": {
    "doc": "Regular Expressions",
    "title": "추가 활용 Tip",
    "content": ". | ( ) 사용해서 더 precise하게 찾기 x = 'From stephan.marquard@uct.ac.za Sat Jan 5 09:14:26 2008' y = re.findall('^From (\\S+@\\S+)', x) ## From으로 시작(^) &amp; space &amp; 1개 이상(+)의 Non-whitespace character(\\S) &amp; @ &amp; 1개 이상(+)의 Non-whitespace character(\\S) ## 이 중, () 안에 있는 부분만 추출한다. ('From '는 추출X) print(y) . ['stephan.marquard@uct.ac.za'] . | 주의할 점: Greedy Matching . | The repeat characters (* and +) push outward in both directions (greedy) to match the largest possible string (*나 + 같은 ‘반복’ 문자를 쓰면, 가능한 가장 크게 매칭되려는 경향이 있다) | +?, *? 이런 식으로 ?를 뒤에 붙여주면 non-greedy matching이 가능 | . *Greedy Matching 예시 . x = 'From: Using the : character' y = re.findall('^F.+:', x) print(y) # From:도 매칭되고, 이를 포함하는 From: Using the :도 매칭될 때, 범위가 더 큰 후자로 매칭되게 됨. ['From: Using the :'] . *Non-Greedy Matching 예시 . x = 'From: Using the : character' y = re.findall('^F.+?:', x) print(y) # +?로 해주면 non-greedy한 매칭방식이므로, 가장 빨리 찾아지는 From:을 return하고 끝내게 됨 . ['From:'] . | Escape Character (예외 문자) . | 정규식에서 쓰는 문자인데 그냥 실제 그 모양 그대로의 의미를 담고 싶으면, \\를 앞에 붙여주면 된다 | ex) $(dollar sign) 그 자체를 써야 한다면, \\$ 이렇게 표시. | . x = 'We just received $10.00 for cookies.' y = re.findall('\\$[0-9]+', x) # \\$: 실제 $ 사인과 매칭되는 것을 찾으라는 의미 print(y) . ['$10'] . | 정규표현식을 테스트할 수 있는 사이트: https://regexr.com/ | . ",
    "url": "https://chaelist.github.io/docs/data_handling/regular_expressions/#%EC%B6%94%EA%B0%80-%ED%99%9C%EC%9A%A9-tip",
    "relUrl": "/docs/data_handling/regular_expressions/#추가-활용-tip"
  },"186": {
    "doc": "Regularization",
    "title": "Regularization",
    "content": ". | 편향과 분산 . | 과적합과 과소적합 | Overfitting 방지하기 | . | Overfitting 문제 체험 . | 6차항 모델 준비 | 학습 &amp; 결과 확인 | . | Regularization (가중치 규제) . | L1 Regularization | L2 Regularization | Lasso Regression 구현 | . | . ",
    "url": "https://chaelist.github.io/docs/ml_advanced/regularization/",
    "relUrl": "/docs/ml_advanced/regularization/"
  },"187": {
    "doc": "Regularization",
    "title": "편향과 분산",
    "content": ". | 편향(Bias) . | 모델이 너무 간단해서 데이터의 관계를 잘 학습하지 못하는 경우, 모델의 편향(bias)이 높다고 함 | 높은 차항의 회귀로 training 데이터에 거의 완벽히 맞춘 모델은 편향이 낮은 모델이라고 할 수 있음 | . | 분산(Variance) . | 모델이 얼마나 일관된 성능을 보여주는지를 분산(variance)라고 함 | 다양한 데이터셋 간에 성능 차이가 많이 나면 분산이 높다고 하고, 성능 차이가 별로 없으면 분산이 낮다고 함 | 직선 모델은 어떤 데이터셋에 적용해도 성능이 비슷하게 나옴 (= 분산이 작음) vs 복잡한 곡선 모델은 데이터셋에 따라 성능의 편차가 큼 (= 분산이 큼) | . | . 과적합과 과소적합 . | 과소적합(Underfit) . | 편향이 높고 분산이 낮은 모델을 underfit되었다고 함 | 너무 단순한 모델이라 관계를 제대로 학습하지 못하는 경우. | 대신 모델이 간단하기에 어떤 데이터에 적용해도 일관된 성능을 보임 | . | 과적합(Overfit) . | 편향이 낮고 분산이 높은 모델을 overfit되었다고 함 | training 데이터의 패턴을 학습하는 게 아니라 거의 데이터 자체를 외워버리는 수준으로 모델을 training 데이터에 거의 완벽히 맞추는 경우. | training 데이터에 대한 성능은 아주 높지만, 처음 보는 test 데이터에 대한 성능은 많이 떨어진다 | . | . *편향-분산 트레이드오프(Bias-Variance Tradeoff) : 일반적으로, 편향과 분산은 하나가 줄어들면 다른 하나는 늘어나는 tradeoff 관계 ※ 과소적합과 과적합 사이의 적당한 밸런스를 찾아내는 게 중요하다! . Overfitting 방지하기 . | 독립변수 추가/제거: 관계가 있는 애들은 추가, 관계가 없는 애들은 제거 . | 보통, 학습데이터가 아주 많을수록 모델의 설명력이 좋아짐. 하지만 y와 상관없는 독립변수가 많이 들어있으면 새로운 data를 잘 설명하는 데 도움이 안됨 | . | 독립변수와 종속변수의 관계를 잘 파악 (비선형관계 등을 잘 고려해서 반영) | parameter에 penalty 주기 = Regularization | . ",
    "url": "https://chaelist.github.io/docs/ml_advanced/regularization/#%ED%8E%B8%ED%96%A5%EA%B3%BC-%EB%B6%84%EC%82%B0",
    "relUrl": "/docs/ml_advanced/regularization/#편향과-분산"
  },"188": {
    "doc": "Regularization",
    "title": "Overfitting 문제 체험",
    "content": ": 6차항의 복잡한 모델을 활용해 Overfitting 문제를 체험해본 후, 아래에서 같은 입력변수에 Regularization을 적용해 결과를 비교해볼 예정. from sklearn.linear_model import LinearRegression from sklearn.model_selection import train_test_split from sklearn.metrics import mean_squared_error from sklearn.preprocessing import PolynomialFeatures from sklearn.datasets import load_boston # boston 집값 데이터 from math import sqrt import numpy as np import pandas as pd . 6차항 모델 준비 . boston_dataset = load_boston() X = pd.DataFrame(boston_dataset.data, columns=boston_dataset.feature_names) # 입력변수를 6차항으로 바꾸기 polynomial_transformer = PolynomialFeatures(6) # 6차항 변환기 준비 polynomial_features = polynomial_transformer.fit_transform(X.values) features = polynomial_transformer.get_feature_names(X.columns) # 바꾼 값들을 다시 X로 저장 X = pd.DataFrame(polynomial_features, columns=features) X.head() . |   | 1 | CRIM | ZN | INDUS | CHAS | NOX | RM | AGE | DIS | RAD | … | B^4 LSTAT^2 | B^3 LSTAT^3 | B^2 LSTAT^4 | B LSTAT^5 | LSTAT^6 | . | 0 | 1 | 0.00632 | 18 | 2.31 | 0 | 0.538 | 6.575 | 65.2 | 4.09 | 1 | … | 6.15436e+11 | 7.72203e+09 | 9.68901e+07 | 1.2157e+06 | 15253.7 | . | 1 | 1 | 0.02731 | 0 | 7.07 | 0 | 0.469 | 6.421 | 78.9 | 4.9671 | 2 | … | 2.07308e+12 | 4.77399e+10 | 1.09938e+09 | 2.5317e+07 | 583012 | . | 2 | 1 | 0.02729 | 0 | 7.07 | 0 | 0.469 | 7.185 | 61.1 | 4.9671 | 2 | … | 3.86749e+11 | 3.96761e+09 | 4.07033e+07 | 417571 | 4283.81 | . | 3 | 1 | 0.03237 | 0 | 2.18 | 0 | 0.458 | 6.998 | 45.8 | 6.0622 | 3 | … | 2.09631e+11 | 1.56175e+09 | 1.16351e+07 | 86681.6 | 645.779 | . | 4 | 1 | 0.06905 | 0 | 2.18 | 0 | 0.458 | 7.147 | 54.2 | 6.0622 | 3 | … | 7.04983e+11 | 9.46727e+09 | 1.27137e+08 | 1.70733e+06 | 22927.8 | . 5 rows × 27132 columns .   . # 목표변수도 dataframe으로 정리 y = pd.DataFrame(boston_dataset.target, columns=['MEDV']) y.head() . |   | MEDV | . | 0 | 24 | . | 1 | 21.6 | . | 2 | 34.7 | . | 3 | 33.4 | . | 4 | 36.2 | . 학습 &amp; 결과 확인 . X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=5) model = LinearRegression() model.fit(X_train, y_train) . LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False) . → training data와 test data에서의 예측 성능 비교: . rsquare = model.score(X_train, y_train) print(f'training set에서의 r스퀘어: {rsquare :.3f}') rsquare = model.score(X_test, y_test) print(f'test set에서의 r스퀘어: {rsquare :.3f}') . training set에서의 r스퀘어: 1.000 test set에서의 r스퀘어: -28657.989 . | training set에서는 100%의 설명력을 갖는 반면, test set은 하나도 설명을 하지 못한다. 과적합된 것! | . y_train_predict = model.predict(X_train) y_test_predict = model.predict(X_test) mse = mean_squared_error(y_train, y_train_predict) print(f'training set에서의 rmse: {sqrt(mse) :.3f}') mse = mean_squared_error(y_test, y_test_predict) print(f'test set에서의 rmse: {sqrt(mse) :.3f}') . training set에서의 rmse: 0.000 test set에서의 rmse: 1650.789 . | training set에서는 평균제곱근오차가 0인 반면, test set에서는 1650이 넘는 큰 값이 나온다 | . ",
    "url": "https://chaelist.github.io/docs/ml_advanced/regularization/#overfitting-%EB%AC%B8%EC%A0%9C-%EC%B2%B4%ED%97%98",
    "relUrl": "/docs/ml_advanced/regularization/#overfitting-문제-체험"
  },"189": {
    "doc": "Regularization",
    "title": "Regularization (가중치 규제)",
    "content": ": 모델을 학습시킬 때 θ값(coefficient; 변수별 가중치)들이 너무 커지는 것을 방지해주는 방법 (penalty를 부여하는 개념) . | 데이터에 대한 오차와 θ값 중 어느 걸 줄이는 것이 더 중요할지를 상수 λ(lambda)에 따라 결정 (λ가 클수록 θ값에 대한 제한이 커짐) | . L1 Regularization . | L1: Lasso. 각 parameter의 절대값을 사용 | 손실 함수 J = 평균제곱오차 + λ|θ1| + λ|θ2| + … + λ|θn| | L1 Regularization을 사용하는 회귀 모델을 Lasso 모델이라고 함 | ※ L1 Regularization은 여러 θ값을 아예 0으로 만들어준다. (모델에 중요하지 않다고 생각되는 속성들을 아예 0으로 만들어 없애주는 효과) . | L1 Regularization은 어떤 모델에 쓰이는 속성(변수)의 개수를 줄이고 싶을 때 사용된다. | ex) 속성 20개로 2차 다중 다항 회귀 모델을 만들면 속성은 총 230개가 됨 → 속성이 이렇게 많으면 과적합 가능성이 높을 뿐만이 아니라 모델을 학습시킬 때 많은 자원(RAM, 시간 등)을 소모됨 → L1 Regularization을 사용하면 학습에 사용되는 속성의 수를 많이 줄일 수 있다 | . | . L2 Regularization . | L2: Ridge. 각 parameter의 제곱값을 사용 | 손실 함수 J = 평균제곱오차 + λθ12 + λθ22 + … + λθn2 | L2 Regularization을 사용하는 회귀 모델을 Ridge 모델이라고 함 | ※ L1과 달리 L2 Regularization은 θ값을 아예 0으로 만들지는 않고, 조금씩 가중치를 줄여주기만 한다. | 속성의 개수를 줄일 필요는 없다고 생각되면 L2 Regularization을 쓰면 됨 | . | .   . *L1, L2 Regularization 사용하기: (Regularization은 손실함수를 최소화하는 모든 알고리즘에 적용할 수 있음) . | 다중회귀/다항회귀 모델: Linear Regression 대신 Lasso 또는 Ridge 모델을 사용하면 됨 | Logistic Regression 모델: L2 Regularization를 적용하는 것이 default. | ‘penalty’라는 옵셔널 파라미터를 통해 어떤 Regularization 기법을 사용할지 정해줄 수 있음 LogisticRegression(penalty='none') # Regularization 사용 안함 LogisticRegression(penalty='l1') # L1 Regularization 사용 LogisticRegression(penalty='l2') # L2 Regularization 사용 LogisticRegression() # 위와 똑같음: L2 Regularization 사용 . | . | . +) Deep Learning에서도 regularization이 중요. Lasso Regression 구현 . from sklearn.linear_model import Lasso # 아예 Lasso Regression 모델이 따로 제공된다 from sklearn.model_selection import train_test_split from sklearn.metrics import mean_squared_error from math import sqrt import numpy as np import pandas as pd . *위의 과적합 문제 체험에서 사용한 X, y 그대로 사용 . X.head() . |   | 1 | CRIM | ZN | INDUS | CHAS | NOX | RM | AGE | DIS | RAD | … | B^4 LSTAT^2 | B^3 LSTAT^3 | B^2 LSTAT^4 | B LSTAT^5 | LSTAT^6 | . | 0 | 1 | 0.00632 | 18 | 2.31 | 0 | 0.538 | 6.575 | 65.2 | 4.09 | 1 | … | 6.15436e+11 | 7.72203e+09 | 9.68901e+07 | 1.2157e+06 | 15253.7 | . | 1 | 1 | 0.02731 | 0 | 7.07 | 0 | 0.469 | 6.421 | 78.9 | 4.9671 | 2 | … | 2.07308e+12 | 4.77399e+10 | 1.09938e+09 | 2.5317e+07 | 583012 | . | 2 | 1 | 0.02729 | 0 | 7.07 | 0 | 0.469 | 7.185 | 61.1 | 4.9671 | 2 | … | 3.86749e+11 | 3.96761e+09 | 4.07033e+07 | 417571 | 4283.81 | . | 3 | 1 | 0.03237 | 0 | 2.18 | 0 | 0.458 | 6.998 | 45.8 | 6.0622 | 3 | … | 2.09631e+11 | 1.56175e+09 | 1.16351e+07 | 86681.6 | 645.779 | . | 4 | 1 | 0.06905 | 0 | 2.18 | 0 | 0.458 | 7.147 | 54.2 | 6.0622 | 3 | … | 7.04983e+11 | 9.46727e+09 | 1.27137e+08 | 1.70733e+06 | 22927.8 | . 5 rows × 27132 columns .   . y.head() . |   | MEDV | . | 0 | 24 | . | 1 | 21.6 | . | 2 | 34.7 | . | 3 | 33.4 | . | 4 | 36.2 | .   . X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=5) model = Lasso(alpha=0.001, max_iter=1000, normalize=True) model.fit(X_train, y_train) . Lasso(alpha=0.001, copy_X=True, fit_intercept=True, max_iter=1000, normalize=True, positive=False, precompute=False, random_state=None, selection='cyclic', tol=0.0001, warm_start=False) . | alpha가 λ(lambda)값을 결정해주는 파라미터 | Lasso 모델은 경사하강법을 사용 → max_iter로 경사하강법을 최대 몇 번 할 지 설정 | normalize=True로 설정하면 데이터를 0~1 사이의 값으로 normalize해준다 | +) Lasso 말고 Ridge 모델을 쓰고 싶다면, Lasso 대신 Ridge를 import해서 사용해주면 됨 | .   . → training data와 test data에서의 예측 성능 비교: . rsquare = model.score(X_train, y_train) print(f'training set에서의 r스퀘어: {rsquare :.3f}') rsquare = model.score(X_test, y_test) print(f'test set에서의 r스퀘어: {rsquare :.3f}') . training set에서의 r스퀘어: 0.945 test set에서의 r스퀘어: 0.902 . y_train_predict = model.predict(X_train) y_test_predict = model.predict(X_test) mse = mean_squared_error(y_train, y_train_predict) print(f'training set에서의 rmse: {sqrt(mse) :.3f}') mse = mean_squared_error(y_test, y_test_predict) print(f'test set에서의 rmse: {sqrt(mse) :.3f}') . training set에서의 rmse: 2.099 test set에서의 rmse: 3.058 . | Regularization으로 overfitting을 방지해주니, training set과 test set에서 r스퀘어값과 평균제곱근오차가 비슷하게 맞춰짐! | . ",
    "url": "https://chaelist.github.io/docs/ml_advanced/regularization/#regularization-%EA%B0%80%EC%A4%91%EC%B9%98-%EA%B7%9C%EC%A0%9C",
    "relUrl": "/docs/ml_advanced/regularization/#regularization-가중치-규제"
  },"190": {
    "doc": "Requests & BeautifulSoup",
    "title": "Requests &amp; BeautifulSoup",
    "content": ". | Requests . | Proxy 사용 | Header 사용 | . | BeautifulSoup . | 직접 tag 이름 호출 | find()와 find_all() | tag 내 element 추출 | Nested Tags 접근 | re.compile | . | BeautifulSoup: select() . | CSS 선택자로 접근 | CSS 선택자 조합하여 사용 | . | . ",
    "url": "https://chaelist.github.io/docs/webscraping/requests_beautifulsoup/#requests--beautifulsoup",
    "relUrl": "/docs/webscraping/requests_beautifulsoup/#requests--beautifulsoup"
  },"191": {
    "doc": "Requests & BeautifulSoup",
    "title": "Requests",
    "content": "*역할: web communication. 서버에서 (raw) data (=source codes)를 받아온다. import requests # import해서 사용 url = 'https://www.imdb.com/title/tt0805647/?ref_=vp_vi_tt' r = requests.get(url) ## sends request message &amp; recieves source codes &amp; returns variables print(r.text) ## .text로 간단히 불러온 source code 출력 . &lt;!DOCTYPE html&gt; &lt;html xmlns:og=\"http://ogp.me/ns#\" xmlns:fb=\"http://www.facebook.com/2008/fbml\"&gt; &lt;head&gt; &lt;meta charset=\"utf-8\"&gt; &lt;meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\"&gt; &lt;meta name=\"apple-itunes-app\" content=\"app-id=342792525, app-argument=imdb:///title/tt0805647?src=mdot\"&gt; (생략) . +) header도 별도로 가져올 수 있음 . r.headers . {'Content-Type': 'text/html;charset=UTF-8', 'Transfer-Encoding': 'chunked', 'Connection': 'keep-alive', 'Server': 'Server', 'Date': 'Mon, 21 Dec 2020 12:27:34 GMT', 'x-amz-rid': 'H9QPTE15DWZ8GAJ3F3ZG', 'Set-Cookie': 'uu=BCYhjgIkRCdAZD3iSQrRQiy0YbMFhLNwk8zxrf0rHF5X3OWIKROLTsDOoCR5TLd5OyQ0OMF32iae%0D%0A1Y0pLcflXX3sTAoF6tCuyAmbPBhAOsNT3-PBrLCEP09Zd7kJEDY89VtMKo_UdYkrDrA6PrC082Jb%0D%0AzA%0D%0A; Domain=.imdb.com; Expires=Sat, 08-Jan-2089 15:41:41 GMT; Path=/; Secure, session-id=000-0000000-0000000; Domain=.imdb.com; Expires=Sat, 08-Jan-2089 15:41:41 GMT; Path=/; Secure, session-id-time=2239273654; Domain=.imdb.com; Expires=Sat, 08-Jan-2089 15:41:41 GMT; Path=/; Secure', 'X-Frame-Options': 'SAMEORIGIN', 'Content-Security-Policy': \"frame-ancestors 'self' imdb.com *.imdb.com *.media-imdb.com withoutabox.com *.withoutabox.com amazon.com *.amazon.com amazon.co.uk *.amazon.co.uk amazon.de *.amazon.de translate.google.com images.google.com www.google.com www.google.co.uk search.aol.com bing.com www.bing.com\", 'Content-Language': 'en-US', 'Strict-Transport-Security': 'max-age=47474747; includeSubDomains; preload', 'Vary': 'Content-Type,Accept-Encoding,X-Amzn-CDN-Cache,X-Amzn-AX-Treatment,User-Agent', 'X-Cache': 'Miss from cloudfront', 'Via': '1.1 214d8a3cdb14de6b0331d1f72902cc67.cloudfront.net (CloudFront)', 'X-Amz-Cf-Pop': 'HKG60-C1', 'X-Amz-Cf-Id': '-_bmiycwCogkwse1lduiRQkvFpgQb2evcKf0DlKRZKSiJwYrPimtfw=='} . Proxy 사용 . : proxy를 사용하면 국가를 우회하여 접속할 수 있다 (특정 국가에서만 접속 가능한 사이트에 접속하기) . # Proxy 사용 예시 코드 proxyDict = { \"http\" : \"http://proxy_example:8080\", \"https\" : \"https://proxy_example:8080\" } r = requests.get(url, proxies=proxyDict) . Header 사용 . : headers 옵션을 사용하면 사람인 척 접속할 수 있다 (scraper 차단하는 사이트에 접속하기) . | User Agent는 자신이 사용하는 브라우저에서 정보를 가져와도 되고, fake user agent를 받아와서 사용해도 된다. | . # Header 사용 예시 코드 headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'} r = requests.get(url, headers=headers) . +) fake user agent 쉽게 가져와서 사용할 수 있는 라이브러리 . import requests from fake_useragent import UserAgent # import해서 사용 ua = UserAgent() headers = { 'User-Agent': ua.random, } resp = requests.get(url, headers = headers) . ",
    "url": "https://chaelist.github.io/docs/webscraping/requests_beautifulsoup/#requests",
    "relUrl": "/docs/webscraping/requests_beautifulsoup/#requests"
  },"192": {
    "doc": "Requests & BeautifulSoup",
    "title": "BeautifulSoup",
    "content": "*역할: information extraction. 원하는 정보를 담은 tag를 찾아서, 그 안의 text를 extract해준다 . | 보통 Requests로 웹 상의 source code를 받아온 후에, BeautifulSoup를 통해 원하는 정보를 추출한다 . import requests from bs4 import BeautifulSoup url = 'https://www.imdb.com/title/tt0805647/?ref_=vp_vi_tt' r = requests.get(url) # requests가 url의 source code를 받아온 후, soup = BeautifulSoup(r.text, 'lxml') # BeautifulSoup로 각 tag를 접근할 수 있게 준비 . | parser의 종류: . | html.parser: 각종 기능 완비, 적절한 속도, 관대함(lenient) | lxml: 아주 빠름, 관대함 | html5lib: 아주 관대함 (웹 브라우저 방식으로 페이지를 해석함), 하지만 속도가 매우 느림 | . | parser에 따라 읽어오는 방식이 조금씩 다르기에, parser를 변경하면 크롤링이 가능해지는 경우도 있다. | . 직접 tag 이름 호출 . # 실습용으로 제작된 html code. (Requests로 웹 상의 source code를 받아오는 단계를 대체) html_code = \"\"\"&lt;!DOCTYPE html&gt; &lt;html&gt; &lt;head&gt; &lt;title&gt; Cities and Locations &lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;span class=\"city1\"&gt; &lt;a href = \"http://english.seoul.go.kr/\"&gt;Seoul&lt;/a&gt; &lt;p class = \"location\"&gt; South Korea &lt;/p&gt; &lt;a href = \"https://en.wikipedia.org/wiki/Seoul\"&gt;Wikipedia - Seoul &lt;/p&gt; &lt;/span&gt; &lt;span class=\"city2\"&gt; &lt;a href = \"hhttp://www1.nyc.gov/\"&gt;New York City&lt;/a&gt; &lt;p class = \"location western\"&gt; U.S.A. &lt;/p&gt; &lt;a href = \"https://en.wikipedia.org/wiki/New_York_City\"&gt;Wikipedia - NY City &lt;/p&gt; &lt;/span&gt; &lt;span class=\"city3\"&gt; &lt;a href = \"http://www.ebeijing.gov.cn/\"&gt;Beijing&lt;/a&gt; &lt;p class = \"location\"&gt; China &lt;/p&gt; &lt;a href = \"https://en.wikipedia.org/wiki/Beijing\"&gt;Wikipedia - Beijing &lt;/p&gt; &lt;/span&gt; &lt;/body&gt; &lt;/html&gt;\"\"\" # BeautifulSoup 타입으로 변환 soup = BeautifulSoup(html_code, 'html.parser') . | title tag에 접근하기 . soup.title . &lt;title&gt; Cities and Locations &lt;/title&gt; . | tag의 text만 추출하기 . soup.title.text.strip() ## .text만 해줘도 되지만, 앞뒤에 space가 있을 수 있어서 보통은 strip()을 함께 써준다. Cities and Locations . | . cf) 여러 개의 같은 tag가 있는 경우: . | 직접 tag 이름을 호출해 불러오는 경우는, 같은 tag가 두 개 이상이여도 맨 처음 tag 하나만 불러온다 . soup.p . &lt;p class=\"location\"&gt; South Korea &lt;/p&gt; . | . find()와 find_all() . : html tag 기반 선택자 . | find(): 조건을 충족하는 것 중 가장 처음의 tag 하나만 가져옴 | find_all(): 조건을 충족하는 모든 tag를 list 형태로 다 불러옴 | . | find() soup.find('p') # 맨 처음 p tag만 반환. soup.p랑 동일. &lt;p class=\"location\"&gt; South Korea &lt;/p&gt; . | find_all() soup.find_all('p') # list 형태로 모든 p tag 모두 반환 . [&lt;p class=\"location\"&gt; South Korea &lt;/p&gt;, &lt;p class=\"location western\"&gt; U.S.A. &lt;/p&gt;, &lt;p class=\"location\"&gt; China &lt;/p&gt;] . → list 형태로 반환되면, indexing을 통해 text를 추출하면 된다 . soup.find_all('p')[2].text.strip() . China . | 특정 attribute를 갖는 tag 찾기 soup.find('p', attrs={'class':'western'}) # 'western'이라는 'class'를 가진 p tag 찾기 . &lt;p class=\"location western\"&gt; U.S.A. &lt;/p&gt; . | soup.find('p', attrs={'class':'western'})는 다음과 같이 써도 동일: soup.find('p', 'western'), soup.find('p', class_='western') (편한대로 줄여서 써도 된다) | . | . tag 내 element 추출 . | .get() 함수 soup.find('a') # a tag에 접근 . &lt;a href=\"http://english.seoul.go.kr/\"&gt;Seoul&lt;/a&gt; . → a tag의 ‘href’에서 url 정보 추출하기 . soup.find('a').get('href') . http://english.seoul.go.kr/ . | [ ] 방식 soup.find('a')['href'] ## 이렇게 접근하는 것도 가능 . http://english.seoul.go.kr/ . | . Nested Tags 접근 . | a tag can be a parent / child / sibling of another tag | 특정 parent tag에 먼저 접근한 후, child tag에 접근하면 더 정교한 접근이 가능하다 | . city3_info = soup.find_all('span', 'city3')[0] # parent tag city3_info . &lt;span class=\"city3\"&gt; &lt;a href=\"http://www.ebeijing.gov.cn/\"&gt;Beijing&lt;/a&gt; &lt;p class=\"location\"&gt; China &lt;/p&gt; &lt;a href=\"https://en.wikipedia.org/wiki/Beijing\"&gt;Wikipedia - Beijing &lt;/a&gt;&lt;/span&gt; . → 해당 parent tag 아래에 있는 child tag에 접근해서 정보 추출 . city3_info.a.get('href') . http://www.ebeijing.gov.cn/ . re.compile . : 정규표현식을 활용하면 특정 글자를 포함하는 tag를 모두 찾아오는 것이 가능 . import re tags = soup.find_all('span', attrs={'class':re.compile('city')}) # 이렇게 하면 class attribute에 'city'가 포함된 span tag를 다 찾는다 ## &lt;span class='city1'&gt;, &lt;span class='city2'&gt;, &lt;span class='city3'&gt; 등을 다 찾아옴! tags . [&lt;span class=\"city1\"&gt; &lt;a href=\"http://english.seoul.go.kr/\"&gt;Seoul&lt;/a&gt; &lt;p class=\"location\"&gt; South Korea &lt;/p&gt; &lt;a href=\"https://en.wikipedia.org/wiki/Seoul\"&gt;Wikipedia - Seoul &lt;/a&gt;&lt;/span&gt;, &lt;span class=\"city2\"&gt; &lt;a href=\"hhttp://www1.nyc.gov/\"&gt;New York City&lt;/a&gt; &lt;p class=\"location western\"&gt; U.S.A. &lt;/p&gt; &lt;a href=\"https://en.wikipedia.org/wiki/New_York_City\"&gt;Wikipedia - NY City &lt;/a&gt;&lt;/span&gt;, &lt;span class=\"city3\"&gt; &lt;a href=\"http://www.ebeijing.gov.cn/\"&gt;Beijing&lt;/a&gt; &lt;p class=\"location\"&gt; China &lt;/p&gt; &lt;a href=\"https://en.wikipedia.org/wiki/Beijing\"&gt;Wikipedia - Beijing &lt;/a&gt;&lt;/span&gt;] . ",
    "url": "https://chaelist.github.io/docs/webscraping/requests_beautifulsoup/#beautifulsoup",
    "relUrl": "/docs/webscraping/requests_beautifulsoup/#beautifulsoup"
  },"193": {
    "doc": "Requests & BeautifulSoup",
    "title": "BeautifulSoup: select()",
    "content": ": CSS 기반 선택자 . | select(): find_all()과 유사. 조건을 충족하는 모든 tag를 list 형태로 다 불러옴 | select_one(): find()와 유사. 조건을 충족하는 것 중 가장 처음의 tag 하나만 가져옴 | . ※ 태그 이름, 속성, 속성값을 특정하는 방식은 find()와 같다. 하지만 select()는 이 외에도 다양한 선택자(selector)를 갖기 때문에 여러 요소를 조합하여 태그를 특정하기 쉽다는 장점이 있다 (more flexible!) . html_code = \"\"\"&lt;!DOCTYPE html&gt; &lt;html&gt; &lt;head&gt; &lt;title&gt;Sample Website&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;h2&gt;HTML 연습!&lt;/h2&gt; &lt;p&gt;이것은 첫 번째 문단입니다.&lt;/p&gt; &lt;p&gt;이것은 두 번째 문단입니다!&lt;/p&gt; &lt;ul&gt; &lt;li id=\"coffee\"&gt;커피&lt;/li&gt; &lt;li class=\"green-tea\"&gt;녹차&lt;/li&gt; &lt;li class=\"green-tea favorite\"&gt;Sencha Green Tea&lt;/li&gt; &lt;li class=\"milk\"&gt;우유&lt;/li&gt; &lt;/ul&gt; &lt;img src='https://i.imgur.com/bY0l0PC.jpg' alt=\"coffee\"/&gt; &lt;img src='https://i.imgur.com/fvJLWdV.jpg' alt=\"green-tea\"/&gt; &lt;img src='https://i.imgur.com/rNOIbNt.jpg' alt=\"milk\"/&gt; &lt;/body&gt; &lt;/html&gt;\"\"\" # BeautifulSoup 타입으로 변환 soup = BeautifulSoup(html_code, 'html.parser') . | html tag로 선택 # soup.select() li_tags = soup.select('li') # &lt;li&gt; 태그만 모아서 리스트로 출력 li_tags . [&lt;li id=\"coffee\"&gt;커피&lt;/li&gt;, &lt;li class=\"green-tea\"&gt;녹차&lt;/li&gt;, &lt;li class=\"green-tea favorite\"&gt;Sencha Green Tea&lt;/li&gt;, &lt;li class=\"milk\"&gt;우유&lt;/li&gt;] . → text만 추출 . li_tags[0].text # find()에서와 동일하게 .text를 사용 ## 하나만 추출하고 싶을 때는 soup.select_one('li').text라고 해도 동일 . 커피 . | tag 내 element 추출 img_tags = soup.select('img') img_tags . [&lt;img alt=\"coffee\" src=\"https://i.imgur.com/bY0l0PC.jpg\"/&gt;, &lt;img alt=\"green-tea\" src=\"https://i.imgur.com/fvJLWdV.jpg\"/&gt;, &lt;img alt=\"milk\" src=\"https://i.imgur.com/rNOIbNt.jpg\"/&gt;] . → find()에서와 동일하게, []이나 .get()을 활용하면 tag 내 element 추출 가능 . img_tags[0]['src'] ## img_tags[0].get('src')와 동일 . https://i.imgur.com/bY0l0PC.jpg . | . CSS 선택자로 접근 . | 특정 id의 태그 선택 . | #으로 접근 - ex) id가 coffee인 태그: #coffee | . soup.select('#coffee') #soup.find('#coffee')는 성립X . [&lt;li id=\"coffee\"&gt;커피&lt;/li&gt;] . | 특정 class의 태그 선택 . | .으로 접근 - ex) class가 ‘green-tea’인 태그: .green-tea | . soup.select('.green-tea') . [&lt;li class=\"green-tea\"&gt;녹차&lt;/li&gt;, &lt;li class=\"green-tea favorite\"&gt;Sencha Green Tea&lt;/li&gt;] . | 속성의 이름/값으로 태그 선택 . | [name=\"value\"] 형식으로 접근 | ex) alt 속성의 값이 “green-tea”인 태그: [alt=\"green-tea\"] | ex) href 속성의 값이 “https://chaelist.github.io/”인 태그: [href=\"https://chaelist.github.io/\"] | . soup.select('[alt=\"green-tea\"]') . [&lt;img alt=\"green-tea\" src=\"https://i.imgur.com/fvJLWdV.jpg\"/&gt;] . | . CSS 선택자 조합하여 사용 . : 아래와 같이 다양한 조합으로 접근할 수 있다는 것이 CSS 기반 선택자인 select()의 장점! . | OR 연산 . | , 사용 → 두 CSS 선택자 중 하나라도 해당되면 선택 | . ex1) . # id가 coffee이거나 class가 milk인 태그 soup.select('#coffee, .milk') . [&lt;li id=\"coffee\"&gt;커피&lt;/li&gt;, &lt;li class=\"milk\"&gt;우유&lt;/li&gt;] . ex2) . # 모든 p 태그와 모든 li 태그 soup.select('p, li') . [&lt;p&gt;이것은 첫 번째 문단입니다.&lt;/p&gt;, &lt;p&gt;이것은 두 번째 문단입니다!&lt;/p&gt;, &lt;li id=\"coffee\"&gt;커피&lt;/li&gt;, &lt;li class=\"green-tea favorite\"&gt;녹차&lt;/li&gt;, &lt;li class=\"milk\"&gt;우유&lt;/li&gt;] . | AND 연산 . | 두 CSS 선택자를 붙여씀 → 두 CSS 선택자 모두 해당되는 요소만 선택 | . ex1) . # green-tea 클래스와 favorite 클래스를 모두 가진 태그 soup.select('.green-tea.favorite') . [&lt;li class=\"green-tea favorite\"&gt;Sencha Green Tea&lt;/li&gt;] . ex2) . # favoite 클래스를 가진 li 태그 soup.select('li.favorite') . [&lt;li class=\"green-tea favorite\"&gt;Sencha Green Tea&lt;/li&gt;] . | Nested Tags 접근 (Descendant Combinator) . | 두 CSS 선택자를 띄어씀 → 첫 번째 CSS 선택자로 선택된 태그 내부에서 두 번째 CSS 선택자를 찾는다 | . # ul 태그 아래에 있는 자손 태그 중, milk라는 class를 가진 태그 soup.select('ul .milk') . [&lt;li class=\"milk\"&gt;우유&lt;/li&gt;] . cf) .class1 .class2와 .class1 &gt; .class2의 차이: . | .class1 .class2: class1 클래스를 가진 태그의 “자손” 중에 class2 클래스를 가진 모든 태그를 의미 | .class1 &gt; .class2: class1 클래스를 가진 태그의 “자식” 중에 class2 클래스를 가진 모든 태그를 의미 | “자손”: 부모 요소에 포함된 모든 하위 요소를 의미 vs “자식”: 부모의 바로 아래 자식 요소만을 의미 | .   . | 형제 태그 접근 (Sibling Combinator) . | ~ 사용 → 특정 태그 뒤에 나오는 형제 태그를 선택 | 형제 태그: parent-child 관계가 아닌, 같은 depth를 가진 태그 (병렬적 나열) | . ex1) . # green-tea라는 class를 가진 태그 뒤에 나오는 green-tea라는 class를 가진 태그 soup.select('.green-tea ~ .green-tea') . [&lt;li class=\"green-tea favorite\"&gt;Sencha Green Tea&lt;/li&gt;] . ex2) . # ul 태그 뒤에 나오는 img 태그 (뒤에 나오는 모든 img 태그) soup.select('ul ~ img') . [&lt;img alt=\"coffee\" src=\"https://i.imgur.com/bY0l0PC.jpg\"/&gt;, &lt;img alt=\"green-tea\" src=\"https://i.imgur.com/fvJLWdV.jpg\"/&gt;, &lt;img alt=\"milk\" src=\"https://i.imgur.com/rNOIbNt.jpg\"/&gt;] . cf) 인접형제 선택자 +: 특정 태그 바로 뒤에 나오는 형제 태그를 선택. | ~와 유사하나, 바로 뒤에 있는 1개만 선택한다는 점에서 차이 # ~ 대신 +를 사용하면 바로 뒤에 나오는 태그 1개만 찾아줌 soup.select('ul + img') . [&lt;img alt=\"coffee\" src=\"https://i.imgur.com/bY0l0PC.jpg\"/&gt;] . | . | . ",
    "url": "https://chaelist.github.io/docs/webscraping/requests_beautifulsoup/#beautifulsoup-select",
    "relUrl": "/docs/webscraping/requests_beautifulsoup/#beautifulsoup-select"
  },"194": {
    "doc": "Requests & BeautifulSoup",
    "title": "Requests & BeautifulSoup",
    "content": " ",
    "url": "https://chaelist.github.io/docs/webscraping/requests_beautifulsoup/",
    "relUrl": "/docs/webscraping/requests_beautifulsoup/"
  },"195": {
    "doc": "Seaborn",
    "title": "Seaborn",
    "content": ". | 기본 그래프 . | Bar Plot, Count Plot | Line Plot | Scatter Plot | . | KDE Plot . | 기본 KDE Plot | histplot() | violinplot() | KDE Plot; 등고선 그래프 | . | LM Plot | Joint Plot | Box Plot | Catplot | Heatmap | Pairplot | . *Seaborn: Python data visualization library based on matplotlib . ",
    "url": "https://chaelist.github.io/docs/visualization/seaborn/",
    "relUrl": "/docs/visualization/seaborn/"
  },"196": {
    "doc": "Seaborn",
    "title": "기본 그래프",
    "content": "Bar Plot, Count Plot . import pandas as pd import seaborn as sns # import해야 사용 가능; 보통 sns로 줄여서 import exam_df = pd.read_csv('data/exam.csv') ## 데이터 출처: codeit exam_df.head() . |   | gender | race/ethnicity | parental level of education | lunch | test preparation course | math score | reading score | writing score | . | 0 | female | group B | bachelor’s degree | standard | none | 72 | 72 | 74 | . | 1 | female | group C | some college | standard | completed | 69 | 90 | 88 | . | 2 | female | group B | master’s degree | standard | none | 90 | 95 | 93 | . | 3 | male | group A | associate’s degree | free/reduced | none | 47 | 57 | 44 | . | 4 | male | group C | some college | standard | none | 76 | 78 | 75 | . | barplot sns.barplot(data=exam_df, x=\"race/ethnicity\", y=\"math score\", palette='Blues_d'); # ci=95가 default (ci: Confidence Interval. 신뢰구간) . | barplot의 기본 estimator는 np.mean (평균값). 별도로 설정해주지 않으면 default로 각 카테고리의 평균값을 시각화해준다 | estimator는 np.sum, np.count 등으로 다양하게 변경 가능 | . +) hue 옵션 지정: . sns.barplot(data=exam_df, x=\"race/ethnicity\", y=\"math score\", hue='gender', palette='Set3'); . | countplot: y값에는 자동으로 count가 들어가는 bar plot # 각 race/ethnicity 그룹에 속한 학생 수를 비교 sns.countplot(data=exam_df, x=\"race/ethnicity\", color='skyblue'); . | . Line Plot . import pandas as pd import seaborn as sns flights_df = sns.load_dataset(\"flights\") flights_df.head() . |   | year | month | passengers | . | 0 | 1949 | Jan | 112 | . | 1 | 1949 | Feb | 118 | . | 2 | 1949 | Mar | 132 | . | 3 | 1949 | Apr | 129 | . | 4 | 1949 | May | 121 | . | 연도별 월평균 승객수 추이 sns.lineplot(data=flights_df, x='year', y='passengers', color='skyblue'); # 평균값을 line으로 그려주고, 95% confidence interval을 함께 표시 . | 연도별 총 승객수 추이 . flights_groupby = flights_df.groupby('year')[['passengers']].sum().reset_index() sns.lineplot(data=flights_groupby, x='year', y='passengers', color='skyblue'); . +) 아예 estimator=’sum’으로 설정해서 시각화하는 것도 가능 . # 연도별 총 승객수 추이 (groupby를 거쳐서 그리는 것과 동일한 형태로 시각화 가능) sns.lineplot(data=flights_df, x='year', y='passengers', color='skyblue', estimator='sum', ci=None); . | estimator는 ‘sum’, ‘min’, ‘count’ 등 다양하게 설정 가능 | . | 연도별, 월별 승객수 추이 . sns.lineplot(data=flights_df, x='year', y='passengers', hue='month', palette='pastel'); . +) 아예 pivot된 데이터를 통째로 넣어서 그리는 것도 가능: . flights_groupby2 = flights_df.pivot('year', 'month', 'passengers') sns.lineplot(data=flights_groupby2, palette='pastel'); . | . Scatter Plot . sns.scatterplot(data=exam_df, x='math score', y='reading score', color='skyblue'); . +) hue, style 등의 옵션으로 구분해서 시각화: . import matplotlib.pyplot as plt plt.figure(figsize=(10, 6)) sns.scatterplot(data=exam_df, x='math score', y='reading score', hue='gender', style='race/ethnicity', palette='Blues_d'); # +) size='lunch' 이런 식으로 점의 size에 따른 구분도 추가 가능 . ",
    "url": "https://chaelist.github.io/docs/visualization/seaborn/#%EA%B8%B0%EB%B3%B8-%EA%B7%B8%EB%9E%98%ED%94%84",
    "relUrl": "/docs/visualization/seaborn/#기본-그래프"
  },"197": {
    "doc": "Seaborn",
    "title": "KDE Plot",
    "content": ": 대체로 세상에서 일어나는 대부분의 일들은 비슷한 확률밀도함수(PDF)의 생김새를 갖는다. 그런데, 우리는 무한개의 데이터를 구할 수 없기에, 우리가 구한 데이터로 그래프를 그려보면 매끄러운 확률밀도함수의 모양이 나오지 않는다. → 이를 해결하는 것이 KDE(Kernel Density Estimation). KDE를 사용하면 우리가 구한 데이터를 기반으로 어느 정도 추측을 해서, 실제 분포에 가깝게 매끄러운 그래프를 그려준다. +) PDF: Probability Density Function (확률밀도함수) . | PDF 아래의 모든 면적을 더하면 1이다. (100%) | PDF를 활용해서 값들이 어떻게 분포되어 있는지 나타낼 수 있다 | 특정 구간의 면적이 곧 값이 그 구간에 속할 확률이다 | 특정 ‘점’의 확률을 무조건 0이다. (면적이 0이므로) | . 기본 KDE Plot . body_df = pd.read_csv('data/body.csv') ## 데이터 출처: codeit body_df.head() . |   | Number | Height | Weight | . | 0 | 1 | 176 | 85.2 | . | 1 | 2 | 175.3 | 67.7 | . | 2 | 3 | 168.6 | 75.2 | . | 3 | 4 | 168.1 | 67.1 | . | 4 | 5 | 175.3 | 63 | . → 키 데이터를 순서대로 정렬 . | value_counts()로 각 항목의 개수 파악 | sort_index()로 순서대로 정렬 | . body_df['Height'].value_counts().sort_index() . 154.4 1 155.5 1 157.4 1 157.8 1 158.0 1 .. 190.3 1 191.2 1 191.8 1 192.4 1 193.1 1 Name: Height, Length: 262, dtype: int64 . | ‘Height’ 데이터 분포를 그대로 그래프로 그려보기 body_df['Height'].value_counts().sort_index().plot() ## 이렇게 우리가 가진 데이터로만 그래프를 그리면 보기 불편하게 나온다 . | Seaborn의 KED Plot 기능으로 매끄러운 확률밀도함수 그리기 sns.kdeplot(body_df['Height']) . | bandwidth 조절하기 sns.kdeplot(body_df['Height'], bw_adjust=0.5); # bw는 bandwidth의 약자 . | bw_adjust로 얼마나 매끄럽게 확률밀도함수를 조절할 것인지 선택. bw값이 클수록 매끄럽다. | 하지만 bw값이 무조건 클수록 좋은 것만은 아니다. 적절히 인사이트를 얻을 수 있는 만큼으로 조절. | **bw_adjust를 설정하지 않으면, Seaborn이 적당한 값으로 알아서 골라준다. | . | . histplot() . | 기본 히스토그램 . sns.histplot(body_df['Height'], bins=15); . | 히스토그램과 KDE Plot을 한 번에 표현 . sns.histplot(body_df['Height'], kde=True, stat='density', linewidth=0, bins=15); . | kde=True, stat='density', linewidth=0 옵션을 추가해주면 deprecated 기능인 ‘seaborn.distplot’과 동일하게 시각화 가능 | stat='density'라고 별도로 설정해주지 않으면 y는 그냥 ‘Count’가 됨 | linewidth=0은 kde 곡선을 잘 보이게 하기 위해 histogram의 테두리선을 없애주는 역할 | . | . violinplot() . : KDE Plot을 양 옆으로 대칭으로 그려둔 모양이랑 동일. box plot과 유사하게, 값의 분포를 파악 가능. sns.violinplot(y=body_df['Height']) . KDE Plot; 등고선 그래프 . :KDE Plot 2개를 합쳐서 등고선 형태로 나타낸 그래프. sns.kdeplot(body_df['Height'], body_df['Weight']) . | body_df 자료에서의 Height와 Weight의 연관성과 함께, 각각의 분포 모양도 볼 수 있는 그래프. | Height의 KDE Plot과 Weight의 KEP Plot을 함쳐서, 3D 형태의 산으로 나타낸 것을 위에서 바라본 모양. → 등고선처럼 표현됨 | 아래에 각각 그려둔 KDE Plot을 보고 머릿속으로 합쳐보면, 위와 같은 등고선이 이해가 될 것이다 | . sns.kdeplot(body_df['Height']) . sns.kdeplot(body_df['Weight']) . ",
    "url": "https://chaelist.github.io/docs/visualization/seaborn/#kde-plot",
    "relUrl": "/docs/visualization/seaborn/#kde-plot"
  },"198": {
    "doc": "Seaborn",
    "title": "LM Plot",
    "content": ": LM은 Linear Model의 약자. 산점도와 Regression Line(회귀선)을 함께 그려준다. (lineplot + scatterplot) . body_df.head() . |   | Number | Height | Weight | . | 0 | 1 | 176 | 85.2 | . | 1 | 2 | 175.3 | 67.7 | . | 2 | 3 | 168.6 | 75.2 | . | 3 | 4 | 168.1 | 67.1 | . | 4 | 5 | 175.3 | 63 | . | 기본 lmplot 그리기 sns.set_style('darkgrid') ## 이렇게 grid의 style도 지정 가능 sns.lmplot(data=body_df, x='Height', y='Weight') . | ci (Confidence Interval, 신뢰구간) 조절 . | lmplot의 default setting은 ci=95 (95% 신뢰구간 표시) → ci=None으로 꺼버리거나, ci=80 이런 식으로 변경할 수 있다 | . sns.set_style('ticks') ## style: dict, None, or one of {darkgrid, whitegrid, dark, white, ticks} sns.lmplot(data=body_df, x='Height', y='Weight', ci=None) . | hue 옵션 지정 . exam_df.head() . |   | gender | race/ethnicity | parental level of education | lunch | test preparation course | math score | reading score | writing score | . | 0 | female | group B | bachelor’s degree | standard | none | 72 | 72 | 74 | . | 1 | female | group C | some college | standard | completed | 69 | 90 | 88 | . | 2 | female | group B | master’s degree | standard | none | 90 | 95 | 93 | . | 3 | male | group A | associate’s degree | free/reduced | none | 47 | 57 | 44 | . | 4 | male | group C | some college | standard | none | 76 | 78 | 75 | . → hue 옵션을 사용해 ‘gender’ 별로 데이터 비교해보기 . sns.lmplot(data=exam_df, x='math score', y='writing score', hue='gender', palette='Set2', height=4); ## palette(그래프가 그려지는 컬러 팔레트를 결정), height(그래프 크기를 결정)도 지정 가능. | . ",
    "url": "https://chaelist.github.io/docs/visualization/seaborn/#lm-plot",
    "relUrl": "/docs/visualization/seaborn/#lm-plot"
  },"199": {
    "doc": "Seaborn",
    "title": "Joint Plot",
    "content": ". | 2차원 실수형 데이터는 jointplot을 활용하면 다각도로 살피기 용이하다 | jointplot을 사용하면 scatterplot + 각 변수의 히스토그램을 함께 그려준다 | kind=’kde’이면 커널 밀도 히스토그램을 그린다 (등고선 모양 그래프 + 각 변수의 KDE Plot) | . | 기본 jointplot: scatterplot + 변수별 히스토그램 sns.jointplot(data=body_df, x='Height', y='Weight') . | KDE jointplot: 등고선 그래프 + 변수별 KDE Plot sns.jointplot(data=body_df, x='Height', y='Weight', kind='kde') . | . ",
    "url": "https://chaelist.github.io/docs/visualization/seaborn/#joint-plot",
    "relUrl": "/docs/visualization/seaborn/#joint-plot"
  },"200": {
    "doc": "Seaborn",
    "title": "Box Plot",
    "content": ": Seaborn을 활용하면 boxplot을 더 다양하게 그릴 수 있다 . cf) pandas 내장 기능으로 그릴 수 있는 boxplot은 이 정도 . exam_df.plot(kind='box', y='math score'); . * Seaborn을 활용하면 x축, hue도 지정해서 각각 비교해 볼 수 있다. sns.boxplot(data=exam_df, x='test preparation course', y='math score', hue='gender', palette='Set3') . | showfliers=False 옵션을 추가하면 outlier를 제외하고 시각화할 수 있다 sns.boxplot(data=exam_df, x='test preparation course', y='math score', hue='gender', palette='Set3', showfliers=False) . | . +) 아래에서 배울 catplot을 활용해도 boxplot과 거의 동일하게 표현 가능 . sns.catplot(data=exam_df, x='test preparation course', y='math score', hue='gender', kind='box') . | catplot(kind=’box’) 역시 마찬가지로 showfliers=False 옵션을 추가할 수 있다 sns.catplot(data=exam_df, x='test preparation course', y='math score', hue='gender', kind='box', showfliers=False) . | . ",
    "url": "https://chaelist.github.io/docs/visualization/seaborn/#box-plot",
    "relUrl": "/docs/visualization/seaborn/#box-plot"
  },"201": {
    "doc": "Seaborn",
    "title": "Catplot",
    "content": ": 카테고리별 비교를 위한 시각화에 적합 . laptops_df = pd.read_csv('data/laptops.csv') ## 데이터 출처: codeit laptops_df.head() . |   | brand | model | ram | hd_type | hd_size | screen_size | price | processor_brand | processor_model | clock_speed | graphic_card_brand | graphic_card_size | os | weight | comments | . | 0 | Dell | Inspiron 15-3567 | 4 | hdd | 1024 | 15.6 | 40000 | intel | i5 | 2.5 | intel | NaN | linux | 2.5 | NaN | . | 1 | Apple | MacBook Air | 8 | ssd | 128 | 13.3 | 55499 | intel | i5 | 1.8 | intel | 2 | mac | 1.35 | NaN | . | 2 | Apple | MacBook Air | 8 | ssd | 256 | 13.3 | 71500 | intel | i5 | 1.8 | intel | 2 | mac | 1.35 | NaN | . | 3 | Apple | MacBook Pro | 8 | ssd | 128 | 13.3 | 96890 | intel | i5 | 2.3 | intel | 2 | mac | 3.02 | NaN | . | 4 | Apple | MacBook Pro | 8 | ssd | 256 | 13.3 | 112666 | intel | i5 | 2.3 | intel | 2 | mac | 3.02 | NaN | . → ‘os’에 몇 개의 unique한 값이 있나 확인 . laptops_df['os'].unique() . array(['linux', 'mac', 'windows'], dtype=object) . → os별 가격 비교: . | box plot으로 비교 sns.catplot(data=laptops_df, x='os', y='price', kind='box'); . | violin plot으로 비교 sns.catplot(data=laptops_df, x='os', y='price', kind='violin') . | strip plot으로 비교: default option. kind= 안쓰면 자동으로 strip plot으로 그려짐 sns.catplot(data=laptops_df, x='os', y='price', kind='strip') . | 이 경우에는 각 항목의 분포 뿐 아니라, 어떤 항목에 데이터가 더 많은지도 알면 좋기 때문에 strip plot이 가장 적합해 보인다. | windows가 데이터가 가장 많고, mac은 몇 개 없는데 많이 분산되어 있다는 것을 한 눈에 알 수 있다. | . | 추가) 프로세서 브랜드에 따라 색 나눠주기 laptops_df['processor_brand'].unique() # 몇 개의 unique한 값이 있나 확인 . array(['intel', 'amd'], dtype=object) . → hue= 옵션을 사용해 프로세서 브랜드별로 구분 . sns.catplot(data=laptops_df, x='os', y='price', kind='strip', hue='processor_brand') . | amd 프로세서를 사용한 노트북은 몇 개 없고, 대체로 저렴한 축에 속한다는 걸 알 수 있다 | . | swarm plot으로 비교: 값이 뭉쳐있는 부분의 데이터가 펼쳐져서 보여진다 sns.catplot(data=laptops_df, x='os', y='price', kind='swarm', hue='processor_brand'); . | . ",
    "url": "https://chaelist.github.io/docs/visualization/seaborn/#catplot",
    "relUrl": "/docs/visualization/seaborn/#catplot"
  },"202": {
    "doc": "Seaborn",
    "title": "Heatmap",
    "content": ". | 변수간 상관계수 시각화 . exam_df.head() . |   | gender | race/ethnicity | parental level of education | lunch | test preparation course | math score | reading score | writing score | . | 0 | female | group B | bachelor’s degree | standard | none | 72 | 72 | 74 | . | 1 | female | group C | some college | standard | completed | 69 | 90 | 88 | . | 2 | female | group B | master’s degree | standard | none | 90 | 95 | 93 | . | 3 | male | group A | associate’s degree | free/reduced | none | 47 | 57 | 44 | . | 4 | male | group C | some college | standard | none | 76 | 78 | 75 | . → DatFrame의 corr() 메소드를 사용해 숫자형 변수간의 상관계수 확인 . exam_df.corr() . |   | math score | reading score | writing score | . | math score | 1 | 0.81758 | 0.802642 | . | reading score | 0.81758 | 1 | 0.954598 | . | writing score | 0.802642 | 0.954598 | 1 | . → Seaborn의 heatmap() 메소드를 사용해 상관계수를 한눈에 시각화 . sns.heatmap(exam_df.corr()) . +) annot=True 옵션을 추가해주면 숫자도 함께 확인 가능 . sns.heatmap(exam_df.corr(), annot=True) . | 연도별, 월별 항공기 승객수 추이를 Heatmap으로 시각화 . ## Seaborn 제공 flights 데이터셋 (연도 및 월별 항공기 승객수를 기록한 데이터셋) flights = sns.load_dataset('flights') flights.head() . |   | year | month | passengers | . | 0 | 1949 | Jan | 112 | . | 1 | 1949 | Feb | 118 | . | 2 | 1949 | Mar | 132 | . | 3 | 1949 | Apr | 129 | . | 4 | 1949 | May | 121 | . → 행: 연도, 열: 월 기준으로 데이터 재구성 . flights_pivot = flights.pivot('month', 'year', 'passengers') flights_pivot.head() . | month | 1949 | 1950 | 1951 | 1952 | 1953 | 1954 | 1955 | 1956 | 1957 | 1958 | 1959 | 1960 | . | Jan | 112 | 115 | 145 | 171 | 196 | 204 | 242 | 284 | 315 | 340 | 360 | 417 | . | Feb | 118 | 126 | 150 | 180 | 196 | 188 | 233 | 277 | 301 | 318 | 342 | 391 | . | Mar | 132 | 141 | 178 | 193 | 236 | 235 | 267 | 317 | 356 | 362 | 406 | 419 | . | Apr | 129 | 135 | 163 | 181 | 235 | 227 | 269 | 313 | 348 | 348 | 396 | 461 | . | May | 121 | 125 | 172 | 183 | 229 | 234 | 270 | 318 | 355 | 363 | 420 | 472 | . → Heatmap으로 시각화 . sns.heatmap(flights_pivot, annot=True, fmt='d', cmap=\"YlGnBu\") # annot=True로 하면 안에 수치가 함께 쓰여짐 # fmt(format는 안에 쓰여지는 수치의 형태를 결정. 'd'는 정수형. '.1f'는 소숫점 한자리까지 # cmap으로 colormap 변경 가능 . | 해가 지날수록 여름 중심으로 승객수가 점점 많아짐을 한눈에 확인 가능. | . | . ",
    "url": "https://chaelist.github.io/docs/visualization/seaborn/#heatmap",
    "relUrl": "/docs/visualization/seaborn/#heatmap"
  },"203": {
    "doc": "Seaborn",
    "title": "Pairplot",
    "content": ": 3차원 이상의 실수형 데이터를 다각도로 살펴볼 수 있음 . ## Seaborn 제공 iris 데이터셋 iris = sns.load_dataset('iris') iris.head() . |   | sepal_length | sepal_width | petal_length | petal_width | species | . | 0 | 5.1 | 3.5 | 1.4 | 0.2 | setosa | . | 1 | 4.9 | 3 | 1.4 | 0.2 | setosa | . | 2 | 4.7 | 3.2 | 1.3 | 0.2 | setosa | . | 3 | 4.6 | 3.1 | 1.5 | 0.2 | setosa | . | 4 | 5 | 3.6 | 1.4 | 0.2 | setosa | . → 4개 변수(꽃잎/꽃받침의 너비/폭) 중 어느 변수가 종을 구분하는 데 도움이 될 지 pariplot으로 확인 . sns.pairplot(iris, hue='species') plt.show() . ",
    "url": "https://chaelist.github.io/docs/visualization/seaborn/#pairplot",
    "relUrl": "/docs/visualization/seaborn/#pairplot"
  },"204": {
    "doc": "데이터 조회 심화",
    "title": "데이터 조회 심화",
    "content": ". | 집계/산술 함수 . | 집계 함수 | 산술 함수 | . | 문자열 가공 함수 | Null값 다루기 &amp; 중복 제거 . | COALESCE(): NULL값 대체 | DISTINCT(): 중복 제거 확인 | . | 컬럼 간 계산/연결 | CASE: 조건문으로 가공 | GROUP BY: 그루핑해서 보기 . | HAVING | WITH ROLLUP | . | SELECT문: 각 절의 작성 순서 | . ",
    "url": "https://chaelist.github.io/docs/sql/select_advanced/",
    "relUrl": "/docs/sql/select_advanced/"
  },"205": {
    "doc": "데이터 조회 심화",
    "title": "집계/산술 함수",
    "content": "집계 함수 . : 특정 칼럼의 여러 row 값들을 동시에 고려해서 실행되는 함수. 아래 함수들은 계산할 때 null값은 고려하지 않고 계산된다. (null값은 제외하고 계산) . | COUNT -- email 칼럼에 존재하는 null을 제외한 값의 수를 구해준다 SELECT COUNT(email) FROM tablename; . | 다만, 특정 칼럼에 COUNT()를 적용하면 null값이 제외된 개수를 구해주기 때문에, 전체 테이블의 row 수를 구해주려면 아래와 같이 전체에 COUNT()를 적용해주는 것이 좋다 SELECT COUNT(*) FROM tablename; -- 전체 테이블의 row 수 구하기 . | . | MAX -- 키의 최댓값 구하기 SELECT MAX(height) FROM tablename; . | +) GREATEST(칼럼1, 칼럼2, 칼럼3): 각 row에서, 가장 큰 값을 가진 칼럼의 값을 찾아줌. (cf. MAX는 특정 칼럼 내의 최댓값을 계산) | . | MIN -- 키의 최솟값 구하기 SELECT MIN(height) FROM tablename; . | +) LEAST(칼럼1, 칼럼2, 칼럼3): 각 row에서, 가장 작은 값을 가진 칼럼의 값을 찾아줌. (cf. MIN는 특정 칼럼 내의 최솟값을 계산) | . | AVG -- 키의 평균값 구하기 SELECT AVG(height) FROM tablename; . | SUM -- 나이의 총합 구하기 SELECT SUM(age) FROM tablename; . | STD - 나이의 표준편차 구하기 SELECT STD(age) FROM tablename; . | . 산술 함수 . : 특정 칼럼의 각 row마다 실행되는 함수 . | ABS: 절대값을 구하는 함수 | SQRT: 제곱근을 구하는 함수 | CEIL: 올림을 해주는 함수 . | ex) 178.4라는 데이터 → CEIL(height)로 출력하면 179라고 출력됨 | . | FLOOR: 내림을 해주는 함수 | ROUND: 반올림을 해주는 함수 . | ex) ROUND(값, 1)이라고 하면 소수점 첫번째 자리까지 반올림해서 출력 | +) ROUND는 소수점 둘째자리의 수를 기반으로 반올림을 해주는 반면, TRUNCATE(값, 1)이라고 하면 소수점 첫번째 자리까지 내림을 해서 출력해준다 (그냥 단순하게 둘째자리부터는 버림) | . | POW: 제곱을 해주는 함수 . | ex) POW(값, 2)라고 하면 제곱, POW(값, 3)이라고 하변 세제곱 | . | . +) 그 외 다양한 산술 함수: https://dev.mysql.com/doc/refman/8.0/en/mathematical-functions.html . ",
    "url": "https://chaelist.github.io/docs/sql/select_advanced/#%EC%A7%91%EA%B3%84%EC%82%B0%EC%88%A0-%ED%95%A8%EC%88%98",
    "relUrl": "/docs/sql/select_advanced/#집계산술-함수"
  },"206": {
    "doc": "데이터 조회 심화",
    "title": "문자열 가공 함수",
    "content": ". | SUBSTRING(칼럼, m, n): 특정 칼럼의 m번째 위치부터 n개의 글자만 추출 SELECT SUBSTRING(address, 3, 3) FROM tablename; -- '서울특별시 중구 서소문로 00'와 같은 주소 → '특별시'라고 출력됨 (3번째 위치에서부터 3개의 글자만 추출) . | LEFT, RIGHT: 좌우 몇 개의 글자만 잘라서 추출해주는 함수 . -- address 중 왼쪽으로부터 2개의 글자만 잘라서 추출 (ex. '서울특별시 신촌로'라면 '서울'만 추출) SELECT LEFT(address, 2) FROM tablename; . -- address 중 오른쪽으로부터 3개의 글자만 잘라서 추출 (ex. '서울특별시 신촌로'라면 '신촌로'만 추출) SELECT RIGHT(address, 3) FROM tablename; . | LENGTH(): 문자열의 길이를 구해줌 SELECT LENGTH(address) FROM tablename; -- `안드로메다 128행성`이라는 주소 → LENGHT()의 결과는 25 . | ※ 다만, LENGTH() 함수는 문자의 Byte길이를 가져오기 때문에 한글의 경우 정확한 길이를 알기 어렵다 | 대신 CHAR_LENGTH() 함수를 사용하면 몇 개의 글자가 사용되었는지 파악 가능. (공백도 포함해 계산) SELECT CHAR_LENGTH(address) FROM tablename; -- `안드로메다 128행성`이라는 주소 → CHAR_LENGTH()의 결과는 11 . | . | UPPER, LOWER -- 모든 문자를 대문자로 변환 SELECT UPPER(address) FROM tablename; . -- 모든 문자를 소문자로 변환 SELECT LOWER(address) FROM tablename; . | LPAD, RPAD: 문자열의 왼쪽(LPAD) 또는 오른쪽(RPAD)을 특정 문자열로 채워주는 함수 (*보통 어떤 숫자의 자리수를 맞출 때 자주 사용) . -- age 컬럼의 값을, 왼쪽에 문자 0을 붙여서 총 10자리로 만들어 준다 SELECT email, LPAD(age, 10, '0') FROM tablename; -- ex) 28 → 0000000028, 101 → 0000000101 . -- age 컬럼의 값을, 오른쪽에 !를 붙여서 총 5자리로 만들어 준다 SELECT email, RPAD(age, 5, '!') FROM tablename; -- ex) 28 → 28!!!, 101 → 101!! . | ※ 사실 age 칼럼은 INT 타입이지만, 숫자도 문자열 함수 안에 인자로 넣어주면 그 값이 자동으로 문자열로 형 변환이 되어 계산된다 | . | TRIM, LTRIM, RTRIM: 문자열 양 끝에 존재하는 공백을 제거하는 함수 . | LTRIM(word): 왼쪽 공백 삭제 | RTRIM(word): 오른쪽 공백 삭제 | TRIM(word): 왼쪽, 오른쪽 양쪽 공백 모두 삭제 | . | REPLACE(칼럼, ‘문자1’, ‘문자2’): 특정 칼럼에서 문자1을 문자2로 모두 대체해줌 -- address에서 '서울'이라는 모든 문자를 '부산'으로 변경해서 출력 SELECT REPLACE(address, '서울', '부산') FROM tablename; . | +) INT 타입에도 그대로 적용 가능: -- 정수형 타입인 'salary'에서 숫자 중간 중간에 들어가는 0을 모두 삭제해서 출력 SELECT REPLACE(salary, '0', '') FROM tablename; . | . | . ",
    "url": "https://chaelist.github.io/docs/sql/select_advanced/#%EB%AC%B8%EC%9E%90%EC%97%B4-%EA%B0%80%EA%B3%B5-%ED%95%A8%EC%88%98",
    "relUrl": "/docs/sql/select_advanced/#문자열-가공-함수"
  },"207": {
    "doc": "데이터 조회 심화",
    "title": "Null값 다루기 &amp; 중복 제거",
    "content": ". | Null값만 / Null값 제외하고 출력 . | NULL은 어떤 값도 아니기에, = NULL, != NULL 이렇게는 표현할 수 없고, IS NULL 혹은 IS NOT NULL이라고 표현해야 함 | . -- address 칼럼이 null값인 데이터만 조회 SELECT * FROM tablename WHERE address IS NULL; . -- address 칼럼이 null값이 아닌 데이터만 조회 SELECT * FROM tablename WHERE address IS NOT NULL; . -- address, height, weight 세 칼럼 중 하나라도 null값이 있는 데이터 조회 SELECT * FROM tablename WHERE address IS NULL OR height IS NULL OR weight IS NULL; . | . COALESCE(): NULL값 대체 . | COALESCE(값1, 값2): null이 아니면 값1을, null이면 값2를 돌려줌 SELECT COALESCE(height, '####'), COALESCE(weight, '---'), COALESCE(address, '@@@') FROM tablename; -- height 칼럼의 null 부분은 ####가, weight 칼럼은 ---, address는 @@@가 출력됨 . | . +) NULL값을 변환하는 다양한 방법 . | COALESCE(height, 'N/A') – height 칼럼의 null값을 ‘N/A’라는 text로 채워줌 | COALESCE(height, weight * 2.3, 'N/A') – height 칼럼의 null값을 우선 weight에 2.3을 곱한 값으로 넣어주고, 만약 weight 칼럼도 NULL이면 ‘N/A’로 채워줌 | IFNULL(height, 'N/A') – height 칼럼의 null값을 ‘N/A’라는 text로 채워줌 . | COALESCE(height, ‘N/A’)와 같지만, IFNULL은 mysql에서만 지원하는 함수이고 COALESCE는 SQL 표준 함수이다 | . | IF(height IS NOT NULL, height, 'N/A') – height가 NULL이 아니면 height값을 return하고, NULL이면 ‘N/A’라는 text를 return . | IF(조건식, 결과1, 결과2): 조건식이 True면 결과1을, False면 결과2를 return하라는 구문 | IF 구문은 NULL값 반환 이외의 다양한 조건에도 응용할 수 있다 | . | . DISTINCT(): 중복 제거 확인 . | DISTINCT 함수: Unique한 값만 확인하게 해주는 함수 -- gender 칼럼의 고유값만 확인 SELECT DISTINCT(gender) FROM tablename; . -- address 칼럼의 가장 앞 두 글자(ex. 경기, 서울, 전라)만 추출해서 고유값만 확인 SELECT DISTINCT(SUBSTRING(address, 1, 2)) FROM tablename; . | +) 고유값 개수 구하기 -- gender 칼럼에 있는 고유값의 개수를 세어줌 SELECT COUNT(DISTINCT(gender)) FROM tablename; . | . ",
    "url": "https://chaelist.github.io/docs/sql/select_advanced/#null%EA%B0%92-%EB%8B%A4%EB%A3%A8%EA%B8%B0--%EC%A4%91%EB%B3%B5-%EC%A0%9C%EA%B1%B0",
    "relUrl": "/docs/sql/select_advanced/#null값-다루기--중복-제거"
  },"208": {
    "doc": "데이터 조회 심화",
    "title": "컬럼 간 계산/연결",
    "content": ". | 컬럼 산술연산: + (더하기), - (빼기), * (곱하기), / (나누기), % (나머지 구하기) -- 이메일, 키, 몸무게, BMI (몸무게(kg) / 키(m)^2)를 출력 -- BMI 계산할 때 키는 보통 m단위로 넣으므로 100을 나눠서 제곱해줌 SELECT email, height, weight, weight / ((height/100) * (height/100)) FROM tablename; . | 연산시, 한 컬럼이라도 NULL이면 계산식의 결과도 NULL | . | alias 붙이기: ‘AS’ 사용 . | 원래의 컬럼 이름을 다른 이름으로 교체해서 보여주는 것. | 단순히 계산식만 보이는 것보다, 보다 직관적인 alias를 붙여서 보여주면 이해하기 쉽다 | SELECT column AS alias와 같은 식으로 AS를 사용하거나, SELECT column alias와 같이 빈칸을 한 칸 두고 뒤에 써줘도 된다 (하지만 AS를 쓰는 습관을 들이면 조금 더 직관적으로 이해하기 쉬운 코드가 된다) | . SELECT email, height, weight, weight / ((height/100) * (height/100)) AS BMI FROM tablename; -- 이렇게 적으면 weight / ((height/100) * (height/100)) 계산 결과 칼럼은 'BMI'라는 이름으로 출력됨 . | CONCAT: 여러 컬럼을 원하는 형식으로 연결해 출력 -- '165.5cm, 67.3kg'와 같은 형식으로, height와 weight 컬럼이 결합되어 출력됨 SELECT CONCAT(height, 'cm, ', weight, 'kg') AS '키와 몸무게' FROM tablename; . | cf) ‘키와 몸무게’처럼 alias에 빈칸이 있을 경우, ‘‘나 ““로 감싸서 구분해줘야 한다 | . | . ",
    "url": "https://chaelist.github.io/docs/sql/select_advanced/#%EC%BB%AC%EB%9F%BC-%EA%B0%84-%EA%B3%84%EC%82%B0%EC%97%B0%EA%B2%B0",
    "relUrl": "/docs/sql/select_advanced/#컬럼-간-계산연결"
  },"209": {
    "doc": "데이터 조회 심화",
    "title": "CASE: 조건문으로 가공",
    "content": ". | 단순 CASE 함수: CASE 뒤에 컬럼을 적고, 그 컬럼의 값과 어떤 특정한 값이 같은지(=) 비교 . | 구조: (WHEN ~ THEN ~ 은 원하는 만큼 사용 가능) CASE 칼럼 이름 WHEN 값 THEN 출력할 내용 ELSE 출력할 내용 END . | 예시: -- age 값이 29면 '스물 아홉'으로, 30이면 '서른'으로 바꿔서 출력 SELECT email, CASE age WHEN 29 THEN ‘스물 아홉’ WHEN 30 THEN ‘서른’ ELSE age END FROM tablename; . | . | 검색 CASE 함수: 특정 조건에 따라 특정 값을 돌려준다. (단순 CASE 함수보다 다양한 연산이 가능) . | 구조: (WHEN ~ THEN ~ 은 원하는 만큼 사용 가능) . CASE WHEN 조건 THEN 출력할 내용 ELSE 출력할 내용 END . | 예시: . SELECT CONCAT(height, 'cm, ', weight, 'kg') AS '키와 몸무게', weight / ((height/100) * (height/100)) AS BMI, (CASE WHEN weight IS NULL or height IS NULL THEN '비만 여부 알 수 없음' WHEN weight / ((height/100) * (height/100)) &gt;= 25 THEN '과체중' WHEN weight / ((height/100) * (height/100)) &gt;= 18.25 AND weight / ((height/100) * (height/100)) &lt; 25 THEN '정상' ELSE '저체중' END) AS obesity_check FROM tablename ORDER BY obesity_check ASC; . | BMI에 따라 ‘과체중’ , ‘정상’, ‘저체중’으로 나누어 결과를 보여주는 칼럼을 새로 생성해 출력 | +) CASE의 경우, alias를 붙여주는 게 좋다 (이 경우는 obesity_check라는 이름으로 출력됨) | +) ORDER BY obesity_check ASC도 붙여줬으므로 obesity_check 칼럼을 기준으로 오름차순 정렬되어 출력 | . | . +) 위 SQL문을 서브쿼리를 활용해 더 간결하게 표현하기: . SELECT email, BMI, (CASE WHEN weight IS NULL OR height IS NULL THEN '비만 여부 알 수 없음' WHEN BMI &gt;= 25 THEN '과체중' WHEN BMI &gt;= 18.5 AND BMI &lt; 25 THEN '정상' ELSE '저체중' END) AS obesity_check FROM (SELECT *, weight/((height/100) * (height/100)) AS BMI FROM tablename) AS subquery_for_BMI ORDER BY obesity_check ASC; . ",
    "url": "https://chaelist.github.io/docs/sql/select_advanced/#case-%EC%A1%B0%EA%B1%B4%EB%AC%B8%EC%9C%BC%EB%A1%9C-%EA%B0%80%EA%B3%B5",
    "relUrl": "/docs/sql/select_advanced/#case-조건문으로-가공"
  },"210": {
    "doc": "데이터 조회 심화",
    "title": "GROUP BY: 그루핑해서 보기",
    "content": ". | 집계함수(COUNT, AVG, MIN, MAX 등)과 함께 사용 . | ※규칙: GROUP BY를 사용할 때는, SELECT 절에 (1) GROUP BY 뒤에 나오는 칼럼들 또는 (2) 집계함수만 쓸 수 있다! | . -- 각 gender별 회원 수 조회 (m: 15, f: 9) ex) SELECT gender, COUNT(*) FROM tablename GROUP BY gender; -- 각 gender별 평균 키를 조회 (m: 176.55, f:173.7) ex) SELECT gender, AVG(height) FROM tablename GROUP BY gender; -- 각 gender별 최소 몸무게를 조회 (m: 58, f: 47) SELECT gender, MIN(weight) FROM tablename GROUP BY gender; . | 기존 컬럼이 아닌, 새로운 기준을 만들어 그루핑하는 것도 가능 -- 지역별(ex. 서울, 경기, 대전) 회원 수 확인 -- address의 맨 첫글자 2개만 추출해서 활용 (ex. 서울) SELECT SUBSTRING(address, 1, 2) AS region, COUNT(*) FROM tablename GROUP BY SUBSTRING(address, 1, 2); . | 2가지 이상의 기준으로도 그루핑 가능 -- 지역별*성별 회원 수 확인 SELECT SUBSTRING(address, 1, 2) AS region, gender, COUNT(*) FROM tablename GROUP BY SUBSTRING(address, 1, 2), gender; . | 그루핑을 하고 나면, 깔끔하게 정렬을 해주는 게 좋다 SELECT SUBSTRING(address, 1, 2) as region, gender, COUNT(*) FROM tablename GROUP BY region, -- MySQL의 경우, GROUP BY 뒤에 바로 alias를 활용하는 것도 가능 gender ORDER BY region ASC, gender DESC ; . | . HAVING . : 그루핑 후, 특정 조건의 데이터만 확인 . SELECT SUBSTRING(address, 1, 2) as region, gender, COUNT(*) FROM tablename GROUP BY region, gender HAVING region = '서울' ORDER BY region ASC, gender DESC; -- → 이렇게 하면 region이 서울인 데이터만 조회됨 -- MySQL의 경우, HAVING 뒤에 바로 alias를 활용 가능 . ※ WHERE와 HAVING의 차이: . | WHERE는 테이블에서 맨 처음 row들을 조회할 때 조건을 설정하기 위한 구문이고, | HAVING은 이미 조회된 row들을 그루핑했을 때 그 그룹들 내에서 다시 필터링할 때 쓰는 구문 | . WITH ROLLUP . : 부분 총계를 포함해 그루핑하기 . SELECT SUBSTRING(address, 1, 2) as region, gender, COUNT(*) FROM tablename GROUP BY region, gender WITH ROLLUP ORDER BY region ASC, gender DESC; . | 이렇게 WITH ROLLUP이 들어가면, 세부 그룹들을 좀 더 큰 단위의 그룹으로 중간중간에 합쳐준다. | ex) 성*연령 교차 데이터라고 하면, 그냥 그루핑하면 ‘여성-10대’, ‘여성-20대’ 등의 그룹만 생성되는 한편, WITH ROLLUP이 들어가면 ‘NULL-10대’ (성별전체-10대를 의미) 이런 식으로 부분총계 값 생성됨 | . | WITH ROLLUP은 GROUP BY 뒤에 나오는 그루핑 기준의 순서에 맞춰서 계층적인 부분 총계를 보여줌 . | ex) GROUP BY gender, age 순서면 ‘여성-NULL’(=여성-나이전체)는 출력되지만, ‘NULL-여성’(=나이전체-여성)은 출력되지 않는다 | . | . +) ‘Null값을 나타내는 NULL’과 ‘WITH ROLLUP의 결과로 부분 총계를 표현하기 위해 쓰인 NULL’ 구분: . | GROUPING() 함수 사용: . | 그 인자를 그루핑 기준에서 고려하지 않은 부분 총계인 경우에 1을 리턴, 그렇지 않은 경우 0을 리턴. (같은 NULL이더라도 원래 NULL이 있던 곳은 0이 출력되고, 부분 총계를 나타내기 위해 NULL이 쓰인 곳은 1이 출력됨) | . SELECT YEAR(sign_up_day) AS s_year, gender, SUBSTRING(address, 1, 2) as region, GROUPING(YEAR(sign_up_day)), GROUPING(gender), GROUPING(SUBSTRING(address, 1, 2)), COUNT(*) FROM tablename GROUP BY s_year, gender, region WITH ROLLUP ORDER BY s_year DESC; . | . ",
    "url": "https://chaelist.github.io/docs/sql/select_advanced/#group-by-%EA%B7%B8%EB%A3%A8%ED%95%91%ED%95%B4%EC%84%9C-%EB%B3%B4%EA%B8%B0",
    "relUrl": "/docs/sql/select_advanced/#group-by-그루핑해서-보기"
  },"211": {
    "doc": "데이터 조회 심화",
    "title": "SELECT문: 각 절의 작성 순서",
    "content": "(+. 해석/실행되는 순서도 별도로 표시) . | SELECT: 모든 컬럼 또는 특정 컬럼들을 조회 (해석 순서: 5) . | SELECT 절에서 컬럼 이름에 alias를 붙인 게 있다면, 이 이후 단계(ORDER BY, LIMIT)부터는 해당 alias를 사용할 수 있다 | MySQL에서는 GROUP BY나 HAVING에서도 SELECT절에서 붙인 alias를 사용 가능 | . | FROM: 어느 테이블을 대상으로 할 지 결정 (해석 순서: 1) | WHERE: 해당 테이블에서 특정 조건(들)을 만족하는 row들만 선별 (해석 순서: 2) | GROUP BY: row들을 그루핑 기준에 따라 그루핑 (해석 순서: 3) | HAVING: 그루핑 작업 후 생성된 여러 그룹 중에서, 특정 조건(들)을 만족하는 그룹만 선별 (해석 순서: 4) | ORDER BY: 각 row를 특정 기준에 따라 정렬 (해석 순서: 6) | LIMIT: 이전 단계까지 조회된 row들 중 일부 row만 선별해서 출력 (해석 순서: 7) | . +) 그 외 작성 순서: https://dev.mysql.com/doc/refman/8.0/en/select.html . ",
    "url": "https://chaelist.github.io/docs/sql/select_advanced/#select%EB%AC%B8-%EA%B0%81-%EC%A0%88%EC%9D%98-%EC%9E%91%EC%84%B1-%EC%88%9C%EC%84%9C",
    "relUrl": "/docs/sql/select_advanced/#select문-각-절의-작성-순서"
  },"212": {
    "doc": "데이터 조회 기초",
    "title": "데이터 조회 기초",
    "content": ". | SQL 기초 . | DB, 테이블 구조 파악하기 | SQL문 작성 형식 | SELECT, FROM | . | WHERE: 조건 걸기 . | 조건 표현 방식 | AND, OR 연산 | . | DATE 데이터 타입 다루기 | ORDER BY: 데이터 정렬 . | CAST(data AS 타입) | . | LIMIT: 데이터 일부만 출력 | . ",
    "url": "https://chaelist.github.io/docs/sql/select_basics/",
    "relUrl": "/docs/sql/select_basics/"
  },"213": {
    "doc": "데이터 조회 기초",
    "title": "SQL 기초",
    "content": "DB, 테이블 구조 파악하기 . | 존재하는 데이터베이스 파악 SHOW DATABASES; . | information_schema / mysql / performance_schema / sys는 MySQL이라는 DBMS의 구동을 위해 존재하는 기본 데이터베이스들 | . | 존재하는 테이블 파악 SHOW FULL TABLES IN 데이터베이스명; . | 각 테이블의 이름과 함께 Table_type을 파악할 수 있다 (BASE TABLE / VIEW) | cf) 그냥 SHOW TABLES라고만 하면 type 정보 없이 각 테이블의 이름만 출력됨 | . | 테이블의 구조 파악 DESCRIBE 테이블명; . | 테이블의 컬럼 구조, 각 컬럼의 데이터 타입, 속성을 확인 가능 | DESC 테이블명;이라고 줄여서 입력해도 동일 | . ex)   . | Field : 컬럼의 이름 | Type : 컬럼의 데이터 타입 | Null : 컬럼의 Null 속성 유무 | Key : Primary Key, Unique 속성 여부 | Default : 컬럼의 기본값 | Extra : AUTO_INCREMENT 등의 기타 속성 | . | Foreign Key 관계 파악 . | Foreign Key 관계를 파악해두면, 데이터베이스가 어떻게 설계되었는지, 각 데이터가 어떤 관계를 갖는지 파악하는 데에 도움이 된다 | 아래는 MySQL에서 사용되는 코드. Foreign Key 정보를 조회하는 SQL문은 DBMS마다 차이가 있으므로 유의! | . SELECT i.TABLE_SCHEMA, i.TABLE_NAME, i.CONSTRAINT_TYPE, i.CONSTRAINT_NAME, k.REFERENCED_TABLE_NAME, k.REFERENCED_COLUMN_NAME FROM information_schema.TABLE_CONSTRAINTS i LEFT JOIN information_schema.KEY_COLUMN_USAGE k ON i.CONSTRAINT_NAME = k.CONSTRAINT_NAME WHERE i.CONSTRAINT_TYPE = 'FOREIGN KEY'; . | . SQL문 작성 형식 . | SQL 문 끝에는 항상 세미콜론(;)을 써줘야한다 | SQL 문 안에는 공백/줄바꿈 등을 자유롭게 넣을 수 있다 (실행에는 영향 없음. 가독성을 위한 것) | SELECT, FROM 등 기본 예약어들은 대문자로 쓰는 게 좋다 (대소문자 여부는 실행에는 영향 없음.) | 사용할 데이터베이스를 명시해주는 방법: . | 데이터 조회시 SELECT * FROM dbname.tablename 이렇게 데이터베이스명도 함께 적어준다 | 왼쪽 SCHEMAS 패널 부분에서 사용할 데이터베이스를 클릭해 활성화해둔다 | SQL문 맨 처음에 USE dbname;이라고 명시해두고 시작한다 | . | . SELECT, FROM . : 보통 SELECT 대상 FROM 테이블명의 구조가 SQL 데이터 조회의 기초 . | SELECT: 테이블의 어떤 칼럼을 조회할지 . | *: 해당 테이블의 모든 칼럼을 다 조회하겠다는 의미 SELECT * FROM tablename; . | 특정 칼럼명을 넣어주면 해당 칼럼들만 조회 SELECT column1, column2 FROM tablename; . | . | FROM: 어떤 테이블의 데이터를 조회할지 . | 데이터베이스명.테이블명의 구조로 적어줘도 되고, (같은 테이블명이 여러 DB에 존재한다면 이런 형태로 적어주는 게 좋다) SELECT * FROM dbname.tablename; . | 어떤 데이터베이스를 사용하는지 명확한 경우라면 테이블명만 적어줘도 된다 SELECT * FROM tablename . | . | . ",
    "url": "https://chaelist.github.io/docs/sql/select_basics/#sql-%EA%B8%B0%EC%B4%88",
    "relUrl": "/docs/sql/select_basics/#sql-기초"
  },"214": {
    "doc": "데이터 조회 기초",
    "title": "WHERE: 조건 걸기",
    "content": ". | WHERE로 특정 조건의 데이터만 조회할 수 있다 | . 조건 표현 방식 . | 기초 . | =: 같음 -- ‘age’ 칼럼의 값이 27인 행(row)만 조회 SELECT * FROM tablename WHERE age = 27; . | !=, &gt;&lt;: 같지 않음 -- 성별이 ‘m’이 아닌 = 여성인 회원 데이터만 조회 SELECT * FROM tablename WHERE gender != ‘m’; . | IN: ( ) 안의 여러 값들 중 해당하는 값이 있는 row들만 선택 -- 나이가 20이나 30인 회원 데이터만 조회 ex) SELECT * FROM tablename WHERE age IN (20, 30); . | NOT IN: ( ) 안의 여러 값들 중 어느 것과도 일치하지 않는 row만 선택 -- 나이가 20이나 30이 아닌 회원 데이터만 조회 ex) SELECT * FROM tablename WHERE age NOT IN (20, 30); . | . | 숫자형 칼럼 (INT, DOUBLE 등) . | 부등호 이용 (&gt;, &lt;, &gt;=, &lt;=) -- 나이가 27 이상인 회원 데이터만 조회 SELECT * FROM tablename WHERE age &gt;= 27; . | BETWEEN a AND b: a와 b 사이의 값을 갖는 데이터만 조회 (양 끝(a, b)도 포함) -- 30대 회원 데이터만 조회 SELECT * FROM tablename WHERE age BETWEEN 30 AND 39; . | NOT을 붙이면 해당 조건을 제외하고 조회됨 -- 30대가 아닌 회원 데이터만 조회 SELECT * FROM tablename WHERE age NOT BETWEEN 30 AND 39; . | . | 날짜형 칼럼 (DATE, DATETIME 등) . | 숫자형처럼, 부등호나 BETWEEN 사용 가능 SELECT * FROM tablename WHERE sign_up_day &gt; '2019-01-01'; . SELECT * FROM tablename WHERE sign_up_day BETWEEN '2018-01-01' AND '2018-12-31'; . | . | 문자형 칼럼 (TEXT 등) . | LIKE ‘패턴’ 매칭 -- address가 ‘서울’로 시작하는 모든 행과 매칭 SELECT * FROM tablename WHERE address LIKE '서울%'; . -- address에 ‘고양시’가 포함되어 있는 모든 행과 매칭 SELECT * FROM tablename WHERE address LIKE '%고양시%'; . | +) NOT LIKE 매칭도 가능 -- address가 ‘서울’로 시작하지 않는 모든 행과 매칭 SELECT * FROM tablename WHERE address NOT LIKE '서울%'; . | %: 임의의 길이를 가진 문자열을 나타냄 (0자도 포함) | _: 문자 하나를 의미 -- 이메일 주소가 c로 시작하고, 그 뒤에 임의의 문자 5개가 온 후, @가 붙고 그 뒤에 무언가 임의의 길이를 가진 문자열이 붙는다는 의미 SELECT * FROM tablename WHERE email LIKE ‘c_____@%’; -- candy@google.com 등의 이메일 주소가 해당 패턴과 매칭됨 . | . +) 문자형조회 Escaping: \\(backslash)를 사용 . | ex) \\% – 실제 %(퍼센트 기호)를 포함한 문자를 찾을 수 있음 | ‘(작은 따옴표), _(언더바) ,”(큰 따옴표)도 \\를 활용해 escape | . +) 대소문자 구분해서 조회하기 . | Table collation 설정이 ci(Case Insensitive) 설정으로 되어 있으면, 문자형 데이터를 조회할 때 g나 G가 똑같이 간주됨. | 이럴 때 g와 G를 구분해서 조회하고 싶으면, BINARY라고 써주면 된다 -- 소문자 g가 포함된 문자열만 조회 SELECT * FROM tablename WHERE sentence LIKE BINARY ‘%g%’; . -- 대문자 G가 포함된 문자열만 조회 ex) SELECT * FROM tablename WHERE sentence LIKE BINARY ‘%G%’; . | . | ANY, ALL . | ANY: 여러 조건 중 하나라도 만족되면 TRUE가 반환됨 (‘SOME’을 사용해도 동일) SELECT * FROM tablename WHERE view_count &gt; ANY(150000, 250000, 300000); -- 150000, 250000, 300000 중 하나보다 큰 view_count를 가지고 있으면 출력됨 -- (사실 이 경우는 그냥 MIN을 사용하는 것과 동일) . | ALL: 여러 조건 모두가 만족되면 TRUE가 반환됨 SELECT * FROM tablename WHERE view_count &gt; ALL(150000, 250000, 300000); -- 150000, 250000, 300000 중 모두보다 큰 view_count를 가지고 있으면 출력됨 -- (사실 이 경우는 그냥 MAX를 사용하는 것과 동일) . | . | . AND, OR 연산 . : 여러 개의 조건 걸기 . | AND . | ex) 남성이면서 주소가 서울로 시작하는 회원 데이터 조회 SELECT * FROM tablename WHERE gender = ‘m’ AND address LIKE ‘서울%; . | ex) 조건 3개를 연결: 남성이면서 주소가 서울이고 나이가 25~29인 회원 데이터 조회 SELECT * FROM tablename WHERE gender = ‘m’ AND address LIKE ‘서울% AND age BETWEEN 25 and 29; . | . | OR . | ex) 봄(3~5월) 혹은 가을(9~11월)에 가입한 회원 데이터 조회 SELECT * FROM tablename WHERE MONTH(sign_up_day) BETWEEN 3 AND 5 OR MONTH(sign_up_day) BEWEEN 9 AND 11; . | 주의사항: WHERE id = 1 OR id = 2와 같이 같은 칼럼에서의 조건을 함께 적는 경우, WHERE id = 1 OR 2 이렇게 적으면 안된다! (두 조건의 칼럼이 같더라도 다 적어줘야 함) | +) 사실, WHERE id = 1 OR id = 2 같은 경우는 WHERE id IN (1, 2) 라고 표현하면 더 짧게 가능 | . | AND와 OR 복합 . | ex) 180 이상인 남자 회원, 또는 170 이상인 여자 회원 데이터 조회 SELECT * FROM tablename WHERE (gender = 'm' AND height &gt;= 180) OR (gender = 'f' AND height &gt;= 170); . | AND와 OR를 섞어서 사용할 경우, 어떤 부분을 먼저 고려해야 하는지 ()로 표시해주는 게 좋다! | ()로 우선순위를 표시하지 않으면, AND가 OR보다 우선순위가 높게 간주된다. (AND 부분이 먼저 실행됨) | 하지만 그냥 AND이든 OR이든 먼저 실행되어야 하는 부분에 ()를 씌워 표시해주는 습관을 들이면 더 직관적으로 이해하기 쉬운 코드가 된다 | . | . ",
    "url": "https://chaelist.github.io/docs/sql/select_basics/#where-%EC%A1%B0%EA%B1%B4-%EA%B1%B8%EA%B8%B0",
    "relUrl": "/docs/sql/select_basics/#where-조건-걸기"
  },"215": {
    "doc": "데이터 조회 기초",
    "title": "DATE 데이터 타입 다루기",
    "content": ". | YEAR, MONTH, DAYOFMONTH, DAYOFWEEK . | YEAR 함수: 날짜에서 연도만 추출 -- 1992년에 태어난 회원 데이터만 조회 SELECT * FROM tablename WHERE YEAR(birthday) = ‘1992’; . | MONTH 함수: 날짜에서 월만 추출 -- 여름(6, 7, 8월)에 가입한 회원 데이터만 조회 SELECT * FROM tablename WHERE MONTH(sign_up_day) IN (6, 7, 8); . | DAYOFMONTH 함수: 날짜에서 일만 추출 -- 각 달의 후반부(15일~31일)에 가입한 회원 데이터만 조회 SELECT * FROM tablename WHERE DAYOFMONTH(sign_up_day) BETWEEN 15 AND 31; . | DAYOFWEEK 함수: 날짜에서 요일을 추출 -- 토요일에 가입한 회원 데이터만 조회 (1: 일요일, 2: 월요일, ..., 7: 토요일) SELECT * FROM tablename WHERE DAYOFWEEK(sign_up_day) = 7; . | . | 날짜 간 차이 구하기: DATEDIFF . | DATEDIFF(날짜 a, 날짜 b) → ‘날짜 a - 날짜 b’를 해서 그 차이 일수를 알려준다 | ex) DATEDIFF(’2018-01-05’, ’2018-01-03’)의 값은 2 -- 가입한 날로부터 2019.01.01까지의 기간을 함께 조회 SELECT email, sign_up_day, DATEDIFF(sign_up_day, ‘2019-01-01’) FROM tablename; . | +) CURDATE(): 오늘 날짜를 구하는 함수 -- 가입한 날로부터 오늘 날짜까지의 기간을 함께 조회 SELECT email, sign_up_day, DATEDIFF(sign_up_day, CURDATE()) FROM tablename; . | +) DATEDIFF(날짜 a, 날짜 b) / 365 이렇게 계산하면 기간을 ‘연(year)’ 단위로 확인할 수 있음 | +) DATEDIFF(날짜 a, 날짜 b)는 날짜 a - 날짜 b와 동일 (어느 날짜가 더 큰지 아는 경우, 단순 빼기로도 계산 가능) | . | DATE_FORMAT(datetime, ‘custom_format’) . | datetime 포맷의 날짜를 원하는 형태로 출력해줌 -- 로그인 날짜를 '2022-03-04'와 같은 포맷으로 출력 SELECT DATE_FORMAT(login_date, '%Y-%m-%d') FROM tablename; . | . | STR_TO_DATE(date_string, ‘assigned_format’) . | 문자열 포맷으로 담긴 날짜를 datetime 포맷으로 변환해준다. 문자열 날짜가 어떤 형태로 담겨있는지 그 형식을 잘 적어줘야 함. -- '20220304'와 같은 형식의 문자열로 담겨 있던 로그인 날짜를 datetime으로 바꿔서 출력 SELECT STR_TO_DATE(login_date_str, '%Y%m%d') FROM tablename; . | . | . ",
    "url": "https://chaelist.github.io/docs/sql/select_basics/#date-%EB%8D%B0%EC%9D%B4%ED%84%B0-%ED%83%80%EC%9E%85-%EB%8B%A4%EB%A3%A8%EA%B8%B0",
    "relUrl": "/docs/sql/select_basics/#date-데이터-타입-다루기"
  },"216": {
    "doc": "데이터 조회 기초",
    "title": "ORDER BY: 데이터 정렬",
    "content": ". | 오름차순 정렬: ASC -- 키가 작은 회원부터 순서대로 출력 SELECT * FROM tablename ORDER BY height ASC; . | +) 아래와 같이 ASC를 안써줘도 ASC가 default이기 때문에 똑같이 오름차순 정렬로 실행된다 (그렇지만 나중에 보고 빨리 이해하기 위해선 ASC를 써주는 게 더 좋음) SELECT * FROM tablename ORDER BY height; . | . | 내림차순 정렬: DESC -- 키가 큰 회원부터 순서대로 출력 SELECT * FROM tablename ORDER BY height DESC; . | +) null값은 가장 작은 값으로 취급됨 | +) TEXT 타입 칼럼의 경우, ASC으로 정렬하면 알파벳 순서대로 정렬됨 . | +) 여러 기준을 두고 정렬: ‘ORDER BY 기준1, 기준2’ 이렇게 쓰면 기준1부터 우선 정렬됨 -- 가입 연도를 기준으로 우선 내림차순 정렬한 후, 가입 연도가 같은 회원들은 이메일 기준으로 오름차순 정렬됨 ex) SELECT sign_up_day, email FROM tablename ORDER BY YEAR(sign_up_day) DESC, email ASC; . | . CAST(data AS 타입) . : 데이터 타입 일시적으로 바꿔주기 . | TEXT 타입 칼럼에 있는 숫자값을 정렬하면, 120 &lt; 19 &lt; 230 &lt; 27의 순서로 오름차순 정렬됨. (TEXT 타입은 INT 타입과 달리, 첫번째 자리부터 한 문자 한문자씩 비교해서 정렬하는 것이기 때문) | TEXT 타입인 칼럼에 있는 숫자값을 숫자형처럼 정렬해주고 싶으면, CAST 함수로 데이터 타입을 일시적으로 변경해주면 된다! SELECT * FROM tablename ORDER BY CAST(data AS signed) ASC; -- 이렇게 써주면 19 &lt; 27 &lt; 120 &lt; 230 이렇게 잘 정렬됨 . | signed: 양수와 음수를 포함한 모든 정수를 나타낼 수 있는 데이터 타입. | +) decimal: 소수점이 있는 수를 나타내는 타입 | . | . ",
    "url": "https://chaelist.github.io/docs/sql/select_basics/#order-by-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%A0%95%EB%A0%AC",
    "relUrl": "/docs/sql/select_basics/#order-by-데이터-정렬"
  },"217": {
    "doc": "데이터 조회 기초",
    "title": "LIMIT: 데이터 일부만 출력",
    "content": ". | ‘LIMIT n’: 맨 처음부터 n개의 데이터만 출력 -- 가장 최근에 가입한 회원 10명의 데이터만 출력 SELECT * FROM tablename ORDER BY sign_up_day DESC LIMIT 10; . | ‘LIMIT m, n’: m+1번째 데이터부터 n개의 데이터만 출력 (0, 1, 2, … , m 이런 식으로 행을 세는 것이라, m은 m+1번째를 의미) -- 가입일자가 가장 최근인 회원부터 정렬된 상태에서, 9번째 &amp; 10번째 row의 데이터만 출력 -- (index가 8, 9인 row) SELECT * FROM tablename ORDER BY sign_up_day DESC LIMIT 8, 2; . | . ",
    "url": "https://chaelist.github.io/docs/sql/select_basics/#limit-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%9D%BC%EB%B6%80%EB%A7%8C-%EC%B6%9C%EB%A0%A5",
    "relUrl": "/docs/sql/select_basics/#limit-데이터-일부만-출력"
  },"218": {
    "doc": "Selenium",
    "title": "Selenium",
    "content": ". | Selenium 기초 . | driver 실행 및 종료 | 웹브라우저 제어하기 | find_element(s) | click() | send_keys() | . | Selenium 추가 기능들 . | 로딩 대기 (Wait) | iframe 속 요소 다루기 | Select 요소 다루기 | Scroll Down (Keys 활용) | Headless 모드로 이용 | . | window(창) 제어 . | 창 크기 조절 및 스크린샷 | 웹 브라우저 창 여러 개 다루기 | . | . *Selenium: 웹 브라우저를 자동화하는 도구. 웹 자동 테스트, 데이터 수집 등에 사용 . ",
    "url": "https://chaelist.github.io/docs/webscraping/selenium/",
    "relUrl": "/docs/webscraping/selenium/"
  },"219": {
    "doc": "Selenium",
    "title": "Selenium 기초",
    "content": "*Selenium 사용을 위한 준비 사항: . | pip이나 conda를 활용해서 install해줘야 한다 | driver를 다운받아야 함 (크롬드라이버 다운) | . driver 실행 및 종료 . | driver 실행 . | 아래 코드대로 실행하면, chromedriver가 켜진다 | . from selenium import webdriver # import해줘야 사용 가능 driver = webdriver.Chrome('driver/chromedriver.exe') ## driver 경로 적어주기 # driver가 같은 파일에 있으면 그냥 이렇게만 써도 된다: `driver = webdriver.Chrome()` . | URL 접속하기 driver.get('https://chaelist.github.io/') # chromedriver가 해당 url에 접속된다 . | driver 종료 . | driver로 필요한 작업을 모두 수행한 후에는 종료하는 것이 좋다 | . driver.quit() ## 창이 하나만 열려 있다면 `driver.close()`이라고 해도 됨 . | . 웹브라우저 제어하기 . | 현재 url 가져오기 driver.current_url . | 뒤로가기(이전 페이지로 이동) driver.back() . | 앞으로가기(이후 페이지로 이동) driver.forward() . | 새로고침하기 driver.refresh() . | . find_element(s) . | find_element 계열은 첫번째로 찾아지는 요소 하나만을 반환 | find_elements 계열은 list 형태로 찾아지는 모든 요소를 반환 | . | find_element(s)_by_css_selector element = driver.find_element_by_css_selector('summary.fs-5') # &lt;summary class=\"fs-5\"&gt; 태그 . | element를 받아와서 클릭하거나, 값을 입력하거나, 필요한 text를 추출하면 된다 element.text ## .text를 해주면 해당 element에 들어있는 text를 추출할 수 있음 . 'Python 기초' . | cf) element.get_attribute('innerHTML')도 element.text와 같이 안에 있는 text를 추출 | cf)element.get_attribute('outerHTML'): 태그와 text를 모두 보여줌 element.get_attribute('outerHTML') . '&lt;summary class=\"fs-5 fw-500\"&gt; Python 기초 &lt;/summary&gt;' . | . +) 일치하는 모든 요소를 list로 받아오기 . element_list = driver.find_elements_by_css_selector('summary.fs-5') [element.text for element in element_list] # list의 각 요소에 접근해 text를 추출 . ['Python 기초', 'Data Handling', 'Numpy', 'Pandas', '데이터 시각화', 'Web Scraping'] . | find_element(s)_by_tag_name element = driver.find_element_by_tag_name('footer') # &lt;footer&gt; 태그 . | find_element(s)_by_class_name element = driver.find_element_by_class_name('main') # &lt;div class=\"main\"&gt; 태그 . | find_element_by_id . | id는 늘 하나뿐이기에, find_element_by_id는 존재하지 않는다 | . element = driver.find_element_by_id('contents') # &lt;h1 id=\"contents\"&gt; 태그 . | find_element(s)_by_xpath . | xpath(XML Path Language)를 통해 특정 element를 접근할 수 있다 | 개발자 모드(ctrl+shift+i)에서 특정 태그의 xpath를 쉽게 복사 가능. | 보통, 복사해 온 xpath를 “\"”xpath””” 이렇게 적어준다 (흔히 xpath 안에 따옴표가 들어가기에, “”” 안에 넣어주는 것) | . element = driver.find_element_by_xpath(\"\"\"//*[@id=\"main-content\"]/footer/p\"\"\") . | . click() . | element.click(): 해당 element를 클릭하는 효과 | . # naver.com에서 [NAVER 로그인] 버튼 누르기 driver = webdriver.Chrome('driver/chromedriver.exe') driver.get('https://www.naver.com/') driver.find_element_by_css_selector(\"a.link_login\").click() . send_keys() . | element.send_keys(): 해당 element에 값을 입력하는 효과 | . # (위 코드에서 연결) ID를 입력하는 부분을 찾아서, 'Your ID'라는 값을 입력하기 driver.find_element_by_css_selector(\"#id\").send_keys('Your ID') . → 네이버 로그인 자동화 코드 FULL . driver = webdriver.Chrome('driver/chromedriver.exe') driver.get('https://www.naver.com/') driver.find_element_by_css_selector(\"a.link_login\").click() # [NAVER 로그인] 링크 누르기 driver.find_element_by_css_selector(\"#id\").send_keys('Your ID') # ID를 입력 driver.find_element_by_css_selector(\"#pw\").send_keys('Your Password') # Password를 입력 driver.find_element_by_id(\"log.login\").click() # [로그인] 버튼 누르기 . ",
    "url": "https://chaelist.github.io/docs/webscraping/selenium/#selenium-%EA%B8%B0%EC%B4%88",
    "relUrl": "/docs/webscraping/selenium/#selenium-기초"
  },"220": {
    "doc": "Selenium",
    "title": "Selenium 추가 기능들",
    "content": "로딩 대기 (Wait) . Selenium은 웹과 직접 상호작용하기 때문에, 웹의 요소들이 로딩될 때까지 기다리지 않으면 error가 날 때가 있다. 웹 요소들이 로딩될 때까지 기다려주는 것을 Wait이라고 한다. | Implicit Wait . | 처음에 한 번 설정해 주면, 찾으려는 웹 요소가 없을 때마다 최대 X초를 암묵적으로 기다려 준다 | 만약 설정한 시간보다 빨리 요소가 찾아지면, 더 기다리지 않고 다음 코드로 넘어간다 | ex) driver.implicitly_wait(3): 웹 요소가 존재할 때까지 (찾아질 때까지) 최대 3초를 기다려준다 | . driver = webdriver.Chrome('driver/chromedriver.exe') driver.implicitly_wait(3) # 이렇게 driver에 맨 처음 한 번만 설정해두면 된다 driver.get('https://www.naver.com/') driver.find_element_by_css_selector(\"a.link_login\").click() # [NAVER 로그인] 버튼 누르기 . | time.sleep() . | time.sleep()도 코드 진행을 멈춰서 로딩을 기다려 줄 수 있는 방법 중 하나. | time.sleep(3)을 코드 중간에 넣어주면, 정확히 3초간 멈췄다가 아래 코드로 넘어간다 | . import time # import해줘야 사용 가능 driver = webdriver.Chrome('driver/chromedriver.exe') driver.get('https://www.naver.com/') time.sleep(3) # 페이지가 로딩되길 3초 동안 기다렸다가 아래 코드로 넘어간다 driver.find_element_by_css_selector(\"a.link_login\").click() # [NAVER 로그인] 버튼 누르기 . ※implicitly_wait(3)은 찾고자 하는 요소가 1초만에 찾아지면 더 기다리지 않고 진행되지만, time.sleep(3)은 반드시 3초를 모두 멈췄다가 진행되기에 implictly_wait(3)이 더 효율적인 경우가 많다 . | Explicit Wait . | 특정 element에 대해 어떤 상태가 될 때까지 최대 얼마나 기다려줄 것인지를 일일이 명시해주는 것. | Implicit Wait과 Explicit Wait을 동시에 사용하는 것은 권장되지 않는다. | . ## 아래 3가지를 추가로 import해줘야 한다 from selenium.webdriver.common.by import By from selenium.webdriver.support.ui import WebDriverWait from selenium.webdriver.support import expected_conditions as EC driver = webdriver.Chrome('driver/chromedriver.exe') driver.get('https://chaelist.github.io/') # &lt;summary class=\"fs-5\"&gt; 태그가 존재할 때까지 최대 5초를 기다려서 element를 받아온다 element = WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.CSS_SELECTOR, \"summary.fs-5\"))) element.click() # 위에서 받아온 element를 클릭 . *다양한 wait조건들* . | presence_of_element_located(): element가 존재할 때까지 기다림 . | 특정 값을 추출해오는 목적이라면 presence만 체크해도 괜찮다 | . | visibility_of_element_located(): element가 실제로 보이는 상태가 될 때까지 기다림 . | element와 interact해서 반응을 체크하는 등의 목적이라면 visibility까지 체크해주는 게 좋다 | . | element_to_be_clickable(): element가 클릭 가능한 상태가 될 때까지 기다림 . | element를 찾아서 클릭하려는 목적이라면 clickable한지 체크해주는 게 좋다 | . | invisibility_of_element_located(): element가 안 보일 때까지 기다림. | text_to_be_present_in_element(): element 안에 텍스트가 로딩될 때까지 기다림. | text_to_be_present_in_element(locator, 확인하고자 하는 text) 이렇게 두 개의 값을 넣어줘야 한다 | . | . → 네이버 로그인 자동화 코드 각 줄에 Explicit Wait 사용해보기 . from selenium import webdriver from selenium.webdriver.common.by import By from selenium.webdriver.support.ui import WebDriverWait from selenium.webdriver.support import expected_conditions as EC driver = webdriver.Chrome('driver/chromedriver.exe') driver.get('https://www.naver.com/') wait = WebDriverWait(driver, 3) # 아래에서 모두 같은 시간(3초)을 설정해줄 거면 이렇게 미리 저장해둬도 된다 # [NAVER 로그인] 링크가 clickable 상태가 될 때까지 최대 3초 기다려서 클릭하기 login_link = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, 'a.link_login'))) login_link.click() # ID 입력창이 visible할 때까지 최대 3초 기다려서 ID 입력하기 id_box = wait.until(EC.visibility_of_element_located((By.CSS_SELECTOR, '#id'))) id_box.send_keys('Your ID') # ID를 입력 # Password 입력창이 visible할 때까지 최대 3초 기다려서 Password 입력하기 password_box = wait.until(EC.visibility_of_element_located((By.CSS_SELECTOR, '#pw'))) password_box.send_keys('Your Password') # [로그인] 버튼이 clickable 상태가 될 때까지 최대 3초 기다려서 클릭하기 login_button = wait.until(EC.element_to_be_clickable((By.ID, 'log.login'))) login_button.click() # [로그인] 버튼 누르기 . | . iframe 속 요소 다루기 . | 태그: HTML 문서 안에 또 다른 HTML 문서를 삽입할 때 사용하는 태그. | 흔히 광고 같은 서드파티 콘텐츠를 하나의 iframe 안에 담는다 | . | iframe 태그는 실제 HTML 문서를 담고 있는 것이 아니라 HTML 문서를 참조하기 때문에, iframe 안에 있는 element가 개발자도구에서는 찾아진다 해도 Selenium으로 찾으려고 하면 실패한다 . ex) naver.com의 아래 ‘트렌드쇼핑’ 영역은 하나의 iframe 안에 담겨 있다 . → li.goods_item (광고 아이템 리스트)은 iframe 안에 들어있기에, 아래와 같이 코드를 쓰면 가져올 수 없다 . # 원하는 결과가 나오지 않는 코드 driver = webdriver.Chrome('driver/chromedriver.exe') driver.get('https://www.naver.com/') driver.implicitly_wait(3) element_list = driver.find_elements_by_css_selector('li.goods_item') [element.text for element in element_list] . [] . | iframe에 있는 코드를 접근하려면 iframe으로 이동해야 한다 – driver.switch_to.frame()을 사용하면 프레임으로 이동하는 것이 가능 . *li.goods_item (광고 아이템 리스트)가 담긴 iframe은 &lt;iframe id=\"shopcast_iframe\"&gt; . → 해당 iframe으로 이동하려면 다음과 같은 코드 중 하나를 써주면 된다 . # 1) 웹 요소 파라미터로 사용 driver.switch_to.frame(driver.find_element_by_css_selector('#shopcast_iframe') # 2) iframe의 id 또는 name 속성값 사용 driver.switch_to.frame('shopcast_iframe') # 3)iframe 인덱스 사용 driver.switch_to.frame(2) ##찾은 iframe 태그가 3번째 iframe이여서 . | 최종: #shopcast_iframe iframe으로 이동해서 li.goods_item element에 접근, text 가져오기 . driver = webdriver.Chrome('driver/chromedriver.exe') driver.get('https://www.naver.com/') time.sleep(1) # 'mainFrame'으로 이동 driver.switch_to.frame('shopcast_iframe') element_list = driver.find_elements_by_css_selector('li.goods_item') [element.text for element in element_list] # 아래 text는 광고가 그때그때 달라지므로 계속 가져올때마다 바뀜. ['올겨울 최강한파~\\n포근하고따뜻한옷', '살균수가대신해요\\n변기살균청소까지', '서둘러요~최대80%\\n고민하면 놓쳐요!', '이렇게 쉬워도돼?\\n완전~중독됐어!', '이렇게 맛있는데\\n왜너만몰라?', '최대 50% SALE\\n시즌오프 진행중!', '북극발한파 추위~\\nBEST롱패딩64%↓', '미려한 디자인~\\n한눈에 반했어!', '자신있는 퀄리티\\n귀걸이 당일발송', '운동복 일상복OK\\n겨울초특가세일', '비싸게사지마~\\n퀄리티 비교불가!', 'KF94 100장쟁여\\n장당200원대 특가', '그윽한 분위기 아이팔레트', '목욕탕대신 이거! 지금3+1'] . | . +) Nested iframes . | ‘A’ iframe 안에 ‘B’ iframe이 들어있다면, 메인 페이지에서 바로 B로 들어갈 수는 없다. 아래와 같이 순서대로 switch해줘야 함. | . driver.get('URL') driver.switch_to.frame('A') driver.switch_to.frame('B') . +) iframe에서 밖으로 / 상위 iframe으로 나오기 . # 상위 iframe으로 이동 driver.switch_to.parent_frame() # 웹 페이지 / 최상위 프레임 / 메인 프레임으로 이동 driver.switch_to.default_content() . Select 요소 다루기 . *Select요소: 드롭다운 메뉴에서 여러 옵션 중 하나를 선택하는 형태의 웹 요소 (아래 예시) . | Select 요소는 select 태그로 돼있고, 안에 있는 옵션들은 option 태그로 되어 있다. | select 태그 예시: &lt;select id='cityCode' name='cityCode' tabindex=\"3\"&gt; | option 태그 예시: &lt;option value=\"1100\" &gt;서울특별시&lt;/option&gt; | . | . *Select 요소는 각 element를 찾아서 클릭하는 방식으로 다뤄도 되지만, 아래와 같이 Selenium의 ‘Select’를 사용하면 훨씬 간단하게 다룰 수 있다. from selenium.webdriver.support.ui import Select # Select 툴을 import해준다 driver = webdriver.Chrome('driver/chromedriver.exe') driver.get('http://info.nec.go.kr/main/showDocument.xhtml?electionId=0020200415&amp;topMenuId=CP&amp;secondMenuId=CPRI03') driver.implicitly_wait(3) # '국회의원선거'탭 클릭 driver.find_element_by_css_selector('#electionId2').click() # select 웹 요소를 찾아서 Select 안에 넣어 준다 cityCode_select = Select(driver.find_element_by_css_selector('#cityCode')) # &lt;select id='cityCode'&gt; 태그 # '서울특별시' option 선택 cityCode_select.select_by_visible_text('서울특별시') . cf) Select 요소에서 option을 선택하는 다양한 방법 . # 1) 옵션 이름으로 선택 (웹사이트에서 보이는 옵션 이름) cityCode_select.select_by_visible_text('서울특별시') # 2) 옵션의 value로 선택 ('서울특별시' 옵션의 value는 1100) cityCode_select.select_by_value('1100') # 3) 옵션의 인덱스로 선택 ('서울특별시'는 두 번째 옵션) cityCode_select.select_by_index(1) . Scroll Down (Keys 활용) . scroll down해줘야만 아래에 있는 element가 로딩되는 경우가 많다. | 100번 scroll down하기 from selenium.webdriver.common.keys import Keys ## Keys를 import해준다 body = driver.find_element_by_tag_name(\"body\") # &lt;body&gt; element 저장해두기 pagedowns = 1 while pagedowns &lt; 100: body.send_keys(Keys.PAGE_DOWN) #scroll down ### ---작업--- ### pagedowns += 1 driver.quit() . | 특정 조건이 만족될 때까지 scroll down #--예시1---------------------------------------------- body = driver.find_element_by_tag_name(\"body\") month = 0 while int(month) &lt; 10: ## 1월~9월까지만 데이터를 가져오려는 경우 body.send_keys(Keys.PAGE_DOWN) #scroll down ### ---작업--- ### #--예시2---------------------------------------------- body = driver.find_element_by_tag_name(\"body\") app_list = [] while '애니팡' not in app_list: ## '애니팡' 데이터가 수집될 때까지 scroll down body.send_keys(Keys.PAGE_DOWN) #scroll down ### ---작업--- ### . | . Headless 모드로 이용 . *Headless 모드: 웹 브라우저가 눈에 보이지 않는 상태로, 백그라운드에서 실행되는 모드 . | Headless 모드를 사용하면 컴퓨터의 자원(CPU, RAM 등)이 덜 소모되고, 더 빠르게 동작한다 | . from selenium.webdriver.chrome.options import Options # Options를 별도로 import options = Options() options.add_argument(\"--headless\") ## headless 모드로 설정 options.add_argument(\"window-size=1920,1080\") ## 브라우저 창 크기 설정(1920x1080) - 너무 크기가 작으면 로딩이 잘 안될 수 있다 driver = webdriver.Chrome('driver/chromedriver.exe', options=options) ## -- 작업 -- ## . ",
    "url": "https://chaelist.github.io/docs/webscraping/selenium/#selenium-%EC%B6%94%EA%B0%80-%EA%B8%B0%EB%8A%A5%EB%93%A4",
    "relUrl": "/docs/webscraping/selenium/#selenium-추가-기능들"
  },"221": {
    "doc": "Selenium",
    "title": "window(창) 제어",
    "content": "창 크기 조절 및 스크린샷 . | 풀스크린으로 보기 driver.fullscreen_window() . | 창 크기 최대화(□) driver.maximize_window() . | 창 크기 최소화(-) driver.minimize_window() . | 창 코기 조절 driver.set_window_size(800, 600) # 창 크기 (800, 600)으로 설정 . | 스크린샷 저장 driver.get_screenshot_as_file('image.png') # image.png라는 파일에 스크린샷 저장 ## driver.save_screenshot('image.png)라고 해도 동일한 효과 . +) 특정 부분만 캡처하기 . # 예시: naver.com 메인 배너 광고 부분만 캡처하기 driver = webdriver.Chrome('driver/chromedriver.exe') driver.get('https://www.naver.com/') element = driver.find_element_by_css_selector('#veta_top') # 캡처할 element를 선택해준다 element_png = element.screenshot_as_png with open(\"naver_main_banner.png\", \"wb\") as file: ## naver_main_baneer.png라는 파일에 저장 file.write(element_png) driver.quit() . | . 웹 브라우저 창 여러 개 다루기 . : 웹사이트에서 링크를 클릭하면 새로운 창에 웹사이트가 열리는 경우에 유용. ex) 쿠팡의 경우, 상품을 클릭하면 상품의 상세 페이지가 새로운 창으로 열린다. | 쿠팡 ‘커피’ 검색 결과 페이지 접속 → 첫번째 item 클릭 import time from selenium import webdriver driver = webdriver.Chrome('driver/chromedriver.exe') driver.implicitly_wait(3) # 쿠팡 '커피' 검색 결과 페이지 접속 driver.get('https://www.coupang.com/np/search?component=&amp;q=%EC%BB%A4%ED%94%BC&amp;channel=user') time.sleep(1) # 첫번째 item을 찾아서 클릭 products = driver.find_elements_by_css_selector('li.search-product') products[0].click() . → 첫 번째 아이템에 대한 상세 페이지가 새로운 탭에 열린다. | ※ 하지만 현재 웹 브라우저에서 열린 탭과, Selenium이 포커스된 탭은 다르다 | Selenium이 포커스된 탭은, Selenium 웹 드라이버가 현재 ‘다루고 있는’ 탭으로, 직접 설정해줘야 바뀐다. | 그렇기 때문에 아래와 같이 두번째 item을 클릭하는 코드를 써줘도 자연스럽게 실행되어 새로운 탭이 1개 더 열린다. (탭 총 3개) | . products[1].click() . | Selenium이 포커스된 탭 바꾸기: driver.switch_to.window(driver.window_handles[i]) # 열려 있는 탭 리스트 조회 print(driver.window_handles) # 현재 포커스된 탭 -- 아직 탭을 바꿔주지 않았기 때문에 처음 .get()으로 접속한 탭 (=첫번째 탭) print(driver.current_window_handle) # 두 번째 탭으로 바꿔주기 driver.switch_to.window(driver.window_handles[1]) # 현재 포커스된 탭 -- 이제 두번째 탭으로 바뀌었음을 확인 가능 print(driver.current_window_handle) . ['CDwindow-9C390D79522E1EAE4B301DD80A05D3FA', 'CDwindow-83BB35AAF356770405A2644797340DE6', 'CDwindow-E2CFF69328DFC8E29C4DF6DA6F16967F'] CDwindow-9C390D79522E1EAE4B301DD80A05D3FA CDwindow-83BB35AAF356770405A2644797340DE6 . | driver.close로 현재 포커스된 탭 닫기 . | 더 이상 안쓰는 탭을 계속 열어두면 자원이 소모되기에 그때그때 닫아주는 것이 좋다. | . driver.close() # 탭 닫기 driver.switch_to.window(driver.window_handles[0]) # 포커스된 탭이 없어졌으므로, 다시 첫번째 탭으로 포커스를 이동시킨다 . | . ",
    "url": "https://chaelist.github.io/docs/webscraping/selenium/#window%EC%B0%BD-%EC%A0%9C%EC%96%B4",
    "relUrl": "/docs/webscraping/selenium/#window창-제어"
  },"222": {
    "doc": "Semantic Network Analysis",
    "title": "Semantic Network Analysis",
    "content": ". | Semantic NA: English . | 전처리 (Preprocessing) | Semantic Network 형성 | 네트워크 시각화 | Centrality 계산 | . | Semantic NA: 한글 . | 전처리 (Preprocessing) | Semantic Network 형성 | 네트워크 시각화 | Centrality 계산 | . | . *Semantic Network Analysis: 언어 네트워크 분석. 특정 문서 내 단어들 사이의 관계를 시각화해 내용을 한 눈에 파악할 수 있게 한다. ",
    "url": "https://chaelist.github.io/docs/network_analysis/semantic_network/",
    "relUrl": "/docs/network_analysis/semantic_network/"
  },"223": {
    "doc": "Semantic Network Analysis",
    "title": "Semantic NA: English",
    "content": ". | New York Times 기사를 예시로 활용 | 기사 내용 및 단어 사이의 관계를 한 눈에 파악하는 것이 목적 | . # 분석에 사용할 text 준비 import requests from bs4 import BeautifulSoup url = 'https://www.nytimes.com/2017/06/12/well/live/having-friends-is-good-for-you.html' r = requests.get(url) soup = BeautifulSoup(r.text, 'lxml') title = soup.title.text.strip() content = soup.find('section', attrs={'name':'articleBody'}).text content . 'Hurray for the HotBlack Coffee cafe in Toronto for declining to offer Wi-Fi to its customers. There are other such cafes, to be sure, including seven of the eight New York City locations of Café Grumpy.But it’s HotBlack’s reason for the electronic blackout that is cause for hosannas. As its president, Jimson Bienenstock, explained, his aim is to get customers to talk with one another instead of being buried in their portable devices.“It’s about creating a social vibe,” he told a New York Times reporter. “We’re a vehicle for human interaction, otherwise it’s just a commodity.” (생략) . 전처리 (Preprocessing) . *영어 전처리에 대한 자세한 설명 . | Text Cleaning import re filtered_content = re.sub('[^.,?!\\s\\w]','',content) # 정규표현식 re 활용. filtered_content = filtered_content.replace('Mr.', 'Mr').replace('Dr.', 'Dr') # 추가로 더 제거 . | Case Conversion filtered_content = filtered_content.lower() . | Tokenization import nltk word_tokens = nltk.word_tokenize(filtered_content) . | POS tagging tokens_pos = nltk.pos_tag(word_tokens) . | Select Noun words NN_words = [] for word, pos in tokens_pos: if 'NN' in pos: NN_words.append(word) . | Lemmatization # nltk의 WordNetLemmatizer을 이용 wlem = nltk.WordNetLemmatizer() lemmatized_words = [] for word in NN_words: new_word = wlem.lemmatize(word) lemmatized_words.append(new_word) . | Stopwords removal # 1차적으로 nltk에서 제공하는 불용어사전을 이용해서 불용어를 제거 from nltk.corpus import stopwords stopwords_list = stopwords.words('english') #nltk에서 제공하는 불용어사전 이용 unique_NN_words = set(lemmatized_words) #set을 사용해 중복 제거 final_NN_words = lemmatized_words for word in unique_NN_words: if word in stopwords_list: while word in final_NN_words: final_NN_words.remove(word) # 아래와 같이 추가로 직접 만든 불용어사전을 이용해 불용어 제거 customized_stopwords = ['be', 'today', 'yesterday', 'new', 'york', 'time'] # 직접 만든 불용어 사전 unique_NN_words1 = set(final_NN_words) for word in unique_NN_words1: if word in customized_stopwords: while word in final_NN_words: final_NN_words.remove(word) ## final_NN_words 출력해보기 print(final_NN_words) . ['hurray', 'hotblack', 'coffee', 'cafe', 'toronto', 'wifi', 'customer', 'cafe', 'city', 'location', 'café', 'grumpy.but', 'hotblacks', 'reason', 'blackout', 'cause', 'hosanna', 'president', 'jimson', 'bienenstock', 'aim', 'customer', 'devices.its', 'vibe', 'vehicle', 'interaction', 'commodity.what', 'idea', 'bienenstock', 'science', 'decade', 'interaction', 'contributor', 'health', 'evidence', 'value', 'connection', 'experience', 'morning', 'walk', 'woman', 'swim', 'locker', 'room', 'ymca', 'use', 'device', 'locker', 'room', 'experience', 'friend', 'share', 'joy', 'sorrow', 'woman', 'problem', 'board', 'advice', 'counsel', 'laugh', 'day.and', 'study', 'life.as', 'harvard', 'health', 'watch', 'dozen', 'study', 'people', 'relationship', 'family', 'friend', 'community', 'health', 'problem', 'longer.in', 'study', 'men', 'woman', 'county', 'calif.', 'berkman', 'syme', 'people', 'others', 'nineyear', 'study', 'people', 'tie', 'john', 'robbins', 'book', 'health', 'longevity', 'difference', 'survival', 'people', 'age', 'gender', 'health', 'practice', 'health', 'status', 'fact', 'researcher', 'tie', 'lifestyle', 'smoking', 'obesity', 'lack', 'exercise', 'tie', 'living', 'habit', 'mr', 'robbins', 'people', 'lifestyle', 'tie', 'all.in', 'study', 'journal', 'medicine', 'researcher', 'health', 'insurance', 'plan', 'men', 'heart', 'attack', 'connection', 'people', 'quarter', 'risk', 'death', 'year', 'connectedness.researchers', 'duke', 'university', 'center', 'tie', 'death', 'people', 'condition', 'brummett', (생략)] . | . Semantic Network 형성 . | Frequency Analysis . | 단어의 빈도를 파악하고, 가장 많이 나오는 단어 10개를 추출 | . from collections import Counter c = Counter(final_NN_words) print(c.most_common(10)) # 가장 빈번하게 나오는 10개의 단어 출력 # 가장 많이 나오는 단어 10개 저장 list_of_words = [] for word, count in c.most_common(10): list_of_words.append(word) print(list_of_words) . [('health', 11), ('people', 10), ('study', 6), ('tie', 6), ('researcher', 6), ('interaction', 5), ('friend', 4), ('others', 4), ('exercise', 4), ('connection', 3)] ['health', 'people', 'study', 'tie', 'researcher', 'interaction', 'friend', 'others', 'exercise', 'connection'] . | 원본 text 문장 단위로 쪼개기 # 위에서 만들어두었던 filtered_content을 활용해서 문장을 쪼갬. (., ?, ! 등 문장 구분자 꼭 필요) sentences = filtered_content.split('.\\n') sentences1 = [] sentences2 = [] sentences3 = [] for sentence in sentences: sentences1.extend(sentence.split('. ')) for sentence in sentences1: sentences2.extend(sentence.split('!')) for sentence in sentences2: sentences3.extend(sentence.split('?')) article_sentences = sentences3 print(article_sentences) ## 각 문장을 element로 담고 있는 list . ['hurray for the hotblack coffee cafe in toronto for declining to offer wifi to its customers', 'there are other such cafes, to be sure, including seven of the eight new york city locations of café grumpy.but its hotblacks reason for the electronic blackout that is cause for hosannas', 'as its president, jimson bienenstock, explained, his aim is to get customers to talk with one another instead of being buried in their portable devices.its about creating a social vibe, he told a new york times reporter', 'were a vehicle for human interaction, otherwise its just a commodity.what a novel idea', ' perhaps mr bienenstock instinctively knows what medical science has been increasingly demonstrating for decades social interaction is a critically important contributor to good health and longevity.personally, i dont need researchbased evidence to appreciate the value of making and maintaining social connections', 'i experience it daily during my morning walk with up to three women, then before and after my swim in the locker room of the ymca where the use of electronic devices is not allowed.the locker room experience has been surprisingly rewarding', 'ive made many new friends with whom i can share both joys and sorrows', (생략)] . | 단어들을 node로 생성 # 가장 많이 출현하는 10개의 명사 단어들을 node로 하는 network 생성 import networkx as nx G = nx.Graph() # undirected graph 생성 G.add_nodes_from(list_of_words) # node 생성 (가장 많았던 명사 단어 10개) print(G.nodes()) # nodes print(G.edges()) # edge, 즉 node 간의 관계는 아직 없는 상황 . ['health', 'people', 'study', 'tie', 'researcher', 'interaction', 'friend', 'others', 'exercise', 'connection'] [] . | edge(=tie) 만들기 import itertools for sentence in article_sentences: # 각 문장을 element로 담고 있는 list sentence = sentence.lower() word_tokens = nltk.word_tokenize(sentence) tokens_pos = nltk.pos_tag(word_tokens) NN_words = [] for word, pos in tokens_pos: if 'NN' in pos: NN_words.append(word) wlem = nltk.WordNetLemmatizer() lemmatized_words = [] for word in NN_words: new_word = wlem.lemmatize(word) lemmatized_words.append(new_word) selected_words = [] for word in lemmatized_words: if word in list_of_words: selected_words.append(word) # 빈도 top 10에 포함된 단어만 append selected_words = set(selected_words) # 중복을 제거하기 위해 set(집합자료형)으로 변환 for pair in list(itertools.combinations(list(selected_words), 2)): # itertools.combinations: selected_words 리스트에서 2개씩 골라 조합을 만들어준다 if pair in G.edges(): weight = G[pair[0]][pair[1]]['weight'] weight += 1 G[pair[0]][pair[1]]['weight'] = weight else: G.add_edge(pair[0], pair[1], weight=1) # 생성된 edge 확인해보기 print(nx.get_edge_attributes(G, 'weight')) . {('health', 'connection'): 2, ('health', 'interaction'): 3, ('health', 'people'): 4, ('health', 'friend'): 1, ('health', 'study'): 3, ('health', 'tie'): 3, ('health', 'others'): 1, ('health', 'researcher'): 3, ('health', 'exercise'): 1, ('people', 'friend'): 1, ('people', 'study'): 4, ('people', 'tie'): 3, ('people', 'others'): 3, ('people', 'connection'): 2, ('people', 'researcher'): 3, ('people', 'interaction'): 1, ('study', 'friend'): 1, ('study', 'tie'): 2, ('study', 'others'): 2, ('study', 'connection'): 1, ('study', 'researcher'): 1, ('tie', 'others'): 1, ('tie', 'exercise'): 1, ('tie', 'researcher'): 3, ('tie', 'connection'): 1, ('tie', 'interaction'): 1, ('researcher', 'exercise'): 2, ('researcher', 'connection'): 2, ('researcher', 'interaction'): 2, ('researcher', 'others'): 1, ('interaction', 'connection'): 1, ('interaction', 'exercise'): 1, ('friend', 'exercise'): 1, ('others', 'connection'): 1} . | . +) itertools: 효율적인 반복을 가능하게 하는 library . | itertools.permutations(list, num): 리스트의 원소들을 (num)개씩 골라 순열을 만들어준다 | itertools.combinations(list, num): 리스트의 원소들을 (num)개씩 골라 조합을 만들어준다 . | ※ 조합은 순열과 달리 순서가 없음 | . | . &gt;&gt; 예시: . alphabets = ['A', 'B', 'C'] print(list(itertools.permutations(alphabets, 2))) . [('A', 'B'), ('A', 'C'), ('B', 'A'), ('B', 'C'), ('C', 'A'), ('C', 'B')] . print(list(itertools.combinations(alphabets, 2))) # 조합은 순서가 중요치 않기에, ('A', 'B')와 ('B', 'A')를 같은 것으로 간주, 한 개만 만든다. [('A', 'B'), ('A', 'C'), ('B', 'C')] . 네트워크 시각화 . | networkx의 다양한 layout: https://networkx.org/documentation/stable/reference/drawing.html#layout | . import matplotlib.pyplot as plt ## 노드의 degree에 따라 color 다르게 설정하기 color_map = [] for node in G: if G.degree(node) &gt;= 8: # 중요한 노드 (degree가 8 이상) color_map.append('pink') else: color_map.append('beige') plt.figure(figsize=(8, 6)) # size 설정 pos = nx.spring_layout(G, scale=0.2) # spring layout 사용, 글씨가 잘리는 것을 방지하기 위해 scale=0.2로 조정 nx.draw_networkx(G, pos, node_color=color_map, edge_color='grey') plt.axis('off') # turn off axis plt.show() . → 사람들 간의 친구 관계가 건강에 중요하다는 연구에 대한 내용이라고 유추 가능. +) circular layout으로도 시각화해보기 . plt.figure(figsize=(8, 7)) pos = nx.circular_layout(G, scale=0.2) # circular layout 모양 nx.draw_networkx(G, pos, node_color=color_map, edge_color='grey') # 위에서 지정한 color_map 그대로 사용 plt.axis('off') # turn off axis plt.show() . Centrality 계산 . : Centrality를 계산해 단어의 중요도 판단하기 . | 예시로 Degree Centrality만 계산해 봄 (+. 다양한 Centrality 계산법) | . # degree centrality print(nx.degree_centrality(G)) . {'health': 1.0, 'people': 0.8888888888888888, 'study': 0.7777777777777777, 'tie': 0.8888888888888888, 'researcher': 0.8888888888888888, 'interaction': 0.6666666666666666, 'friend': 0.4444444444444444, 'others': 0.6666666666666666, 'exercise': 0.5555555555555556, 'connection': 0.7777777777777777} . ",
    "url": "https://chaelist.github.io/docs/network_analysis/semantic_network/#semantic-na-english",
    "relUrl": "/docs/network_analysis/semantic_network/#semantic-na-english"
  },"224": {
    "doc": "Semantic Network Analysis",
    "title": "Semantic NA: 한글",
    "content": ". | 중앙일보(joins.com) 기사를 예시로 활용 | . # 분석에 사용할 text 준비 import requests from bs4 import BeautifulSoup url = 'https://news.joins.com/article/23904235' r = requests.get(url) soup = BeautifulSoup(r.text, 'lxml') title = soup.select_one('#article_title').text.strip() content = soup.select_one('#article_body').text.strip() print(content) . 강찬수 환경전문기자 택배 노동자들이 잇따라 숨지고 있다. 전국 택배연대 노조 등에 따르면 이달에만 CJ대한통운 서울 강북지점, 한진택배 서울 동대문지사, 경북 칠곡 쿠팡 물류센터에서 일했던 노동자가 연달아 숨졌다. 배달 물량 급증으로 인한 과로사라는 지적이 쏟아진다. 실제로 국토교통부가 국회에 제출한 자료에 따르면, 2020년 6월 생활물류 택배 물동량은 2억 9340만 개로 지난해 6월보다 36.3% 늘었다. 신종 코로나바이러스 감염증(코로나19) 탓에 시민들이 외출과 매장 쇼핑을 꺼린 탓이다. 1회용 마스크 재질이 플라스틱먼지 치솟으면 음식 배달도 늘어낙동강 물고기에도 미세플라스틱다회용기 사용 등 대안모델 필요 배달음식 주문도 늘었다. 공정거래위원회에 따르면 올해 국내 배달 음식 시장 규모는 대략 20조원으로, 지난해보다 17%가량 증가할 것으로 전망된다. 당장은 노동자의 열악한 상황이 문제지만, 겹겹이 포장된 택배 상자와 플라스틱 용기가 가득 든 배달 음식은 심각한 환경문제이기도 하다. 코로나19 대응 과정에서도 플라스틱 쓰레기는 쏟아진다. 마스크도 플라스틱으로 만들어진다. 세계보건기구(WHO)는 코로나19 대응을 위해 세계적으로 매달 8900만 개의 의료용 마스크와 7600만 개의 검사용 장갑이 필요할 것으로 추정했다. 일반 시민이 사용하는 것은 제외한 수치다. 국내에서만 매주 2억장의 마스크를 생산한다. 경기도 수원시 자원순환센터에 쌓인 플라스틱 재활용 쓰레기. 최근 택배와 음식 배달이 늘면서 플라스틱 쓰레기도 크게 늘었다. [연합뉴스] 최근에는 미세먼지 오염이 플라스틱 쓰레기를 늘린다는 연구 결과도 나왔다. 싱가포르 연구팀이 지난 22일 ‘네이처 인간 행동’ 저널에 발표한 논문이다. 베이징 등 중국 3개 도시에서 실시한 조사에서 미세먼지 농도가 ㎥당 100㎍(마이크로그램) 상승하면, 사무실 노동자의 음식 배달 주문이 43%나 늘었다. 중국 전역으로 확대하면 배달 음식이 260만 개 늘어난다는 의미다. (생략) . 전처리 (Preprocessing) . *한글 전처리에 대한 자세한 설명 . | Text Cleaning import re cleaned_content = re.sub('[^,.?!\\w\\s]','', content) # 정규표현식 사용 cleaned_content = cleaned_content.replace('강찬수', '').replace('연합뉴스', '').replace('환경전문기자', '') # 추가로 더 제거 . | Tokenization + Lemmatization + POS tagging from konlpy.tag import Kkma kkma = Kkma() NN_words = [] kkma_pos = kkma.pos(cleaned_content) for word, pos in kkma_pos: if 'NN' in pos: NN_words.append(word) . ['택배', '노동자', '전국', '택배', '연대', '노조', '등', '이달', '통운', '서울', '강북', '지점', '한진', '택배', '서울', '동대문', '지사', '경북', '곡', '물류', '센터', '노동자', '배달', '물량', '급증', '과로', '사', '지적', '국토', '교통부', '국회', '제출', (생략)] . | Stopwords 제거 customized_stopwords = ['것', '등', '탓', '바', '용', '년', '개', '당', '면', '말'] unique_NN_words = set(NN_words) for word in unique_NN_words: if word in customized_stopwords: while word in NN_words: NN_words.remove(word) . | . Semantic Network 형성 . | 단어의 빈도 파악 from collections import Counter c = Counter(NN_words) print(c.most_common(20)) # 가장 빈번하게 나오는 20개의 단어 출력 # 가장 많이 나오는 단어 20개 저장 list_of_words = [] for word, count in c.most_common(20): list_of_words.append(word) . [('플라스틱', 28), ('미세', 13), ('쓰레기', 12), ('배달', 8), ('연구', 8), ('음식', 7), ('톤', 7), ('택배', 6), ('팀', 6), ('코로나', 5), ('생산', 5), ('발표', 5), ('논문', 5), ('노동자', 4), ('마스크', 4), ('먼지', 4), ('물고기', 4), ('최근', 4), ('바다', 4), ('지난해', 3)] . | 원본 text 문장 단위로 쪼개기 sentences = cleaned_content.split('.\\n') sentences1 = [] sentences2 = [] sentences3 = [] for sentence in sentences: sentences1.extend(sentence.strip().split('. ')) for sentence in sentences1: sentences2.extend(sentence.strip().split('!')) for sentence in sentences2: sentences3.extend(sentence.strip().split('?')) article_sentences = sentences3 print(article_sentences) . ['택배 노동자들이 잇따라 숨지고 있다', '전국 택배연대 노조 등에 따르면 이달에만 CJ대한통운 서울 강북지점, 한진택배 서울 동대문지사, 경북 칠곡 쿠팡 물류센터에서 일했던 노동자가 연달아 숨졌다', '배달 물량 급증으로 인한 과로사라는 지적이 쏟아진다', '실제로 국토교통부가 국회에 제출한 자료에 따르면, 2020년 6월 생활물류 택배 물동량은 2억 9340만 개로 지난해 6월보다 36.3 늘었다', '신종 코로나바이러스 감염증코로나19 탓에 시민들이 외출과 매장 쇼핑을 꺼린 탓이다', '1회용 마스크 재질이 플라스틱먼지 치솟으면 음식 배달도 늘어낙동강 물고기에도 미세플라스틱다회용기 사용 등 대안모델 필요 배달음식 주문도 늘었다', '공정거래위원회에 따르면 올해 국내 배달 음식 시장 규모는 대략 20조원으로, 지난해보다 17가량 증가할 것으로 전망된다', '당장은 노동자의 열악한 상황이 문제지만, 겹겹이 포장된 택배 상자와 플라스틱 용기가 가득 든 배달 음식은 심각한 환경문제이기도 하다', '코로나19 대응 과정에서도 플라스틱 쓰레기는 쏟아진다', '마스크도 플라스틱으로 만들어진다', '세계보건기구WHO는 코로나19 대응을 위해 세계적으로 매달 8900만 개의 의료용 마스크와 7600만 개의 검사용 장갑이 필요할 것으로 추정했다', '일반 시민이 사용하는 것은 제외한 수치다', '국내에서만 매주 2억장의 마스크를 생산한다', '경기도 수원시 자원순환센터에 쌓인 플라스틱 재활용 쓰레기', '최근 택배와 음식 배달이 늘면서 플라스틱 쓰레기도 크게 늘었다', (생략)] . | 단어들을 node로 생성 # 가장 많이 출현하는 20개의 명사 단어들에 대해서 네트워크 생성하기 import networkx as nx G = nx.Graph() G.add_nodes_from(list_of_words) # node 생성 (가장 많았던 명사 단어 20개) print(G.nodes()) # nodes print(G.edges()) # edge, 즉 node 간의 관계는 아직 없는 상황 . ['플라스틱', '미세', '쓰레기', '배달', '연구', '음식', '톤', '택배', '팀', '코로나', '생산', '발표', '논문', '노동자', '마스크', '먼지', '물고기', '최근', '바다', '지난해'] [] . | edge(=tie) 만들기 import itertools for sentence in article_sentences: selected_words = [] NN_words = [] kkma_pos = kkma.pos(sentence) for word, pos in kkma_pos: if 'NN' in pos: NN_words.append(word) for word in NN_words: if word in list_of_words: selected_words.append(word) selected_words = set(selected_words) for pair in list(itertools.combinations(list(selected_words), 2)): if pair in G.edges(): weight = G[pair[0]][pair[1]]['weight'] weight += 1 G[pair[0]][pair[1]]['weight'] = weight else: G.add_edge(pair[0], pair[1], weight=1 ) # 생성된 edge 확인해보기 print(nx.get_edge_attributes(G, 'weight')) . {('플라스틱', '음식'): 3, ('플라스틱', '미세'): 9, ('플라스틱', '배달'): 3, ('플라스틱', '먼지'): 2, ('플라스틱', '물고기'): 2, ('플라스틱', '마스크'): 2, ('플라스틱', '노동자'): 1, ('플라스틱', '택배'): 2, ('플라스틱', '쓰레기'): 11, ('플라스틱', '코로나'): 1, ('플라스틱', '최근'): 4, ('플라스틱', '연구'): 6, ('플라스틱', '팀'): 4, ('플라스틱', '생산'): 3, ('플라스틱', '톤'): 3, ('플라스틱', '발표'): 3, ('플라스틱', '논문'): 3, ('플라스틱', '지난해'): 1, ('플라스틱', '바다'): 1, ('미세', '음식'): 2, ('미세', '배달'): 2, ('미세', '먼지'): 4, ('미세', '물고기'): 2, ('미세', '마스크'): 1, ('미세', '최근'): 1, ('미세', '쓰레기'): 2, ('미세', '연구'): 4, ('미세', '노동자'): 1, ('미세', '코로나'): 1, ('미세', '바다'): 1, ('미세', '팀'): 2, ('미세', '생산'): 1, ('미세', '톤'): 1, ('미세', '발표'): 1, ('미세', '논문'): 1, ('미세', '지난해'): 1, ('쓰레기', '코로나'): 1, ('쓰레기', '최근'): 2, ('쓰레기', '음식'): 1, ('쓰레기', '배달'): 1, ('쓰레기', '택배'): 1, ('쓰레기', '연구'): 2, ('쓰레기', '먼지'): 1, ('쓰레기', '바다'): 2, ('쓰레기', '팀'): 1, ('쓰레기', '톤'): 1, ('쓰레기', '발표'): 1, ('쓰레기', '논문'): 1, ('쓰레기', '물고기'): 1, ('배달', '음식'): 6, ('배달', '먼지'): 2, ('배달', '물고기'): 1, ('배달', '마스크'): 1, ('배달', '지난해'): 1, ('배달', '노동자'): 2, ('배달', '택배'): 2, ('배달', '최근'): 1, ('연구', '최근'): 3, ('연구', '먼지'): 1, ('연구', '논문'): 5, (생략)} . | . 네트워크 시각화 . import matplotlib.pyplot as plt from matplotlib import font_manager as fm ## 한글로 시각화할 경우, 아래와 같이 font를 가져와줘야 함 path = 'c:/Windows/Fonts/malgun.ttf' font_name = fm.FontProperties(fname=path).get_name() ## 노드의 degree에 따라 color 다르게 설정하기 color_map = [] for node in G: if G.degree(node) &gt;= 15: # 중요한 노드 (degree가 15 이상) color_map.append('pink') else: color_map.append('beige') plt.figure(figsize=(9, 7)) pos = nx.spring_layout(G) # spring layout 사용 nx.draw_networkx(G, pos, node_color=color_map, edge_color='grey', font_family=font_name) plt.axis('off') # turn off axis plt.show() . → 플라스틱 쓰레기, 특히 미세 플라스틱으로 인한 문제에 관한 내용이라고 유추 가능. +) circular layout으로도 시각화해보기 . | node가 많은 경우, circular layout으로 시각화하면 개별 node와 edge를 더 쉽게 식별 가능 | . plt.figure(figsize=(9, 7)) pos = nx.circular_layout(G) nx.draw_networkx(G, pos, node_color=color_map, edge_color='grey', font_family=font_name) plt.axis('off') plt.show() . Centrality 계산 . : Centrality를 계산해 단어의 중요도 판단하기 . # degree centrality print(nx.degree_centrality(G)) . {'플라스틱': 1.0, '미세': 0.9473684210526315, '쓰레기': 0.7894736842105263, '배달': 0.5789473684210527, '연구': 0.631578947368421, '음식': 0.5789473684210527, '톤': 0.5263157894736842, '택배': 0.3684210526315789, '팀': 0.5789473684210527, '코로나': 0.2631578947368421, '생산': 0.5263157894736842, '발표': 0.5263157894736842, '논문': 0.5263157894736842, '노동자': 0.3157894736842105, '마스크': 0.42105263157894735, '먼지': 0.5789473684210527, '물고기': 0.42105263157894735, '최근': 0.6842105263157894, '바다': 0.5263157894736842, '지난해': 0.3684210526315789} . ",
    "url": "https://chaelist.github.io/docs/network_analysis/semantic_network/#semantic-na-%ED%95%9C%EA%B8%80",
    "relUrl": "/docs/network_analysis/semantic_network/#semantic-na-한글"
  },"225": {
    "doc": "영화 리뷰 감성 분석",
    "title": "영화 리뷰 감성 분석",
    "content": ". | 감성분석(Sentiment Analysis)이란? | 영화 리뷰 감성 분석: 준비 . | 영화 평점 - 리뷰 데이터 준비 | 학습용 데이터로 가공 | . | Logistic Regression으로 학습 . | train_test_split &amp; vector화 | 학습 &amp; test data 예측 | 예측에 중요한 역할을 하는 단어들 확인 | 새로운 댓글의 긍정/부정 예측해보기 | . | . ",
    "url": "https://chaelist.github.io/docs/ml_application/sentiment_analysis/",
    "relUrl": "/docs/ml_application/sentiment_analysis/"
  },"226": {
    "doc": "영화 리뷰 감성 분석",
    "title": "감성분석(Sentiment Analysis)이란?",
    "content": ": 특정 주제에 대한 글의 감성/태도를 파악하는 것 (긍정 / 부정) . | ML-based (기계 학습 기반) . | Supervised learning. (training data: 긍정/부정의 label이 있는 text 데이터) | 단점: 유사한 domain의 학습 데이터를 준비해야 하는데, 긍정-부정이 명확히 드러나 있는 학습 데이터를 만들기가 어렵다. | . | Lexicon-based (감성어 사전 기반) . | Lexicon(감성어 사전)에 단어별로 감성 점수가 들어있다 (긍정적인 단어는 + 값, 부정적인 단어는 - 값) | 사전을 바탕으로 text의 전체적인 감성 점수를 매기고, 값이 0보다 크면 긍정 / 작으면 부정으로 해석 | 단점: 사전을 만드는 것 자체가 어렵고, 사전이 있다고 해도 이걸 문서에 적용해서 정확한 감성점수를 계산하는 게 쉽지만은 않다. (ex. the movie was not fun → fun은 긍정이지만 not fun은 부정) | . | . ",
    "url": "https://chaelist.github.io/docs/ml_application/sentiment_analysis/#%EA%B0%90%EC%84%B1%EB%B6%84%EC%84%9Dsentiment-analysis%EC%9D%B4%EB%9E%80",
    "relUrl": "/docs/ml_application/sentiment_analysis/#감성분석sentiment-analysis이란"
  },"227": {
    "doc": "영화 리뷰 감성 분석",
    "title": "영화 리뷰 감성 분석: 준비",
    "content": ": Machine Learning 기반 감성 분석을 위해, 긍정/부정이 구분되는 데이터인 영화 평점 &amp; 리뷰 데이터를 수집 . # 사용할 library를 먼저 모두 import import pandas as pd import numpy as np import konlpy from sklearn.feature_extraction.text import CountVectorizer # tf-idf 방식을 사용하려면 대신 TfidfVectorizer를 import from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression from sklearn.metrics import accuracy_score . 영화 평점 - 리뷰 데이터 준비 .   . ⁣1. 네이버 영화 ‘현재 상영 영화’ 100개의 평점-리뷰 데이터 수집 (2021.03.30 기준) . | https://movie.naver.com/movie/running/current.nhn | . ## '네이버 영화'에서 직접 수집해 온 데이터를 dataframe으로 정리해 둠 reviews_df.head() . |   | movie_num | movie_title | star | comment | . | 0 | 191637 | 고질라 VS. 콩 | 10 | 시리즈 중 최고였던거 같아요;; 극장가서 보길 잘한듯 | . | 1 | 191637 | 고질라 VS. 콩 | 9 | 시원시원하게 두드려 패고 맞는 영화. 종종 봐줘야 마음이 상쾌해진다. | . | 2 | 191637 | 고질라 VS. 콩 | 10 | We need Kong, The world needs him. | . | 3 | 191637 | 고질라 VS. 콩 | 10 | 방사능 왕도마뱀과 인류 조상 격 에이프의 환상적인 좌충우돌 액션 콜라보 | . | 4 | 191637 | 고질라 VS. 콩 | 10 | 콩 점프하고 고질라 수영하고 전투씬 나올때마다 포디로 보니까 ㄹㅇ 존잼임ㅋㅋ 님들아 꼭 나믿고 포디로 보셔라 | .   . reviews_df.info() . &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 170388 entries, 0 to 170387 Data columns (total 4 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 movie_num 170388 non-null int64 1 movie_title 170388 non-null object 2 star 170388 non-null int64 3 comment 170388 non-null object dtypes: int64(2), object(2) . ⁣2. comment에 내용이 없는 행 파악해서 삭제 . | 평점만 있고 연결된 text가 없다면 감성 분석에 사용할 수 없음 | . # comment에 내용이 없는 행 파악 print(len(reviews_df[reviews_df['comment'] == ''])) # comment에 내용이 없는 행은 지워서 새 df에 저장 reviews_with_comment_df = reviews_df[reviews_df['comment'] != ''] reviews_with_comment_df.reset_index(drop=True, inplace=True) print(len(reviews_with_comment_df)) # comment가 있는 리뷰의 수 . 5370 165018 . 학습용 데이터로 가공 . | 평점 8 이상 혹은 3 이하만 저장 (8 이상: 긍정적, 3 이하: 부정적) | 각 text를 tokenize한 후, 동사, 형용사, 명사만 저장 (konlpy의 Okt 사용) | . # 텍스트를 tokenize해서 adjective, verb, noun만 추출하는 함수 def tokenize_korean_text(text): text_filtered = re.sub('[^,.?!\\w\\s]','', text) okt = konlpy.tag.Okt() Okt_morphs = okt.pos(text_filtered) words = [] for word, pos in Okt_morphs: if pos == 'Adjective' or pos == 'Verb' or pos == 'Noun': words.append(word) words_str = ' '.join(words) return words_str . X_texts = [] y = [] for star, comment in zip(reviews_with_comment_df['star'], reviews_with_comment_df['comment']): if 4 &lt;= star &lt;= 7: continue # 평점이 4~7인 영화는 애매하기 때문에 학습데이터로 사용하지 않음 tokenized_comment = tokenize_korean_text(comment) # 위에서 만들었던 함수로 comment 쪼개기 X_texts.append(tokenized_comment) y.append(1 if star &gt; 7 else -1) # 평점이 8 이상이면(8,9,10) 값을 1로 지정 (positive) # 평점이 3 이하이면(1,2,3) 값을 -1로 지정 (negative) print(f'원래 text 수: {len(reviews_with_comment_df)}') print(f'평점 3 이하 혹은 8 이상인 text 수: {len(X_texts)}') print(X_texts[:5]) . 원래 text 수: 165018 평점 3 이하 혹은 8 이상인 text 수: 149373 ['시리즈 중 최고 였던거 같아요 장가 보길 한듯', '시원시원하게 두드려 패 맞는 영화 종종 봐줘야 마음 상쾌해진다', '', '방사 능 왕 도마뱀 인류 조상 격 이프 환상 좌충우돌 액션 콜라보', '콩 점프 고질라 수영 전투씬 나올 때 포디 보 존잼임 님들 꼭 나 믿고 포디 보셔라'] . ",
    "url": "https://chaelist.github.io/docs/ml_application/sentiment_analysis/#%EC%98%81%ED%99%94-%EB%A6%AC%EB%B7%B0-%EA%B0%90%EC%84%B1-%EB%B6%84%EC%84%9D-%EC%A4%80%EB%B9%84",
    "relUrl": "/docs/ml_application/sentiment_analysis/#영화-리뷰-감성-분석-준비"
  },"228": {
    "doc": "영화 리뷰 감성 분석",
    "title": "Logistic Regression으로 학습",
    "content": "train_test_split &amp; vector화 . # train_test_split X_train_texts, X_test_texts, y_train, y_test = train_test_split(X_texts, y, test_size=0.2, random_state=0) . # CountVectorizer로 vector화 tf_vectorizer = CountVectorizer(min_df=1, ngram_range=(1,1)) X_train_tf = tf_vectorizer.fit_transform(X_train_texts) # training data에 맞게 fit &amp; training data를 transform X_test_tf = tf_vectorizer.transform(X_test_texts) # test data를 transform vocablist = [word for word, number in sorted(tf_vectorizer.vocabulary_.items(), key=lambda x:x[1])] # 단어들을 번호 기준 내림차순으로 저장 . *CountVectorizer: 각 단어의 frequency를 기반으로 text를 vector화. | ngram_range: (1, 1) means only unigrams, (1, 2) means unigrams and bigrams, and (2, 2) means only bigrams. (default: (1,1)) | min_df: ignore terms that have a document frequency strictly lower than the given threshold. (default: 1) . | float 값을 쓰면: document에 어느 정도 비율 아래로 들어 있으면 무시한다는 뜻 | int 값을 쓰면 (ex. 1): 절대적으로 몇 번 아래로 포함된 단어면 무시한다는 뜻 (1이면 1번도 안 나온 단어는 무시한다는 뜻) | . | tf_vectorizer.fit_transform(X_train_texts): . | fit: training data에 있는 각 단어별로 번호가 붙어서 저장됨 | transform: training data의 각 값마다, 어떤 단어가 몇 번 나오는지 데이터가 저장됨 – ex) (0, 2442) 1이라고 하면 0번째 변수에 2442번 단어가 1번 나온다는 뜻 | . | tf_vectorizer.transform(X_train_texts): . | test data의 각 값마다, 어떤 단어가 몇 번 나오는지의 데이터가 저장됨 (단, fit_transform이 아니므로, 앞서 training data에 있던 단어들에 한해서 분석됨) | . | . ## 확인해보기 print(X_train_tf[:1], '\\n') print(X_test_tf[:1], '\\n') print(vocablist[:3]) . (0, 48016) 1 (0, 30089) 2 (0, 1146) 1 (0, 68104) 1 (0, 57682) 1 (0, 1277) 1 (0, 34835) 1 (0, 51567) 1 (0, 172) 1 (0, 37975) 1 (0, 1485) 1 (0, 15929) 1 (0, 64573) 1 (0, 66001) 1 ['가가', '가각', '가감'] . +) Text Vectorization: TF와 TF-IDF 방식 . | TF (Term Frequency) . | text 내 단어별 frequency를 기반으로 vector를 형성 | . (+. 위와 같이, 각 문서의 단어를 나타내는 vector들로 만든 행렬을 DTM(Document Term Matrix)이라고 함) . | TF-IDF (Term Frequency - Inverse Document Frequency) . | 특정 단어가 특정 문서의 uniqueness를 얼마나 나타내는가를 계산하기 위해 사용 | TF-IDF가 높을수록 해당 단어는 다른 문서에서는 적게 사용되고, 해당 문서에서 많이 사용되고 있다는 뜻으로, 해당 단어가 해당 문서의 uniqueness를 많이 나타낸다고 볼 수 있음. | . (출처: bloter) . | . 학습 &amp; test data 예측 . model = LogisticRegression(C=0.1, penalty='l2', random_state=0) model.fit(X_train_tf, y_train) # 학습 . LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True, intercept_scaling=1, l1_ratio=None, max_iter=100, multi_class='auto', n_jobs=None, penalty='l2', random_state=0, solver='lbfgs', tol=0.0001, verbose=0, warm_start=False) . → Test data 예측: . y_test_pred = model.predict(X_test_tf) print('Misclassified samples: {} out of {}'.format((y_test_pred != y_test).sum(), len(y_test))) print('Accuracy: {:.2f}'.format(accuracy_score(y_test, y_test_pred))) # model.score(X_test_tf, y_test)로 계산해도 됨 . Misclassified samples: 2490 out of 29875 Accuracy: 0.92 . | 92%를 잘 예측함! | . 예측에 중요한 역할을 하는 단어들 확인 . coefficients = model.coef_.tolist() sorted_coefficients = sorted(enumerate(coefficients[0]), key=lambda x:x[1], reverse=True) # coefficients(계수)가 큰 값부터 내림차순으로 정렬 print('긍정적인 단어 Top 10 (높은 평점과 상관관계가 강한 단어들)') for word_num, coef in sorted_coefficients[:10]: print('{0:}({1:.3f})'.format(vocablist[word_num], coef)) print('\\n부정적인 단어 Top 10 (낮은 평점과 상관관계가 강한 단어들)') for word_num, coef in sorted_coefficients[-10:]: print('{0:}({1:.3f})'.format(vocablist[word_num], coef)) . 긍정적인 단어 Top 10 (높은 평점과 상관관계가 강한 단어들) 최고(2.163) 여운(1.829) 좋았습니다(1.626) 재밌어요(1.617) 좋았어요(1.524) 재밌었어요(1.456) 재밌게(1.435) 좋아요(1.389) 봤어요(1.330) 최고다(1.317) 부정적인 단어 Top 10 (낮은 평점과 상관관계가 강한 단어들) 재미없다(-1.818) 재미없어요(-1.836) 게이(-1.842) 재미없음(-1.936) 쓰레기(-2.012) 아깝다(-2.045) 미화(-2.196) 똥꼬(-2.597) 노잼(-2.748) 최악(-2.767) . 새로운 댓글의 긍정/부정 예측해보기 . : 학습에 포함되지 않았던 영화인 ‘인턴‘에서 댓글을 가져와서 테스트 . # 긍정/부정 테스트용 함수 생성 def guess_good_or_bad(text): text_filtered = text.replace('.', '').replace(',','').replace(\"'\",\"\").replace('·', ' ').replace('=','') okt = konlpy.tag.Okt() Okt_morphs = okt.pos(text_filtered) words = [] for word, pos in Okt_morphs: if pos == 'Adjective' or pos == 'Verb' or pos == 'Noun': words.append(word) words_str = ' '.join(words) new_text_tf = tf_vectorizer.transform([words_str]) if model.predict(new_text_tf) == 1: print('긍정') else: print('부정') . → 리뷰 3개를 각각 테스트: . guess_good_or_bad('로버트 드니로가 첫출근 전날밤에 (다음날 입을) 옷과 신발을 준비하는 장면이 왜그렇게 가슴찡하던지..왠지 가슴설레고 긴장되고, 가기 싫었던 온갖 감정들이 15년만에 다시 기억이 나더군요 재밌고 따뜻한 영화입니다~') . 긍정 . guess_good_or_bad('시작하고 몇분간은 재밌을거같았다. 하지만 계속 진행될수록 뭔가 이상함과 뜬금없음과 ㅈ같음을 느꼈다. 난중에 감독이 여자인것을 알고 납득이 가버렸다.') . 부정 . guess_good_or_bad('울컥... 한 영화...') . 긍정 . ",
    "url": "https://chaelist.github.io/docs/ml_application/sentiment_analysis/#logistic-regression%EC%9C%BC%EB%A1%9C-%ED%95%99%EC%8A%B5",
    "relUrl": "/docs/ml_application/sentiment_analysis/#logistic-regression으로-학습"
  },"229": {
    "doc": "Social Network Analysis",
    "title": "Social Network Analysis",
    "content": ". | Harry Potter Network Analysis . | Network 데이터 준비 | 인물 간 관계 Graph 생성 | Circos Plot으로 시각화 | 가장 가까운 인물 10명 찾기 | . | 인물의 중요도 변화 확인 (HP 1~7) . | 인물의 중요도 변화: Degree Centrality | 인물의 중요도 변화: Betweenness Centrality | 인물의 중요도 변화: Eigenvector Centrality | 인물의 중요도 변화: PageRank | Correlation between different measures | . | . *Social Network Analysis: 사람들로 이루어진 네트워크, 즉 사람들 간의 관계를 분석. ",
    "url": "https://chaelist.github.io/docs/network_analysis/social_network/",
    "relUrl": "/docs/network_analysis/social_network/"
  },"230": {
    "doc": "Social Network Analysis",
    "title": "Harry Potter Network Analysis",
    "content": ": 예시로, 소설 &lt; Harry Potter &gt; 시리즈 속 인물들을 바탕으로 Social Network를 구현해 봄. (→ 실제 사람들 사이의 관계에도 유사한 분석을 적용 가능) . Network 데이터 준비 . | bookwarm library를 사용: 책 text를 바탕으로 인물 간 관계 데이터를 구성. | source, target, value(=weight) 3개의 칼럼으로 구성. (방향성은 없지만 편의상 source와 target으로 나눔) | 우선은 1권 &lt; Harry Potter and the Philosopher’s Stone &gt; 데이터만 사용 | . import pandas as pd # 미리 bookwarm library를 사용해서 구축해 놓은 인물 간 관계 데이터 book1 = pd.read_csv('harrypotter/network_HarryPotter1.csv', index_col=0) book1.head() . |   | source | target | value | . | 0 | vernon | petunia | 24 | . | 1 | vernon | dudley | 29 | . | 2 | vernon | harry | 75 | . | 3 | vernon | hagrid | 6 | . | 4 | vernon | marge | 21 | . 인물 간 관계 Graph 생성 . import networkx as nx import matplotlib.pyplot as plt # Create an empty graph object G = nx.Graph() # Iterate through the DataFrame &amp; add edges for _, edge in book1.iterrows(): G.add_edge(edge['source'], edge['target'], weight=edge['value']) nx.draw_networkx(G) plt.axis('off') # turn off axis plt.show() . | ※ 이렇게 node가 너무 많은 경우 뭉친 부분이 잘 안 보인다 | . Circos Plot으로 시각화 . | nxviz의 Circos Plot을 사용: node가 많아 graph가 뭉치는 현상을 해결 | +) 각 node의 Degree Centrality 계산, Centrality에 따라 색을 다르게 해서 그리기 | +) Centrality 값에 따라 node를 정렬해서 중요도 높은 node 순으로 판별 가능하게 하기 | . # degree centrality 계산 deg_cen = nx.degree_centrality(G) # 각 node에 'centrality' attribute 넣어주기 for node in G.nodes(): G.nodes[node]['centrality'] = deg_cen[node] # centrality 값의 크기를 색으로 구분해 circos plot 그리기 import nxviz as nv c = nv.CircosPlot(G, node_labels=True, node_label_layout=\"rotation\", # node끼리 서로 가리지 않도록 label를 rotate node_color='centrality', # centrality 값을 기반으로 색을 구분 node_order='centrality', # centrality 값 순서대로 정렬 figsize=(12,12)) c.draw() . 가장 가까운 인물 10명 찾기 . : 특정 node와 연결된 edge의 weight가 큰 인물 순서대로 정렬해 찾기 . # 네트워크 G 안에서 특정 노드(name) 사이의 weight가 큰 neighbor 순서대로 10명 출력하는 함수 def find_ten_closest(G, name): neighbors_list = list(G.neighbors(f\"{name}\")) temp_dict = {} for n in neighbors_list: weight = G.edges[f\"{name}\", n]['weight'] temp_dict[n] = weight sorted_list = sorted(temp_dict.items(), key=(lambda x:x[1]), reverse=True) for n, w in sorted_list[0:10]: # 1~10위 print(f'{n}: {w}') # harry와 가장 가까운 인물 top10과 weight 출력 - 1권 기준 find_ten_closest(G, 'harry') . ron: 367 hagrid: 183 hermione: 177 snape: 94 draco: 90 quirrell: 87 dudley: 84 dumbledore: 83 vernon: 75 neville: 66 . ",
    "url": "https://chaelist.github.io/docs/network_analysis/social_network/#harry-potter-network-analysis",
    "relUrl": "/docs/network_analysis/social_network/#harry-potter-network-analysis"
  },"231": {
    "doc": "Social Network Analysis",
    "title": "인물의 중요도 변화 확인 (HP 1~7)",
    "content": "import pandas as pd import networkx as nx # 미리 bookwarm library를 사용해서 만들어 둔 인물 간 관계 데이터: 1권~7권 book1 = pd.read_csv('harrypotter/network_HarryPotter1.csv', index_col=0) book2 = pd.read_csv('harrypotter/network_HarryPotter2.csv', index_col=0) book3 = pd.read_csv('harrypotter/network_HarryPotter3.csv', index_col=0) book4 = pd.read_csv('harrypotter/network_HarryPotter4.csv', index_col=0) book5 = pd.read_csv('harrypotter/network_HarryPotter5.csv', index_col=0) book6 = pd.read_csv('harrypotter/network_HarryPotter6.csv', index_col=0) book7 = pd.read_csv('harrypotter/network_HarryPotter7.csv', index_col=0) # 각 book으로부터 네트워크를 만들어 G_books list에 append books = [book1, book2, book3, book4, book5, book6, book7] G_books = [] for book in books: G_book = nx.Graph() for _, edge in book.iterrows(): G_book.add_edge(edge['source'], edge['target'], weight=edge['value']) G_books.append(G_book) . 인물의 중요도 변화: Degree Centrality . # Creating a list of degree centrality of all the books evol = [nx.degree_centrality(G) for G in G_books] # Creating a DataFrame from the list of degree centralities in all the books index = ['Book1', 'Book2', 'Book3', 'Book4', 'Book5', 'Book6', 'Book7'] degree_evol_df = pd.DataFrame.from_records(evol, index=index).fillna(0) # 모든 책에 모든 캐릭터가 있는 게 아니므로, N/A는 0으로 채워줌 # 각 책의 중요도 top 5 인물 선택 set_of_char = set() for i in index: # 1~7권 set_of_char |= set(list(degree_evol_df.T[i].sort_values(ascending=False)[0:5].index)) # |=는 왼쪽과 오른쪽의 합집합을 왼쪽에 assign해주는 역할 list_of_char = list(set_of_char) # Evolution of Top Characters 시각화 degree_evol_df[list_of_char].plot(figsize=(13, 7)); . 인물의 중요도 변화: Betweenness Centrality . # Creating a list of betweenness centrality of all the books evol = [nx.betweenness_centrality(G, weight='weight') for G in G_books] # Making a DataFrame from the list index = ['Book1', 'Book2', 'Book3', 'Book4', 'Book5', 'Book6', 'Book7'] betweenness_evol_df = pd.DataFrame.from_records(evol, index=index).fillna(0) # 각 책의 중요도 top 5 인물 선택 set_of_char = set() for i in index: # 1~7권 set_of_char |= set(list(betweenness_evol_df.T[i].sort_values(ascending=False)[0:5].index)) list_of_char = list(set_of_char) # Evolution of Top Characters 시각화 betweenness_evol_df[list_of_char].plot(figsize=(13, 7)); . 인물의 중요도 변화: Eigenvector Centrality . # Creating a list of eigenvector centrality of all the books evol = [nx.eigenvector_centrality(G, weight='weight') for G in G_books] # Making a DataFrame from the list index = ['Book1', 'Book2', 'Book3', 'Book4', 'Book5', 'Book6', 'Book7'] eigenvector_evol_df = pd.DataFrame.from_records(evol, index=index).fillna(0) # 각 책의 중요도 top 5 인물 선택 set_of_char = set() for i in index: # 1~7권 set_of_char |= set(list(eigenvector_evol_df.T[i].sort_values(ascending=False)[0:5].index)) list_of_char = list(set_of_char) # Evolution of Top Characters 시각화 eigenvector_evol_df[list_of_char].plot(figsize=(13, 7)); . 인물의 중요도 변화: PageRank . | PageRank는 Eigenvector Centrality를 기반으로 node의 ranking을 매기는 알고리즘 | Google에서 web page 간의 link 관계를 바탕으로 ranking을 매기는 방법으로 고안된 방식 | 원래는 directed graph를 위해 디자인된 알고리즘이지만 undirected graph에서도 활용 가능 | . # Creating a list of pagerank of all the characters in all the books evol = [nx.pagerank(G) for G in G_books] # Making a DataFrame from the list index = ['Book1', 'Book2', 'Book3', 'Book4', 'Book5', 'Book6', 'Book7'] pagerank_evol_df = pd.DataFrame.from_records(evol, index=index).fillna(0) # 각 책의 중요도 top 5 인물 선택 set_of_char = set() for i in index: # 1~7권 set_of_char |= set(list(pagerank_evol_df.T[i].sort_values(ascending=False)[0:5].index)) list_of_char = list(set_of_char) # Evolution of Top Characters 시각화 pagerank_evol_df[list_of_char].plot(figsize=(13, 7)); . | Eigenvector Centrality로 계산했을 때와 매우 유사한 결과 | . Correlation between different measures . 4개의 Centrality 계산 방식끼리 얼마나 유사한 결과를 내는지 비교 . | Correlation 계산: 1권 기준 # List of 4 centrality scores of all the characters in book 1 measures = [nx.pagerank(G_books[0]), nx.eigenvector_centrality(G_books[0], weight='weight'), nx.betweenness_centrality(G_books[0], weight='weight'), nx.degree_centrality(G_books[0])] # DataFrame 만들어서 Correlation 계산 cor = pd.DataFrame.from_records(measures, index=['PageRank', 'Eigenvector', 'Betweenness', 'Degree']) cor.T.corr() . |   | PageRank | Eigenvector | Betweenness | Degree | . | PageRank | 1 | 0.936511 | 0.932695 | 0.957365 | . | Eigenvector | 0.936511 | 1 | 0.880642 | 0.970408 | . | Betweenness | 0.932695 | 0.880642 | 1 | 0.926985 | . | Degree | 0.957365 | 0.970408 | 0.926985 | 1 | . | 1권에서는 4개의 인물 중요도 계산 방식이 모두 서로 유사한 결과를 냄 | .   . | Correlation 계산: 3권 기준 # Creating a list of all centrality of all the characters in book 3 measures = [nx.pagerank(G_books[2]), nx.eigenvector_centrality(G_books[2], weight='weight'), nx.betweenness_centrality(G_books[2], weight='weight'), nx.degree_centrality(G_books[2])] # DataFrame 만들어서 Correlation 계산 cor = pd.DataFrame.from_records(measures, index=['PageRank', 'Eigenvector', 'Betweenness', 'Degree']) cor.T.corr() . |   | PageRank | Eigenvector | Betweenness | Degree | . | PageRank | 1 | 0.946001 | 0.508885 | 0.908312 | . | Eigenvector | 0.946001 | 1 | 0.561182 | 0.925465 | . | Betweenness | 0.508885 | 0.561182 | 1 | 0.705608 | . | Degree | 0.908312 | 0.925465 | 0.705608 | 1 | . | 3권에서는 인물 중요도 계산에서 유독 Betweenness Centrality가 다른 방법들과 차이를 보임 | PageRank와 Eigenvector Centrality 방식이 특히 유사한 결과를 냄 | . | . ",
    "url": "https://chaelist.github.io/docs/network_analysis/social_network/#%EC%9D%B8%EB%AC%BC%EC%9D%98-%EC%A4%91%EC%9A%94%EB%8F%84-%EB%B3%80%ED%99%94-%ED%99%95%EC%9D%B8-hp-17",
    "relUrl": "/docs/network_analysis/social_network/#인물의-중요도-변화-확인-hp-17"
  },"232": {
    "doc": "SQL",
    "title": "SQL",
    "content": " ",
    "url": "https://chaelist.github.io/docs/sql",
    "relUrl": "/docs/sql"
  },"233": {
    "doc": "STEM Salaries 1",
    "title": "STEM Salaries 1",
    "content": ". | 데이터 파악 및 전처리 . | 불필요한 칼럼 삭제 | 중복값 제거 | 결측치 채우기 | 이상한 값이 있는지 확인 | company 칼럼의 표기 통일 | . | 기업별, 직업별 salary . | 기업별 평균 salary | title별 평균 salary | 기업별 title별 treemap | . | 성별, 인종, 교육수준별 salary . | gender별 평균 salary | race별 평균 salary | education별 평균 salary | . | location별 salary . | 가장 많은 회사가 위치한 location | 가장 평균 salary가 높은 location | . | . *분석 대상 데이터셋: Data Science and STEM Salaries . | 데이터셋 출처 | levels.fyi에서 가져온 62,642개의 salary data (Data Science &amp; STEM 직종) | 2017.06.07 ~ 2021.08.17의 기간에 기록된 약 1060개의 회사, 15개 job title에 대한 데이터 | . ",
    "url": "https://chaelist.github.io/docs/kaggle/stem_salaries/",
    "relUrl": "/docs/kaggle/stem_salaries/"
  },"234": {
    "doc": "STEM Salaries 1",
    "title": "데이터 파악 및 전처리",
    "content": "# 필요한 라이브러리 import import pandas as pd import numpy as np import plotly.express as px import plotly.io as pio pio.templates.default = \"plotly_white\" # default template을 지정 . salary_df = pd.read_csv('data/Levels_Fyi_Salary_Data.csv') salary_df.head(3) . |   | timestamp | company | level | title | totalyearlycompensation | location | yearsofexperience | yearsatcompany | tag | basesalary | stockgrantvalue | bonus | gender | otherdetails | cityid | dmaid | rowNumber | Masters_Degree | Bachelors_Degree | Doctorate_Degree | Highschool | Some_College | Race_Asian | Race_White | Race_Two_Or_More | Race_Black | Race_Hispanic | Race | Education | . | 0 | 6/7/2017 11:33:27 | Oracle | L3 | Product Manager | 127000 | Redwood City, CA | 1.5 | 1.5 | nan | 107000 | 20000 | 10000 | nan | nan | 7392 | 807 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | nan | nan | . | 1 | 6/10/2017 17:11:29 | eBay | SE 2 | Software Engineer | 100000 | San Francisco, CA | 5 | 3 | nan | 0 | 0 | 0 | nan | nan | 7419 | 807 | 2 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | nan | nan | . | 2 | 6/11/2017 14:53:57 | Amazon | L7 | Product Manager | 310000 | Seattle, WA | 8 | 0 | nan | 155000 | 0 | 0 | nan | nan | 11527 | 819 | 3 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | nan | nan | . → 칼럼 정보 파악 . salary_df.info() . &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 62642 entries, 0 to 62641 Data columns (total 29 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 timestamp 62642 non-null object 1 company 62637 non-null object 2 level 62523 non-null object 3 title 62642 non-null object 4 totalyearlycompensation 62642 non-null int64 5 location 62642 non-null object 6 yearsofexperience 62642 non-null float64 7 yearsatcompany 62642 non-null float64 8 tag 61788 non-null object 9 basesalary 62642 non-null float64 10 stockgrantvalue 62642 non-null float64 11 bonus 62642 non-null float64 12 gender 43102 non-null object 13 otherdetails 40137 non-null object 14 cityid 62642 non-null int64 15 dmaid 62640 non-null float64 16 rowNumber 62642 non-null int64 17 Masters_Degree 62642 non-null int64 18 Bachelors_Degree 62642 non-null int64 19 Doctorate_Degree 62642 non-null int64 20 Highschool 62642 non-null int64 21 Some_College 62642 non-null int64 22 Race_Asian 62642 non-null int64 23 Race_White 62642 non-null int64 24 Race_Two_Or_More 62642 non-null int64 25 Race_Black 62642 non-null int64 26 Race_Hispanic 62642 non-null int64 27 Race 22427 non-null object 28 Education 30370 non-null object dtypes: float64(6), int64(13), object(10) memory usage: 13.9+ MB . 불필요한 칼럼 삭제 . | cityid, dmaid, rowNumber 칼럼은 불필요하다고 판단 | Masters_Degree ~ Race_Hispanic 칼럼은 ‘Race’와 ‘Education’의 정보를 더미변수화해놓은 칼럼이므로 우선 삭제 | . drop_columns = salary_df.loc[:, 'cityid':'Race_Hispanic'].columns salary_df.drop(drop_columns, axis='columns', inplace=True) salary_df.head(3) . |   | timestamp | company | level | title | totalyearlycompensation | location | yearsofexperience | yearsatcompany | tag | basesalary | stockgrantvalue | bonus | gender | otherdetails | Race | Education | . | 0 | 6/7/2017 11:33:27 | Oracle | L3 | Product Manager | 127000 | Redwood City, CA | 1.5 | 1.5 | nan | 107000 | 20000 | 10000 | nan | nan | nan | nan | . | 1 | 6/10/2017 17:11:29 | eBay | SE 2 | Software Engineer | 100000 | San Francisco, CA | 5 | 3 | nan | 0 | 0 | 0 | nan | nan | nan | nan | . | 2 | 6/11/2017 14:53:57 | Amazon | L7 | Product Manager | 310000 | Seattle, WA | 8 | 0 | nan | 155000 | 0 | 0 | nan | nan | nan | nan | . 중복값 제거 . salary_df.duplicated().sum() # 중복값이 포함되어 있나 확인 (모든 열의 데이터가 같은 경우) . 44 . → 모든 컬럼이 중복되는 경우는 실수로 중복 기입된 것이라 간주, 하나의 row만 남기고 삭제 . print('중복 제거 이전: ', len(salary_df)) salary_df.drop_duplicates(inplace=True, ignore_index=True) print('중복 제거 이후: ', len(salary_df)) . 중복 제거 이전: 62642 중복 제거 이후: 62598 . 결측치 채우기 . salary_df.isna().sum() . timestamp 0 company 5 level 119 title 0 totalyearlycompensation 0 location 0 yearsofexperience 0 yearsatcompany 0 tag 854 basesalary 0 stockgrantvalue 0 bonus 0 gender 19529 otherdetails 22467 Race 40171 Education 32232 dtype: int64 . → gender, Race, Education의 null값을 모두 ‘Unknown’으로 채워줌 (분석에 사용하기 위함) . salary_df[['gender', 'Race', 'Education']] = salary_df[['gender', 'Race', 'Education']].fillna('Unknown') salary_df.head(3) . |   | timestamp | company | level | title | totalyearlycompensation | location | yearsofexperience | yearsatcompany | tag | basesalary | stockgrantvalue | bonus | gender | otherdetails | Race | Education | . | 0 | 6/7/2017 11:33:27 | Oracle | L3 | Product Manager | 127000 | Redwood City, CA | 1.5 | 1.5 | nan | 107000 | 20000 | 10000 | Unknown | nan | Unknown | Unknown | . | 1 | 6/10/2017 17:11:29 | eBay | SE 2 | Software Engineer | 100000 | San Francisco, CA | 5 | 3 | nan | 0 | 0 | 0 | Unknown | nan | Unknown | Unknown | . | 2 | 6/11/2017 14:53:57 | Amazon | L7 | Product Manager | 310000 | Seattle, WA | 8 | 0 | nan | 155000 | 0 | 0 | Unknown | nan | Unknown | Unknown | . → 잘 채워졌는지 확인 . salary_df.isna().sum() . timestamp 0 company 5 level 119 title 0 totalyearlycompensation 0 location 0 yearsofexperience 0 yearsatcompany 0 tag 854 basesalary 0 stockgrantvalue 0 bonus 0 gender 0 otherdetails 22467 Race 0 Education 0 dtype: int64 . 이상한 값이 있는지 확인 . | 숫자형 칼럼 점검 . salary_df.describe() . |   | totalyearlycompensation | yearsofexperience | yearsatcompany | basesalary | stockgrantvalue | bonus | . | count | 62642.0 | 62642.0 | 62642.0 | 62642.0 | 62642.0 | 62642.0 | . | mean | 216300.37364707384 | 7.2041350850866825 | 2.7020929408384147 | 136687.28129689346 | 51486.08073259315 | 19334.746587752627 | . | std | 138033.7463773671 | 5.84037534823308 | 3.263655591673307 | 61369.27805673717 | 81874.56939076641 | 26781.292039968368 | . | min | 10000.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . | 25% | 135000.0 | 3.0 | 0.0 | 108000.0 | 0.0 | 1000.0 | . | 50% | 188000.0 | 6.0 | 2.0 | 140000.0 | 25000.0 | 14000.0 | . | 75% | 264000.0 | 10.0 | 4.0 | 170000.0 | 65000.0 | 26000.0 | . | max | 4980000.0 | 69.0 | 69.0 | 1659870.0 | 2800000.0 | 1000000.0 | . | 전반적으로 특별히 이상한 값은 없다고 생각됨 | . fig = px.box(salary_df, x='yearsatcompany', height=300) fig.show() . salary_df.query('yearsatcompany == 69') . |   | timestamp | company | level | title | totalyearlycompensation | location | yearsofexperience | yearsatcompany | tag | basesalary | stockgrantvalue | bonus | gender | otherdetails | Race | Education | . | 46988 | 4/3/2021 11:04:46 | Disney | 5 | Product Designer | 102000 | Crapo, MD | 69 | 69 | Interaction Design | 100000 | 2000 | 0 | Unknown | Title: Joiooooo | Unknown | Unknown | . | yeartsatcompany == 69라는 값이 다른 값과 너무 크기가 다르긴 하지만, 그럴 수 있다고 생각됨. | . | gender 칼럼 점검 . salary_df['gender'].unique() . array(['Unknown', 'Male', 'Female', 'Other', 'Title: Senior Software Engineer'], dtype=object) . | gender에 ‘Title: Senior Software Engineer’라는 값이 들어가 있는 것은 잘못 기입된 것이라 생각됨 | . salary_df.query('gender == \"Title: Senior Software Engineer\"') . |   | timestamp | company | level | title | totalyearlycompensation | location | yearsofexperience | yearsatcompany | tag | basesalary | stockgrantvalue | bonus | gender | otherdetails | Race | Education | . | 11010 | 9/17/2019 6:23:02 | GitHub | E4 | Software Engineer | 205000 | Buda, TX | 15 | 4 | Distributed Systems (Back-End) | 177000 | 31000 | 1000 | Title: Senior Software Engineer | nan | Unknown | Unknown | . → gender == “Title: Senior Software Engineer”라고 기입되어 있던 index=10985 행의 gender 값을 ‘Unknown’으로 바꿔줌 . salary_df.loc[10985, 'gender'] = 'Unknown' . | Race, Education, title 칼럼 점검 . | 특별히 이상한 값은 없음 | . salary_df['Race'].unique() . array(['Unknown', 'White', 'Asian', 'Black', 'Two Or More', 'Hispanic'], dtype=object) . salary_df['Education'].unique() . array(['Unknown', 'PhD', \"Master's Degree\", \"Bachelor's Degree\", 'Some College', 'Highschool'], dtype=object) . list(salary_df['title'].unique()) . ['Product Manager', 'Software Engineer', 'Software Engineering Manager', 'Data Scientist', 'Solution Architect', 'Technical Program Manager', 'Human Resources', 'Product Designer', 'Marketing', 'Business Analyst', 'Hardware Engineer', 'Sales', 'Recruiter', 'Mechanical Engineer', 'Management Consultant'] . | . company 칼럼의 표기 통일 . salary_df['company'].nunique() . 1631 . list(salary_df['company'].unique()) . ['Oracle', 'eBay', 'Amazon', 'Apple', 'Microsoft', 'Salesforce', 'Facebook', 'Uber', 'Oath', 'Google', 'Netflix', 'Pinterest', 'Linkedin', 'Adobe', 'LinkedIn', 'amazon', 'Symantec', 'Intel Corporation', 'Intel', (생략)] . | ‘Apple’ = ‘apple’, ‘Intel’ = ‘Intel Corporation’ 등 통일되지 않은 이름들이 꽤 있음 | . | 공백을 제거하고 uppercase로 맞춰줌 salary_df['company'] = salary_df['company'].str.strip().str.upper() . | llc, org, ltd 등 뒤에 붙는 다양한 단어들을 지워줌 salary_df['company'] = salary_df['company'].str.replace(' LLC', '').str.replace('.ORG', '').str.replace(' LTD', '').str.replace(' CORPORATION', '').str.replace(' INC', '') salary_df['company'] = salary_df['company'].str.replace(' MEDIA', '').str.replace(' GROUP', '').str.replace(' TECHNOLOGY', '').str.replace(' TECHNOLOGIES', '').str.strip() . salary_df['company'].nunique() . 1081 . | 같은 회사인데 이름이 다른 경우가 더 있는지 확인 . | 앞에서부터 6글자를 떼어서 groupby로 묶어서 확인 | . temp = salary_df[['company']] temp['6letters'] = temp['company'].str[:6] temp_groupby = temp.groupby('6letters')['company'].agg([pd.Series.nunique, set]) temp_groupby.query('nunique &gt; 1') . | 6letters | nunique | set | . | AMAZON | 3 | {‘AMAZON.COM’, ‘AMAZON’, ‘AMAZON WEB SERVICES’} | . | AMERIC | 3 | {‘AMERICAN FAMILY INSURANCE’, ‘AMERICAN EXPRESS’, ‘AMERICAN AIRLINES’} | . | ARISTA | 2 | {‘ARISTA NETWORKS’, ‘ARISTA’} | . | BANK O | 2 | {‘BANK OF AMERICA MERRILL LYNCH’, ‘BANK OF AMERICA’} | . | BETTER | 3 | {‘BETTER.COM’, ‘BETTERMENT’, ‘BETTER MORTGAGE’} | . | (생략) | … | … | . | 같은 회사인데 표기가 다른 경우를 함수로 만들어서 정리해줌 . | ‘amazon.com’과 ‘amazon web services’처럼 계열사지만 다른 기업이라고 판단되는 경우는 그냥 둠 | . def clean_company_names(name): try: if name.startswith('AMAZON.COM'): final_name = 'AMAZON' elif name.startswith('ARISTA'): final_name = 'ARISTA' elif name.startswith('BLOOMBERG'): final_name = 'BLOOMBERG' elif name.startswith('BOOKING'): final_name = 'BOOKING.COM' elif name.startswith('DELOITTE CONSULTING'): final_name = 'DELOITTE CONSULTING' elif name.startswith('FORD'): final_name = 'FORD' elif name.startswith('CADENCE'): final_name = 'CADENCE' elif name.startswith('MCKINSEY'): final_name = 'MCKINSEY &amp; COMPANY elif name.startswith('MOTOROLA'): final_name = 'MOTOROLA' elif name.startswith('NUANCE'): final_name = 'NUANCE elif name.startswith('COSTCO'): final_name = 'COSTCO' elif name.startswith('TOYOTA'): final_name = 'TOYOTA' elif name.startswith('WALMART'): final_name = 'WALMART' elif name.startswith('MOODY'): final_name = \"MOODY'S\" elif name.startswith('MACY'): final_name = \"MACY'S\" elif name.startswith('ERNST'): final_name = \"ERNST &amp; YOUNG\" elif name.startswith('JANE STREET'): final_name = 'JANE STREET CAPITAL' elif name.startswith('LIBERTY MUTUAL'): final_name = 'LIBERTY MUTUAL' elif name.startswith('SAMSUNG'): final_name = 'SAMSUNG' else: final_name = name except: final_name = name return final_name salary_df['company'] = salary_df['company'].apply(lambda name: clean_company_names(name)) . | . → 최종적으로 정리 완료된 기업 수: . salary_df['company'].nunique() . 1060 . ",
    "url": "https://chaelist.github.io/docs/kaggle/stem_salaries/#%EB%8D%B0%EC%9D%B4%ED%84%B0-%ED%8C%8C%EC%95%85-%EB%B0%8F-%EC%A0%84%EC%B2%98%EB%A6%AC",
    "relUrl": "/docs/kaggle/stem_salaries/#데이터-파악-및-전처리"
  },"235": {
    "doc": "STEM Salaries 1",
    "title": "기업별, 직업별 salary",
    "content": "기업별 평균 salary . | 평균 salary가 많은 기업 확인 . | 100명 이상의 기록이 있는 기업에 한해 분석 (수가 적으면 1명의 값에 영향을 너무 크게 받으므로) | . sal_comp = salary_df.groupby(['company'])[['totalyearlycompensation']].agg(['mean', 'count']) sal_comp = sal_comp['totalyearlycompensation'].reset_index() sal_comp = sal_comp.query('count &gt;= 100') fig = px.bar(sal_comp.sort_values('mean', ascending=False).head(10), x='company', y='mean', color='mean', hover_name='company', hover_data=['count'], # hover했을 때의 제목 &amp; 정보 추가 labels={'mean':'average yearly compensation', 'count':'data count'}, color_continuous_scale = 'Brwnyl') fig.update(layout_coloraxis_showscale=False) # 원래 오른쪽에 나오게 되는 colorbar를 숨김 fig.show() . | 100개 이상의 기록이 있는 기업 중 가장 평균 yearly compensation이 높은 기업은 ‘Netflix’ | . | 평균 salary가 많은 기업 top 10: gender별로 나눠서 확인 . sal_comp2 = salary_df.groupby(['company', 'gender'])[['totalyearlycompensation']].agg(['mean', 'count']) sal_comp2 = sal_comp2['totalyearlycompensation'].reset_index() top_10 = sal_comp.sort_values('mean', ascending=False).head(10)['company'] sal_comp2 = sal_comp2[sal_comp2['company'].isin(top_10)] fig = px.scatter(sal_comp2, x='company', y='mean', color='gender', category_orders={'company':top_10}, # 가장 평균 salary가 많은 기업부터 순서대로 시각화 hover_name='company', hover_data=['count'], # hover했을 때의 제목 &amp; 정보 추가 labels={'mean':'average yearly compensation', 'count':'data count'}, color_discrete_sequence=px.colors.qualitative.Antique) fig.update_traces(marker={'size': 10}) # marker의 size를 키워줌 fig.show() . | 10개 기업 모두 Male이 Female보다 평균 yearly compensation이 높음 | . | 평균 salary가 많은 기업 top 10: job title별로 나눠서 확인 . sal_comp3 = salary_df.groupby(['company', 'title'])[['totalyearlycompensation']].agg(['mean', 'count']) sal_comp3 = sal_comp3['totalyearlycompensation'].reset_index() top_10 = sal_comp.sort_values('mean', ascending=False).head(10)['company'] sal_comp3 = sal_comp3[sal_comp3['company'].isin(top_10)] fig = px.scatter(sal_comp3, x='company', y='mean', color='title', category_orders={'company':top_10}, # 가장 평균 salary가 많은 기업부터 순서대로 시각화 hover_name='title', hover_data=['count'], # hover했을 때의 제목 &amp; 정보 추가 labels={'mean':'average yearly compensation', 'count':'data count'}, color_discrete_sequence=px.colors.qualitative.Antique) fig.update_traces(marker={'size': 10}) # marker의 size를 키워줌 fig.show() . | 10개 기업 모두 ‘Software Engineering Manager’ title이 평균 yearly compensation이 가장 높음 | . | . title별 평균 salary . | job title별 평균 salary 확인 . sal_title = salary_df.groupby(['title'])[['totalyearlycompensation']].agg(['mean', 'count']) sal_title = sal_title['totalyearlycompensation'].reset_index() fig = px.bar(sal_title.sort_values('mean', ascending=False), x='title', y='mean', color='mean', hover_name='title', hover_data=['count'], # hover했을 때의 제목 &amp; 정보 추가 labels={'mean':'average yearly compensation', 'count':'data count'}, color_continuous_scale = 'Brwnyl') fig.update(layout_coloraxis_showscale=False) # 원래 오른쪽에 나오게 되는 colorbar를 숨김 fig.show() . | ‘Software Engineering Manager’ title이 가장 평균 yearly compensation이 높음 | 그 다음으로 평균 yearly compensation이 높은 건 manager 직급인 ‘Product Manager’, ‘Technical Program Manager’ title | . | job title별 평균 salary: gender별로 나눠서 확인 . sal_title2 = salary_df.groupby(['title', 'gender'])[['totalyearlycompensation']].agg(['mean', 'count']) sal_title2 = sal_title2['totalyearlycompensation'].reset_index() top_order = sal_title.sort_values('mean', ascending=False)['title'] fig = px.scatter(sal_title2, x='title', y='mean', color='gender', category_orders={'title':top_order}, # 가장 평균 salary가 많은 title부터 순서대로 시각화 hover_name='title', hover_data=['count'], # hover했을 때의 제목 &amp; 정보 추가 labels={'mean':'average yearly compensation', 'count':'data count'}, color_discrete_sequence=px.colors.qualitative.Antique) fig.update_traces(marker={'size': 10}) # marker의 size를 키워줌 fig.show() . | 대체로 모든 직군에서 Male이 Female보다 평균 yearly compensation이 높음 | Mechanical Engineer, Recruiter, Business Analyst 직군은 남녀 차이가 거의 없음 | . | job title별 평균 salary: education level별로 나눠서 확인 . sal_title3 = salary_df.groupby(['title', 'Education'])[['totalyearlycompensation']].agg(['mean', 'count']) sal_title3 = sal_title3['totalyearlycompensation'].reset_index() top_order = sal_title.sort_values('mean', ascending=False)['title'] fig = px.scatter(sal_title3, x='title', y='mean', color='Education', category_orders={'title':top_order}, # 가장 평균 salary가 많은 title부터 순서대로 시각화 hover_name='title', hover_data=['count'], # hover했을 때의 제목 &amp; 정보 추가 labels={'mean':'average yearly compensation', 'count':'data count'}, color_discrete_sequence=px.colors.qualitative.Antique) fig.update_traces(marker={'size': 10}) # marker의 size를 키워줌 fig.show() . | 대부분의 직종에서 PhD를 취득한 사람이 평균 yearly compensation이 가장 높음 | . | . 기업별 title별 treemap . | 기록된 직원 정보가 많은 기업 top 20에 대해서 treemap을 그려봄 (회사별 규모 정보가 따로 있으면 좋겠지만, 없으므로 기록된 정보 수를 규모라고 간주하고 분석) | . top_20 = salary_df.groupby('company')[['title']].count().sort_values(by='title', ascending=False).head(20).index fig = px.treemap(salary_df[(salary_df['company'].notnull()) &amp; (salary_df['company'].isin(top_20))], path=[px.Constant('top 20'), 'company', 'title'], color='totalyearlycompensation', color_continuous_scale='Brwnyl') fig.update_layout(margin = dict(t=50, l=25, r=25, b=25)) fig.show() . | 대부분의 기업에서 ‘Software Engineering Manager’ title이 가장 평균 yearly compensation이 높음 | . ",
    "url": "https://chaelist.github.io/docs/kaggle/stem_salaries/#%EA%B8%B0%EC%97%85%EB%B3%84-%EC%A7%81%EC%97%85%EB%B3%84-salary",
    "relUrl": "/docs/kaggle/stem_salaries/#기업별-직업별-salary"
  },"236": {
    "doc": "STEM Salaries 1",
    "title": "성별, 인종, 교육수준별 salary",
    "content": "gender별 평균 salary . sal_gend = salary_df.groupby('gender')[['totalyearlycompensation']].agg(['mean', 'count']) sal_gend = sal_gend['totalyearlycompensation'].reset_index() fig = px.bar(sal_gend, x='gender', y='mean', color='gender', hover_name='gender', hover_data=['count'], # hover했을 때의 제목 &amp; 정보 추가 labels={'mean':'average yearly compensation', 'count':'data count'}, color_discrete_sequence=px.colors.qualitative.Antique) fig.update_traces(showlegend=False) # 원래 오른쪽에 나타나게 되는 legend를 숨김 fig.show() . | Male이 Female보다 평균 yearly compensation이 높음 | . race별 평균 salary . | race별 평균 yearly compensation . sal_race = salary_df.groupby('Race')[['totalyearlycompensation']].agg(['mean', 'count']) sal_race = sal_race['totalyearlycompensation'].reset_index() fig = px.bar(sal_race, x='Race', y='mean', color='Race', category_orders={'Race':['White', 'Asian', 'Black', 'Hispanic']}, hover_name='Race', hover_data=['count'], # hover했을 때의 제목 &amp; 정보 추가 labels={'mean':'average yearly compensation', 'count':'data count'}, color_discrete_sequence=px.colors.qualitative.Antique) fig.update_traces(showlegend=False) # 원래 오른쪽에 나타나게 되는 legend를 숨김 fig.show() . | Two Or More, Unknown을 제외하면 백인이 가장 평균 yearly compensation이 많고, 흑인이 가장 적음 | . | gender * race 교차해서 비교 . sal_race_gend = salary_df.groupby(['gender', 'Race'])[['totalyearlycompensation']].agg(['mean', 'count']) sal_race_gend = sal_race_gend['totalyearlycompensation'].reset_index() fig = px.scatter(sal_race_gend, x='Race', y='mean', color='gender', category_orders={'Race':['White', 'Asian', 'Black', 'Hispanic']}, hover_data=['count'], # hover했을 때의 제목 &amp; 정보 추가 labels={'mean':'average yearly compensation', 'count':'data count'}, color_discrete_sequence=px.colors.qualitative.Antique) fig.update_traces(marker={'size': 10}) # marker의 size를 키워줌 fig.show() . | Two Or More, Unknown을 제외하면 백인 남성이 가장 평균 yearly compensation이 많음 | 혼혈(Two Or More)은 특이하게도 여성이 남성보다 평균 yearly compensation이 많음 | . | race별 gender별 분포 비교 . fig = px.violin(salary_df.query('gender == \"Male\" or gender == \"Female\"'), x='Race', y='totalyearlycompensation', color='gender', category_orders={'Race':['White', 'Asian', 'Black', 'Hispanic'], 'gender':['Female', 'Male']}, hover_name='company', hover_data=['title'], labels={'totalyearlycompensation':'average yearly compensation'}, color_discrete_sequence=px.colors.qualitative.Antique) fig.show() . | 최상위권에는 white / asian male이 가장 많음 (Unknown 제외) | white는 female도 최상위권에 꽤 분포 | Black은 1M 이상이 1명, Hispanic은 없음 | . | . education별 평균 salary . | education별 평균 yearly compensation . sal_edu = salary_df.groupby('Education')[['totalyearlycompensation']].agg(['mean', 'count']) sal_edu = sal_edu['totalyearlycompensation'].reset_index() fig = px.bar(sal_edu, x='Education', y='mean', color='Education', category_orders={'Education':['PhD', \"Master's Degree\", \"Bachelor's Degree\", 'Some College', 'Highschool']}, hover_name='Education', hover_data=['count'], # hover했을 때의 제목 &amp; 정보 추가 labels={'mean':'average yearly compensation', 'count':'data count'}, color_discrete_sequence=px.colors.qualitative.Antique) fig.update_traces(showlegend=False) # 원래 오른쪽에 나타나게 되는 legend를 숨김 fig.show() . | PhD (박사 학위)를 취득한 사람이 평균 yearly compensation이 가장 높음 | . | gender * education 교차해서 비교 . sal_edu_gend = salary_df.groupby(['gender', 'Education'])[['totalyearlycompensation']].agg(['mean', 'count']) sal_edu_gend = sal_edu_gend['totalyearlycompensation'].reset_index() fig = px.scatter(sal_edu_gend, x='Education', y='mean', color='gender', category_orders={'Education':['PhD', \"Master's Degree\", \"Bachelor's Degree\", 'Some College', 'Highschool']}, hover_data=['count'], # hover했을 때의 제목 &amp; 정보 추가 labels={'mean':'average yearly compensation', 'count':'data count'}, color_discrete_sequence=px.colors.qualitative.Antique) fig.update_traces(marker={'size': 10}) # marker의 size를 키워줌 fig.show() . | PhD까지 취득한 남성이 평균 yearly compensation이 가장 높음 | Bachelor’s Degree (학사 학위)를 취득한 여성이 평균 yearly compensation이 가장 낮음 (Highschool이 최종학력인 여성의 경우 데이터 양이 적어서 유의미한 비교라고 하기는 애매) | . | educaiton별 gender별 분포 비교 . orders_dict = {'Education':['PhD', \"Master's Degree\", \"Bachelor's Degree\", 'Some College', 'Highschool'], 'gender':['Female', 'Male']} fig = px.violin(salary_df.query('gender == \"Male\" or gender == \"Female\"'), x='Education', y='totalyearlycompensation', color='gender', category_orders = orders_dict, hover_name='company', hover_data=['title'], labels={'totalyearlycompensation':'average yearly compensation'}, color_discrete_sequence=px.colors.qualitative.Antique) fig.show() . | 오히려 최상위권(1M 이상)에는 PhD보다 Masters’s &amp; Bacher’s Degree 소지자가 더 많음 | . | . ",
    "url": "https://chaelist.github.io/docs/kaggle/stem_salaries/#%EC%84%B1%EB%B3%84-%EC%9D%B8%EC%A2%85-%EA%B5%90%EC%9C%A1%EC%88%98%EC%A4%80%EB%B3%84-salary",
    "relUrl": "/docs/kaggle/stem_salaries/#성별-인종-교육수준별-salary"
  },"237": {
    "doc": "STEM Salaries 1",
    "title": "location별 salary",
    "content": "가장 많은 회사가 위치한 location . | 가장 많은 회사가 위치한 location top 10 . # 데이터에 기록된 회사만을 대상으로 계산 comp_loc = salary_df.groupby(['location'])[['company']].nunique().reset_index() fig = px.bar(comp_loc.sort_values('company', ascending=False).head(10), x='location', y='company', color='company', hover_name='location', labels={'company':'# of companies'}, color_continuous_scale = 'Brwnyl') fig.update(layout_coloraxis_showscale=False) # 원래 오른쪽에 나오게 되는 colorbar를 숨김 fig.show() . | 약 400개의 회사가 위치한 San Francisco가 가장 인기 있는 location | . | 가장 많은 회사가 위치한 location top 10에 대해, 그 곳에 위치한 회사 &amp; 화사별 salary 정보를 쪼개어 확인 . sal_loc2 = salary_df.groupby(['location', 'company'])[['totalyearlycompensation']].agg(['mean', 'count']) sal_loc2 = sal_loc2['totalyearlycompensation'].reset_index() top_comp_locations = comp_loc.sort_values('company', ascending=False).head(10)['location'] fig = px.treemap(sal_loc2[sal_loc2['location'].isin(top_comp_locations)], path = [px.Constant('top locations'), 'location', 'company'], values='count', color = 'mean', hover_data=['count'], labels={'mean':'average yearly compensation', 'count':'data count'}, color_continuous_scale = 'Brwnyl') fig.update_layout(margin = dict(t=50, l=25, r=25, b=25)) fig.show() . | 가장 많은 회사가 위치한 San Francisco의 경우, Google, Uber, Salesforce, Amazon, Facebook 등이 대표적인 회사 | San Francisco의 평균 yearly compensation이 Seattle이나 New York보다 다소 높은 편 | . | . 가장 평균 salary가 높은 location . | 평균 salary가 높은 location top 10 . | 100명 이상의 기록이 있는 위치에 한해 분석 (수가 적으면 1명의 값에 영향을 너무 크게 받으므로) | . sal_loc = salary_df.groupby(['location'])[['totalyearlycompensation']].agg(['mean', 'count']) sal_loc = sal_loc['totalyearlycompensation'].reset_index() sal_loc = sal_loc.query('count &gt;= 100') fig = px.bar(sal_loc.sort_values('mean', ascending=False).head(10), x='location', y='mean', color='mean', hover_name='location', hover_data=['count'], # hover했을 때의 제목 &amp; 정보 추가 labels={'mean':'average yearly compensation', 'count':'data count'}, color_continuous_scale = 'Brwnyl') fig.update(layout_coloraxis_showscale=False) # 원래 오른쪽에 나오게 되는 colorbar를 숨김 fig.show() . | 100명 이상의 기록이 있는 위치 중, 평균 yearly compensation이 가장 높은 location은 Los Gatos | . | 평균 salary가 가장 많은 위치 top 10에 대해, 그 곳에 위치한 회사 &amp; 화사별 salary 정보를 쪼개어 확인 . sal_loc2 = salary_df.groupby(['location', 'company'])[['totalyearlycompensation']].agg(['mean', 'count']) sal_loc2 = sal_loc2['totalyearlycompensation'].reset_index() top_locations = sal_loc.sort_values('mean', ascending=False).head(10)['location'] fig = px.treemap(sal_loc2[sal_loc2['location'].isin(top_locations)], path = [px.Constant('top locations'), 'location', 'company'], values='count', color = 'mean', hover_data=['count'], labels={'mean':'average yearly compensation', 'count':'data count'}, color_continuous_scale = 'Brwnyl') fig.update_layout(margin = dict(t=50, l=25, r=25, b=25)) fig.show() . | Los Gatos는 Netflix가 있어서 평균 salary가 높게 나온 듯 | Los Gatos는 Netflix, Menlo Park은 Facebook, Cupertino는 Apple의 평균 salary가 큰 영향 | . | . ",
    "url": "https://chaelist.github.io/docs/kaggle/stem_salaries/#location%EB%B3%84-salary",
    "relUrl": "/docs/kaggle/stem_salaries/#location별-salary"
  },"238": {
    "doc": "STEM Salaries 2",
    "title": "STEM Salaries 2",
    "content": ". | 데이터 정리 . | company 표기 통일 | . | 기간별 비교 . | datetime 타입 가공 | 분기별 추이 비교 | . | basesalary, stock, bonus . | totalyearlycompensation과의 관계 | 기업별 salary, bonus, stock grant value | . | Data Scientist 정보 파악 . | 분기별 추이 | gender, race, education | Top 기업 파악 | . | . *분석 대상 데이터셋: Data Science and STEM Salaries . | 데이터셋 출처 | levels.fyi에서 가져온 62,642개의 salary data (Data Science &amp; STEM 직종) | 2017.06.07 ~ 2021.08.17의 기간에 기록된 약 1060개의 회사, 15개 job title에 대한 데이터 | . ",
    "url": "https://chaelist.github.io/docs/kaggle/stem_salaries2/",
    "relUrl": "/docs/kaggle/stem_salaries2/"
  },"239": {
    "doc": "STEM Salaries 2",
    "title": "데이터 정리",
    "content": "# 필요한 라이브러리 import import pandas as pd import numpy as np import plotly.express as px import plotly.io as pio pio.templates.default = \"plotly_white\" # default template을 지정 . salary_df = pd.read_csv('data/Levels_Fyi_Salary_Data.csv') salary_df.head(3) . |   | timestamp | company | level | title | totalyearlycompensation | location | yearsofexperience | yearsatcompany | tag | basesalary | stockgrantvalue | bonus | gender | otherdetails | cityid | dmaid | rowNumber | Masters_Degree | Bachelors_Degree | Doctorate_Degree | Highschool | Some_College | Race_Asian | Race_White | Race_Two_Or_More | Race_Black | Race_Hispanic | Race | Education | . | 0 | 6/7/2017 11:33:27 | Oracle | L3 | Product Manager | 127000 | Redwood City, CA | 1.5 | 1.5 | nan | 107000 | 20000 | 10000 | nan | nan | 7392 | 807 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | nan | nan | . | 1 | 6/10/2017 17:11:29 | eBay | SE 2 | Software Engineer | 100000 | San Francisco, CA | 5 | 3 | nan | 0 | 0 | 0 | nan | nan | 7419 | 807 | 2 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | nan | nan | . | 2 | 6/11/2017 14:53:57 | Amazon | L7 | Product Manager | 310000 | Seattle, WA | 8 | 0 | nan | 155000 | 0 | 0 | nan | nan | 11527 | 819 | 3 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | nan | nan | . | 불필요한 칼럼 삭제 . | cityid, dmaid, rowNumber 칼럼은 불필요하다고 판단 | Masters_Degree ~ Race_Hispanic 칼럼은 ‘Race’와 ‘Education’의 정보를 더미변수화해놓은 칼럼이므로 우선 삭제 | . drop_columns = salary_df.loc[:, 'cityid':'Race_Hispanic'].columns salary_df.drop(drop_columns, axis='columns', inplace=True) . | 중복값 제거 . | 모든 컬럼이 중복되는 경우는 실수로 중복 기입된 것이라 간주, 하나의 row만 남기고 삭제 | . print('중복 제거 이전: ', len(salary_df)) salary_df.drop_duplicates(inplace=True, ignore_index=True) print('중복 제거 이후: ', len(salary_df)) . 중복 제거 이전: 62642 중복 제거 이후: 62598 . | 결측치 채우기 . salary_df[['gender', 'Race', 'Education']] = salary_df[['gender', 'Race', 'Education']].fillna('Unknown') . | 이상한 값 처리 . # gender == \"Title: Senior Software Engineer\"라고 기입되어 있던 행의 gender 값을 'Unknown'으로 바꿔줌 salary_df.loc[10985, 'gender'] = 'Unknown' . | . company 표기 통일 . | 공백을 제거하고 uppercase로 맞춰줌 salary_df['company'] = salary_df['company'].str.strip().str.upper() . | llc, org, ltd 등 뒤에 붙는 다양한 단어들을 지워줌 salary_df['company'] = salary_df['company'].str.replace(' LLC', '').str.replace('.ORG', '').str.replace(' LTD', '').str.replace(' CORPORATION', '').str.replace(' INC', '') salary_df['company'] = salary_df['company'].str.replace(' MEDIA', '').str.replace(' GROUP', '').str.replace(' TECHNOLOGY', '').str.replace(' TECHNOLOGIES', '').str.strip() . salary_df['company'].nunique() . 1081 . | 같은 회사인데 표기가 다른 경우를 함수로 만들어서 정리 . | ‘amazon.com’과 ‘amazon web services’처럼 계열사지만 다른 기업이라고 판단되는 경우는 그냥 둠 | . def clean_company_names(name): try: if name.startswith('AMAZON.COM'): final_name = 'AMAZON' elif name.startswith('ARISTA'): final_name = 'ARISTA' elif name.startswith('BLOOMBERG'): final_name = 'BLOOMBERG' elif name.startswith('BOOKING'): final_name = 'BOOKING.COM' elif name.startswith('DELOITTE CONSULTING'): final_name = 'DELOITTE CONSULTING' elif name.startswith('FORD'): final_name = 'FORD' elif name.startswith('CADENCE'): final_name = 'CADENCE' elif name.startswith('MCKINSEY'): final_name = 'MCKINSEY &amp; COMPANY elif name.startswith('MOTOROLA'): final_name = 'MOTOROLA' elif name.startswith('NUANCE'): final_name = 'NUANCE elif name.startswith('COSTCO'): final_name = 'COSTCO' elif name.startswith('TOYOTA'): final_name = 'TOYOTA' elif name.startswith('WALMART'): final_name = 'WALMART' elif name.startswith('MOODY'): final_name = \"MOODY'S\" elif name.startswith('MACY'): final_name = \"MACY'S\" elif name.startswith('ERNST'): final_name = \"ERNST &amp; YOUNG\" elif name.startswith('JANE STREET'): final_name = 'JANE STREET CAPITAL' elif name.startswith('LIBERTY MUTUAL'): final_name = 'LIBERTY MUTUAL' elif name.startswith('SAMSUNG'): final_name = 'SAMSUNG' else: final_name = name except: final_name = name return final_name salary_df['company'] = salary_df['company'].apply(lambda name: clean_company_names(name)) . → 최종적으로 정리 완료된 기업 수: . salary_df['company'].nunique() . 1060 . | . ",
    "url": "https://chaelist.github.io/docs/kaggle/stem_salaries2/#%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%A0%95%EB%A6%AC",
    "relUrl": "/docs/kaggle/stem_salaries2/#데이터-정리"
  },"240": {
    "doc": "STEM Salaries 2",
    "title": "기간별 비교",
    "content": "datetime 타입 가공 . | datetime 타입으로 변환 . salary_df['timestamp'] = pd.to_datetime(salary_df['timestamp']) . | timestamp 칼럼은 levels.fyi에 데이터가 기록된 시점을 의미 | . | 기간을 월별, 분기별로 묶어줌 . salary_df['yearmonth'] = salary_df['timestamp'].dt.strftime('%Y%m') salary_df['yearquarter'] = salary_df['timestamp'].dt.to_period(\"Q\").astype('str') salary_df[['timestamp', 'yearmonth', 'yearquarter']].head() . |   | timestamp | yearmonth | yearquarter | . | 0 | 2017-06-07 11:33:27 | 201706 | 2017Q2 | . | 1 | 2017-06-10 17:11:29 | 201706 | 2017Q2 | . | 2 | 2017-06-11 14:53:57 | 201706 | 2017Q2 | . | 3 | 2017-06-17 00:23:14 | 201706 | 2017Q2 | . | 4 | 2017-06-20 10:58:51 | 201706 | 2017Q2 | . | . 분기별 추이 비교 . | 분기별 평균 yearly compensation . quarterly_sal = salary_df.groupby(['yearquarter'])[['totalyearlycompensation']].agg(['mean', 'count']) quarterly_sal = quarterly_sal['totalyearlycompensation'].reset_index() fig = px.line(quarterly_sal, x='yearquarter', y='mean', hover_data=['count'], labels={'mean':'average yearly compensation', 'yearquarter':'posted quarter', 'count':'data count'}, color_discrete_sequence=px.colors.qualitative.Antique) fig.show() . | 2018Q1 이후 평균 yearly compensation은 약간 하락세를 보임. 하지만 점점 사이트에 등록된 data 수가 증가하면서 yearly compensation의 평균값이 평균으로 회귀하는 것일 수도 있으므로… 평균 salary가 하락세라고 단정적으로 말하기는 어렵다. | . | 분기별 등록된 data의 수 . fig = px.histogram(salary_df, x='yearquarter', labels={'yearquarter':'posted quarter'}, color_discrete_sequence=px.colors.qualitative.Antique) fig.show() . | . ",
    "url": "https://chaelist.github.io/docs/kaggle/stem_salaries2/#%EA%B8%B0%EA%B0%84%EB%B3%84-%EB%B9%84%EA%B5%90",
    "relUrl": "/docs/kaggle/stem_salaries2/#기간별-비교"
  },"241": {
    "doc": "STEM Salaries 2",
    "title": "basesalary, stock, bonus",
    "content": "totalyearlycompensation과의 관계 . | totalyearlycompensation (연간 총소득)과 다른 변수들이 어떤 관계가 있는지 확인 | . | 변수들 간의 상관관계를 파악 . plt.figure(figsize=(8, 6)) sns.heatmap(salary_df.corr(), annot=True, cmap='pink_r') plt.xticks(rotation=45); . | basesalary와 stockgrantvalue가 가장 totalyearlycompensation과 상관관계가 강한 편 | . | basesalary와 totalyearlycompensation 사이의 관계 . fig = px.scatter(salary_df, x='basesalary', y='totalyearlycompensation', trendline='ols', hover_name='company', hover_data = ['gender', 'Race', 'Education'], color_discrete_sequence=px.colors.qualitative.Antique, trendline_color_override='peachpuff') fig.show() . | stockgrantvalue와 totalyearlycompensation 사이의 관계 . fig = px.scatter(salary_df, x='stockgrantvalue', y='totalyearlycompensation', trendline='ols', hover_name='company', hover_data = ['gender', 'Race', 'Education'], color_discrete_sequence=px.colors.qualitative.Antique, trendline_color_override='peachpuff') fig.show() . | . 기업별 salary, bonus, stock grant value . | 평균 yearly compensation이 높은 기업 top 10 . | 100명 이상의 기록이 있는 기업에 한해 분석 (수가 적으면 1명의 값에 영향을 너무 크게 받으므로) | . sal_comp = salary_df.groupby(['company'])[['totalyearlycompensation']].agg(['mean', 'count']) sal_comp = sal_comp['totalyearlycompensation'].reset_index() sal_comp = sal_comp.query('count &gt;= 100') fig = px.bar(sal_comp.sort_values('mean', ascending=False).head(10), x='company', y='mean', color='mean', hover_name='company', hover_data=['count'], labels={'mean':'average yearly compensation', 'count':'data count'}, color_continuous_scale = 'Brwnyl') fig.update(layout_coloraxis_showscale=False) fig.show() . | 평균 base salary가 높은 기업 top 10 . | 100명 이상의 기록이 있는 기업에 한해 분석 | basesalary &gt; 0인 경우에 한해서 분석 (base salary가 없을 수는 없으므로, 0이라고 적힌 것은 결측치라고 간주) | . base_comp = salary_df.query('basesalary &gt; 0').groupby(['company'])[['basesalary']].agg(['mean', 'count']) base_comp = base_comp['basesalary'].reset_index() base_comp = base_comp.query('count &gt;= 100') fig = px.bar(base_comp.sort_values('mean', ascending=False).head(10), x='company', y='mean', color='mean', hover_name='company', hover_data=['count'], labels={'mean':'average base salary', 'count':'data count'}, color_continuous_scale = 'Brwnyl') fig.update(layout_coloraxis_showscale=False) fig.show() . | Netflix를 제외하고는 total yearly compensation과 순위가 많이 달라짐 | Netflix의 경우 total yearly compensation의 평균과 base salary의 평균이 크게 다르지 않다는 것을 알 수 있음 | . | 변수 간 관계 파악: Netflix . plt.figure(figsize=(8, 6)) sns.heatmap(salary_df.query('company == \"NETFLIX\"').corr(), annot=True, cmap='pink_r') plt.xticks(rotation=45); . | Netflix의 경우, total yearly compensation은 base salary와 가장 강한 관계를 보이고, stock grant value나 bonus와는 관계가 없다는 것이 확인됨 | Netflix는 stock grant나 bonus를 통해 보상을 크게 지급하기보다는 base salary 자체를 높게 가져가는 연봉 체계를 가지고 있는 것으로 추측됨 | . | 평균 bonus가 많은 기업 top 10 . | 100명 이상의 기록이 있는 기업에 한해 분석 | . bonus_comp = salary_df.groupby(['company'])[['bonus']].agg(['mean', 'count']) bonus_comp = bonus_comp['bonus'].reset_index() bonus_comp = bonus_comp.query('count &gt;= 100') fig = px.bar(bonus_comp.sort_values('mean', ascending=False).head(10), x='company', y='mean', color='mean', hover_name='company', hover_data=['count'], labels={'mean':'average bonus', 'count':'data count'}, color_continuous_scale = 'Brwnyl') fig.update(layout_coloraxis_showscale=False) fig.show() . | 변수 간 관계 파악: Cruise . plt.figure(figsize=(8, 6)) sns.heatmap(salary_df.query('company == \"CRUISE\"').corr(), annot=True, cmap='pink_r') plt.xticks(rotation=45); . | Cruise의 경우, base salary뿐 아니라 stock grant value와 bonus도 totaly yearly compensation과 강한 관계를 보임 | . | 평균 stock grant value가 많은 기업 top 10 . | 100명 이상의 기록이 있는 기업에 한해 분석 | . stock_comp = salary_df.groupby(['company'])[['stockgrantvalue']].agg(['mean', 'count']) stock_comp = stock_comp['stockgrantvalue'].reset_index() stock_comp = stock_comp.query('count &gt;= 100') fig = px.bar(stock_comp.sort_values('mean', ascending=False).head(10), x='company', y='mean', color='mean', hover_name='company', hover_data=['count'], # hover했을 때의 제목 &amp; 정보 추가 labels={'mean':'average stock grant value', 'count':'data count'}, color_continuous_scale = 'Brwnyl') fig.update(layout_coloraxis_showscale=False) # 원래 오른쪽에 나오게 되는 colorbar를 숨김 fig.show() . | 변수 간 관계 파악: Snap . plt.figure(figsize=(8, 6)) sns.heatmap(salary_df.query('company == \"SNAP\"').corr(), annot=True, cmap='pink_r') plt.xticks(rotation=45); . | 다소 약하긴 하지만, stock grant value가 total yearly compensation과 어느 정도의 상관관계를 보임 | . | 변수 간 관계 파악: Lyft . plt.figure(figsize=(8, 6)) sns.heatmap(salary_df.query('company == \"LYFT\"').corr(), annot=True, cmap='pink_r') plt.xticks(rotation=45); . | Lyft는 stock grant value가 total yearly compensation가 가장 강한 관계를 보임 | . | . &gt;&gt; 결론: 기업마다 각자의 보상 체계에 따라, base salary가 total yearly compensation을 거의 결정하기도 하고, bonus나 stock grant value가 total yearly compensation의 많은 부분을 차지하기도 한다 (total yearly compensation과 각 변수 간의 관계는 기업에 따라 달라진다) . ",
    "url": "https://chaelist.github.io/docs/kaggle/stem_salaries2/#basesalary-stock-bonus",
    "relUrl": "/docs/kaggle/stem_salaries2/#basesalary-stock-bonus"
  },"242": {
    "doc": "STEM Salaries 2",
    "title": "Data Scientist 정보 파악",
    "content": ": 최근 유망한 직종인 Data Scientist에 대해 집중 탐구 . 분기별 추이 . | 분기별 data 수의 변화 . quarterly_ds_sal = ds_salary.groupby(['yearquarter'])[['totalyearlycompensation']].agg(['mean', 'count']) quarterly_ds_sal = quarterly_ds_sal['totalyearlycompensation'].reset_index() fig = px.bar(quarterly_ds_sal, x='yearquarter', y='count', labels={'yearquarter':'posted quarter', 'count':'data count'}, color_discrete_sequence=px.colors.qualitative.Antique) fig.show() . | 2021Q3은 2021.08.17까지밖에 없어서 적은 것 | 점점 data scientist에 대한 기록이 증가하는 것으로 보이나, 전반적으로 이 dataset에서는 직업에 관계 없이 최근 분기일수록 더 기록이 많아지기 때문에 data scientist에 대한 관심이 증가한 것이라고 해석하기는 어려움 | . | 분기별 data scientist 기록의 전체 대비 % . | data scientist 뿐 아니라 다른 data도 수가 같이 증가하므로, 전체 대비 %를 계산해봄 | . temp1 = salary_df.groupby(['yearquarter'])[['timestamp']].count() temp1.rename(columns={'timestamp':'total_count'}, inplace=True) temp2 = ds_salary.groupby(['yearquarter'])[['timestamp']].count() temp2.rename(columns={'timestamp':'ds_count'}, inplace=True) temp3 = temp1.join(temp2) temp3.fillna(0, inplace=True) temp3['ds_ratio'] = temp3['ds_count'] / temp3['total_count'] fig = px.bar(temp3.reset_index(), x='yearquarter', y='ds_ratio', labels={'yearquarter':'posted quarter'}, color_discrete_sequence=px.colors.qualitative.Antique) fig.layout.yaxis.tickformat = ',.2%' fig.show() . | 2018년 Q4부터는 Data Scientist에 대한 기록이 꾸준히 전체의 4% 내외로 유지됨 | 물론 levels.fyi에 기록되지 않은 데이터가 많겠지만, 주어진 데이터만 보면 Data Scientist의 수가 다른 STEM 직종에 비해 유독 증가하고 있다고 보기는 어려움 | . | 분기별 data scientist의 평균 yearly compensation . fig = px.bar(quarterly_ds_sal, x='yearquarter', y='mean', hover_data=['count'], labels={'mean':'average yearly compensation', 'yearquarter':'posted quarter', 'count':'data count'}, color_discrete_sequence=px.colors.qualitative.Antique) fig.show() . | 2018Q2는 data 수가 너무 적으므로 제외하고 생각하면, 대체로 data scientist의 평균 yearly compensation은 비슷한 수준으로 유지됨 | . | . gender, race, education . | Gender별 기록 수 . | levels.fyi에 기록된 데이터에만 한정된 분석이지만, Data Scientist라는 직업에 어느 성별이 더 많은지 대강 파악할 수 있음 | . fig = px.pie(ds_salary.query('gender != \"Unknown\"'), names='gender', # Unkown은 제외하고 계산 color_discrete_sequence=px.colors.qualitative.Antique, hole=0.4) fig.show() . | 남성이 3/4 이상을 차지 | . | Race별 기록 수 . fig = px.pie(ds_salary.query('Race != \"Unknown\"'), names='Race', # Unkown은 제외하고 계산 color_discrete_sequence=px.colors.qualitative.Antique, hole=0.4) fig.show() . | Asian이 과반수를 차지, 그 다음은 White | . | Education별 기록 수 . fig = px.pie(ds_salary.query('Education != \"Unknown\"'), names='Education', color_discrete_sequence=px.colors.qualitative.Antique, hole=0.4) fig.show() . | Master’s Degree 소지자가 과반수, 그 다음은 PhD | 석사 학위 이상을 소지한 사람이 유독 많은 직업군으로 보임 (cf. 전체 데이터 중에서는 학사 학위 소지자가 약 41.5%) | . | Gender별 평균 yearly compensation . ds_sal_gend = ds_salary.groupby('gender')[['totalyearlycompensation']].agg(['mean', 'count']) ds_sal_gend = ds_sal_gend['totalyearlycompensation'].reset_index() fig = px.bar(ds_sal_gend, x='gender', y='mean', color='gender', hover_name='gender', hover_data=['count'], labels={'mean':'average yearly compensation', 'count':'data count'}, color_discrete_sequence=px.colors.qualitative.Antique) fig.update_traces(showlegend=False) fig.show() . | Male이 Female보다 다소 높지만, 차이가 아주 유의미하다고 보기는 어려울 듯 (cf. 평균 차이를 t-test로 검정하면 p값은 0.02 정도) | . | Race별 평균 yearly compensation . ds_sal_race = ds_salary.groupby('Race')[['totalyearlycompensation']].agg(['mean', 'count']) ds_sal_race = ds_sal_race['totalyearlycompensation'].reset_index() fig = px.bar(ds_sal_race, x='Race', y='mean', color='Race', category_orders={'Race':['White', 'Asian', 'Black', 'Hispanic']}, hover_name='Race', hover_data=['count'], labels={'mean':'average yearly compensation', 'count':'data count'}, color_discrete_sequence=px.colors.qualitative.Antique) fig.update_traces(showlegend=False) fig.show() . | 등록된 Data Scientist의 수는 Asian이 더 많지만, 연간 보상의 평균은 White가 Asian보다 다소 높음 | . | Education별 평균 yearly compensation . ds_sal_edu = ds_salary.groupby('Education')[['totalyearlycompensation']].agg(['mean', 'count']) ds_sal_edu = ds_sal_edu['totalyearlycompensation'].reset_index() fig = px.bar(ds_sal_edu, x='Education', y='mean', color='Education', category_orders={'Education':['PhD', \"Master's Degree\", \"Bachelor's Degree\", 'Some College', 'Highschool']}, hover_name='Education', hover_data=['count'], labels={'mean':'average yearly compensation', 'count':'data count'}, color_discrete_sequence=px.colors.qualitative.Antique) fig.update_traces(showlegend=False) fig.show() . | 등록된 Data Scientist의 수는 Master’s Degree 소지자가 가장 많지만, 연간 보상의 평균은 PhD 소지자가 가장 높음 | . | . Top 기업 파악 . | 등록된 data scientist 수가 많은 기업 확인 . ds_count = ds_salary.groupby(['company'])[['timestamp']].count().reset_index() fig = px.bar(ds_count.sort_values('timestamp', ascending=False).head(10), x='company', y='timestamp', color='timestamp', hover_name='company', labels={'timestamp':'data count'}, color_continuous_scale = 'Brwnyl') fig.update(layout_coloraxis_showscale=False) fig.show() . | 가장 Data Scientist를 많이 고용하는 회사는 Amazon, Microsoft, Facebook이라고 추정 (회사의 규모와 비례하는 듯) | . | Salary Top 기업 . | 평균 yearly compensation이 높은 top 10 기업 파악 | 20명 이상의 기록이 있는 기업에 한해 분석 (수가 적으면 평균이 1명의 영향을 크게 받으므로) | . ds_sal_comp = ds_salary.groupby(['company'])[['totalyearlycompensation']].agg(['mean', 'count']) ds_sal_comp = ds_sal_comp['totalyearlycompensation'].reset_index() ds_sal_comp = ds_sal_comp.query('count &gt;= 20') fig = px.bar(ds_sal_comp.sort_values('mean', ascending=False).head(10), x='company', y='mean', color='mean', hover_name='company', hover_data=['count'], # hover했을 때의 제목 &amp; 정보 추가 labels={'mean':'average yearly compensation', 'count':'data count'}, color_continuous_scale = 'Brwnyl') fig.update(layout_coloraxis_showscale=False) # 원래 오른쪽에 나오게 되는 colorbar를 숨김 fig.show() . | 가장 평균 yearly compensation이 높은 기업은 Netflix | . | . ",
    "url": "https://chaelist.github.io/docs/kaggle/stem_salaries2/#data-scientist-%EC%A0%95%EB%B3%B4-%ED%8C%8C%EC%95%85",
    "relUrl": "/docs/kaggle/stem_salaries2/#data-scientist-정보-파악"
  },"243": {
    "doc": "Text 분석",
    "title": "Text 분석",
    "content": " ",
    "url": "https://chaelist.github.io/docs/text_analysis",
    "relUrl": "/docs/text_analysis"
  },"244": {
    "doc": "시계열 데이터 예측",
    "title": "시계열 데이터 예측",
    "content": ". | Prophet 기초 . | 이론 | 시계열 데이터 준비 | Prophet으로 시계열 예측 | . | 파라미터 조정하기 . | Trend 조절 (changepoint 조절) | Seasonality 반영 | Holiday 반영 | Outlier 제외하기 | . | . *Facebook의 오픈소스 라이브러리인 ‘Prophet‘을 활용 . ",
    "url": "https://chaelist.github.io/docs/ml_application/time_series/",
    "relUrl": "/docs/ml_application/time_series/"
  },"245": {
    "doc": "시계열 데이터 예측",
    "title": "Prophet 기초",
    "content": "이론 . | 세 개의 주요 요소를 활용해 예측: Trend, Seasonality, Holiday | $ y(t) = g(t) + s(t) + h(t) + \\epsilon_i $ . | $g(t)$: the trend function. models non-periodic changes in the value of the time series. 주기적이지 않은 변화를 반영 (트렌드를 반영) | $s(t)$: represents periodic changes (ex. weekly / yearly seasonality) 주기적인 변화를 반영 (주, 일, 연 등의 기간에 따라 주기적으로 나타나는 흐름) | $h(t)$: represents the effects of holidays 불규칙한 event인 휴일의 영향을 반영 | $\\epsilon_i$: the error term. represents any unusual changes not accommodated by the model (정규분포를 따른다고 가정) | . | . 시계열 데이터 준비 . # 필요한 library를 import import pandas as pd import matplotlib.pyplot as plt %matplotlib inline from datetime import datetime from pandas_datareader import data . ⁣1. Alphabet (Google 모기업) 주식 정보 가져오기 . start_date = datetime(2020, 1, 1) end_date = datetime(2021, 5, 1) ## cf) datetime.now()라고 하면 오늘 날짜로 가져옴 Google = data.DataReader('GOOGL','yahoo', start_date, end_date) # yahoo 주식 데이터에서 Alphabet 주식 데이터 가져옴 Google.head() . | Date | High | Low | Open | Close | Volume | Adj Close | . | 2020-01-02 | 1368.68 | 1346.49 | 1348.41 | 1368.68 | 1363900 | 1368.68 | . | 2020-01-03 | 1373.75 | 1347.32 | 1348 | 1361.52 | 1170400 | 1361.52 | . | 2020-01-06 | 1398.32 | 1351 | 1351.63 | 1397.81 | 2338400 | 1397.81 | . | 2020-01-07 | 1403.5 | 1391.56 | 1400.46 | 1395.11 | 1716500 | 1395.11 | . | 2020-01-08 | 1411.85 | 1392.63 | 1394.82 | 1405.04 | 1765700 | 1405.04 | .   . # 종가(Close) 기준으로 그래프 그려보기 Google['Close'].plot(figsize=(11,5), grid=True); .   . ⁣2. 2021.01 전까지로 데이터를 자르기 . # 2020-12-31까지의 데이터만을 사용: 2021-05-01까지의 데이터를 예측해볼 예정 Google_2020 = Google[:'2020-12-31'] Google_2020.tail() . | Date | High | Low | Open | Close | Volume | Adj Close | . | 2020-12-24 | 1742.41 | 1724.35 | 1729 | 1734.16 | 465600 | 1734.16 | . | 2020-12-28 | 1787 | 1741.82 | 1744.91 | 1773.96 | 1382500 | 1773.96 | . | 2020-12-29 | 1788.47 | 1755.11 | 1787.23 | 1757.76 | 986300 | 1757.76 | . | 2020-12-30 | 1767.76 | 1728 | 1765 | 1736.25 | 1051300 | 1736.25 | . | 2020-12-31 | 1757.5 | 1736.09 | 1737.27 | 1752.64 | 1053500 | 1752.64 | . ⁣3. Prophet에 학습시키기 위한 포맷으로 맞추기 . | pandas.DataFrame with ‘y’ and ‘ds’ columns | . # Prophet에 학습시키려면 아래와 같은 포맷이여야 함 df = pd.DataFrame({'ds':Google_2020.index, 'y':Google_2020['Close']}) df.reset_index(inplace=True, drop=True) df.head() . |   | ds | y | . | 0 | 2020-01-02 | 1368.68 | . | 1 | 2020-01-03 | 1361.52 | . | 2 | 2020-01-06 | 1397.81 | . | 3 | 2020-01-07 | 1395.11 | . | 4 | 2020-01-08 | 1405.04 | . Prophet으로 시계열 예측 . | 먼저 pip install pystan, pip install prophet으로 설치해줘야 사용 가능 (설치 방법) | Google Colaboratory에서는 별도의 설치 없이 import해서 사용 가능 | . from fbprophet import Prophet m = Prophet() m.fit(df) . | yearly seasonality와 daily seasonality는 따로 써주지 않으면 반영되지 않음 | .   . ⁣1. 2021.05.01까지의 주가를 예측해 봄 . future = m.make_future_dataframe(periods=121) # 2021.05.01까지의 데이터를 예측할 예정 (=121일) forecast = m.predict(future) forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail() . |   | ds | yhat | yhat_lower | yhat_upper | . | 369 | 2021-04-27 | 2044.79 | 1870.67 | 2224.93 | . | 370 | 2021-04-28 | 2050.44 | 1880.35 | 2231.62 | . | 371 | 2021-04-29 | 2046.42 | 1869.55 | 2222.84 | . | 372 | 2021-04-30 | 2050.11 | 1872.39 | 2221.89 | . | 373 | 2021-05-01 | 2032.17 | 1848.47 | 2218.29 | . ⁣2. forecast 시각화해서 확인 . m.plot(forecast); . ⁣3. 요소별로 확인해보기 . m.plot_components(forecast); . ⁣4. 2021년의 실제 데이터와 예측값을 비교해서 그려보기 . plt.figure(figsize=(12,6)) plt.plot(Google.index, Google['Close'], label='real') plt.plot(forecast['ds'], forecast['yhat'], label='forecast') plt.grid() plt.legend() plt.show() . ",
    "url": "https://chaelist.github.io/docs/ml_application/time_series/#prophet-%EA%B8%B0%EC%B4%88",
    "relUrl": "/docs/ml_application/time_series/#prophet-기초"
  },"246": {
    "doc": "시계열 데이터 예측",
    "title": "파라미터 조정하기",
    "content": "Trend 조절 (changepoint 조절) . | changepoint_range: changepoint 설정 가능 범위를 조절 (default: 80%) | changepoint_prior_scale: changepoint의 유연성 조절 (default: 0.5) | changepoints: 트렌드 변화 시점을 리스트로 직접 assign | n_changepoints: changepoint의 개수를 조절 | .   . ## 기본으로 잡히는 changepoint 시각화해보기 from fbprophet.plot import add_changepoints_to_plot # 필요한 함수 m = Prophet(yearly_seasonality=False, daily_seasonality=True) m.fit(df) future = m.make_future_dataframe(periods=121) forecast = m.predict(future) fig = m.plot(forecast) add_changepoints_to_plot(fig.gca(), m, forecast); . 1. changepoint_range 조절 . | 기본적으로 Prophet은 시계열 데이터의 80% 크기에서 ChangePoint를 지정 (overfitting을 피하기 위해 전체 데이터가 아닌 학습 데이터의 앞부분 80%의 데이터만을 사용해 변동점을 찾는 것) | . # changepoint_range를 0.5로 변경 # 시계열 데이터의 앞 50%에서만 changepoint를 지정하라는 뜻 m = Prophet(yearly_seasonality=False, daily_seasonality=True, changepoint_range=0.5) m.fit(df) future = m.make_future_dataframe(periods=121) forecast = m.predict(future) fig = m.plot(forecast) add_changepoints_to_plot(fig.gca(), m, forecast); . 2. changepoint_prior_scale 조절 . | ChangePoint의 유연성을 조정하는 방법 | 기본 값은 0.05이며, 값을 늘리면 더 유연해지고(=underfitting 해결), 값을 줄이면 유연성 감소(=overfitting 해결) | . # changepoint_prior_scale를 0.1로 늘려봄. # 0.05일 때(=default)보다 유동적으로 changepoint를 찾는 것을 알 수 있다. (→ overfitting 가능성) m = Prophet(yearly_seasonality=False, daily_seasonality=True, changepoint_prior_scale=0.1) m.fit(df) future = m.make_future_dataframe(periods=121) forecast = m.predict(future) fig = m.plot(forecast) a = add_changepoints_to_plot(fig.gca(), m, forecast) . 3. changepoints로 직접 날짜 assign . | changepoint일 수 있는 날짜들을 직접 지정해주는 것. | changepoints에 값을 명시해주지 않으면 자동으로 changepoint들이 선택된다 | . # changepoint를 내가 직접 지정해줌 m = Prophet(yearly_seasonality=False, daily_seasonality=True, changepoints=['2020-03-20', '2020-09-25']) m.fit(df) future = m.make_future_dataframe(periods=121) forecast = m.predict(future) fig = m.plot(forecast) a = add_changepoints_to_plot(fig.gca(), m, forecast) . Seasonality 반영 . | yearly_seasonality: 연 주기의 계절성을 반영. default는 10. | weekly_seasonality: 주간 주기의 계절성을 반영. default는 3 | daily_seasonality: 일 주기의 계절성을 반영. default는 4 | seasonality_mode: ‘additive’ 혹은 ‘multiplicative’ (‘additive’가 default) | seasonality_prior_scale: 계절성 반영 강도 | .   . 1. yearly_seasonality . # yearly_seasonality=True라고 해주면 default값인 10으로 설정됨 m = Prophet(yearly_seasonality=True) m.fit(df) future = m.make_future_dataframe(periods=121) forecast = m.predict(future) m.plot_components(forecast); . # 연 단위의 seasonality가 있다고 간주→ 2020년과 동일하게 3월에 급감하는 모양으로 예측 m.plot(forecast); . +) yearly_seasonality의 강도 높여보기 . # yearly_seasonality를 20으로 설정해주면, default값인 10에 비해 더 연 계절성을 강하게 반영 m = Prophet(yearly_seasonality=20) m.fit(df) future = m.make_future_dataframe(periods=121) forecast = m.predict(future) m.plot_components(forecast); . m.plot(forecast); # yearly_seasonlity가 더 강하게 반영됨 . 2. weekly_seasonality . # weekly_seasonality는 따로 써주지 않아도 default로 3이 들어감. 20으로 넣으면 더 강하게 반영. m = Prophet(weekly_seasonality=20) m.fit(df) future = m.make_future_dataframe(periods=121) forecast = m.predict(future) m.plot_components(forecast); . m.plot(forecast); # weekly_seasonlity가 강하게 반영됨 . 3. daily_seasonality . # daily_seasonality=True라고 해주면 default값인 4으로 설정됨 m = Prophet(daily_seasonality=True) m.fit(df) future = m.make_future_dataframe(periods=121) forecast = m.predict(future) m.plot_components(forecast); . m.plot(forecast); # daily_seasonlity가 반영됨 . +) daily_seasonality의 강도 높여보기 . m = Prophet(daily_seasonality=20) # default: 4 m.fit(df) future = m.make_future_dataframe(periods=121) forecast = m.predict(future) m.plot_components(forecast); . m.plot(forecast); # daily_seasonlity가 강하게 반영됨 . 4. custom seasonality 추가 . | m.add_seasonality()로 직접 seasonality를 만들어 추가할 수 있다 | . # monthly seasonality 만들어서 추가해보기 m = Prophet() m.add_seasonality(name='monthly', period=30.5, fourier_order=5) m.fit(df) future = m.make_future_dataframe(periods=121) forecast = m.predict(future) m.plot_components(forecast); . m.plot(forecast); . 5. multiplicative seasonality . | seasonality_mode: additive가 default | additive는 seasonality가 일정함을 의미하고, multiplicative는 seasonality가 트렌드와 함께 점점 증가함을 의미 | multiplicative seasonality가 필요한 경우: number of air passengers 예시 | . # seasonality_mode='multiplicative'라고 설정해주면 점점 seasonality가 증가하는 것으로 반영됨 m = Prophet(yearly_seasonality=False, daily_seasonality=True, seasonality_mode='multiplicative') m.fit(df) future = m.make_future_dataframe(periods=121) forecast = m.predict(future) m.plot(forecast); # 이 데이터에서는 사실 multiplicative seasonality가 필요하지 않음 . Holiday 반영 . | holidays: 휴일 / 이벤트 기간을 명시한 데이터프레임 . | ‘lower_window’, ‘upper_window’를 설정해 전후 기간에 미치는 휴일의 영향을 반영 가능 | . | holiday_prior_scale: holiday 반영 강도 (default는 10) . | holiday의 영향을 키우고 싶으면 숫자를 키우고, 영향을 줄이고 싶으면 숫자를 줄여서 써주면 됨 | . | m.add_country_holidays(country_name=’US’) 이렇게 하면 특정 국가의 공휴일 정보를 가져다 쓸 수 있음. | . # 직접 holiday 정보를 담은 dataframe을 생성 # lower_window=0, upper_window=1: 휴일이 전날에는 영향을 주지 않고, 다음날 하루 정도는 영향을 더 준다는 가정 holiday = pd.DataFrame({ 'holiday': 'holiday', 'ds': pd.to_datetime(['2020-01-24', '2020-01-25', '2020-01-26', '2021-02-11', '2021-02-12', '2021-02-13']), 'lower_window': 0, 'upper_window': 1 }) m = Prophet(holidays=holiday, daily_seasonality=True) m.add_country_holidays(country_name='US') # 내장되어 있는 휴일 정보를 불러와서 추가 m.fit(df) future = m.make_future_dataframe(periods=121) forecast = m.predict(future) m.plot(forecast); . m.plot_components(forecast); . +) 어떤 holidy들이 training에 반영되었는지 확인: . m.train_holiday_names . 0 holiday 1 New Year's Day 2 Martin Luther King Jr. Day 3 Washington's Birthday 4 Memorial Day 5 Independence Day 6 Independence Day (Observed) 7 Labor Day 8 Columbus Day 9 Veterans Day 10 Thanksgiving 11 Christmas Day dtype: object . Outlier 제외하기 . | 특정 기간의 데이터가 유독 트렌드를 벗어나게 이상한 수치를 보일 경우, 제외하고 학습시킬 수도 있다 | . ## 코로나 pandemic으로 주가가 급락-급증한 2020년 상반기 일부를 제외하고 학습 df.loc[(df['ds'] &gt; '2020-02-01') &amp; (df['ds'] &lt; '2020-05-01'), 'y'] = None m = Prophet(yearly_seasonality=False, daily_seasonality=True) m.fit(df) future = m.make_future_dataframe(periods=121) m.plot(m.predict(future)); . ",
    "url": "https://chaelist.github.io/docs/ml_application/time_series/#%ED%8C%8C%EB%9D%BC%EB%AF%B8%ED%84%B0-%EC%A1%B0%EC%A0%95%ED%95%98%EA%B8%B0",
    "relUrl": "/docs/ml_application/time_series/#파라미터-조정하기"
  },"247": {
    "doc": "Topic Modeling (LDA)",
    "title": "Topic Modeling (LDA)",
    "content": ". | Topic Modeling이란? . | 잠재 디리클레 할당(Latent Dirichlet Allocation, LDA) | LDA의 수행 과정 | . | LDA: scikit-learn . | 데이터 준비: 앱 리뷰 | Tokenization (한글 형태소 단위로 쪼개기) | Vectorization &amp; LDA | LDA 시각화: pyLDAvis | 문서별 토픽 할당 | 토픽별 정리 | . | LDA: gensim . | Tokenization | Vectorization &amp; LDA | LDA 시각화: pyLDAvis | 문서별 토픽 할당 | . | . ",
    "url": "https://chaelist.github.io/docs/ml_application/topic_modeling/",
    "relUrl": "/docs/ml_application/topic_modeling/"
  },"248": {
    "doc": "Topic Modeling (LDA)",
    "title": "Topic Modeling이란?",
    "content": ": 문서에서 주제(topic)을 추출하는 기법 . | 관련이 높은 단어들끼리 묶어 토픽을 구성 → 단어의 조합으로 토픽의 핵심을 정의 가능 | 각 문서가 어떤 단어들로 구성되는지에 따라 가장 유사한 토픽으로 문서를 할당 | . (출처: medium.com/@connectwithghosh) . 잠재 디리클레 할당(Latent Dirichlet Allocation, LDA) . : 대표적인 토픽 모델링 기법. 다수의 문서에서 잠재적으로 의미 있는 토픽을 발견하는 절차적 확률 분포 모델 . | 단어들의 집합이 어떤 토픽들로 묶인다고 가정하고, 이 단어들이 각각의 토픽에 구성될 확률을 계산하여 결과 값을 토픽에 해당할 가능성이 높은 단어들의 집합으로 추출하는 방식 | 문서의 다양성에 비해 토픽의 수를 너무 적게 지정하거나, 하나의 문서에 다양한 주제가 혼용되어 있는 경우에는 토픽끼리 겹치는 결과가 나올 수 있다. 토픽 수를 정밀하게 지정하는 것이 중요! | . (출처: bookdown.org/Maxine/tidy-text-mining/) . LDA의 수행 과정 . | 토픽의 개수 k를 정한다 | 모든 문서의 모든 단어를 k개의 토픽 중 하나에 랜덤으로 할당한다 . | 이 작업이 끝나면 각 문서는 토픽의 분포를 가지며, 토픽은 단어의 분포를 갖게 된다 | . | 모든 문서의 모든 단어에 대해 아래 과정을 반복: . | 단어 w 외에 다른 단어들은 모두 올바른 토픽에 할당되어 있다고 가정하고, 1) 단어 w가 속한 문서 doc1의 단어들이 어떤 토픽에 해당하는지, 2) 전체 문서에서 단어 w가 보통 어떤 토픽에 속해 있는지 두 가지 기준을 참고하여 w에 토픽을 재할당한다 | . | . (참고자료: 딥러닝을 이용한 자연어 처리 입문) . (출처: donghwa-kim.github.io) . ",
    "url": "https://chaelist.github.io/docs/ml_application/topic_modeling/#topic-modeling%EC%9D%B4%EB%9E%80",
    "relUrl": "/docs/ml_application/topic_modeling/#topic-modeling이란"
  },"249": {
    "doc": "Topic Modeling (LDA)",
    "title": "LDA: scikit-learn",
    "content": ". | sklearn.decomposition.LatentDirichletAllocation을 사용 | . 데이터 준비: 앱 리뷰 . | google play store에서 ‘Netflix’의 2020.1.1~2021.5.28 사이의 1점 리뷰를 수집 (총 4242개) | . ## 직접 수집해 온 데이터를 dataframe으로 정리해 둠 review_df.head() . |   | Score | Content | Date | . | 0 | 1 | 사기 | 2021-05-28 13:04:04 | . | 1 | 1 | 갤럭시s6입니다. 잘 되다가 한번씩 무한로딩 걸립니다. 근데 무한로딩 한번 걸리면… | 2021-05-28 11:30:37 | . | 2 | 1 | 로딩이 너무 느려서 자꾸 멈춰. 사운드는 나오는데 영상이 멈추는 경우도 있고, 그러다… | 2021-05-28 08:15:41 | . | 3 | 1 | 언제까지 로드 중만 뜰지 궁금함 빈센조 7화 보다가 다시 보려니까 무한로딩,, 내가… | 2021-05-28 01:12:33 | . | 4 | 1 | 아까까진 잘 되다 갑자기 튕기더니 그 다음부터는 폰을 끄고 다시 켜서 들어가도 디바이스… | 2021-05-27 19:37:03 | . Tokenization (한글 형태소 단위로 쪼개기) . import konlpy import re # tokenization 함수를 만들어둠 def tokenize_korean_text(text): text = re.sub(r'[^,.?!\\w\\s]','', text) ## ,.?!와 문자+숫자+_(\\w)와 공백(\\s)만 남김 # 앞에 r을 붙여주면 deprecation warning이 안뜸 (raw string으로 declare) okt = konlpy.tag.Okt() Okt_morphs = okt.pos(text) # stem=True로 설정하면 동사원형으로 바꿔서 return words = [] for word, pos in Okt_morphs: if pos == 'Adjective' or pos == 'Verb' or pos == 'Noun': # 이 경우에는 형용사, 동사, 명사만 남김 words.append(word) words_str = ' '.join(words) return words_str # review_df['Content']를 하나씩 tokenize해서 list로 저장 tokenized_list = [] for text in review_df['Content']: tokenized_list.append(tokenize_korean_text(text)) print(len(tokenized_list)) print(tokenized_list[0]) . 4242 사기 . +) 단어가 1-2개만 포함된 corpus는 삭제 . drop_corpus = [] for index in range(len(tokenized_list)): corpus = tokenized_list[index] if len(set(corpus.split())) &lt; 3: # 같은 단어 1-2개만 반복되는 corpus도 지우기 위해 set()을 사용 review_df.drop(index, axis='index', inplace=True) drop_corpus.append(corpus) for corpus in drop_corpus: tokenized_list.remove(corpus) review_df.reset_index(drop=True, inplace=True) print(len(tokenized_list)) print(len(review_df)) . 3936 3936 . Vectorization &amp; LDA . from sklearn.feature_extraction.text import CountVectorizer from sklearn.decomposition import LatentDirichletAllocation . ⁣1. vector화 . #LDA 는 Count기반의 Vectorizer만 적용 count_vectorizer = CountVectorizer(max_df=0.1, max_features=1000, min_df=2, ngram_range=(1,2)) # 2개의 문서 미만으로 등장하는 단어는 제외, 전체의 10% 이상으로 자주 등장하는 단어는 제외 # bigram도 포함 feat_vect = count_vectorizer.fit_transform(tokenized_list) print('CountVectorizer Shape:', feat_vect.shape) . CountVectorizer Shape: (3936, 1000) . ⁣2. 토픽모델링: LDA . lda = LatentDirichletAllocation(n_components=6) # 토픽 수는 6개로 설정 lda.fit(feat_vect) . LatentDirichletAllocation(batch_size=128, doc_topic_prior=None, evaluate_every=-1, learning_decay=0.7, learning_method='batch', learning_offset=10.0, max_doc_update_iter=100, max_iter=10, mean_change_tol=0.001, n_components=6, n_jobs=None, n_topics=None, perp_tol=0.1, random_state=None, topic_word_prior=None, total_samples=1000000.0, verbose=0) . ⁣3. 토픽별 연관어 출력 . def display_topics(model, feature_names, num_top_words): for topic_index, topic in enumerate(model.components_): print('Topic #', topic_index) # components_ array에서 가장 값이 큰 순으로 정렬했을 때, 그 값의 array index를 반환. topic_word_indexes = topic.argsort()[::-1] top_indexes=topic_word_indexes[:num_top_words] # top_indexes대상인 index별로 feature_names에 해당하는 word feature 추출 후 join으로 concat feature_concat = ' '.join([feature_names[i] for i in top_indexes]) print(feature_concat) # CountVectorizer객체내의 전체 word들의 명칭을 get_features_names( )를 통해 추출 feature_names = count_vectorizer.get_feature_names() # Topic별 가장 연관도가 높은 word를 10개만 추출 display_topics(lda, feature_names, 10) . Topic # 0 무료 가입 했는데 해지 아니 이용 환불 사용 계정 해서 Topic # 1 자막 영화 재생 영상 넷플릭스 소리 문제 나오고 기능 드라마 Topic # 2 로딩 자꾸 플립 진짜 화질 내고 무한 무한 로딩 보는데 입니다 Topic # 3 안됨 해주세요 노트 실행 깔아도 삭제 갑자기 어플 시청 다운 Topic # 4 영상 재생 해결 소리 해주세요 안되요 멈추고 제발 개선 에러 Topic # 5 로그인 실행 설치 어플 삭제 했는데 안되고 안되네요 해도 고객 . LDA 시각화: pyLDAvis . | pip install pyLDAvis로 설치해서 사용 | . import pyLDAvis.sklearn # sklearn의 ldamodel에 최적화된 라이브러리 pyLDAvis.enable_notebook() vis = pyLDAvis.sklearn.prepare(lda, feat_vect, count_vectorizer) pyLDAvis.display(vis) . (이미지를 클릭하면 html로 구현된 버전으로 확인 가능) . 문서별 토픽 할당 . ⁣1. 각 문서별로 가장 가까운 topic으로 할당 . # 문서별로, 가장 확률이 높은 topic으로 할당해줌 doc_topic = lda.transform(feat_vect) doc_per_topic_list = [] for n in range(doc_topic.shape[0]): topic_most_pr = doc_topic[n].argmax() topic_pr = doc_topic[n].max() doc_per_topic_list.append([n, topic_most_pr, topic_pr]) doc_topic_df = pd.DataFrame(doc_per_topic_list, columns=['Doc_Num', 'Topic', 'Percentage']) doc_topic_df.head() . |   | Doc_Num | Topic | Percentage | . | 0 | 0 | 2 | 0.735108 | . | 1 | 1 | 1 | 0.455361 | . | 2 | 2 | 3 | 0.544513 | . | 3 | 3 | 0 | 0.241815 | . | 4 | 4 | 2 | 0.880451 | . → 실제 review 내용과 join . doc_topic_df = doc_topic_df.join(review_df) doc_topic_df.head() . |   | Doc_Num | Topic | Percentage | Score | Content | Date | . | 0 | 0 | 2 | 0.735108 | 1 | 갤럭시s6입니다. 잘 되다가 한번씩 무한로딩 걸립니다. 근데 무한로딩 한번 걸리면… | 2021-05-28 11:30:37 | . | 1 | 1 | 1 | 0.455361 | 1 | 로딩이 너무 느려서 자꾸 멈춰. 사운드는 나오는데 영상이 멈추는 경우도 있고, 그러다… | 2021-05-28 08:15:41 | . | 2 | 2 | 3 | 0.544513 | 1 | 언제까지 로드 중만 뜰지 궁금함 빈센조 7화 보다가 다시 보려니까 무한로딩,, 내가… | 2021-05-28 01:12:33 | . | 3 | 3 | 0 | 0.241815 | 1 | 아까까진 잘 되다 갑자기 튕기더니 그 다음부터는 폰을 끄고 다시 켜서 들어가도 디바이스… | 2021-05-27 19:37:03 | . | 4 | 4 | 2 | 0.880451 | 1 | 무한로딩중……빠른 문제해결 부탁드립니다. | 2021-05-27 17:22:05 | . ⁣2. 토픽별 문서 수 계산 . doc_topic_df.groupby('Topic')[['Doc_Num']].count() . | Topic | Doc_Num | . | 0 | 654 | . | 1 | 818 | . | 2 | 600 | . | 3 | 531 | . | 4 | 702 | . | 5 | 631 | . ⁣3. 토픽별로, 가장 높은 확률로 할당된 문서 top 3 확인 . for topic in range(len(doc_topic_df['Topic'].unique())): print('Topic #', topic, '-----------------------------') top_pr_topics = doc_topic_df[doc_topic_df['Topic'] == topic].sort_values(by='Percentage', ascending=False) print(top_pr_topics['Content'].iloc[0]) print(top_pr_topics['Content'].iloc[1]) print(top_pr_topics['Content'].iloc[2], '\\n') . Topic # 0 ----------------------------- 혼자 사용해서 베이직 요금제를 선택했는데 첫달 무료로 사용하면서 스탠다드로 쓸수 있게 해준다 해서 스탠다드로 한달 사용하고 그 다음달 베이직으로 바꿨는데 스탠다드 요금 나옴. 문의전화 하니까 데이터 상 내가 베이직으로 바꾼후 바로 스탠다드로 다시 바꿨다함 그런적 없다 하니 데이터상 그렇기 때문에 어쩔수 없다 함 베이직으로 바뀐시간과 스탠다드로 바뀐시간이 동일하며 초까지는 안나와서 모르겠으나 시간과 분이 일치하므로 몇초이내에 바꿨을거라 함. 난 베이직으로 바꾸고 스탠다드로 바로 바꾼적 없다 하니까 내가 베이직으로 바꾼후 베이직으로 바뀐걸 계정 들어가서 한번더 확인 안했으니 내 잘못이라 함. 나도 모르게 멤버십 바뀔수 있으니 결제될 때마다 베이직 스탠다드 확인하란 얘긴가요? 카드결재 주의하시고 해지도 조심하세요. 계정이 두개라 한개만 결재했는데 한달후보니 두개다 결재돼고 결재 5일전에 해지했는데 또 결재돼고 전화했더니 두계정결재 건은 내가 아직모르는 상태라고 상담사가 말도 안해주고 한번만 환불 가능하다고 요번 해지 결재건만 환불 처리 해줍디다. 한개정만 카드결재하고 한달 시청해는데 다른한개정은 정지상태인데 왜 결재돼냐고요. 오늘 넷플릭스 처음 이용 해봤는데 회원가입하고 첫 달 무료라면서 카드 등록하니까 바로 돈 빠져나가는 거 뭡니까? 당황해서 다급히 멤버십 해지.했는데도 돈 도 안들어오고... 사기 당해서 기분 나쁘네요.... 첫 달 무료라니 뭐라니 이런 거짓말은.하지 마세요... 당한 입장으로서 엄청 별로입니다... Topic # 1 ----------------------------- 넷플릭스에 올라오는 작품들은 만족스럽고 작품성이 뛰어난데 한글 번역은 왜 그따위인가요? 분명 본문은 그런 내용이 아닌데 혐오적 표현을 쓴다던가.. 아니 대체 영어를 한글로 번역하면서 부인은 남편한테 존대쓰고 남편은 부인한테 반말 쓰는 80년대적 발상은 어디서 나온 거죠?? 캐릭터 성격상 꼬박꼬박 존대 쓰고 반말 쓰는게 어색한데도 죽어도 놓질 못하시네요,, 이제는 영어 영화는 그냥 보고 스페인이나 프랑스 영화같은건 영어로 번역해서 보고 있는데.. 제발 그 글러먹은 사상좀 어디 버리고 와주세요 번역가님. 번역 신고 기능이 있어서 계속 신고할래도 본질이 글러먹은 것 같아서 평점 기능에 남겨봅니다 자체 제작하는 콘텐츠, 들여오는 영화나 드라마 모두 좋아요 콘텐츠 면에선 모두 만족합니다 그러나 자막이나 번역에 심각한 오의역이 존재하는 경우를 많이 보았어요. 예를 들어 드라마 글리 자막의 경우 주인공이 눈에 돌을 맞아 거의 실명할 뻔 하는 사건이 나오는데 이를 눈에 소금을 맞았다고 잘못 번역이 되어 있습니다.. 이럴 경우 영화나 드라마에 심각한 영향을 끼칠 것 같아요 또 이 정도의 오역이 아니어도 심심한 오류를 많이 보았습니다 자막이나 번역에 조금 더 힘써 주셨으면 합니다. 정말 잘 사용하고 있다가 최근에 업데이트 이후 v50s 듀얼스크린 재생이 되지를 않습니다. 영상 재생 중에 카카오톡이나 인터넷 검색 등 멀티 기능을 활용할 수 있었는데 지금은 환경설정 창만 내려도 영상이 정지가 되네요. 하루빨리 다시 예전처럼 영상이 지속적으로 재생될 수 있도록 조치해 주시면 대단히 감사드리겠습니다. Topic # 2 ----------------------------- 저도제트플립사용자입니다 제트플립 사용자는 전부 그러나봐요 저도 로딩만뜨고 넘어가질않는데 문제해결과 안내공지가 있어야하지않을까여? 왠 돈받고 저질서비스임? 제트플립 사용자인데 업데이트하고 갑자기 무한로딩... 못보고있는데 다른 리뷰들도 제트플립 안된다고 하네요... 돈은 돈대로 나가는데 보지는 못하고 이거 무슨 상황입니까? 저도 아래 리뷰처럼 제트플립 사용자인데 무한로드 반복하고 그 어느 컨텐츠 하나 재생이 안됩니다. 이거 뭐 며칠동안 돈날려먹으란것도 아니고 보상도 안바라니까 해결이나 빨리해줘요. Topic # 3 ----------------------------- 핸드폰을 시청후 껐을때 다른기기로 재시청을하면 기존에 시청했던 프로그램 시간대로 다시 재시청이 가능한데 어는 순간부터는 기기마다 시청했던 시간대로만 재시청이 가능해지네요 다른기기로 재시청시 기존에 시청했던 시간대로 시청할수있게끔 업데이트좀 부탁드려요 아니면 설정 방법좀 알려주세요 내가 찜한 콘텐츠 눌러도 확인해보면 추가 안되어있고 예전에는 찜한 콘텐츠삭제하는것도 됐었는데 지금은 왜 안될까요? 다 본건 지우고 새로 볼 것만 추가해두고 싶은데 삭제했다가 다시 깔아도 안되고 제가 뭘 어떻게 해요. 갤럭시 노트10+ 유저입니다. 501-109??오류메시지 뜨면서 앱실행이 2주째 안되고있고 매달 결재는 되는상황입니다. 업뎃하라해서 다했고.. 뭡니까 이게.. 환불이라도 해주던가 앱 제대로 관리하던가! 환불해줘요 !!! Topic # 4 ----------------------------- 아니 오늘 업데이트하라해서 했는데 30초마다 영상이 중지되네요ㅡㅡ 어제까지만 해도 아니 업뎃전까지만 해도 멀쩡했는데 왜 이러는거죠? 영상이 중지되서 다시 보려고 하면 재생 버튼은 눌리지도 않아요. 그래서 다시 창 끄고 영상 누르고 30초후에 중지. 또 끄고 영상 누르고. 개선좀해주세요 쉬는날 넷플 보려는데 오류때문에 시간 다날렸어요 영상이 자꾸 끊겨요 원래도 잠깐씩 끊긴 적이 있긴 했지만 오늘은 좀 심하네요... 아예 한 장면에서 계속 멈추고 10초 전이나 10초 후로 돌려도 영상 시간만 가고 앱을 아예 닫았다가 다시 들어가도 똑같아요 영상은 멈추고 소리만 들릴 때도 많고 너무 화나네요 ㅠ 고쳐 주세요 제발 아니 저만 이런가요? 한 일주일 전부터 영상 재생하면 한 10초?은 재생되다가 갑자기 화면 멈추고 소리만 나오네요ㅡㅡ 해결책좀요 제발ㅡㅡ Topic # 5 ----------------------------- ㅡㅡ저기요 결제정보변경해서 결제했더니ㅡㅡ계정정보를불러올수없다길래 로그아웃했다다시로그인하려하니 로그인도안되고ㅡㅡ삭제했다 다시깔아도안되고 고객센터연결도안되고 홈페이지도안들어가지고 뭡니까? 넷플릭스에 접속할 수 없습니다? 방금 전까지만 해도 됐었는데.... 갑자기 접속이 안되길래 휴대폰 껐켰, 어플 삭제 후 재설치, 데이터 삭제 등 할 수 있는건 다 했는데 여전히 먹통이네요... 장난하는것도 아니고 결제하니까 안들어가집니다. 재설치 하려고 지웠다 설치중인데 설치도 계속 안되고 있네요. 뭐하자는건지 개선이 안되고 있는데 어쩌자는 겁니까? . 토픽별 정리 . | keywords, 문서 예시 등을 통해 토픽을 정의/설명하면 토픽 모델링의 결과에서 인사이트를 끌어내는 데 도움이 된다. 하지만 모든 토픽이 정확히 설명 가능하게 분류되지는 않을 수 있으므로 주의. | . *Topic #0: “결제 관련 불만 사항 - 무료 가입 후 자동 결재, 해지/환불 절차에 대한 불만” . | keywords: 무료 가입 했는데 해지 아니 이용 환불 사용 계정 해서 | 문서 예시: “카드결재 주의하시고 해지도 조심하세요. 계정이 두개라 한개만 결재했는데 한달후보니 두개다 결재돼고 결재 5일전에 해지했는데 또 결재돼고”, “오늘 넷플릭스 처음 이용 해봤는데 회원가입하고 첫 달 무료라면서 카드 등록하니까 바로 돈 빠져나가는 거 뭡니까?” | . *Topic #1: “자막 및 영상 기능 관련 불만 사항 - 자막 오류, 오역, 기능상의 결함에 대한 불만” . | keywords: 자막 영화 재생 영상 넷플릭스 소리 문제 나오고 기능 드라마 | 문서 예시: “넷플릭스에 올라오는 작품들은 만족스럽고 작품성이 뛰어난데 한글 번역은 왜 그따위인가요?”, “자막이나 번역에 심각한 오의역이 존재하는 경우를 많이 보았어요.”, “업데이트 이후 v50s 듀얼스크린 재생이 되지를 않습니다.” | . *Topic #2: “무한 로딩 문제 - 특정 기종(제트플립) 관련 문제가 다수” . | keywords: 로딩 자꾸 플립 진짜 화질 내고 무한 무한 로딩 보는데 입니다 | 문서 예시: “제트플립 사용자는 전부 그러나봐요 저도 로딩만뜨고 넘어가질않는데”, “제트플립 사용자인데 업데이트하고 갑자기 무한로딩…” | . *Topic #3: “다양한 주제의 불만 사항 및 요청 사항” . | keywords: 안됨 해주세요 노트 실행 깔아도 삭제 갑자기 어플 시청 다운 | 문서 예시: “다른기기로 재시청시 기존에 시청했던 시간대로 시청할수있게끔 업데이트좀 부탁드려요”, “내가 찜한 콘텐츠 눌러도 확인해보면 추가 안되어있고 예전에는 찜한 콘텐츠삭제하는것도 됐었는데 지금은 왜 안될까요?” | ※ 찜하기, 알림, 이어보기 등 특정 기능 관련, 앱 실행 오류 관련… 다양한 주제가 mix된 토픽 - 하나로 정의하기 어렵다 | . *Topic #4: “영상 재생 관련 불만 사항 - 재생 경험에서의 불쾌감” . | keywords: 영상 재생 해결 소리 해주세요 안되요 멈추고 제발 개선 에러 | 문서 예시: “아니 오늘 업데이트하라해서 했는데 30초마다 영상이 중지되네요”, “영상이 자꾸 끊겨요 원래도 잠깐씩 끊긴 적이 있긴 했지만 오늘은 좀 심하네요” | Topic #2와 유사 (pyLDAvis로 시각화한 Intertopic Distance Map에서도 유사하게 나타남) | . *Topic #5: “접속 장애 관련 불만 사항 - 접속 불가, 로그인 불가 등의 문제” . | keywords: 로그인 실행 설치 어플 삭제 했는데 안되고 안되네요 해도 고객 | 문서 예시: “넷플릭스에 접속할 수 없습니다? 방금 전까지만 해도 됐었는데”, “장난하는것도 아니고 결제하니까 안들어가집니다. 재설치 하려고 지웠다 설치중인데 설치도 계속 안되고 있네요” | . ",
    "url": "https://chaelist.github.io/docs/ml_application/topic_modeling/#lda-scikit-learn",
    "relUrl": "/docs/ml_application/topic_modeling/#lda-scikit-learn"
  },"250": {
    "doc": "Topic Modeling (LDA)",
    "title": "LDA: gensim",
    "content": ". | gensim.models.ldamodel.LdaModel을 사용 | gensim: 자연어 처리에 특화된 라이브러리 | . ## sklearn을 활용한 LDA에서와 같은 데이터를 사용 review_df.head() . |   | Score | Content | Date | . | 0 | 1 | 사기 | 2021-05-28 13:04:04 | . | 1 | 1 | 갤럭시s6입니다. 잘 되다가 한번씩 무한로딩 걸립니다. 근데 무한로딩 한번 걸리면… | 2021-05-28 11:30:37 | . | 2 | 1 | 로딩이 너무 느려서 자꾸 멈춰. 사운드는 나오는데 영상이 멈추는 경우도 있고, 그러다… | 2021-05-28 08:15:41 | . | 3 | 1 | 언제까지 로드 중만 뜰지 궁금함 빈센조 7화 보다가 다시 보려니까 무한로딩,, 내가… | 2021-05-28 01:12:33 | . | 4 | 1 | 아까까진 잘 되다 갑자기 튕기더니 그 다음부터는 폰을 끄고 다시 켜서 들어가도 디바이스… | 2021-05-27 19:37:03 | . Tokenization . import konlpy import re def tokenize_korean_text(text): text = re.sub(r'[^,.?!\\w\\s]','', text) okt = konlpy.tag.Okt() Okt_morphs = okt.pos(text) words = [] for word, pos in Okt_morphs: if pos == 'Adjective' or pos == 'Verb' or pos == 'Noun': words.append(word) ## word를 이어붙인 string 형태가 아닌 word의 list를 return해주는 게 sklearn lda 준비 과정과의 차이 return words tokenized_list = [] for text in review_df['Content']: tokenized_list.append(tokenize_korean_text(text)) print(len(tokenized_list)) print(tokenized_list[1]) . 4242 ['사기'] . +) 단어가 1-2개만 포함된 corpus는 삭제 . drop_corpus = [] for index in range(len(tokenized_list)): corpus = tokenized_list[index] if len(set(corpus)) &lt; 3: # corpus 자체가 list 형태인게 sklearn lda 준비 과정과의 차이 review_df.drop(index, axis='index', inplace=True) drop_corpus.append(corpus) for corpus in drop_corpus: tokenized_list.remove(corpus) review_df.reset_index(drop=True, inplace=True) . Vectorization &amp; LDA . # bigram 생성에 필요한 library from gensim.models import Phrases from gensim.models.phrases import Phraser # vectorize &amp; lda에 필요한 library from gensim import corpora from gensim.models.ldamodel import LdaModel . ⁣1. bigram을 포함하기 위한 추가 세팅 . # Build the bigram models bigram = Phrases(tokenized_list, min_count=4, threshold=10) bigram_mod = Phraser(bigram) # See example print(bigram_mod[tokenized_list[0]]) . ['갤럭시', '입니다', '잘_되다가', '한번', '무한_로딩', '걸립니다', '무한_로딩', '한번', '걸리면', '한', '주간', '넷플릭스', '앱', '아예', '켜지지가', '않습니다', '원인', '모르겠고', '이럴', '때', '피씨', '봐', '야해서', '번거롭습니다'] . | gensim.models.Phrases: 자동으로 ngram colloation을 detect하는 모델. Phrases를 반복 사용하면 bigram뿐 아니라 trigram과 그 이상도 생성 가능 . | min_count: 최소한 min_count보다 많이 등장한 token이 대상 | threshold: default는 10, 값이 작을수록 덜 빈번한 단어 조합이더라도 bigram으로 생성해준다. 아주 빈번하게 사용되는 최소한의 bigram만 만들고자 하면 threshold를 높여주면 된다 | . | . # tokenized_list의 모든 문서에 대해 bigram을 생성해줌 words_bigram = [bigram_mod[doc] for doc in tokenized_list] . ⁣2. vector화 . dictionary = corpora.Dictionary(words_bigram) # 각 단어에 번호를 할당해줌 # bigram 포함하는 과정을 생략하고 싶으면, 그냥 바로 여기에 tokenized_list를 넣어주면 됨 dictionary.filter_extremes(no_below=2, no_above=0.05) # 2개의 문서 미만으로 등장하는 단어는 제외, 전체의 5% 이상으로 자주 등장하는 단어는 제외 corpus = [dictionary.doc2bow(text) for text in words_bigram] print(corpus[0]) # 첫번째 corpus를 테스트로 출력: 몇 번째 단어가 몇 번 나왔는지 저장되어 있음 . [(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 2), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 2)] . ⁣3. LDA 모델 학습 &amp; 연관어 확인 . ldamodel = LdaModel(corpus, num_topics=6, id2word=dictionary, passes=20, iterations=500) # 토픽 수: 6 ldamodel.print_topics(num_words=7) # num_words=10이 default . [(0, '0.011*\"로그인\" + 0.006*\"안\" + 0.006*\"고객_센터\" + 0.006*\"환불\" + 0.006*\"해도\" + 0.005*\"하면\" + 0.005*\"잘\"'), (1, '0.009*\"앱_실행\" + 0.009*\"안되네요\" + 0.008*\"이\" + 0.008*\"결재\" + 0.008*\"다운\" + 0.007*\"안되요\" + 0.007*\"디바이스_오류\"'), (2, '0.014*\"뭐\" + 0.010*\"가입\" + 0.010*\"영화\" + 0.009*\"했는데\" + 0.008*\"무료\" + 0.007*\"어떻게\" + 0.006*\"해지\"'), (3, '0.022*\"화질\" + 0.017*\"자막\" + 0.008*\"영화\" + 0.007*\"제발\" + 0.006*\"없고\" + 0.006*\"볼\" + 0.005*\"생각\"'), (4, '0.009*\"무한_로딩\" + 0.008*\"삭제\" + 0.007*\"안됩니다\" + 0.007*\"입니다\" + 0.007*\"해결\" + 0.007*\"로딩\" + 0.007*\"문제\"'), (5, '0.011*\"돈_내고\" + 0.009*\"안\" + 0.009*\"제발\" + 0.008*\"보는데\" + 0.006*\"자막\" + 0.006*\"문제\" + 0.006*\"아니고\"')] . | LdaModel의 파라미터: . | passes: Number of passes through the corpus during training. (default: 1) | iterations: Maximum number of iterations through the corpus when inferring the topic distribution of a corpus. (default: 50) | corpus 수가 적으면 passes를 높여주는 것이 유용할 수 있고, 시간만 충분하다면 iterations를 높여주면 더 학습이 잘 된다 (iterations가 낮으면 제대로 수렴하기 전에 학습이 종료될 수 있음) | . | . LDA 시각화: pyLDAvis . import pyLDAvis.gensim # gensim의 ldamodel에 최적화된 라이브러리 vis = pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary) pyLDAvis.display(vis) . (이미지를 클릭하면 html로 구현된 버전으로 확인 가능) . 문서별 토픽 할당 . doc_per_topic_list = [] for n in range(len(corpus)): doc_topic = ldamodel[corpus[n]] doc_topic = sorted(doc_topic, key=lambda x: (x[1]), reverse=True) topic_most_pr = doc_topic[0][0] topic_pr = doc_topic[0][1] doc_per_topic_list.append([n, topic_most_pr, topic_pr]) doc_topic_df = pd.DataFrame(doc_per_topic_list, columns=['Doc_Num', 'Topic', 'Percentage']) # 실제 review 내용과 join doc_topic_df = doc_topic_df.join(review_df) doc_topic_df.head() . |   | Doc_Num | Topic | Percentage | Score | Content | Date | . | 0 | 0 | 4 | 0.959966 | 1 | 갤럭시s6입니다. 잘 되다가 한번씩 무한로딩 걸립니다. 근데 무한로딩 한번 걸리면… | 2021-05-28 11:30:37 | . | 1 | 1 | 2 | 0.955639 | 1 | 로딩이 너무 느려서 자꾸 멈춰. 사운드는 나오는데 영상이 멈추는 경우도 있고, 그러다… | 2021-05-28 08:15:41 | . | 2 | 2 | 0 | 0.836514 | 1 | 언제까지 로드 중만 뜰지 궁금함 빈센조 7화 보다가 다시 보려니까 무한로딩,, 내가… | 2021-05-28 01:12:33 | . | 3 | 3 | 1 | 0.976007 | 1 | 아까까진 잘 되다 갑자기 튕기더니 그 다음부터는 폰을 끄고 다시 켜서 들어가도 디바이스… | 2021-05-27 19:37:03 | . | 4 | 4 | 4 | 0.83286 | 1 | 무한로딩중……빠른 문제해결 부탁드립니다. | 2021-05-27 17:22:05 | . ",
    "url": "https://chaelist.github.io/docs/ml_application/topic_modeling/#lda-gensim",
    "relUrl": "/docs/ml_application/topic_modeling/#lda-gensim"
  },"251": {
    "doc": "Twitter 데이터 수집",
    "title": "Twitter 데이터 수집",
    "content": ". | Tweepy로 수집 (Twitter API v1.1) . | 준비 작업 | 특정 user의 timeline 수집 | Cursor를 활용해 수집 | 특정 user의 follower 수집 | 특정 query를 포함한 tweet 수집 | . | . ",
    "url": "https://chaelist.github.io/docs/webscraping/twitter_api/",
    "relUrl": "/docs/webscraping/twitter_api/"
  },"252": {
    "doc": "Twitter 데이터 수집",
    "title": "Tweepy로 수집 (Twitter API v1.1)",
    "content": ". | Tweepy: Twitter API를 python으로 쉽게 활용할 수 있게 해주는 라이브러리 . | ※Twitter는 자사 서비스에서 발생하는 다양한 데이터를 쉽게 활용할 수 있도록 API를 제공한다 | . | pip install tweepy로 설치해서 사용 | . 준비 작업 . ⁣1. Twitter API 권한 얻기 . | Tweepy를 사용하기 위해선, 우선 Twitter API를 활용할 수 있는 권한이 필요하다 | https://developer.twitter.com/en/apply-for-access에서 계정을 만들고, application을 생성하면 API에 접속 가능한 token 등을 받을 수 있다 | . ⁣2. 권한 인증 &amp; API instance 생성 . import tweepy # 트위터 API에 접근하기 위한 개인 키를 입력 consumer_key = \"\" consumer_secret = \"\" access_token = \"\" access_token_secret = \"\" # OAuth 핸들러 생성 &amp; 개인정보 인증 요청 auth = tweepy.OAuthHandler(consumer_key, consumer_secret) # 액세스 요청 auth.set_access_token(access_token, access_token_secret) # api instace 생성 api = tweepy.API(auth) . 특정 user의 timeline 수집 . | References: tweepy, developer.twitter | . API.user_timeline(user_id, screen_name, since_id, count, max_id, trim_user, exclude_replies, include_rts) . | user_id나 screen_name 중 하나의 parameter로 timeline을 가져올 user를 명시 | cout: default는 20. maximum 200까지 가능 | . | 트윗 내용 가져오기: status.text . | 예시로, 삼성전자 뉴스룸(@SamsungNewsroom)의 타임라인 데이터를 가져옴 | . status = api.user_timeline(screen_name = 'SamsungNewsroom', count=1)[0] status.text . '삼성전자, 보호종료 청소년 자립 돕는 ‘삼성 희망디딤돌’ 광주센터 개소\\n\\nhttps://t.co/TUZGjUwlUn . | 트윗 작성 날짜 가져오기: status.created_at print(status.created_at) # datetime.datetime 형식 # 아래와 같이 원하는 형식으로 바꿔줘도 좋다 import datetime print(datetime.datetime.strftime(status.created_at, '%Y.%m.%d')) . 2021-06-02 03:34:26 2021.06.02 . | retweet된 횟수 가져오기: status.retweet_count status.retweet_count . 3 . | 좋아요(하트) 수 가져오기: status.favorite_count status.favorite_count . 12 . | tweet 작성자 정보: status.author # status.author에 접근 후, 아래와 같이 한 단계씩 더 접근해서 각 정보를 추출 print(status.author.name) print(status.author.screen_name) print(status.author.location) print(status.author.description) . 삼성전자 뉴스룸 SamsungNewsroom Seoul, Korea 삼성전자 공식 트위터입니다. (Global Newsroom now uses @Samsung) . | retweet된 글인지 여부 확인 # status.retweeted_status.text가 존재하는지 확인하면 됨 try: print(status.retweeted_status.text) except: print('not retweeted') . not retweeted . | . Cursor를 활용해 수집 . | Reference: tweepy | Cursor를 활용하면 limit없이 원하는 만큼의 tweet을 수집할 수 있다 | tweepy.Cursor()안에 api.user_timeline 등 원하는 수집 옵션을 넣어주면 된다 | .items()로 limit(수집을 원하는 양)을 넘겨준다 | . import pandas as pd tweet_list = [] for status in tweepy.Cursor(api.user_timeline, id='SamsungNewsroom').items(400): temp_list = [status.text, status.created_at,status.retweet_count, status.favorite_count] tweet_list.append(temp_list) df = pd.DataFrame(tweet_list, columns=['Tweets', 'Created_Date', '#_of_Retweets', '#_of_Likes']) print(len(df)) df.head() . 400 . |   | Tweets | Created_Date | #_of_Retweets | #_of_Likes | . | 0 | 삼성전자, 보호종료 청소년 자립 돕는 ‘삼성 희망디딤돌’ 광주센터 개소\\n\\nhttps://t.co/TUZGjUwlUn | 2021-06-02 03:34:26+00:00 | 3 | 12 | . | 1 | 삼성전자, 차세대 기업 서버용 ‘ZNS SSD’ 출시\\n\\nhttps://t.co/fZuY9s5J7P | 2021-06-02 02:24:53+00:00 | 3 | 17 | . | 2 | 삼성전자, 인공지능으로 완전히 새로워진 시스템에어컨 ‘DVM S2’ 출시\\n\\nhttps://t.co/CXdr48M34c | 2021-06-01 02:02:53+00:00 | 3 | 14 | . | 3 | 삼성전자, 시청각 장애인용 TV 보급사업 공급자로 선정\\n\\nhttps://t.co/TmByXt3uLA | 2021-05-30 01:09:24+00:00 | 2 | 18 | . | 4 | 삼성전자, 세계 최고 광효율에 색품질 혁신을 더한 LM301B EVO 패키지 출시\\n\\nhttps://t.co/gOurN6OYZL | 2021-05-27 02:05:31+00:00 | 2 | 10 | . +) ‘TooManyRequests’ error . | twitter api를 사용해 데이터를 가져올 때에는 일정 기간 동안 가져올 수 있는 횟수가 제한되어 있다. | 특히, Cursor로 무제한 수집하다보면 rate limit이 초과되어 error가 날 수 있기에, 아래와 같은 코드를 추가해주면 rate limit이 초과될 때 일정 시간을 기다렸다 작업해주게 된다 | . def limit_handled(cursor): while True: try: yield next(cursor) except tweepy.TooManyRequests: time.sleep(15 * 60) # 15분 기다려줌. for status in limit_handled(tweepy.Cursor(api.user_timeline, screen_name = '').items()): ## -- 코드 -- ## . | tweepy exception의 종류: https://docs.tweepy.org/en/latest/exceptions.html | . 특정 user의 follower 수집 . | References: tweepy, developer.twitter | . API.followers(user_id, screen_name, cursor, count, skip_status, include_user_entities) . | 이름, screen_name 등 정보 가져오기 . | 예시로, 방탄소년단(@BTS_twt)의 follower 정보를 수집 | . followers = api.followers(screen_name = 'BTS_twt') follower = followers[0] print(follower.name) # 이름 print(follower.screen_name) # screen name (@뒤에 붙은 이름) print(follower.created_at) # 계정이 생성된 날 print(follower.description) # description print(follower.followers_count) # 팔로워 수 print(follower.friends_count) # 팔로잉하는 사람 수 . Suci Suci90898274 2021-06-03 01:28:22+00:00 💜💜 0 13 . | Cursor를 이용해 다량을 한번에 수집 follower_list = [] for follower in tweepy.Cursor(api.followers, screen_name = 'BTS_twt').items(400): temp_list = [follower.name, follower.screen_name, follower.created_at, follower.description, follower.followers_count, follower.friends_count] follower_list.append(temp_list) df = pd.DataFrame(follower_list, columns=['Name', 'ID', 'Created_Date', 'Description', 'Followers', 'Following']) df.head() . |   | Name | ID | Created_Date | Description | Followers | Following | . | 0 | felix | felix22210574 | 2021-06-03 01:54:20+00:00 |   | 0 | 2 | . | 1 | aily | 2OOrb | 2021-06-02 23:46:19+00:00 | she / they ; 🔞 | 0 | 11 | . | 2 | delin | delin73271563 | 2021-06-03 01:54:00+00:00 |   | 0 | 1 | . | 3 | Muniri Iri | IriMuniri | 2021-06-03 01:49:15+00:00 | nikmatilah hidup selagi masih bernafas🙂 | 0 | 10 | . | 4 | Rhealyn Mamaril | MamarilRhealyn | 2021-06-03 01:52:35+00:00 | yolo | 0 | 5 | . | . 특정 query를 포함한 tweet 수집 . | References: tweepy, developer.twitter | . API.search_tweets(q, geocode, lang, locale, result_type, count, until, since_id, max_id, include_entities) . | q: 알고 싶은 query를 입력. 해당 query가 포함된 tweet만 수집되어 온다 | result_type: default는 ‘mixed’ . | mixed: include both popular &amp; real time results | recent: return only the most recent results | popular: return only the most popular results | . | until: 특정 일자 이전의 tweet만 검색. 날짜는 ‘YYYY-MM-DD’형태로 입력. | ※ search index는 7-day limit이 있기에, 일주일 이전의 tweet은 검색되지 않는다 | . | . | tweet 정보 가져오기 # 이런식으로 특정 geocode에서의 tweet만 검색해올 수도 있다 korea_geo = \"%s,%s,%s\" % (\"35.95\", \"128.25\", \"1000km\") statuses = api.search_tweets(q='방탄', geocode=korea_geo, count=1) status = statuses[0] print(status.text) # tweet 내용 print(status.created_at) # 게시 일자 print(status.retweet_count) # retweet된 횟수 print(status.favorite_count) # 좋아요 받은 횟수 . RT @corwin1129: 맥도날드가 비겁(?)하게 방탄을 끌고 오는 바람에.... 2021-06-03 02:03:36+00:00 2797 0 . | tweet 작성자의 정보 가져오기 print(status.user.id) # ID print(status.user.name) # 이름 print(status.user.screen_name) # screen name (@뒤에 붙은 이름) print(status.user.description) # description print(status.user.followers_count) # 팔로워 수 print(status.user.friends_count) # 팔로잉하는 사람 수 . 3018274394 🍎BABARAMAN🍉 BKB_shot 겜덕 551042289 체인블락씁니다 71 98 . | retweet된 글의 경우, 원본 확인 . | retweeted_status가 존재하면 retweet된 글이다 | . # 원본 글 확인 print(status.retweeted_status.text) # 원본 글 작성자 정보 확인 print(status.retweeted_status.user.id) print(status.retweeted_status.user.name) print(status.retweeted_status.user.screen_name) . 맥도날드가 비겁(?)하게 방탄을 끌고 오는 바람에.... https://t.co/cA3C1FCC1H 42567931 일기통관 corwin1129 . | Cursor를 이용해 다량을 한번에 수집 . tweet_list = [] korea_geo = \"%s,%s,%s\" % (\"35.95\", \"128.25\", \"1000km\") for status in tweepy.Cursor(api.search_tweets, q='방탄', geocode=korea_geo, until='2021-06-01').items(400): temp_list = [status.text, status.created_at, status.retweet_count, status.favorite_count] try: temp_list.append(status.retweeted_status.text) except: temp_list.append('Not Retweeted') temp_list.extend([status.user.name, status.user.followers_count]) tweet_list.append(temp_list) df = pd.DataFrame(tweet_list, columns=['Text', 'Created_Date', '#_of_Retweets', '#_of_Likes', 'Original_Text', 'User', 'User_#_of_Followers']) df.head() . |   | Text | Created_Date | #_of_Retweets | #_of_Likes | Original_Text | User | User_#_of_Followers |   | . | 0 | RT @borahe7p: 스밍인증하면 아이스크림에 세차권에 목걸이라………………. 더 방탄 스밍이랑 뮤스 죽어라고 달리겠습니다. https://t.co/1H6y54MHPZ | 2021-05-31 23:57:28+00:00 | 605 | 0 | 스밍인증하면 아이스크림에 세차권에 목걸이라………………. 더 방탄 스밍이랑 뮤스 죽어라고 달리겠습니다. https://t.co/1H6y54MHPZ | ᴮᴱpurpleU4ever | 77 |   | . | 1 | @guitar_aeong 작년보다 더 빡세요 ㅋㅋㅋ 우리 방탄 힘 실어 주기…(생략) | 2021-05-31 23:57:20+00:00 | 0 | 1 | Not Retweeted | 최애는7명차애는아미 | 211 |   | . | 2 | RT @soulmate91_bts: 다마가 11억뷰가 되었어요!!🎉🎉🎉🎉…(생략) | 2021-05-31 23:54:24+00:00 | 15 | 0 | 다마가 11억뷰가 되었어요!!🎉🎉🎉🎉…(생략) | 다정미니💕🐤 𝔅𝔲𝔱𝔱𝔢𝔯🧈 | 2434 |   | . | 3 | RT @BABO_bts0613: 버터뮤비합산..출처가 어딘지는 모르지만…합산된적이 없었기 때문에 위험한겁니다…(생략) | 2021-05-31 23:53:31+00:00 | 301 | 0 | 버터뮤비합산..출처가 어딘지는 모르지만…합산된적이 없었기 때문에 위험한겁니다…(생략) | ⟭⟬ 휘비 ⟬⟭ | 422 |   | . | 4 | RT @borahe7p: 스밍인증하면 아이스크림에 세차권에 목걸이라………………. 더 방탄 스밍이랑 뮤스 죽어라고 달리겠습니다. https://t.co/1H6y54MHPZ | 2021-05-31 23:53:02+00:00 | 605 | 0 | 스밍인증하면 아이스크림에 세차권에 목걸이라………………. 더 방탄 스밍이랑 뮤스 죽어라고 달리겠습니다. https://t.co/1H6y54MHPZ | 동경 | 510 |   | . | . ",
    "url": "https://chaelist.github.io/docs/webscraping/twitter_api/#tweepy%EB%A1%9C-%EC%88%98%EC%A7%91-twitter-api-v11",
    "relUrl": "/docs/webscraping/twitter_api/#tweepy로-수집-twitter-api-v11"
  },"253": {
    "doc": "UK Ecommerce Data 1",
    "title": "UK Ecommerce Data",
    "content": ". | 데이터 파악 및 전처리 . | 데이터 타입 정리 | 중복값 확인 | 결측치 확인 | 이상치 / 불필요한 값 정리 | 결측치 재확인 | 새로운 칼럼 생성 | . | 월별 매출액, 구매자수 | 코호트 분석 . | 코호트별 재방문률 | 코호트별 총매출액 | 코호트별 평균 지출액 | . | 고객별 이탈 가능성 파악 | 월별 이탈률 . | 월별 이탈률 추이 | . | 월별 신규 유입률 . | 월별 신규유입률 추이 | . | . *분석 대상 데이터셋: UK E-Commerce Data . | 데이터셋 출처 | UK-based retailer의 2020-12-01에서 2011-12-09 사이의 모든 거래를 기록한 데이터 | gift를 주로 판매하며, 대다수의 고객은 wholesaler임 | . ",
    "url": "https://chaelist.github.io/docs/kaggle/uk_ecommerce/#uk-ecommerce-data",
    "relUrl": "/docs/kaggle/uk_ecommerce/#uk-ecommerce-data"
  },"254": {
    "doc": "UK Ecommerce Data 1",
    "title": "데이터 파악 및 전처리",
    "content": "# 필요한 라이브러리 import import pandas as pd import numpy as np import scipy.stats as stats from matplotlib import pyplot as plt import seaborn as sns import plotly.express as px import plotly.io as pio pio.templates.default = \"plotly_white\" # default template을 지정 . ecom_df = pd.read_csv('data/uk_ecommerce_data.csv') ecom_df.head() . |   | InvoiceNo | StockCode | Description | Quantity | InvoiceDate | UnitPrice | CustomerID | Country | . | 0 | 536365 | 85123A | WHITE HANGING HEART T-LIGHT HOLDER | 6 | 12/1/2010 8:26 | 2.55 | 17850 | United Kingdom | . | 1 | 536365 | 71053 | WHITE METAL LANTERN | 6 | 12/1/2010 8:26 | 3.39 | 17850 | United Kingdom | . | 2 | 536365 | 84406B | CREAM CUPID HEARTS COAT HANGER | 8 | 12/1/2010 8:26 | 2.75 | 17850 | United Kingdom | . | 3 | 536365 | 84029G | KNITTED UNION FLAG HOT WATER BOTTLE | 6 | 12/1/2010 8:26 | 3.39 | 17850 | United Kingdom | . | 4 | 536365 | 84029E | RED WOOLLY HOTTIE WHITE HEART. | 6 | 12/1/2010 8:26 | 3.39 | 17850 | United Kingdom | . → 칼럼 정보 파악 . ecom_df.info() . &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 541909 entries, 0 to 541908 Data columns (total 8 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 InvoiceNo 541909 non-null object 1 StockCode 541909 non-null object 2 Description 540455 non-null object 3 Quantity 541909 non-null int64 4 InvoiceDate 541909 non-null object 5 UnitPrice 541909 non-null float64 6 CustomerID 406829 non-null float64 7 Country 541909 non-null object dtypes: float64(2), int64(1), object(5) memory usage: 33.1+ MB . 데이터 타입 정리 . | Customer ID: string 형태로 변경 . | 현재는 Null값이 섞이 칼럼이라 float 형태로 되어 있음 | 소수점을 없애기 위해 정수로 먼저 변경한 후 str로 바꿔줌 | . ecom_df['CustomerID'].fillna(0, inplace=True) # 일단 N/A를 0으로 바꿔줌 ecom_df['CustomerID'] = ecom_df['CustomerID'].astype('int').astype('str') # 정수로 변경한 후, str로 바꿔줌 ecom_df.replace({'CustomerID': '0'}, 'N/A', inplace=True) # 0은 'N/A'로 바꿔줌 print(ecom_df['CustomerID'].dtypes) . object . | InvoiceDate: datetime 형태로 변경 . ecom_df['InvoiceDate'] = pd.to_datetime(ecom_df['InvoiceDate']) print(ecom_df['InvoiceDate'].dtypes) ecom_df.head(3) . datetime64[ns] . |   | InvoiceNo | StockCode | Description | Quantity | InvoiceDate | UnitPrice | CustomerID | Country | . | 0 | 536365 | 85123A | WHITE HANGING HEART T-LIGHT HOLDER | 6 | 2010-12-01 08:26:00 | 2.55 | 17850 | United Kingdom | . | 1 | 536365 | 71053 | WHITE METAL LANTERN | 6 | 2010-12-01 08:26:00 | 3.39 | 17850 | United Kingdom | . | 2 | 536365 | 84406B | CREAM CUPID HEARTS COAT HANGER | 8 | 2010-12-01 08:26:00 | 2.75 | 17850 | United Kingdom | . +) 기간 범위 확인: . print(min(ecom_df['InvoiceDate'])) print(max(ecom_df['InvoiceDate'])) . 2010-12-01 08:26:00 2011-12-09 12:50:00 . | . 중복값 확인 . # 중복값의 수를 확인 ecom_df.duplicated().sum() . 5268 . → 전체 값이 다 중복된 경우, 기록 과정에서 중복 기입된 것이라고 판단. → 가장 위에 있는 행만 남기고 drop . print(len(ecom_df)) ecom_df.drop_duplicates(inplace=True) ecom_df.reset_index(drop=True, inplace=True) print(len(ecom_df)) . 541909 536641 . 결측치 확인 . | Description 칼럼 . ecom_df['Description'].isna().sum() . 1454 . → StockCode와 Description이 1:1 대응되는 개념인지 확인 (같은 StockCode의 Description으로 null값을 채워넣을 수 있을지 점검) . temp1 = ecom_df.groupby('StockCode')[['Description']].nunique() print(len(temp1.query('Description &gt; 1'))) print(len(temp1.query('Description == 1'))) print(len(temp1.query('Description &lt; 1'))) . 650 3308 112 . | StockCode 1가지에도 Description이 2개 이상 붙기도 해서, 단순하게 StockCode를 바탕으로 Description을 채워넣을 수는 없을 듯. | 우선 Description의 null값을 그대로 둠. | . | CustemerID 칼럼 . | . sum(ecom_df['CustomerID'] == 'N/A') . 135037 . # 같은 InvoiceNo라면 한 명이 한번에 구매한 것이므로, InvoiceNo의 unique 값으로도 확인해봄 ecom_df.query('CustomerID == \"N/A\"')['InvoiceNo'].nunique() . 3710 . | 개별 transaction으로 따지면, 3710개의 구매가 구매자가 남겨져 있지 않음 | 우선은 지우지 않고 둠. | . 이상치 / 불필요한 값 정리 . ecom_df.describe() . |   | Quantity | UnitPrice | . | count | 536641 | 536641 | . | mean | 9.62003 | 4.63266 | . | std | 219.13 | 97.2331 | . | min | -80995 | -11062.1 | . | 25% | 1 | 1.25 | . | 50% | 3 | 2.08 | . | 75% | 10 | 4.13 | . | max | 80995 | 38970 | . | Quanity나 UnitPrice에 -값이 섞어 있음 | . | UnitPrice가 0인 경우 체크 . sum(ecom_df['UnitPrice'] == 0) . 2510 . | UnitPrice = 0인 데이터는 매출과 관계가 없으므로 삭제해준다 | 아마 재고 관리를 위해 Price=0인 칼럼을 덧붙여서 Quantity를 조정해준 듯.. | . print(len(ecom_df)) drop_index = ecom_df[ecom_df['UnitPrice'] == 0].index ecom_df.drop(drop_index, axis='index', inplace=True) print(len(ecom_df)) # reset index ecom_df.reset_index(drop=True, inplace=True) . 536641 534131 . | UnitPrice가 -인 경우 체크 . ecom_df.query('UnitPrice &lt; 0') ## Stock Code가 'B'임 . |   | InvoiceNo | StockCode | Description | Quantity | InvoiceDate | UnitPrice | CustomerID | Country | . | 297646 | A563186 | B | Adjust bad debt | 1 | 2011-08-12 14:51:00 | -11062.1 | N/A | United Kingdom | . | 297647 | A563187 | B | Adjust bad debt | 1 | 2011-08-12 14:52:00 | -11062.1 | N/A | United Kingdom | . | StockCode 점검 . # Stock Code에 상품코드와 조금 다른 값들이 있음 print(sorted(ecom_df['StockCode'].unique())[-30:]) . ['90214T', '90214U', '90214V', '90214W', '90214Y', '90214Z', 'AMAZONFEE', 'B', 'BANK CHARGES', 'C2', 'CRUK', 'D', 'DCGS0003', 'DCGS0004', 'DCGS0069', 'DCGS0070', 'DCGS0076', 'DCGSSBOY', 'DCGSSGIRL', 'DOT', 'M', 'PADS', 'POST', 'S', 'gift_0001_10', 'gift_0001_20', 'gift_0001_30', 'gift_0001_40', 'gift_0001_50', 'm'] . | ‘gift_숫자’ 형태는 gift voucher 상품 | . ※ StockCode 중 제품이 아닌 것 정리: . | StockCode | Desciption | 처리 방향성 | . | S | SAMPLES | 무료로 샘플을 주는 데에 쓴 돈이라고 추정 → 매출과 관련이 적으므로 일단 drop | . | POST | POSTAGE | 우편 요금으로 추정… Country가 주로 해외인 것을 보면 일종의 상품이라고 생각되기도 함 → keep | . | PADS | PADS TO MATCH ALL CUSHIONS | Price=0.001 → 함께 판매하는 부속품으로 추정 → keep | . | M | Manual | 제품명이 정확하게 찍히지 않은 구매/환불건에 대해 Manual이라고 적어놓은 것으로 추정 → keep | . | DOT | DOTCOM POSTAGE | POST와 마찬가지로, 일종의 상품이라고 간주 → keep | . | D | Discount | 구매할 때 Discount해준 금액 → keep | . | CRUK | CRUK Commission | 특정 단체(CRUK)에 내는 Commision이라고 추정 → 매출과 관련이 적으므로 일단 drop | . | C2 | CARRIAGE | 운송비 → keep | . | BANK CHARGES | Bank Charges | 매출과 관련이 적으므로 일단 drop | . | B | Adjust bad debt | 매출과 관련이 적으므로 일단 drop | . | AMAZONFEE | AMAZON FEE | 아마존에 내는 수수료 → 매출과 관련이 적으므로 일단 drop | . → 제품 판매로 인한 매출과 관계 없는 Stock Code는 drop . # CRUK, BANK CHARGES, B, AMAZONFEE, S 삭제 print(len(ecom_df)) drop_index = ecom_df.query(\"StockCode in ['CRUK', 'BANK CHARGES', 'B', 'S', 'AMAZONFEE']\").index ecom_df.drop(drop_index, axis='index', inplace=True) print(len(ecom_df)) # reset index ecom_df.reset_index(drop=True, inplace=True) . 534131 533979 . | Quantity가 -인 경우 체크 . ecom_df.query('Quantity &lt; 0').head(3) . |   | InvoiceNo | StockCode | Description | Quantity | InvoiceDate | UnitPrice | CustomerID | Country | . | 141 | C536379 | D | Discount | -1 | 2010-12-01 09:41:00 | 27.5 | 14527 | United Kingdom | . | 154 | C536383 | 35004C | SET OF 3 COLOURED FLYING DUCKS | -1 | 2010-12-01 09:49:00 | 4.65 | 15311 | United Kingdom | . | 235 | C536391 | 22556 | PLASTERS IN TIN CIRCUS PARADE | -12 | 2010-12-01 10:24:00 | 1.65 | 17548 | United Kingdom | . | Quantity가 -인 경우는 refund, discount, 혹은 기타 지출을 의미하는 듯 | refund나 discount 금액은 매출과 관련이 있으므로 Quantity가 -인 행은 다 남겨둠. | cf) Quantity가 0인 경우는 없음 | . | . 결측치 재확인 . | Description 칼럼 . ecom_df['Description'].isna().sum() . 0 . | 정리하고 나니 Description이 nan인 행은 모두 사라짐 | . | CustemerID 칼럼 . # Invoice 기준으로 묶어주면 1531명의 Customer가 null ecom_df.query('CustomerID == \"N/A\"')['InvoiceNo'].nunique() . 1531 . | nan인 값은 ‘등록되지 않은 구매자’ 개념인 듯. (비회원 구매 개념) | 신규 이용자 / 이탈자 등을 분석할 때는 제외하는 것이 맞을 것 같지만, 인기 상품 / 기간별 매출 등을 파악할 때에는 미등록 이용자의 구매도 중요하므로 일단 남겨둠. | . | . 새로운 칼럼 생성 . | InvoiceMonth, InvoiceWeekday 칼럼 생성 . ecom_df['InvoiceMonth'] = ecom_df['InvoiceDate'].dt.strftime('%Y%m') # YYYYmm 형식으로 정리 ecom_df['InvoiceWeekday'] = ecom_df['InvoiceDate'].dt.weekday # 월: 0 ~ 일: 6 ecom_df[['InvoiceDate', 'InvoiceMonth', 'InvoiceWeekday']].head() . |   | InvoiceDate | InvoiceMonth | InvoiceWeekday | . | 0 | 2010-12-01 08:26:00 | 201012 | 2 | . | 1 | 2010-12-01 08:26:00 | 201012 | 2 | . | 2 | 2010-12-01 08:26:00 | 201012 | 2 | . | 3 | 2010-12-01 08:26:00 | 201012 | 2 | . | 4 | 2010-12-01 08:26:00 | 201012 | 2 | . | TotalSpending 칼럼 생성: Quantity * UnitPrice . ecom_df['TotalSpending'] = ecom_df['Quantity'] * ecom_df['UnitPrice'] ecom_df[['Quantity', 'UnitPrice', 'TotalSpending']].head() . |   | Quantity | UnitPrice | TotalSpending | . | 0 | 6 | 2.55 | 15.3 | . | 1 | 6 | 3.39 | 20.34 | . | 2 | 8 | 2.75 | 22 | . | 3 | 6 | 3.39 | 20.34 | . | 4 | 6 | 3.39 | 20.34 | . | . ",
    "url": "https://chaelist.github.io/docs/kaggle/uk_ecommerce/#%EB%8D%B0%EC%9D%B4%ED%84%B0-%ED%8C%8C%EC%95%85-%EB%B0%8F-%EC%A0%84%EC%B2%98%EB%A6%AC",
    "relUrl": "/docs/kaggle/uk_ecommerce/#데이터-파악-및-전처리"
  },"255": {
    "doc": "UK Ecommerce Data 1",
    "title": "월별 매출액, 구매자수",
    "content": ". | 201112는 9일밖에 없으므로 제외하고 그려줌 | . | 월별 총 매출액 fig = px.bar(ecom_df.query('InvoiceMonth != \"201112\"').groupby('InvoiceMonth')[['TotalSpending']].sum().reset_index(), x='InvoiceMonth', y='TotalSpending', color='TotalSpending', color_continuous_scale = 'Teal') fig.update(layout_coloraxis_showscale=False) fig.show() . | 월별 순구매자수 ecom_no_na = ecom_df.query('CustomerID != \"N/A\"') # CustomerID가 N/A인 걸 제외한 df를 따로 저장해둠 fig = px.bar(ecom_no_na.query('InvoiceMonth != \"201112\"').groupby('InvoiceMonth')[['CustomerID']].nunique().reset_index(), x='InvoiceMonth', y='CustomerID', color='CustomerID', labels = {'CustomerID':'# of Customers'}, color_continuous_scale = 'Teal') fig.update(layout_coloraxis_showscale=False) fig.show() . | 월별 매출액, 구매자수 모두 2011.09 ~ 2011.11 기간에 급증하는 추세 | 현재로서는 전망이 긍정적으로 판단되지만, 겨울 seasonality의 영향인지, 실적이 전반적으로 좋아지는 추세인 것인지 구분하려면 조금 더 장기간의 데이터가 필요할 듯 | . | 월별 고객 1인당 지출 . # CustomerID = N/A인 값은 제외하고 계산 cus_spend = ecom_no_na.query('InvoiceMonth != \"201112\"').groupby('InvoiceMonth') cus_spend = cus_spend.agg({'CustomerID':pd.Series.nunique, 'TotalSpending':'sum'}) cus_spend['SpendingPerCustomer'] = cus_spend['TotalSpending'] / cus_spend['CustomerID'] fig = px.bar(cus_spend.reset_index(), x='InvoiceMonth', y='SpendingPerCustomer', color='SpendingPerCustomer', labels = {'SpendingPerCustomer':'Spending per Customer'}, color_continuous_scale = 'Teal') fig.update(layout_coloraxis_showscale=False) fig.show() . | 1인당 지출 금액이 크게 증가하는 추세는 아님. | 2011.09 ~ 2011.11 기간에 비교적 1인당 지출이 높게 나타나지만, 겨울 seasonality의 영향인지, 실제로 1인당 지출이 증가하는 추세인 것인지 구분하려면 조금 더 장기간의 데이터가 필요할 듯 | . | . ",
    "url": "https://chaelist.github.io/docs/kaggle/uk_ecommerce/#%EC%9B%94%EB%B3%84-%EB%A7%A4%EC%B6%9C%EC%95%A1-%EA%B5%AC%EB%A7%A4%EC%9E%90%EC%88%98",
    "relUrl": "/docs/kaggle/uk_ecommerce/#월별-매출액-구매자수"
  },"256": {
    "doc": "UK Ecommerce Data 1",
    "title": "코호트 분석",
    "content": ". | 201112는 9일밖에 없으므로 제외하고 계산 | 첫 구매월을 기준으로 cohort 분류 | . ecom_cohort_df = ecom_no_na.query('InvoiceMonth != \"201112\"') min_month =ecom_cohort_df.groupby('CustomerID')[['InvoiceMonth']].min() min_month.rename(columns={'InvoiceMonth': 'CohortGroup'}, inplace=True) ecom_cohort_df = pd.merge(ecom_cohort_df, min_month, on='CustomerID', how='left') ecom_cohort_df.head(3) . |   | InvoiceNo | StockCode | Description | Quantity | InvoiceDate | UnitPrice | CustomerID | Country | InvoiceMonth | InvoiceWeekday | TotalSpending | CohortGroup | . | 0 | 536365 | 85123A | WHITE HANGING HEART T-LIGHT HOLDER | 6 | 2010-12-01 08:26:00 | 2.55 | 17850 | United Kingdom | 201012 | 2 | 15.3 | 201012 | . | 1 | 536365 | 71053 | WHITE METAL LANTERN | 6 | 2010-12-01 08:26:00 | 3.39 | 17850 | United Kingdom | 201012 | 2 | 20.34 | 201012 | . | 2 | 536365 | 84406B | CREAM CUPID HEARTS COAT HANGER | 8 | 2010-12-01 08:26:00 | 2.75 | 17850 | United Kingdom | 201012 | 2 | 22 | 201012 | . → CohortGroup을 기준으로 정리 . ecom_cohort_df = ecom_cohort_df.groupby(['CohortGroup', 'InvoiceMonth']) ecom_cohort_df = ecom_cohort_df.agg({'CustomerID':pd.Series.nunique, 'TotalSpending':['sum', 'mean']}) ecom_cohort_df.columns = ['Customers', 'TotalSpending', 'AverageSpending'] # 칼럼명 정리 ecom_cohort_df.head() . | CohortGroup | InvoiceMonth | Customers | TotalSpending | AverageSpending | . | 201012 | 201012 | 947 | 552358 | 20.9623 | . | ^^ | 201101 | 361 | 271937 | 25.2237 | . | ^^ | 201102 | 317 | 230416 | 25.3427 | . | ^^ | 201103 | 367 | 301779 | 25.2345 | . | ^^ | 201104 | 341 | 200541 | 20.0741 | . → 첫구매월을 기준으로 얼마나 지났는지 표기 . def cohort_period(df): df['CohortPeriod'] = np.arange(len(df)) + 1 return df ecom_cohort_df = ecom_cohort_df.groupby(level=0).apply(cohort_period).reset_index() ecom_cohort_df.head() . |   | CohortGroup | InvoiceMonth | Customers | TotalSpending | AverageSpending | CohortPeriod | . | 0 | 201012 | 201012 | 947 | 552358 | 20.9623 | 1 | . | 1 | 201012 | 201101 | 361 | 271937 | 25.2237 | 2 | . | 2 | 201012 | 201102 | 317 | 230416 | 25.3427 | 3 | . | 3 | 201012 | 201103 | 367 | 301779 | 25.2345 | 4 | . | 4 | 201012 | 201104 | 341 | 200541 | 20.0741 | 5 | . 코호트별 재방문률 . cohorts1 = pd.pivot_table(ecom_cohort_df, index='CohortGroup', columns='CohortPeriod', values='Customers', aggfunc='sum') cohort_retention = cohorts1.divide(cohorts1[1], axis=0) cohort_retention.loc['MEAN'] = cohort_retention.mean(axis=0) cohort_retention = cohort_retention.reindex(['MEAN', '201012', '201101', '201102', '201103', '201104', '201105', '201106', '201107', '201108', '201109', '201110', '201111']) # heatmap으로 시각화 plt.figure(figsize=(15,7)) sns.heatmap(cohort_retention, annot=True, cmap='GnBu', fmt='.0%') plt.yticks(rotation=0); . | 2010.12에 첫구매한 cohort가 가장 이후 재방문률이 놓은 편. (2010.12 이전부터 지속적으로 구매해오던 이용자 역시 이 그룹으로 계산되었기 때문일 수도 있음) | 2010.12에 첫구매한 cohort의 50%가 2011.11에 다시 방문 → 겨울 seasonality가 있는 상품에 대한 수요가 있는 편이라고 추정 | 2011.09, 2011.10에 첫구매한 cohort의 경우 이전 cohort에 비해 다음달 재방문률이 다소 높은 편이지만, ‘최근에 유입된 고객일수록 재방문률이 높다’고 말하기는 어려움 | . 코호트별 총매출액 . cohort_spending = pd.pivot_table(ecom_cohort_df, index='CohortGroup', columns='CohortPeriod', values='TotalSpending', aggfunc='sum') cohort_spending.loc['MEAN'] = cohort_spending.mean(axis=0) cohort_spending = cohort_spending.reindex(['MEAN', '201012', '201101', '201102', '201103', '201104', '201105', '201106', '201107', '201108', '201109', '201110', '201111']) # heatmap으로 시각화 plt.figure(figsize=(15,7)) sns.heatmap(cohort_spending, annot=True, cmap='GnBu', fmt='.0f') plt.yticks(rotation=0); . | 2010.12에 첫구매한 cohort가 총매출에 지속적으로 가장 큰 기여를 함 | 특히, 2010.12에 첫구매한 cohort의 경우, 2011.09 ~ 2010.11 기간에도 거의 2010.12에 버금가는 큰 금액을 지출 | . 코호트별 평균 지출액 . cohort_avgspend = pd.pivot_table(ecom_cohort_df, index='CohortGroup', columns='CohortPeriod', values='AverageSpending', aggfunc='sum') cohort_avgspend.loc['MEAN'] = cohort_avgspend.mean(axis=0) cohort_avgspend = cohort_avgspend.reindex(['MEAN', '201012', '201101', '201102', '201103', '201104', '201105', '201106', '201107', '201108', '201109', '201110', '201111']) # heatmap으로 시각화 plt.figure(figsize=(15,7)) sns.heatmap(cohort_avgspend, annot=True, cmap='GnBu', fmt='.2f') plt.yticks(rotation=0); . | 2010.12에 첫구매한 cohort가 지속적으로 평균 지출액이 높은 편. 첫구매월 이후 오히려 평균 지출액이 증가하는 모습을 보임 | 오히려 최근에 유입된 cohort들의 경우, 2번째 달 지출액이 이전 cohort대비 낮은 편으로 나타남 → 긍정적이지 않은 신호라고 생각됨 (서비스가 개선되고 있다면, 점점 첫달 이후 지출액 감소분이 줄어드는 것이 이상적) | . ",
    "url": "https://chaelist.github.io/docs/kaggle/uk_ecommerce/#%EC%BD%94%ED%98%B8%ED%8A%B8-%EB%B6%84%EC%84%9D",
    "relUrl": "/docs/kaggle/uk_ecommerce/#코호트-분석"
  },"257": {
    "doc": "UK Ecommerce Data 1",
    "title": "고객별 이탈 가능성 파악",
    "content": ". | 고객별 구매 내역을 월별로 정리 . ecom_purchase = ecom_no_na.query('TotalSpending &gt;= 0') # TotalSpending이 양수인 값만 반영. (-는 반품한 내역으로 추정) temp = ecom_purchase.query('InvoiceMonth != \"201112\"') # 201112는 제외하고 계산 customer_usage = pd.pivot_table(temp, index='CustomerID', columns='InvoiceMonth', values='InvoiceNo', aggfunc='count') customer_usage = customer_usage.applymap(lambda x: 'O' if x &gt; 0 else 'X') customer_usage.head() . | CustomerID | 201012 | 201101 | 201102 | 201103 | 201104 | 201105 | 201106 | 201107 | 201108 | 201109 | 201110 | 201111 | . | 12346 | X | O | X | X | X | X | X | X | X | X | X | X | . | 12347 | O | O | X | X | O | X | O | X | O | X | O | X | . | 12348 | O | O | X | X | O | X | X | X | X | O | X | X | . | 12349 | X | X | X | X | X | X | X | X | X | X | X | O | . | 12350 | X | X | O | X | X | X | X | X | X | X | X | X | . | 고객별로, 구매와 구매 사이에 걸리는 평균 기간을 계산 . | 2011.10 ~ 2011.11 기간에 한 번만 구매한 고객은 ‘New Customer’라고 기재 (이탈자라고 보기에는 최근에 새로 구입한 고객이므로) | 2010.12 ~ 2011.09 기간에 한 번 구매한 이후 다시 돌아오지 않은 고객은 ‘Never Returned’라고 기재 | . for index in customer_usage.index: templist = customer_usage.loc[index].to_list() month_btw = [] for i in range(12): if templist[i] == 'O': for j in range(1, 12 - i): if templist[i + j] == 'O': month_btw.append(j) break if month_btw: month_btw_avg = sum(month_btw) / len(month_btw) else: if (customer_usage.loc[index, '201111'] == 'O') or (customer_usage.loc[index, '201110'] == 'O'): month_btw_avg = 'New Customer' else: month_btw_avg = 'Never Returned' customer_usage.loc[index, 'avg_month_btw_purchases'] = month_btw_avg customer_usage.head() . | CustomerID | 201012 | 201101 | 201102 | 201103 | 201104 | 201105 | 201106 | 201107 | 201108 | 201109 | 201110 | 201111 | avg_month_btw_purchases | . | 12346 | X | O | X | X | X | X | X | X | X | X | X | X | Never Returned | . | 12347 | O | O | X | X | O | X | O | X | O | X | O | X | 2.0 | . | 12348 | O | O | X | X | O | X | X | X | X | O | X | X | 3.0 | . | 12349 | X | X | X | X | X | X | X | X | X | X | X | O | New Customer | . | 12350 | X | X | O | X | X | X | X | X | X | X | X | X | Never Returned | . → ‘Never Returned’인 고객의 수 파악: . len(customer_usage.query('avg_month_btw_purchases == \"Never Returned\"')) . 1097 . | 2010.12 ~ 2011.09 기간에 한 번 구매한 이후 다시 돌아오지 않은 고객이 1097명 | . +) 지속적으로 구매해 온 고객의 평균 구매 간격 파악: . temp = customer_usage.query('avg_month_btw_purchases not in [\"Never Returned\", \"New Customer\"]') temp['avg_month_btw_purchases'].mean() . 2.870106683984906 . | 평균적으로 2.9개월 간격으로 구매 | . | 마지막 구매 이후 경과한 기간 계산 . # 고객별 마지막 구매월 파악 last_month = ecom_purchase.query('InvoiceMonth != \"201112\"').groupby('CustomerID')[['InvoiceMonth']].max() last_month.rename(columns={'InvoiceMonth':'LastMonth'}, inplace=True) customer_usage = customer_usage.join(last_month) # 마지막 구매 이후 경과한 기간 계산: 현재시점(2010.12) - 마지막 구매월 for index in customer_usage.index: lastmonth = customer_usage.loc[index, 'LastMonth'] if lastmonth == '201012': lastmonth = 201100 customer_usage.loc[index, 'months_after_latest_purchase'] = 201112 - int(lastmonth) customer_usage.head() . | CustomerID | 201012 | 201101 | 201102 | 201103 | 201104 | 201105 | 201106 | 201107 | 201108 | 201109 | 201110 | 201111 | avg_month_btw_purchases | LastMonth | months_after_latest_purchase | . | 12346 | X | O | X | X | X | X | X | X | X | X | X | X | Never Returned | 201101 | 11 | . | 12347 | O | O | X | X | O | X | O | X | O | X | O | X | 2.0 | 201110 | 2 | . | 12348 | O | O | X | X | O | X | X | X | X | O | X | X | 3.0 | 201109 | 3 | . | 12349 | X | X | X | X | X | X | X | X | X | X | X | O | New Customer | 201111 | 1 | . | 12350 | X | X | O | X | X | X | X | X | X | X | X | X | Never Returned | 201102 | 10 | . +) ‘Never Returned’ 고객의 평균 구매 후 경과 기간: . customer_usage.query('avg_month_btw_purchases == \"Never Returned\"')['months_after_latest_purchase'].mean() . 7.353691886964448 . | 평균적으로 마지막 구매 이후 7개월 이상이 지남 | . | 구매 지속 고객의 ‘risk ratio’ 계산 . | risk ratio(위험 비율): 마지막 구매 이후 경과한 기간 / 평균 구매 간격 | 고객별로 평균 구매 간격이 다른 경우가 많기 때문에, ‘risk ratio’를 계산해보면 각각의 성격에 맞게 이탈 여부를 판별할 수 있다 | ex) 평균 4달 간격으로 구매하던 고객이 6달 동안 구매하지 않은 경우보다 평균 2달 간격으로 구매하던 고객이 6달 동안 구매하지 않은 경우가 더 강한 이탈 신호라고 판단 | . likely_customer = customer_usage.query('avg_month_btw_purchases not in [\"Never Returned\", \"New Customer\"]') likely_customer['risk_ratio'] = likely_customer['months_after_latest_purchase'] / likely_customer['avg_month_btw_purchases'] likely_customer.head() . | CustomerID | 201012 | 201101 | 201102 | 201103 | 201104 | 201105 | 201106 | 201107 | 201108 | 201109 | 201110 | 201111 | avg_month_btw_purchases | LastMonth | months_after_latest_purchase | risk_ratio | . | 12347 | O | O | X | X | O | X | O | X | O | X | O | X | 2 | 201110 | 2 | 1 | . | 12348 | O | O | X | X | O | X | X | X | X | O | X | X | 3 | 201109 | 3 | 1 | . | 12352 | X | X | O | O | X | X | X | X | X | O | X | O | 3 | 201111 | 1 | 0.333333 | . | 12356 | X | O | X | X | O | X | X | X | X | X | X | O | 5 | 201111 | 1 | 0.2 | . | 12359 | X | O | O | X | X | X | O | X | X | X | O | X | 3 | 201110 | 2 | 0.666667 | . → risk ratio가 1.5보다 높은 이용자수: . print(len(likely_customer.query('risk_ratio &gt; 1.5'))) # 평균 구매 간격의 1.5배가 넘는 기간 동안 구매하지 않고 있는 고객의 수 . 515 . | . &gt; 결론: 2010.12 ~ 2011.11 내의 순구매자 4371명 중 1097명은 아마 이탈한 것이 확정적, 515명은 이탈 위기 . | risk ratio가 일정 수치가 넘는 고객의 경우 특별한 관리를 통해 이탈을 방지하는 것이 필요하다고 생각됨 | . ",
    "url": "https://chaelist.github.io/docs/kaggle/uk_ecommerce/#%EA%B3%A0%EA%B0%9D%EB%B3%84-%EC%9D%B4%ED%83%88-%EA%B0%80%EB%8A%A5%EC%84%B1-%ED%8C%8C%EC%95%85",
    "relUrl": "/docs/kaggle/uk_ecommerce/#고객별-이탈-가능성-파악"
  },"258": {
    "doc": "UK Ecommerce Data 1",
    "title": "월별 이탈률",
    "content": ": 구매 지속 고객의 경우 평균 구매 간격이 2.9개월 정도의 기간라고 계산되었으므로, 구매 이후 3개월 간 이용이 없는 이용자를 ‘이탈자’로 간주하고 월별 변화를 분석 . | customer_usage의 월별 O, X 부분을 복사 . customer_churn = customer_usage.copy() customer_churn.drop(['avg_month_btw_purchases', 'LastMonth', 'months_after_latest_purchase'], axis='columns', inplace=True) # 필요 없는 칼럼은 drop customer_churn.head() . | CustomerID | 201012 | 201101 | 201102 | 201103 | 201104 | 201105 | 201106 | 201107 | 201108 | 201109 | 201110 | 201111 | . | 12346 | X | O | X | X | X | X | X | X | X | X | X | X | . | 12347 | O | O | X | X | O | X | O | X | O | X | O | X | . | 12348 | O | O | X | X | O | X | X | X | X | O | X | X | . | 12349 | X | X | X | X | X | X | X | X | X | X | X | O | . | 12350 | X | X | O | X | X | X | X | X | X | X | X | X | . | 월별 이용 내역을 바탕으로, 이탈 여부 정리 . | 구매가 일어난 월을 기준으로, 그 후 3개월 간 다시 구매하지 않으면 ‘Churn’이라고 표기 | 3개월 이내에 다시 구매한 경우, ‘Returning’이라고 표기 | 구매가 일어나지 않은 월은 ‘N/A’로 표기 | . columns = customer_churn.columns for index in customer_churn.index: usage_list = list(customer_churn.loc[index]) for i in range(len(columns) - 3): if usage_list[i] == 'X': customer_churn.loc[index, columns[i]] = 'N/A' else: if 'O' in usage_list[i+1 : i+4]: customer_churn.loc[index, columns[i]] = 'Returning' else: customer_churn.loc[index, columns[i]] = 'Churn' customer_churn.drop(['201109', '201110', '201111'], axis='columns', inplace=True) customer_churn.head() . | CustomerID | 201012 | 201101 | 201102 | 201103 | 201104 | 201105 | 201106 | 201107 | 201108 | . | 12346 | N/A | Churn | N/A | N/A | N/A | N/A | N/A | N/A | N/A | . | 12347 | Returning | Returning | N/A | N/A | Returning | N/A | Returning | N/A | Returning | . | 12348 | Returning | Returning | N/A | N/A | Churn | N/A | N/A | N/A | N/A | . | 12349 | N/A | N/A | N/A | N/A | N/A | N/A | N/A | N/A | N/A | . | 12350 | N/A | N/A | Churn | N/A | N/A | N/A | N/A | N/A | N/A | . | unstack &amp; rename columns . customer_churn = customer_churn.unstack().reset_index() customer_churn.rename(columns={'level_0':'InvoiceMonth', 0:'ChurnFlag'}, inplace=True) customer_churn.head() . |   | InvoiceMonth | CustomerID | ChurnFlag | . | 0 | 201012 | 12346 | N/A | . | 1 | 201012 | 12347 | Returning | . | 2 | 201012 | 12348 | Returning | . | 3 | 201012 | 12349 | N/A | . | 4 | 201012 | 12350 | N/A | . | 월별 Churn / Returning 고객 수 및 비율을 계산 . # InvoiceMonth, ChurnFlag를 기준으로 groupby ('N/A'는 무시) monthly_churn = customer_churn.query('ChurnFlag != \"N/A\"') monthly_churn = monthly_churn.groupby(['InvoiceMonth', 'ChurnFlag'])[['CustomerID']].count() monthly_churn.rename(columns={'CustomerID':'CustomerCount'}, inplace=True) # 월별로, Churn vs Returning 비율을 계산 monthly_churn['Percentage'] = monthly_churn.groupby(level=0)['CustomerCount'].transform(lambda x: x / x.sum()) monthly_churn.head(6) . | InvoiceMonth | ChurnFlag | CustomerCount | Percentage | . | 201012 | Churn | 357 | 0.403846 | . | ^^ | Returning | 527 | 0.596154 | . | 201101 | Churn | 263 | 0.354926 | . | ^^ | Returning | 478 | 0.645074 | . | 201102 | Churn | 254 | 0.335092 | . | ^^ | Returning | 504 | 0.664908 | . | . 월별 이탈률 추이 . | 월별 재구매자수 &amp; 이탈자수 추이 . fig = px.bar(monthly_churn.reset_index(), x='InvoiceMonth', y='CustomerCount', color='ChurnFlag', barmode='group', category_orders = {'ChurnFlag':['Returning', 'Churn']}, hover_data={'Percentage': ':.0%'}, color_discrete_sequence = [px.colors.sequential.Teal[0], px.colors.sequential.Teal[2]]) fig.show() . | 재구매자: 2010.12 대비 2011.08에 약 33% 증가 | 이탈자 수 자체는 크게 급감하는 추세를 보이지는 않지만, 2011.05 이후에는 비교적 감소 추세 | 하지만 구매자 수가 증가하는 동안 이탈자 수가 증가하지 않고 유지되었다는 것만으로도 긍정적인 성과라고 생각됨 | . | 월별 재구매율 &amp; 이탈률 추이 . fig = px.bar(monthly_churn.reset_index(), x='InvoiceMonth', y='Percentage', color='ChurnFlag', category_orders = {'ChurnFlag':['Returning', 'Churn']}, color_discrete_sequence = [px.colors.sequential.Teal[0], px.colors.sequential.Teal[2]]) fig.update_yaxes(tickformat='.0%') fig.show() . | 월별 전체 구매자 대비 비율로 따지면, ‘이탈률’은 감소 추세라고 할 수 있을 듯. | . | . ",
    "url": "https://chaelist.github.io/docs/kaggle/uk_ecommerce/#%EC%9B%94%EB%B3%84-%EC%9D%B4%ED%83%88%EB%A5%A0",
    "relUrl": "/docs/kaggle/uk_ecommerce/#월별-이탈률"
  },"259": {
    "doc": "UK Ecommerce Data 1",
    "title": "월별 신규 유입률",
    "content": ": 구매 지속 고객의 경우 평균 구매 간격이 2.9개월 정도의 기간라고 계산되었으므로, 구매 이전에 3개월 간 이용이 없었던 이용자를 ‘신규유입자’로 간주하고 월별 변화를 분석 . | 고객별 월별 신규 유입 여부 정리 . | 구매가 발생한 월을 기준으로, 그 전 3개월 간 이용이 없었다면 ‘New’라고 표기 | 3개월 이내에 구매했던 이력이 있다면 ‘Returning’이라고 표기 | 구매하지 않은 월은 ‘N/A’로 표기 | . # customer_usage의 월별 O, X 부분을 복사 new_customer = customer_usage.copy() new_customer.drop(['avg_month_btw_purchases', 'LastMonth', 'months_after_latest_purchase'], axis='columns', inplace=True) # 월별 이용 내역을 바탕으로, 신규 유입 여부 정리 columns = new_customer.columns for index in new_customer.index: usage_list = list(new_customer.loc[index]) for i in range(3, len(columns)): if usage_list[i] == 'X': new_customer.loc[index, columns[i]] = 'N/A' else: if 'O' in usage_list[i-3 : i]: new_customer.loc[index, columns[i]] = 'Returning' else: new_customer.loc[index, columns[i]] = 'New' new_customer.drop(['201012', '201101', '201102'], axis='columns', inplace=True) new_customer.head() . | CustomerID | 201103 | 201104 | 201105 | 201106 | 201107 | 201108 | 201109 | 201110 | 201111 | . | 12346 | N/A | N/A | N/A | N/A | N/A | N/A | N/A | N/A | N/A | . | 12347 | N/A | Returning | N/A | Returning | N/A | Returning | N/A | Returning | N/A | . | 12348 | N/A | Returning | N/A | N/A | N/A | N/A | New | N/A | N/A | . | 12349 | N/A | N/A | N/A | N/A | N/A | N/A | N/A | N/A | New | . | 12350 | N/A | N/A | N/A | N/A | N/A | N/A | N/A | N/A | N/A | . | 월별 Churn / Returning 고객 수 및 비율을 계산 . # unstack &amp; rename new_customer = new_customer.unstack().reset_index() new_customer.rename(columns={'level_0':'InvoiceMonth', 0:'NewFlag'}, inplace=True) # InvoiceMonth, NewFlag를 기준으로 groupby ('N/A'는 무시) monthly_new = new_customer.query('NewFlag != \"N/A\"') monthly_new = monthly_new.groupby(['InvoiceMonth', 'NewFlag'])[['CustomerID']].count() monthly_new.rename(columns={'CustomerID':'CustomerCount'}, inplace=True) # 월별로, New vs Returning 비율을 계산 monthly_new['Percentage'] = monthly_new.groupby(level=0)['CustomerCount'].transform(lambda x: x / x.sum()) monthly_new.head(6) . | InvoiceMonth | NewFlag | CustomerCount | Percentage | . | 201103 | New | 452 | 0.464066 | . | ^^ | Returning | 522 | 0.535934 | . | 201104 | New | 361 | 0.421729 | . | ^^ | Returning | 495 | 0.578271 | . | 201105 | New | 372 | 0.352607 | . | ^^ | Returning | 683 | 0.647393 | . | . 월별 신규유입률 추이 . | 월별 재구매자수 &amp; 신규유입자수 추이 . fig = px.bar(monthly_new.reset_index(), x='InvoiceMonth', y='CustomerCount', color='NewFlag', barmode='group', category_orders = {'NewFlag':['Returning', 'New']}, hover_data={'Percentage': ':.0%'}, color_discrete_sequence = [px.colors.sequential.Teal[0], px.colors.sequential.Teal[2]]) fig.show() . | 재구매자: 2011.03 대비 2011.11에 약 92% 증가 | 2011.09 ~ 2011.11 3개월 간 신규유입자가 500명 위로 높게 유지됨 | . | 월별 재구매율 &amp; 신규유입률 추이 . fig = px.bar(monthly_new.reset_index(), x='InvoiceMonth', y='Percentage', color='NewFlag', category_orders = {'NewFlag':['Returning', 'New']}, color_discrete_sequence = [px.colors.sequential.Teal[0], px.colors.sequential.Teal[2]]) fig.update_yaxes(tickformat='.0%') fig.show() . | 월별 전체 구매자 대비 비율로 따지면, 신규유입률은 여름(2011.06 ~ 2011.08)에 감소 후 가을 이후로 다시 증가하는 경향을 보임 | 1년 정도의 데이터를 더 확보하면 seasonality를 더 명확히 파악하고 전략을 세울 수 있을 거라 생각됨 (ex. 신규 유입이 많은 겨울 시즌에 집중 프로모션) | . | . ",
    "url": "https://chaelist.github.io/docs/kaggle/uk_ecommerce/#%EC%9B%94%EB%B3%84-%EC%8B%A0%EA%B7%9C-%EC%9C%A0%EC%9E%85%EB%A5%A0",
    "relUrl": "/docs/kaggle/uk_ecommerce/#월별-신규-유입률"
  },"260": {
    "doc": "UK Ecommerce Data 1",
    "title": "UK Ecommerce Data 1",
    "content": " ",
    "url": "https://chaelist.github.io/docs/kaggle/uk_ecommerce/",
    "relUrl": "/docs/kaggle/uk_ecommerce/"
  },"261": {
    "doc": "UK Ecommerce Data 2",
    "title": "UK Ecommerce Data",
    "content": ". | 데이터 정리 | 6개월 재구매율 | 고객 특성별 구매당 반품 비율 . | 이탈자와 재구매자 비교 | 이탈 고위험군과 저위험군 비교 | . | 상품별 분석 . | 상품별 구매 대비 반품 | 상품별 구매 증가폭 | . | 국가별 분석 . | 국가별 매출 | 상반기 대비 하반기 비교 | . | . *분석 대상 데이터셋: UK E-Commerce Data . | 데이터셋 출처 | UK-based retailer의 2020-12-01에서 2011-12-09 사이의 모든 거래를 기록한 데이터 | gift를 주로 판매하며, 대다수의 고객은 wholesaler임 | . ",
    "url": "https://chaelist.github.io/docs/kaggle/uk_ecommerce2/#uk-ecommerce-data",
    "relUrl": "/docs/kaggle/uk_ecommerce2/#uk-ecommerce-data"
  },"262": {
    "doc": "UK Ecommerce Data 2",
    "title": "데이터 정리",
    "content": "# 필요한 라이브러리 import import pandas as pd import numpy as np import scipy.stats as stats from matplotlib import pyplot as plt import seaborn as sns import plotly.express as px import plotly.io as pio pio.templates.default = \"plotly_white\" # default template을 지정 . ecom_df = pd.read_csv('data/uk_ecommerce_data.csv') ecom_df.head() . |   | InvoiceNo | StockCode | Description | Quantity | InvoiceDate | UnitPrice | CustomerID | Country | . | 0 | 536365 | 85123A | WHITE HANGING HEART T-LIGHT HOLDER | 6 | 12/1/2010 8:26 | 2.55 | 17850 | United Kingdom | . | 1 | 536365 | 71053 | WHITE METAL LANTERN | 6 | 12/1/2010 8:26 | 3.39 | 17850 | United Kingdom | . | 2 | 536365 | 84406B | CREAM CUPID HEARTS COAT HANGER | 8 | 12/1/2010 8:26 | 2.75 | 17850 | United Kingdom | . | 3 | 536365 | 84029G | KNITTED UNION FLAG HOT WATER BOTTLE | 6 | 12/1/2010 8:26 | 3.39 | 17850 | United Kingdom | . | 4 | 536365 | 84029E | RED WOOLLY HOTTIE WHITE HEART. | 6 | 12/1/2010 8:26 | 3.39 | 17850 | United Kingdom | . | 데이터 타입 정리 . # Customer ID를 string 형태로 바꿔줌 ecom_df['CustomerID'].fillna(0, inplace=True) # 일단 N/A를 0으로 바꿔줌 ecom_df['CustomerID'] = ecom_df['CustomerID'].astype('int').astype('str') # 정수로 -&gt; str로 바꿔줌 ecom_df.replace({'CustomerID': '0'}, 'N/A', inplace=True) # 0은 'N/A'로 바꿔줌 # InvoiceDate를 datetime 형태로 바꿔줌 ecom_df['InvoiceDate'] = pd.to_datetime(ecom_df['InvoiceDate']) . | 중복값 제거 . # 전체 값이 다 중복된 경우, 가장 위에 있는 행만 남기고 drop ecom_df.drop_duplicates(inplace=True) ecom_df.reset_index(drop=True, inplace=True) . | 데이터 정리 - 필요 없는 데이터 삭제 . # UnitPrice = 0인 데이터는 매출과 관계가 없으므로 삭제 drop_index = ecom_df[ecom_df['UnitPrice'] == 0].index ecom_df.drop(drop_index, axis='index', inplace=True) # CRUK, BANK CHARGES, B, AMAZONFEE, S 삭제 (제품 판매로 인한 매출과 관계 없는 Stock Code) drop_index = ecom_df.query(\"StockCode in ['CRUK', 'BANK CHARGES', 'B', 'S', 'AMAZONFEE']\").index ecom_df.drop(drop_index, axis='index', inplace=True) # reset index ecom_df.reset_index(drop=True, inplace=True) . | 분석이 용이하도록 새로운 칼럼 생성 . # InvoiceMonth, InvoiceWeekday 칼럼 생성 ecom_df['InvoiceMonth'] = ecom_df['InvoiceDate'].dt.strftime('%Y%m') # YYYYmm 형식으로 정리 ecom_df['InvoiceWeekday'] = ecom_df['InvoiceDate'].dt.weekday # 월: 0 ~ 일: 6 # TotalSpending 칼럼 생성: Quantity * UnitPrice ecom_df['TotalSpending'] = ecom_df['Quantity'] * ecom_df['UnitPrice'] . | CustomerID == N/A를 제외한 df를 따로 저장 . | nan인 값은 ‘등록되지 않은 구매자’ 개념인 듯. (비회원 구매 개념) | 신규 이용자 / 이탈자 등을 분석할 때는 제외하는 것이 맞겠지만, 인기 상품 / 기간별 매출 등을 파악할 때에는 익명 이용자의 구매도 중요하므로, ecom_df와 ecom_no_na 두 개의 dataframe으로 나눠서 저장해둠 | . ecom_no_na = ecom_df.query('CustomerID != \"N/A\"') . | . ",
    "url": "https://chaelist.github.io/docs/kaggle/uk_ecommerce2/#%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%A0%95%EB%A6%AC",
    "relUrl": "/docs/kaggle/uk_ecommerce2/#데이터-정리"
  },"263": {
    "doc": "UK Ecommerce Data 2",
    "title": "6개월 재구매율",
    "content": ". | 같은 e-commerce 사업이라도 재구매율과 재구매 주기는 취급 항목에 따라 다름 | 재구매율에 따라, 충성 고객 유지에 초점을 맞춘 마케팅이 필요할 수도 있고, 신규 고객 유치에 더 노력을 기울여야 할 수도 있다. (ex. 식료품 판매: 거의 매일 접속하는 이용자가 많음 vs 대형 가전 판매: 대부분 한 번 구매하면 몇 년 동안 사용) | 주어진 데이터의 경우 1년 간의 사업 데이터이므로, 6개월 단위의 재구매율을 통해 사업 방향성을 점검 | . # 상반기: 2010-12-01 ~ 2011-05-31 first_half = ecom_no_na.query('InvoiceDate &lt; \"2011-06-01\"') first_half_customers = set(first_half['CustomerID'].unique()) # 하반기: 2011-06-01 ~ 2011-11-30 second_half = ecom_no_na.query('InvoiceDate &gt;= \"2011-06-01\" &amp; InvoiceDate &lt; \"2011-12-01\"') second_half_customers = set(second_half['CustomerID'].unique()) print('상반기 구매자수: ', len(first_half_customers)) print('상반기&amp;하반기 구매자수: ', len(first_half_customers &amp; second_half_customers)) print('6개월 재구매율: ', len(first_half_customers &amp; second_half_customers) / len(first_half_customers)) . 상반기 구매자수: 2767 상반기&amp;하반기 구매자수: 1934 6개월 재구매율: 0.6989519335019877 . | 6개월 재구매율: 약 69.9% | 재구매율이 높은 편이므로, 충성도 높은 고객에게 초점을 맞춘 마케팅에 신경을 쓰면 좋을 듯 (ex. 포인트 제도 등) | . ",
    "url": "https://chaelist.github.io/docs/kaggle/uk_ecommerce2/#6%EA%B0%9C%EC%9B%94-%EC%9E%AC%EA%B5%AC%EB%A7%A4%EC%9C%A8",
    "relUrl": "/docs/kaggle/uk_ecommerce2/#6개월-재구매율"
  },"264": {
    "doc": "UK Ecommerce Data 2",
    "title": "고객 특성별 구매당 반품 비율",
    "content": ": 고객별 이탈 위험도 (risk ratio)와 구매당 반품 비율의 관계를 확인 . | 고객별 구매 패턴 정리 . ecom_purchase = ecom_no_na.query('TotalSpending &gt;= 0') # TotalSpending이 양수인 값만 반영. (-는 반품한 내역으로 추정) temp = ecom_purchase.query('InvoiceMonth != \"201112\"') # 201112는 제외하고 계산 customer_usage = pd.pivot_table(temp, index='CustomerID', columns='InvoiceMonth', values='InvoiceNo', aggfunc='count') customer_usage = customer_usage.applymap(lambda x: 'O' if x &gt; 0 else 'X') for index in customer_usage.index: templist = customer_usage.loc[index].to_list() month_btw = [] for i in range(12): if templist[i] == 'O': for j in range(1, 12 - i): if templist[i + j] == 'O': month_btw.append(j) break if month_btw: month_btw_avg = sum(month_btw) / len(month_btw) else: if (customer_usage.loc[index, '201111'] == 'O') or (customer_usage.loc[index, '201110'] == 'O'): month_btw_avg = 'New Customer' else: month_btw_avg = 'Never Returned' customer_usage.loc[index, 'avg_month_btw_purchases'] = month_btw_avg last_month = ecom_purchase.query('InvoiceMonth != \"201112\"').groupby('CustomerID')[['InvoiceMonth']].max() last_month.rename(columns={'InvoiceMonth':'LastMonth'}, inplace=True) customer_usage = customer_usage.join(last_month) for index in customer_usage.index: lastmonth = customer_usage.loc[index, 'LastMonth'] if lastmonth == '201012': lastmonth = 201100 customer_usage.loc[index, 'months_after_latest_purchase'] = 201112 - int(lastmonth) customer_usage.head() . | CustomerID | 201012 | 201101 | 201102 | 201103 | 201104 | 201105 | 201106 | 201107 | 201108 | 201109 | 201110 | 201111 | avg_month_btw_purchases | LastMonth | months_after_latest_purchase | . | 12346 | X | O | X | X | X | X | X | X | X | X | X | X | Never Returned | 201101 | 11 | . | 12347 | O | O | X | X | O | X | O | X | O | X | O | X | 2.0 | 201110 | 2 | . | 12348 | O | O | X | X | O | X | X | X | X | O | X | X | 3.0 | 201109 | 3 | . | 12349 | X | X | X | X | X | X | X | X | X | X | X | O | New Customer | 201111 | 1 | . | 12350 | X | X | O | X | X | X | X | X | X | X | X | X | Never Returned | 201102 | 10 | . | 구매 지속 고객의 risk ratio 계산 . | risk ratio(위험 비율): 마지막 구매 이후 경과한 기간 / 평균 구매 간격 | . likely_customer = customer_usage.query('avg_month_btw_purchases not in [\"Never Returned\", \"New Customer\"]') likely_customer['risk_ratio'] = likely_customer['months_after_latest_purchase'] / likely_customer['avg_month_btw_purchases'] likely_customer.head() . | CustomerID | 201012 | 201101 | 201102 | 201103 | 201104 | 201105 | 201106 | 201107 | 201108 | 201109 | 201110 | 201111 | avg_month_btw_purchases | LastMonth | months_after_latest_purchase | risk_ratio | . | 12347 | O | O | X | X | O | X | O | X | O | X | O | X | 2 | 201110 | 2 | 1 | . | 12348 | O | O | X | X | O | X | X | X | X | O | X | X | 3 | 201109 | 3 | 1 | . | 12352 | X | X | O | O | X | X | X | X | X | O | X | O | 3 | 201111 | 1 | 0.333333 | . | 12356 | X | O | X | X | O | X | X | X | X | X | X | O | 5 | 201111 | 1 | 0.2 | . | 12359 | X | O | O | X | X | X | O | X | X | X | O | X | 3 | 201110 | 2 | 0.666667 | . | 고객별 구매 대비 반품 비율 계산 . # 고객별 Purchase와 Refunds 계산해서 join ecom_purchase = ecom_no_na.query('TotalSpending &gt;= 0') ecom_purchase = ecom_purchase.groupby('CustomerID')[['Quantity']].sum() ecom_purchase.rename(columns={'Quantity':'Purchases'}, inplace=True) ecom_refund = ecom_no_na.query('TotalSpending &lt; 0') ecom_refund = ecom_refund.groupby('CustomerID')[['Quantity']].sum() * -1 ecom_refund.rename(columns={'Quantity':'Refunds'}, inplace=True) purchase_refund = ecom_purchase.join(ecom_refund, how='left') purchase_refund['Refunds'].fillna(0, inplace=True) # Refunds per Purchases 수치 계산 purchase_refund['Re_per_Pur'] = purchase_refund['Refunds'] / purchase_refund['Purchases'] * 100 purchase_refund.head() . | CustomerID | Purchases | Refunds | Re_per_Pur | . | 12346 | 74215 | 74215 | 100 | . | 12347 | 2458 | 0 | 0 | . | 12348 | 2341 | 0 | 0 | . | 12349 | 631 | 0 | 0 | . | 12350 | 197 | 0 | 0 | . | 구매당 반품수 &amp; 고객 특성 join . purchase_refund = purchase_refund.join(customer_usage['avg_month_btw_purchases'], how='left') purchase_refund = purchase_refund.join(customer_usage['months_after_latest_purchase'], how='left') purchase_refund = purchase_refund.join(likely_customer['risk_ratio'], how='left') purchase_refund.head() . | CustomerID | Purchases | Refunds | Re_per_Pur | avg_month_btw_purchases | months_after_latest_purchase | risk_ratio | . | 12346 | 74215 | 74215 | 100 | Never Returned | 11 | nan | . | 12347 | 2458 | 0 | 0 | 2.0 | 2 | 1 | . | 12348 | 2341 | 0 | 0 | 3.0 | 3 | 1 | . | 12349 | 631 | 0 | 0 | New Customer | 1 | nan | . | 12350 | 197 | 0 | 0 | Never Returned | 10 | nan | . | . 이탈자와 재구매자 비교 . | 이탈자 vs 1년 이내에 2번 이상 구매한 고객 | . # 'New Customer'는 제외하고 분석 purchase_refund1 = purchase_refund.query('avg_month_btw_purchases != \"New Customer\"') # 고객을 'Never Returned'와 'Returned'로 구분해줌 purchase_refund1['flag1'] = purchase_refund1['avg_month_btw_purchases'].apply(lambda x: 'Never Returned' if x == 'Never Returned' else 'Returned') purchase_refund1.head() . | CustomerID | Purchases | Refunds | Re_per_Pur | avg_month_btw_purchases | months_after_latest_purchase | risk_ratio | flag1 | . | 12346 | 74215 | 74215 | 100 | Never Returned | 11 | nan | Never Returned | . | 12347 | 2458 | 0 | 0 | 2.0 | 2 | 1 | Returned | . | 12348 | 2341 | 0 | 0 | 3.0 | 3 | 1 | Returned | . | 12350 | 197 | 0 | 0 | Never Returned | 10 | nan | Never Returned | . | 12352 | 536 | 66 | 12.3134 | 3.0 | 1 | 0.333333 | Returned | . | 이탈자와 재구매자의 구매당 반품수 비교 . purchase_refund1.groupby('flag1')[['Re_per_Pur']].agg(['mean', 'count']) . |   | Re_per_Pur | |   | . | flag1 | mean | count | . | Never Returned | 2.6610 | 1097 | . | Returned | 1.7426 | 2644 | . → 시각화: . temp = purchase_refund1.groupby('flag1')[['Re_per_Pur']].agg(['mean', 'count']) temp = temp['Re_per_Pur'].reset_index() fig = px.bar(temp, x='flag1', y='mean', color='flag1', hover_data = ['count'], labels = {'mean':'Refunds per Purchase', 'flag1': 'Customer Flag', 'count':'Customer Count'}, category_orders = {'flag1':['Never Returned', 'Returned']}, color_discrete_sequence = [px.colors.sequential.Teal[0], px.colors.sequential.Teal[2]]) fig.update_traces(showlegend=False) fig.show() . | 이탈 고객이 재구매 고객 대비 구매당 반품 비율이 높은 편 | . | t-test로 평균 차이가 유의미한지 검정 . never_returned = purchase_refund1.query('flag1 == \"Never Returned\"') returned = purchase_refund1.query('flag1 == \"Returned\"') # Levene의 등분산 검정 lev_result = stats.levene(never_returned['Re_per_Pur'], returned['Re_per_Pur']) print('LeveneResult(F) : %.2f \\np-value : %.3f' % (lev_result)) . LeveneResult(F) : 5.76 p-value : 0.016 . → p-value가 0.05 미만이므로, 이분산인 독립표본 t-test로 진행: . t_result = stats.ttest_ind(never_returned['Re_per_Pur'], returned['Re_per_Pur'], equal_var=False) print('t statistic : %.2f \\np-value : %.3f' % (t_result)) . t statistic : 1.80 p-value : 0.072 . | t-test 결과 p-value는 0.05 이상이지만, 수치 차이를 볼 때, 이탈한 고객과 재방문 고객 간에 구매당 반품 비율 차이가 약간은 의미가 있다고 생각해도 될 듯. (데이터 수가 더 많으면 p값이 더 낮게 나올 수 있음) | . | . 이탈 고위험군과 저위험군 비교 . # 'risk ratio'가 null인 행은 제외하고 분석 (Never Returned와 New Customer 제외) purchase_refund2 = purchase_refund[purchase_refund['risk_ratio'].notna()] # risk ratio가 1.5보다 높으면 'High Risk', 그 외에는 'Low Risk'로 분류해줌 # risk ratio가 1.5라는 건 평소 구매 주기의 1.5배의 기간 동안 돌아오지 않았다는 의미 purchase_refund2['flag2'] = purchase_refund2['risk_ratio'].apply(lambda x: 'High Risk' if x &gt; 1.5 else 'Low Risk') purchase_refund2.head() . | CustomerID | Purchases | Refunds | Re_per_Pur | avg_month_btw_purchases | months_after_latest_purchase | risk_ratio | flag2 | . | 12347 | 2458 | 0 | 0 | 2 | 2 | 1 | Low Risk | . | 12348 | 2341 | 0 | 0 | 3 | 3 | 1 | Low Risk | . | 12352 | 536 | 66 | 12.3134 | 3 | 1 | 0.333333 | Low Risk | . | 12356 | 1591 | 0 | 0 | 5 | 1 | 0.2 | Low Risk | . | 12359 | 1609 | 10 | 0.621504 | 3 | 2 | 0.666667 | Low Risk | . | 이탈 고위험군과 저위험군의 구매당 반품수 비교 . purchase_refund2.groupby('flag2')[['Re_per_Pur']].agg(['mean', 'count']) . |   | Re_per_Pur | |   | . | flag2 | mean | count | . | High Risk | 2.45521 | 515 | . | Low Risk | 1.55297 | 2088 | . → 시각화: . temp = purchase_refund2.groupby('flag2')[['Re_per_Pur']].agg(['mean', 'count']) temp = temp['Re_per_Pur'].reset_index() fig = px.bar(temp, x='flag2', y='mean', color='flag2', hover_data = ['count'], labels = {'mean':'Refunds per Purchase', 'flag2': 'Customer Flag', 'count':'Customer Count'}, color_discrete_sequence = [px.colors.sequential.Teal[0], px.colors.sequential.Teal[2]]) fig.update_traces(showlegend=False) fig.show() . | 이탈 고위험군인 고객들이 구매당 반품 비율이 더 높은 편으로 파악됨 | . | t-test로 평균 차이가 유의미한지 검정 . high_risk = purchase_refund2.query('flag2 == \"High Risk\"') low_risk = purchase_refund2.query('flag2 == \"Low Risk\"') # Levene의 등분산 검정 lev_result = stats.levene(high_risk['Re_per_Pur'], low_risk['Re_per_Pur']) print('LeveneResult(F) : %.2f \\np-value : %.3f' % (lev_result)) . LeveneResult(F) : 7.13 p-value : 0.008 . → p-value가 0.05 미만이므로, 이분산인 독립표본 t-test로 진행: . t_result = stats.ttest_ind(high_risk['Re_per_Pur'], low_risk['Re_per_Pur'], equal_var=False) print('t statistic : %.2f \\np-value : %.3f' % (t_result)) . t statistic : 1.87 p-value : 0.062 . | p값이 0.05 이상이긴 하지만, 수치 차이를 볼 때, 이탈한 고객과 재방문 고객 간에 구매당 반품 비율 차이가 약간은 의미가 있다고 생각해도 될 듯. (데이터 수가 더 많으면 p값이 더 낮게 나올 수 있음) | 반품을 방지하는 것이 이탈 고위험군 고객의 만족도를 높여 이탈을 방지하는 데에 도움이 될 거라고 가설을 세울 수 있을 듯 | . | . ",
    "url": "https://chaelist.github.io/docs/kaggle/uk_ecommerce2/#%EA%B3%A0%EA%B0%9D-%ED%8A%B9%EC%84%B1%EB%B3%84-%EA%B5%AC%EB%A7%A4%EB%8B%B9-%EB%B0%98%ED%92%88-%EB%B9%84%EC%9C%A8",
    "relUrl": "/docs/kaggle/uk_ecommerce2/#고객-특성별-구매당-반품-비율"
  },"265": {
    "doc": "UK Ecommerce Data 2",
    "title": "상품별 분석",
    "content": "# Description을 기준으로 정리하기 전에, 혹시 모를 사소한 차이를 대비해 모두 upper로 맞춰주고, 양 옆 공백을 없애줌 print(ecom_df['Description'].nunique()) # 정리 이전의 Description 수 ecom_df['Description'] = ecom_df['Description'].str.upper().str.strip() print(ecom_df['Description'].nunique()) # 정리 이후의 Description 수 . 4037 4026 . 상품별 구매 대비 반품 . | 반품을 최소화하기 위해, 구매 대비 반품이 많은 제품을 살펴보기로 함 | . # 상품별로 구매수와 반품수를 집계한 다음 join product_purchase = ecom_df.query('TotalSpending &gt;= 0') product_purchase = product_purchase.groupby('Description')[['Quantity']].sum() product_purchase.rename(columns={'Quantity':'Purchases'}, inplace=True) product_refund = ecom_df.query('TotalSpending &lt; 0') product_refund = product_refund.groupby('Description')[['Quantity']].sum() * -1 product_refund.rename(columns={'Quantity':'Refunds'}, inplace=True) products = product_purchase.join(product_refund, how='left') products['Refunds'].fillna(0, inplace=True) # Refunds per Purchases 수치 계산 products['Re_per_Pur'] = products['Refunds'] / products['Purchases'] * 100 products.head() . | Description | Purchases | Refunds | Re_per_Pur | . | *BOOMBOX IPOD CLASSIC | 1 | 0 | 0 | . | *USB OFFICE MIRROR BALL | 2 | 0 | 0 | . | 10 COLOUR SPACEBOY PEN | 6564 | 172 | 2.62035 | . | 12 COLOURED PARTY BALLOONS | 2134 | 20 | 0.93721 | . | 12 DAISY PEGS IN WOOD BOX | 344 | 0 | 0 | . → 구매 대비 반품비율이 가장 높은 제품 Top 10을 시각화: . # 구매가 너무 적을 경우 한두명의 변심에 따라 반품비율이 너무 높게 나타날 수 있으므로, 구매가 10번 이상인 제품만 대상으로 비교 temp = products.reset_index().query('Purchases &gt;= 10') temp = temp.sort_values(by='Re_per_Pur', ascending=False).head(10) fig = px.bar(temp, x='Re_per_Pur', y='Description', color='Re_per_Pur', hover_data=['Purchases', 'Refunds'], labels = {'Re_per_Pur':'Refunds per Purchase'}, category_orders = {'Description': temp['Description'].to_list()}, color_continuous_scale = 'Teal') fig.update(layout_coloraxis_showscale=False) fig.show() . | 반품 비율이 100% 이상인 제품들: 2010.12 이전 데이터는 기록되어 있지 않기 때문에, 그 이전에 구매한 후 기록 기간 내에 반품된 건이 많아서 그런 듯. | 구매당 반품 비율이 50% 이상인 제품들은 집중 품질 관리가 필요하다고 생각됨. | .   . +) 반품 비율이 가장 높은 제품의 구매월 &amp; 반품월 확인: . # WOODEN BOX ADVENT CALENDAR fig = px.bar(ecom_df.query('Description == \"WOODEN BOX ADVENT CALENDAR\"'), x='InvoiceMonth', y='Quantity', color='Quantity', color_continuous_scale = 'Teal') fig.update(layout_coloraxis_showscale=False) fig.show() . | WOODEN BOX ADVENT CALENDAR 제품의 경우, 주로 2010.12~2011.01 기간에 반품이 몰려 있고, 그 이후에는 반품이 거의 없음. – 2011년 초에 제품 품질 개선이 이미 이루어졌는지 확인해보면 좋을 듯 | . 상품별 구매 증가폭 . | 상반기 대비 하반기 구매량이 가장 많이 증가한 제품 . first_half = ecom_df.query('InvoiceDate &lt; \"2011-06-01\"') temp1 = first_half.groupby('Description')[['Quantity']].sum() temp1.rename(columns={'Quantity':'first_half'}, inplace=True) second_half = ecom_no_na.query('InvoiceDate &gt;= \"2011-06-01\" &amp; InvoiceDate &lt; \"2011-12-01\"') temp2 = second_half.groupby('Description')[['Quantity']].sum() temp2.rename(columns={'Quantity':'second_half'}, inplace=True) prod_purchase_change = temp1.join(temp2, how='outer') prod_purchase_change.fillna(0, inplace=True) prod_purchase_change['change'] = prod_purchase_change['second_half'] - prod_purchase_change['first_half'] prod_purchase_change.sort_values(by='change', ascending=False).head(10) . | Description | first_half | second_half | change | . | POPCORN HOLDER | 0 | 25149 | 25149 | . | RABBIT NIGHT LIGHT | 1130 | 22278 | 21148 | . | MINI PAINT SET VINTAGE | 0 | 14033 | 14033 | . | RED HARMONICA IN BOX | 0 | 12787 | 12787 | . | ROTATING SILVER ANGELS T-LIGHT HLDR | -6887 | 5284 | 12171 | . | PAPER CHAIN KIT 50’S CHRISTMAS | 0 | 11993 | 11993 | . | BROCADE RING PURSE | 0 | 11842 | 11842 | . | PACK OF 12 LONDON TISSUES | 0 | 11763 | 11763 | . | BUBBLEGUM RING ASSORTED | 0 | 11419 | 11419 | . | 60 CAKE CASES VINTAGE CHRISTMAS | 1336 | 12642 | 11306 | . | Popcorn Holder가 가장 구매량이 많이 증가함. | 주로 상반기에는 없었던 신상품들이 구매 증가량이 가장 많은 것을 알 수 있음 | . cf) 전체 기간 구매량 Top 10: . ecom_df.groupby('Description')[['Quantity']].sum().sort_values(by='Quantity', ascending=False).head(10) . | Description | Quantity | . | WORLD WAR 2 GLIDERS ASSTD DESIGNS | 53751 | . | JUMBO BAG RED RETROSPOT | 47256 | . | POPCORN HOLDER | 36322 | . | ASSORTED COLOUR BIRD ORNAMENT | 36282 | . | PACK OF 72 RETROSPOT CAKE CASES | 36016 | . | WHITE HANGING HEART T-LIGHT HOLDER | 35294 | . | RABBIT NIGHT LIGHT | 30631 | . | MINI PAINT SET VINTAGE | 26437 | . | PACK OF 12 LONDON TISSUES | 26095 | . | PACK OF 60 PINK PAISLEY CAKE CASES | 24719 | . | 상반기 대비 하반기 구매 금액이 가장 많이 증가한 제품 . first_half = ecom_df.query('InvoiceDate &lt; \"2011-06-01\"') temp1 = first_half.groupby('Description')[['TotalSpending']].sum() temp1.rename(columns={'TotalSpending':'first_half'}, inplace=True) second_half = ecom_no_na.query('InvoiceDate &gt;= \"2011-06-01\" &amp; InvoiceDate &lt; \"2011-12-01\"') temp2 = second_half.groupby('Description')[['TotalSpending']].sum() temp2.rename(columns={'TotalSpending':'second_half'}, inplace=True) prod_purchase_change2 = temp1.join(temp2, how='outer') prod_purchase_change2.fillna(0, inplace=True) prod_purchase_change2['change'] = prod_purchase_change2['second_half'] - prod_purchase_change2['first_half'] # 'MANUAL'은 하나의 제품은 아니므로 제외 prod_purchase_change2.query('Description != \"MANUAL\"').sort_values(by='change', ascending=False).head(10) . | Description | first_half | second_half | change | . | RABBIT NIGHT LIGHT | 2277.5 | 42033.0 | 39755.5 | . | PICNIC BASKET WICKER 60 PIECES | 0 | 39619.5 | 39619.5 | . | PAPER CHAIN KIT 50’S CHRISTMAS | 0 | 32806.9 | 32806.9 | . | SET OF 3 REGENCY CAKE TINS | 0 | 25219.8 | 25219.8 | . | HOT WATER BOTTLE KEEP CALM | 0 | 21902.0 | 21902.0 | . | SET OF TEA COFFEE SUGAR TINS PANTRY | 0 | 21305.5 | 21305.5 | . | DOORMAT KEEP CALM AND COME IN | 6175.5 | 27088.7 | 20913.2 | . | SPOTTY BUNTING | 7513.9 | 27418.4 | 19904.5 | . | SET OF 3 CAKE TINS PANTRY DESIGN | 0 | 19277.6 | 19277.6 | . | POPCORN HOLDER | 0 | 19109.8 | 19109.8 | . | Rabbit Night Light가 구매 금액이 가장 많이 증가함. | 주로 상반기에는 없었던 신상품들이 구매 금액 증가폭이 가장 크다 | . cf) 전체 기간 구매 금액 Top 10: . # 'DOTCOM POSTAGE', 'POSTAGE'는 판매 제품이라고 보기는 애매하므로 제외 products_spending = ecom_df.groupby('Description')[['TotalSpending']].sum().sort_values(by='TotalSpending', ascending=False) products_spending.query('Description not in [\"DOTCOM POSTAGE\", \"POSTAGE\"]').head(10) . | Description | TotalSpending | . | REGENCY CAKESTAND 3 TIER | 164459.5 | . | WHITE HANGING HEART T-LIGHT HOLDER | 99612.4 | . | PARTY BUNTING | 98243.9 | . | JUMBO BAG RED RETROSPOT | 92175.8 | . | RABBIT NIGHT LIGHT | 66661.6 | . | PAPER CHAIN KIT 50’S CHRISTMAS | 63715.2 | . | ASSORTED COLOUR BIRD ORNAMENT | 58792.4 | . | CHILLI LIGHTS | 53746.7 | . | SPOTTY BUNTING | 42030.7 | . | JUMBO BAG PINK POLKADOT | 41584.4 | . | . ",
    "url": "https://chaelist.github.io/docs/kaggle/uk_ecommerce2/#%EC%83%81%ED%92%88%EB%B3%84-%EB%B6%84%EC%84%9D",
    "relUrl": "/docs/kaggle/uk_ecommerce2/#상품별-분석"
  },"266": {
    "doc": "UK Ecommerce Data 2",
    "title": "국가별 분석",
    "content": "국가별 매출 . | 국가별 총 매출 . # 전체 기간 매출 Top 10 국가 시각화 country_spending = ecom_df.groupby('Country')[['TotalSpending']].sum().sort_values(by='TotalSpending', ascending=False) fig = px.bar(country_spending.head(10).reset_index(), x='Country', y='TotalSpending', color='TotalSpending', color_continuous_scale = 'Teal') fig.update(layout_coloraxis_showscale=False) fig.show() . | UK 기반 업체이므로, UK가 압도적으로 많고, 주로 인접한 유럽 국가들에서 구매액이 높음 | . | 국가별 고객 1인당 매출 . # 국가별로, 고객 1인당 매출 계산 spc_country = ecom_df.groupby('Country').agg({'TotalSpending':'sum', 'CustomerID':pd.Series.nunique}) spc_country['SpendingPerCustomer'] = spc_country['TotalSpending'] / spc_country['CustomerID'] # 고객 1인당 매출 Top 10 국가 시각화 # 고객 수가 5명 이상인 국가로 한정 spc_country_over3 = spc_country.query('CustomerID &gt;= 5').sort_values(by='SpendingPerCustomer', ascending=False) fig = px.bar(spc_country_over3.head(10).reset_index(), x='Country', y='SpendingPerCustomer', color='SpendingPerCustomer', hover_data = ['CustomerID'], labels = {'CustomerID': 'Customer Count'}, color_continuous_scale = 'Teal') fig.update(layout_coloraxis_showscale=False) fig.show() . | Netherlands가 가장 고객 당 매출이 높은 것으로 확인됨. | UK의 경우, 고객수가 압도적으로 많아서 고객 당 매출액이 보다 평균으로 회귀한 결과가 나온 것일 수도 있고, 배송비가 적게 붙어서 그런 영향도 조금 있을 수 있을 듯. | . | . 상반기 대비 하반기 비교 . | 국가별 상반기 대비 하반기 구매금액 . first_half = ecom_no_na.query('InvoiceDate &lt; \"2011-06-01\"') temp1 = first_half.groupby('Country')[['TotalSpending']].sum() temp1.rename(columns={'TotalSpending':'first_half'}, inplace=True) second_half = ecom_no_na.query('InvoiceDate &gt;= \"2011-06-01\" &amp; InvoiceDate &lt; \"2011-12-01\"') temp2 = second_half.groupby('Country')[['TotalSpending']].sum() temp2.rename(columns={'TotalSpending':'second_half'}, inplace=True) spending_change = temp1.join(temp2, how='outer') spending_change.fillna(0, inplace=True) spending_change['change'] = spending_change['second_half'] - spending_change['first_half'] spending_change.sort_values(by='change', ascending=False).head(10) . | Country | first_half | second_half | change | . | United Kingdom | 2535957.8 | 3920927.9 | 1384970.1 | . | EIRE | 78764.5 | 164260.9 | 85496.3 | . | Netherlands | 112906.7 | 160026.8 | 47120.2 | . | France | 71743.5 | 117833.7 | 46090.2 | . | Germany | 91606.4 | 122098.4 | 30492.1 | . | Australia | 55600.0 | 81409.8 | 25809.8 | . | Switzerland | 15438.4 | 40301.0 | 24862.5 | . | Norway | 4722.7 | 27655.1 | 22932.4 | . | Belgium | 13140.0 | 26361.5 | 13221.5 | . | Spain | 21485.9 | 32998.7 | 11512.9 | . → 구매액 증가폭이 높은 국가 Top 10 시각화: . country_spending_change = spending_change.sort_values(by='change', ascending=False).head(10) fig = px.bar(country_spending_change[['first_half', 'second_half']].unstack().reset_index(), x='Country', y=0, color='level_0', barmode='group', labels={'0':'Total Spending', 'level_0':'Time Period'}, category_orders = {'Country': country_spending_change.index}, # 구매액 증가폭이 높은 순서대로 정렬 color_discrete_sequence = [px.colors.sequential.Teal[0], px.colors.sequential.Teal[2]]) fig.update(layout_coloraxis_showscale=False) fig.show() . | 구매액 증가폭 역시 UK가 가장 크고, 그 다음이 아일랜드와 네덜란드. | . | 국가별 상반기 대비 하반기 고객수 . temp1 = first_half.groupby('Country')[['CustomerID']].nunique() temp1.rename(columns={'CustomerID':'first_half'}, inplace=True) temp2 = second_half.groupby('Country')[['CustomerID']].nunique() temp2.rename(columns={'CustomerID':'second_half'}, inplace=True) customer_change = temp1.join(temp2, how='outer') customer_change.fillna(0, inplace=True) customer_change['change'] = customer_change['second_half'] - customer_change['first_half'] customer_change.sort_values(by='change', ascending=False).head(10) . | Country | first_half | second_half | change | . | United Kingdom | 2508 | 3165 | 657 | . | Germany | 57 | 80 | 23 | . | France | 56 | 70 | 14 | . | Spain | 19 | 25 | 6 | . | Norway | 3 | 9 | 6 | . | Switzerland | 11 | 17 | 6 | . | Finland | 4 | 10 | 6 | . | Denmark | 3 | 8 | 5 | . | Portugal | 11 | 14 | 3 | . | Poland | 3 | 6 | 3 | . → 구매 고객수 증가폭이 높은 국가 Top 10 시각화: . country_customer_change = customer_change.sort_values(by='change', ascending=False).head(10) fig = px.bar(country_customer_change[['first_half', 'second_half']].unstack().reset_index(), x='Country', y=0, color='level_0', barmode='group', labels={'0':'Customer Count', 'level_0':'Time Period'}, category_orders = {'Country': country_customer_change.index}, # 고객수 증가폭이 높은 순서대로 정렬 color_discrete_sequence = [px.colors.sequential.Teal[0], px.colors.sequential.Teal[2]]) fig.update(layout_coloraxis_showscale=False) fig.show() . | 구매고객 증가폭 역시 UK가 가장 크고, 그 다음은 독일과 프랑스. | . | . ",
    "url": "https://chaelist.github.io/docs/kaggle/uk_ecommerce2/#%EA%B5%AD%EA%B0%80%EB%B3%84-%EB%B6%84%EC%84%9D",
    "relUrl": "/docs/kaggle/uk_ecommerce2/#국가별-분석"
  },"267": {
    "doc": "UK Ecommerce Data 2",
    "title": "UK Ecommerce Data 2",
    "content": " ",
    "url": "https://chaelist.github.io/docs/kaggle/uk_ecommerce2/",
    "relUrl": "/docs/kaggle/uk_ecommerce2/"
  },"268": {
    "doc": "유용한 Python 내장함수",
    "title": "유용한 Python 내장함수",
    "content": ". | map, zip | Itertools . | 조합형 iterator | 무한 iterator | 조건형 iterator | . | Collections . | Counter | . | . ",
    "url": "https://chaelist.github.io/docs/data_handling/useful_functions/",
    "relUrl": "/docs/data_handling/useful_functions/"
  },"269": {
    "doc": "유용한 Python 내장함수",
    "title": "map, zip",
    "content": ". | map(function, iterable): input으로 받은 함수를 input으로 받은 iterable 내 element에 모두 적용해주는 함수 . print(list(map(int, [1.4, 2.3, 3.5, 4.0]))) # 각 element를 꺼내서 int(x)에 넣어준 후 다시 list에 넣는 느낌 . [1, 2, 3, 4] . +) function 부분에 기존 함수가 아닌 lambda 식을 넣는 것도 가능 . print(list(map(lambda x: x**2, [1, 2, 3, 4, 5]))) . [1, 4, 9, 16, 25] . | zip(iterables): 각 iterable의 element들을 연결해주는 함수 . # 보통은 이런 식으로 반복문에서 index를 붙여줄 때 많이 활용 list1 = ['A', 'B', 'C', 'D', 'E'] for i, element in zip(range(len(list1)), list1): print(i, ':', element) . 0 : A 1 : B 2 : C 3 : D 4 : E . +) zip(*zipped_element)를 해주면 unzip할 수 있음 . print(list(zip(*zip(range(len(list1)), list1)))) . [(0, 1, 2, 3, 4), ('A', 'B', 'C', 'D', 'E')] . | . ",
    "url": "https://chaelist.github.io/docs/data_handling/useful_functions/#map-zip",
    "relUrl": "/docs/data_handling/useful_functions/#map-zip"
  },"270": {
    "doc": "유용한 Python 내장함수",
    "title": "Itertools",
    "content": ": 효율적인 루핑을 위한 이터레이터를 만드는 함수들을 제공하는 모듈 . 조합형 iterator . | product(iterables, repeat=1): input으로 받은 iterable들의 데카르트곱을 반환 . | iterable: 요소를 하나씩 반환할 수 있는 객체. sequence type인 list, str, tuple 등. | 데카르트곱 (= 곱집합): 각 집합의 원소를 각 성분으로 하는 tuple의 집합 | . from itertools import product print(list(product('ABC', 'xyz'))) . [('A', 'x'), ('A', 'y'), ('A', 'z'), ('B', 'x'), ('B', 'y'), ('B', 'z'), ('C', 'x'), ('C', 'y'), ('C', 'z')] . +) iterable이 list로 묶여 있는 경우, *로 list를 해제하고 넣어주면 된다 . list1 = ['ABC', 'xyz'] print(list(product(*list1))) . [('A', 'x'), ('A', 'y'), ('A', 'z'), ('B', 'x'), ('B', 'y'), ('B', 'z'), ('C', 'x'), ('C', 'y'), ('C', 'z')] . +) repeat을 설정하면 iterable이 n번 반복해서 존재하는 것으로 간주 . print(list(product('01', repeat=2))) # product('01', '01)과 동일한 개념 . [('0', '0'), ('0', '1'), ('1', '0'), ('1', '1')] . print(list(product('A', repeat=4))) . [('A', 'A', 'A', 'A')] . print(list(product('01', 'ab', repeat=2))) . [('0', 'a', '0', 'a'), ('0', 'a', '0', 'b'), ('0', 'a', '1', 'a'), ('0', 'a', '1', 'b'), ('0', 'b', '0', 'a'), ('0', 'b', '0', 'b'), ('0', 'b', '1', 'a'), ('0', 'b', '1', 'b'), ('1', 'a', '0', 'a'), ('1', 'a', '0', 'b'), ('1', 'a', '1', 'a'), ('1', 'a', '1', 'b'), ('1', 'b', '0', 'a'), ('1', 'b', '0', 'b'), ('1', 'b', '1', 'a'), ('1', 'b', '1', 'b')] . | combinations(iterable, r): iterable에서 원소 개수가 r개인 ‘조합’ 뽑기 . | 조합: 순서를 생각하지 않고 뽑는 것 | iterable이 n개의 원소를 갖는다면, nCr개의 조합이 가능 | . from itertools import combinations print(list(combinations([1, 2, 3, 4], 2))) # 4C2 = 6가지 경우의 수 . [(1, 2), (1, 3), (1, 4), (2, 3), (2, 4), (3, 4)] . | combinations_with_replacement(iterable, r): iterable에서 원소 개수가 r개인 중복 조합 뽑기 . | iterable이 n개의 원소를 갖는다면, nHr개의 조합이 가능 | nHr = n+r-1Cr | . from itertools import combinations_with_replacement print(list(combinations_with_replacement([1, 2, 3, 4], 2))) # 4H2 = 5C2 = 10가지 경우의 수 . [(1, 1), (1, 2), (1, 3), (1, 4), (2, 2), (2, 3), (2, 4), (3, 3), (3, 4), (4, 4)] . | permutations(iterable, r=None): iterable에서 원소 개수가 r개인 ‘순열’ 뽑기 . | 순열: 순서를 고려하고 뽑는 것 | iterable이 n개의 원소를 갖는다면, nPr개의 순열이 가능 | r값을 지정하지 않거나 r=None이라고 입력하면, r의 기본값은 iterable의 길이이며 가능한 모든 최대 길이 순열이 생성됨 | . from itertools import permutations print(list(permutations([1, 2, 3, 4], 2))) # 4P2 = 12가지 경우의 수 # (1, 2)와 (2, 1)은 다른 것으로 간주됨 . [(1, 2), (1, 3), (1, 4), (2, 1), (2, 3), (2, 4), (3, 1), (3, 2), (3, 4), (4, 1), (4, 2), (4, 3)] . +) r값을 입력하지 않는 경우: . # r을 입력하지 않으면 default로 iterable의 길이인 4가 됨 → 4P4 = 24가지 경우의 수 print(list(permutations([1, 2, 3, 4]))) . [(1, 2, 3, 4), (1, 2, 4, 3), (1, 3, 2, 4), (1, 3, 4, 2), (1, 4, 2, 3), (1, 4, 3, 2), (2, 1, 3, 4), (2, 1, 4, 3), (2, 3, 1, 4), (2, 3, 4, 1), (2, 4, 1, 3), (2, 4, 3, 1), (3, 1, 2, 4), (3, 1, 4, 2), (3, 2, 1, 4), (3, 2, 4, 1), (3, 4, 1, 2), (3, 4, 2, 1), (4, 1, 2, 3), (4, 1, 3, 2), (4, 2, 1, 3), (4, 2, 3, 1), (4, 3, 1, 2), (4, 3, 2, 1)] . | . 무한 iterator . | count(start=0, step=1): start부터 시작해 step만큼씩 무한 증가하는 숫자를 만든다 . | 그냥 사용하면 숫자가 무한하게 계속 생성되므로, 보통은 zip과 같은 함수 안에 넣어서 사용 | . from itertools import count print(list(zip(count(10, 2), ['A', 'B', 'C', 'D']))) . [(10, 'A'), (12, 'B'), (14, 'C'), (16, 'D')] . *반복문에서 index를 붙여줄 때 활용하기 좋음 . list1 = ['A', 'B', 'C', 'D', 'E'] for i, element in zip(count(0, 1), list1): print(i, ':', element) # zip(range(len(list1)), list1)과 결과는 같지만, len(list1)을 계산하지 않아도 된다는 장점이 있다 . 0 : A 1 : B 2 : C 3 : D 4 : E . | cycle(iterable): iterable의 요소를 무한 반복해준다 . | count와 마찬가지로, 그냥 사용하면 무한하게 숫자가 생성되므로 주의해서 사용 | . from itertools import cycle print(list(zip(cycle('ABCD'), [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]))) . [('A', 1), ('B', 2), ('C', 3), ('D', 4), ('A', 5), ('B', 6), ('C', 7), ('D', 8), ('A', 9), ('B', 10)] . | repeat(object, [times]): object를 times만큼 반복한다 (times를 지정하지 않으면 무한 반복됨) . from itertools import repeat print(list(repeat('A', 5))) # 'A'를 5번 반복 . ['A', 'A', 'A', 'A', 'A'] . +) 무한 반복된다는 점을 활용해 이런 식으로 사용 가능: . list(map(pow, range(10), repeat(2))) # pow(x, y): x의 y제곱값을 return # pow: 기본 내장함수도 있고, math.pow()도 있음 (둘 다 가능) . [0, 1, 4, 9, 16, 25, 36, 49, 64, 81] . | . 조건형 iterator . | filterfalse(predicate, iterable): predicate(술어, 조건절)이 False인 요소만 반환 . from itertools import filterfalse print(list(filterfalse(lambda x: x &gt; 5, range(10)))) . [0, 1, 2, 3, 4, 5] . +) return값이 Boolean이 아닌 술어부의 경우, 값이 0인 경우를 False로 간주 . print(list(filterfalse(lambda x: x % 2, range(10)))) . [0, 2, 4, 6, 8] . | takewhile(predicate, iterable): 조건이 True인 동안만 요소를 반환 (iterable을 처음부터 탐색하다가, 조건이 False가 되는 순간 stop) . from itertools import takewhile print(list(takewhile(lambda x: x &gt; 5, [6, 10, 7, 2, 9, 3, 1, 11, 12]))) . [6, 10, 7] . | dropwhile(predicate, iterable): 조건이 False가 되는 순간부터 요소를 반환 (takewhile과 반대) . from itertools import dropwhile print(list(dropwhile(lambda x: x &gt; 5, [6, 10, 7, 2, 9, 3, 1, 11, 12]))) . [2, 9, 3, 1, 11, 12] . | groupby(iterable, key=None): 연속적인 키와 그룹을 반환하는 iterator 생성 (iterable을 처음부터 탐색하며, 같은 element가 연속적으로 나올 경우 이를 묶어준다) . from itertools import groupby dict1 = {} iterator = groupby('AAABBCCCCDDEE') for i, group in iterator: dict1[i] = list(group) print(dict1) . {'A': ['A', 'A', 'A'], 'B': ['B', 'B'], 'C': ['C', 'C', 'C', 'C'], 'D': ['D', 'D'], 'E': ['E', 'E']} . +) 같은 element여도, 연속적으로 나오지 않고 사이에 다른 element가 있으면 함께 묶이지 않는다 . iterator = groupby([1, 1, 1, 1, 2, 3, 1, 1, 4, 4]) # 앞의 1 네 개와 뒤의 1 두 개는 따로 묶임 for i, group in iterator: print(i, ':', list(group)) . 1 : [1, 1, 1, 1] 2 : [2] 3 : [3] 1 : [1, 1] 4 : [4, 4] . | . ",
    "url": "https://chaelist.github.io/docs/data_handling/useful_functions/#itertools",
    "relUrl": "/docs/data_handling/useful_functions/#itertools"
  },"271": {
    "doc": "유용한 Python 내장함수",
    "title": "Collections",
    "content": ": 특수 컨테이너 데이터형을 구현해주는 모듈 . | 파이썬의 범용 내장 컨테이너 dict, list, set 및 tuple에 대한 대안을 제공해준다. (namedtuple, deque 등) | . Counter . | Counter([iterable-or-mappine]): 해시 가능한 객체를 세는 데 사용하는 딕셔너리 서브 클래스. 요소가 딕셔너리 키로 저장되고 개수가 딕셔너리값으로 저장됨 . | iterable로 counter 생성하는 법 예시: c = Counter('gallahad') | mapping으로 counter 생성하는 법 예시: c = Counter({'red': 4, 'blue': 2}) | . | . | 대체로 각 element의 빈도를 세어주는 데에 활용 . from collections import Counter c = Counter('AABCDADCBADFEGEOADGOAEFBBDA') print(c) . Counter({'A': 7, 'D': 5, 'B': 4, 'E': 3, 'C': 2, 'F': 2, 'G': 2, 'O': 2}) . | c.most_common(n): 가장 빈도가 높은 n개의 element를 보여줌 . c.most_common(5) . [('A', 7), ('D', 5), ('B', 4), ('E', 3), ('C', 2)] . | dictionary type에 적용되는 기능들은 대부분 동일하게 활용 가능 . print(c.items()) . dict_items([('A', 7), ('B', 4), ('C', 2), ('D', 5), ('F', 2), ('E', 3), ('G', 2), ('O', 2)]) . | . ",
    "url": "https://chaelist.github.io/docs/data_handling/useful_functions/#collections",
    "relUrl": "/docs/data_handling/useful_functions/#collections"
  },"272": {
    "doc": "데이터 시각화",
    "title": "데이터 시각화",
    "content": " ",
    "url": "https://chaelist.github.io/docs/visualization",
    "relUrl": "/docs/visualization"
  },"273": {
    "doc": "Web Scraping",
    "title": "Web Scraping",
    "content": " ",
    "url": "https://chaelist.github.io/docs/webscraping",
    "relUrl": "/docs/webscraping"
  },"274": {
    "doc": "YouTube Trending Videos",
    "title": "YouTube Trending Videos",
    "content": ". | 데이터 파악 및 전처리 . | 결측치, 중복값 파악 | category_id에 category명 연결 | datetime 타입 변경 | . | trending 동영상 특징 파악 . | 동영상별 trending 횟수 | 월별 카테고리별 trending 동영상 추이 | 등록 요일별 / 시간대별 차이 | . | trending하기까지의 기간 비교 . | 데이터 가공 | 데이터 분포 확인 | 카테고리별 분포 확인 | . | views와 다른 변수 간의 관계 . | views와 likes | views와 dislikes | . | 기능 사용 유무에 따른 비교 . | comments/ratings_disabled 기능 사용 비율 | comments/ratings_disabled 기능의 views에의 영향 | views 기준 사분위 분류 → 분위별 비율 확인 | . | . *분석 대상 데이터셋: YouTube Trending Videos (Korea) . | 데이터셋 출처 | 2017-11-14 ~ 2018-06-14 사이에 trending한 동영상 34,567개의 데이터 | 일별 trending 동영상이 최소 52개에서 최대 192개까지 저장되어 있으며, 이틀 이상 trending한 동영상의 경우 여러 번 중복해서 포함되어 있음 | Columns (16개): ‘video_id’, ‘trending_date’, ‘title’, ‘channel_title’, ‘category_id’, ‘publish_time’, ‘tags’, ‘views’, ‘likes’, ‘dislikes’, ‘comment_count’, ‘thumbnail_link’, ‘comments_disabled’, ‘ratings_disabled’, ‘video_error_or_removed’, ‘description’ | . ",
    "url": "https://chaelist.github.io/docs/kaggle/youtube_trending/",
    "relUrl": "/docs/kaggle/youtube_trending/"
  },"275": {
    "doc": "YouTube Trending Videos",
    "title": "데이터 파악 및 전처리",
    "content": "# 필요한 라이브러리 import import pandas as pd import numpy as np from matplotlib import pyplot as plt import seaborn as sns . 결측치, 중복값 파악 . videos_df = pd.read_csv('data/KRvideos.csv') videos_df.head(3) . |   | video_id | trending_date | title | channel_title | category_id | publish_time | tags | views | likes | dislikes | comment_count | thumbnail_link | comments_disabled | ratings_disabled | video_error_or_removed | description | . | 0 | RxGQe4EeEpA | 17.14.11 | 좋아 by 민서_윤종신_좋니 답가 | 라푸마코리아 | 22 | 2017-11-13T07:07:36.000Z | 라푸마|\"윤종신\"|\"좋니\"|\"좋아\"|\"샬레\"|\"민서\" | 156130 | 1422 | 40 | 272 | https://i.ytimg.com/vi/RxGQe4EeEpA/default.jpg | False | False | False | 윤종신 ‘좋니’의 답가 ‘좋아’ 최초 공개!\\n그 여자의 이야기를 지금 만나보세요… (생략) | . | 1 | hH7wVE8OlQ0 | 17.14.11 | JSA 귀순 북한군 총격 부상 | Edward | 25 | 2017-11-13T10:59:16.000Z | JSA|\"귀순\"|\"북한군\"|\"총격\"|\"부상\"|\"JSA 귀순 북한군 총격 부상\" | 76533 | 211 | 28 | 113 | https://i.ytimg.com/vi/hH7wVE8OlQ0/default.jpg | False | False | False | [채널A단독]北 병사 현재 ‘의식불명’… 혈압 떨어지는 중 \\n[채널A단독]우리측 초소 50m 앞서 의식 잃고 쓰러져… (생략) | . | 2 | 9V8bnWUmE9U | 17.14.11 | 나몰라패밀리 운동화 영상 2탄 (빼빼로데이버젼) | 나몰라패밀리 핫쇼 | 22 | 2017-11-11T07:16:08.000Z | 아디다스|\"빼빼로\"|\"핫쇼\"|\"나몰라패밀리\"|\"대학로\"|\"공연\" | 421409 | 5112 | 166 | 459 | https://i.ytimg.com/vi/9V8bnWUmE9U/default.jpg | False | False | False | 퍼가실때 꼭 출처 부탁드려요 | . | dataframe 정보 확인: data type, null값 여부 . videos_df.info() . &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 34567 entries, 0 to 34566 Data columns (total 16 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 video_id 34567 non-null object 1 trending_date 34567 non-null object 2 title 34567 non-null object 3 channel_title 34567 non-null object 4 category_id 34567 non-null int64 5 publish_time 34567 non-null object 6 tags 34567 non-null object 7 views 34567 non-null int64 8 likes 34567 non-null int64 9 dislikes 34567 non-null int64 10 comment_count 34567 non-null int64 11 thumbnail_link 34567 non-null object 12 comments_disabled 34567 non-null bool 13 ratings_disabled 34567 non-null bool 14 video_error_or_removed 34567 non-null bool 15 description 31404 non-null object dtypes: bool(3), int64(5), object(8) memory usage: 3.5+ MB . | description 칼럼에만 null값이 조금 있고 나머지는 없음 | description 칼럼의 null값은 분석에 큰 영향을 주지 않는다고 생각되어 별다른 처리를 하지는 않음. | . | 중복값 확인 . # 중복값이 포함되어 있나 확인 (모든 열의 데이터가 같은 경우) videos_df.duplicated().sum() . 2316 . → 모든 열의 값이 다 중복되는 데이터는 하나의 row만 남기고 삭제 . print('중복 제거 이전: ', len(videos_df)) videos_df.drop_duplicates(inplace=True, ignore_index=True) print('중복 제거 이후: ', len(videos_df)) . | . category_id에 category명 연결 . | KR_category_id.json 파일 불러오기 . import json with open(\"data/KR_category_id.json\", \"r\", encoding=\"utf-8\") as f: json_data = json.load(f) json_data . {'kind': 'youtube#videoCategoryListResponse', 'etag': '\"XI7nbFXulYBIpL0ayR_gDh3eu1k/1v2mrzYSYG6onNLt2qTj13hkQZk\"', 'items': [{'kind': 'youtube#videoCategory', 'etag': '\"XI7nbFXulYBIpL0ayR_gDh3eu1k/Xy1mB4_yLrHy_BmKmPBggty2mZQ\"', 'id': '1', 'snippet': {'channelId': 'UCBR8-60-B28hp2BmDPdntcQ', 'title': 'Film &amp; Animation', 'assignable': True}}, {'kind': 'youtube#videoCategory', 'etag': '\"XI7nbFXulYBIpL0ayR_gDh3eu1k/UZ1oLIIz2dxIhO45ZTFR3a3NyTA\"', 'id': '2', 'snippet': {'channelId': 'UCBR8-60-B28hp2BmDPdntcQ', 'title': 'Autos &amp; Vehicles', 'assignable': True}}, (생략)} . | category_id_df에 category_id와 category_name 칼럼 정리 . temp_list = [] for item in json_data['items']: temp_list.append([int(item['id']), item['snippet']['title']]) category_id_df = pd.DataFrame(temp_list, columns=['category_id', 'category_name']) print(len(category_id_df)) category_id_df.head() . |   | category_id | category_name | . | 0 | 1 | Film &amp; Animation | . | 1 | 2 | Autos &amp; Vehicles | . | 2 | 10 | Music | . | 3 | 15 | Pets &amp; Animals | . | 4 | 17 | Sports | . | videos_df에 category_id_df 결합 . videos_df = pd.merge(videos_df, category_id_df, on='category_id', how='left') videos_df[['title', 'category_id', 'category_name']].head() . |   | title | category_id | category_name | . | 0 | 좋아 by 민서_윤종신_좋니 답가 | 22 | People &amp; Blogs | . | 1 | JSA 귀순 북한군 총격 부상 | 25 | News &amp; Politics | . | 2 | 나몰라패밀리 운동화 영상 2탄 (빼빼로데이버젼) | 22 | People &amp; Blogs | . | 3 | 이명박 출국 현장, 놓치면 안되는 장면 | 25 | News &amp; Politics | . | 4 | 김장겸은 물러갔다 MBC 노조 환호와 눈물 | 25 | News &amp; Politics | . → null값이 몇 개 생겼나 확인 . videos_df['category_name'].isnull().sum() . 269 . → null값의 원인 파악: json 파일에 29번이 없어서 차이가 생긴 것 . set(videos_df['category_id']) - set(category_id_df['category_id']) . {29} . → category_name 칼럼의 null값에 ‘N/A’라는 string값을 넣어줌 . videos_df['category_name'].fillna('N/A', inplace=True) videos_df['category_name'].isnull().sum() # null값이 잘 대체되었나 확인 . 0 . | . datetime 타입 변경 . | trending_date와 publish_time을 datetime 타입으로 바꿔줌 | . # datetime 타입으로 바꿔줌 videos_df['trending_date'] = pd.to_datetime(videos_df['trending_date'], format='%y.%d.%m') videos_df['publish_time'] = pd.to_datetime(videos_df['publish_time']) # type이 잘 바뀌었는지 확인 videos_df[['trending_date', 'publish_time']].dtypes . trending_date datetime64[ns] publish_time datetime64[ns, UTC] dtype: object . ",
    "url": "https://chaelist.github.io/docs/kaggle/youtube_trending/#%EB%8D%B0%EC%9D%B4%ED%84%B0-%ED%8C%8C%EC%95%85-%EB%B0%8F-%EC%A0%84%EC%B2%98%EB%A6%AC",
    "relUrl": "/docs/kaggle/youtube_trending/#데이터-파악-및-전처리"
  },"276": {
    "doc": "YouTube Trending Videos",
    "title": "trending 동영상 특징 파악",
    "content": "동영상별 trending 횟수 . trending_count = videos_df.groupby('title')[['trending_date']].count().reset_index() trending_count.sort_values(by='trending_date', ascending=False, inplace=True) trending_count.head() . |   | title | trending_date | . | 11072 | 브베 실시간 스트리밍 | 19 | . | 13718 | 이재용 여자관계와 연예인, 배우 이나영, 윤은혜가, 조수빈 | 9 | . | 2074 | Bruno Mars,Charlie Puth,Ed Sheeran Best Christmas Songs,Greatest Hits Pop Playlist Christmas 2018 | 9 | . | 2751 | Marvel Studios’ Avengers: Infinity War Official Trailer | 8 | . | 8552 | 나얼 (Naul) - 기억의 빈자리 (Emptiness in Memory) MV | 8 | . *통계량 확인 . print(trending_count.describe()) . trending_date count 16353.000000 mean 1.972176 std 1.068082 min 1.000000 25% 1.000000 50% 2.000000 75% 3.000000 max 19.000000 . *시각화해서 확인 . fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12, 4)) sns.kdeplot(trending_count['trending_date'], bw_method=0.5, color='skyblue', ax=ax1) sns.countplot(data=trending_count, x=\"trending_date\", color='skyblue', ax=ax2) plt.close(2) plt.close(3) plt.tight_layout() . | 1-2번 trending한 동영상이 가장 많으며, 전체의 75% 이상이 3번 이하 trending | . 월별 카테고리별 trending 동영상 추이 . | month 단위로 정리 . ## 연-월로만 trending_date를 다시 정리 videos_df['trending_month'] = videos_df['trending_date'].dt.strftime('%Y%m') videos_df[['trending_date', 'trending_month']].head() . |   | trending_date | trending_month | . | 0 | 2017-11-14 | 201711 | . | 1 | 2017-11-14 | 201711 | . | 2 | 2017-11-14 | 201711 | . | 3 | 2017-11-14 | 201711 | . | 4 | 2017-11-14 | 201711 | . | 월별 카테고리별 trending video 수 집계 . # nunique()로, 중복을 제거해 집계 (같은 month에 같은 title의 영상이 2번 이상 trending한 경우, 1번으로 count) groupby_df1 = copied_df1.groupby(['trending_month', 'category_name'])[['title']].nunique().reset_index() groupby_df1.rename(columns={'title': 'count'}, inplace=True) groupby_df1.head() . |   | trending_month | category_name | count | . | 0 | 201711 | Autos &amp; Vehicles | 12 | . | 1 | 201711 | Comedy | 62 | . | 2 | 201711 | Education | 19 | . | 3 | 201711 | Entertainment | 354 | . | 4 | 201711 | Film &amp; Animation | 111 | . | 시각화해서 월별 추이를 확인 . plt.figure(figsize=(12, 5)) sns.lineplot(data=groupby_df1, x='trending_month', y='count', hue='category_name', palette='Set3') # legend를 box 밖으로 빼 줌 plt.legend(bbox_to_anchor=(1.01, 1), borderaxespad=0); . | 모든 월에 Entertainment, News &amp; Politics, People &amp; Blogs 카테고리가 가장 인기가 있음을 알 수 있다 | 2018.03월을 기점으로 News &amp; Politics 카테고리가 People &amp; Blogs 카테고리의 인기를 추월 | 2017-11-14 ~ 2018-06-14 사이의 dataset이기 때문에 201711과 201806은 대부분의 카테고리에서 trending 동영상 수가 낮게 나옴 | . | . 등록 요일별 / 시간대별 차이 . # 등록된 '요일'을 별도 칼럼으로 저장 (0: 월 ~ 6: 일) videos_df['publish_weekday'] = videos_df['publish_time'].dt.weekday # 등록된 '시간대'를 별도 칼럼으로 저장 (0 ~ 24) videos_df['publish_hour'] = videos_df['publish_time'].dt.hour videos_df[['publish_time', 'publish_weekday', 'publish_hour']].head() . |   | publish_time | publish_weekday | publish_hour | . | 0 | 2017-11-13 07:07:36+00:00 | 0 | 7 | . | 1 | 2017-11-13 10:59:16+00:00 | 0 | 10 | . | 2 | 2017-11-11 07:16:08+00:00 | 5 | 7 | . | 3 | 2017-11-12 11:19:52+00:00 | 6 | 11 | . | 4 | 2017-11-13 11:08:59+00:00 | 0 | 11 | . | 등록 요일별 비교 sns.countplot(data=videos_df, x='publish_weekday', color='skyblue'); . | 금요일에 publish된 동영상이 비교적 trending한 수가 많은 편 | 한 동영상이 여러 번 trending한 경우도 포함해서 계산 | . | 등록 시간대별 비교 . groupby_df2 = videos_df.groupby(['publish_hour'])[['video_id']].count().reset_index() plt.figure(figsize=(10, 6)) sns.lineplot(data=groupby_df2, x='publish_hour', y='video_id', color='pink'); . | 오전 9시 전후에 publish된 동영상이 trending한 수가 많은 편 | .   . +) 시간대별, 요일별 차이 비교 . groupby_df3 = videos_df.groupby(['publish_hour', 'publish_weekday'])[['video_id']].count().reset_index() plt.figure(figsize=(10, 6)) sns.lineplot(data=groupby_df3, x='publish_hour', y='video_id', hue='publish_weekday'); . | 특히 금요일 오전 9시경에 publish된 동영상이 비교적 trending한 수가 많은 편 | . | . ",
    "url": "https://chaelist.github.io/docs/kaggle/youtube_trending/#trending-%EB%8F%99%EC%98%81%EC%83%81-%ED%8A%B9%EC%A7%95-%ED%8C%8C%EC%95%85",
    "relUrl": "/docs/kaggle/youtube_trending/#trending-동영상-특징-파악"
  },"277": {
    "doc": "YouTube Trending Videos",
    "title": "trending하기까지의 기간 비교",
    "content": ". | 동영상들이 대체로 publish 후 trending하기까지 어느 정도의 기간이 걸렸는지 확인 | . 데이터 가공 . # publish ~ trending 사이의 기간을 계산: integer로 저장 (단위: days) ## dt.date로 두 칼럼의 type을 통일해줘야 빼서 기간을 계산하는 게 가능 videos_df['publish_to_trending'] = (videos_df['trending_date'].dt.date - videos_df['publish_time'].dt.date).dt.days videos_df[['trending_date', 'publish_time', 'publish_to_trending']].head() . |   | trending_date | publish_time | publish_to_trending | . | 0 | 2017-11-14 | 2017-11-13 07:07:36+00:00 | 1 | . | 1 | 2017-11-14 | 2017-11-13 10:59:16+00:00 | 1 | . | 2 | 2017-11-14 | 2017-11-11 07:16:08+00:00 | 3 | . | 3 | 2017-11-14 | 2017-11-12 11:19:52+00:00 | 2 | . | 4 | 2017-11-14 | 2017-11-13 11:08:59+00:00 | 1 | . *같은 video_id &amp; title, 다른 trending_date를 가진 데이터 삭제 . | ※ ‘영상 등록 후 trending되기까지의 기간이 보통 어느 정도 걸리는지’를 확인하고자 하는 것이기에, 하나의 동영상이 처음으로 trending한 시점의 데이터만 가지고 분포를 확인하기로. → 처음으로 trending한 시점의 데이터만 남기고 중복값 삭제 (= 중복된 행들 중 가장 위에 있는 행만 남기고 다 삭제) | . # trending_date 기준으로 오름차순 정렬 videos_df_sorted = videos_df.sort_values(by='trending_date', ignore_index=True) # 영상별로 가장 먼저 trending한 날의 데이터만 남긴 데이터셋을 unique_videos_df에 새로 저장 unique_videos_df = videos_df_sorted.drop_duplicates(subset=['video_id', 'title'], ignore_index=True) print('videos_df: ', len(videos_df)) print('unique_videos_df: ', len(unique_videos_df)) . videos_df: 32251 unique_videos_df: 16393 . 데이터 분포 확인 . plt.figure(figsize=(20,8)) sns.countplot(data=unique_videos_df, x=\"publish_to_trending\", color='skyblue'); . | 대체로 영상 등록 후 trending되기까지의 기간이 3일 이내이며, 특히 등록 후 하루 만에 trending하는 경우가 가장 많음 | . sns.catplot(data=unique_videos_df, x='publish_to_trending', height=5, aspect=3, color='skyblue'); . | 약 20일 정도를 넘어서면, 그 이후에는 상당히 outlier 간의 간격이 크게 분포 | . 카테고리별 분포 확인 . | 어느 카테고리에 outlier가 많이 포진되어 있는지, publish 후 trending하기까지의 평균 기간이 어떻게 다른지 확인 | . # 어느 카테고리에 outlier가 많이 포진되어 있는지 확인 sns.catplot(data=unique_videos_df, x='category_name', y='publish_to_trending', palette='Blues_d', height=5, aspect=3) plt.xticks(rotation=45); . | 특히 Music, People &amp; Blogs, Pets &amp; Animals, Film &amp; Animation 카테고리에서 정상 범주를 크게 벗어나는 데이터가 발견됨 | 크게 유행을 타지 않고 인기를 끌 수 있는 카테고리이기 때문에 trending되기까지의 기간이 긴 경우가 발견되는 편이라고 생각됨 | . # 카테고리별 평균 publish~trending 기간 비교 (선은 95% 신뢰구간) plt.figure(figsize=(20, 6)) sns.barplot(data=unique_videos_df, x='category_name', y='publish_to_trending', palette='Blues_d'); plt.xticks(rotation=45); . | Music, Pets &amp; Animals 등은 편차가 큰 반면, Entertainment, News &amp; Politics, Sports, Gaming 등 시의성이 중요할 수 있는 영상들은 대체로 빠르신 시일 내에 trending한 경우가 많음. | . # Music 카테고리의 outlier top3 확인 videos_music_df = unique_videos_df[unique_videos_df['category_name'] == 'Music'] videos_music_df.sort_values(by='publish_to_trending', ascending=False)[['title', 'trending_date', 'publish_time', 'publish_to_trending']].head(3) . |   | title | trending_date | publish_time | publish_to_trending | . | 7864 | 손성제 - Goodbye | 2018-02-19 00:00:00 | 2011-09-29 13:08:39+00:00 | 2335 | . | 1695 | Taylor swift - See You Again | 2017-12-02 00:00:00 | 2017-02-20 14:25:18+00:00 | 285 | . | 12731 | 갈수록 어려워지는 젓가락 행진곡을 이렇게 친다고???? | 2018-04-28 00:00:00 | 2018-04-09 16:36:09+00:00 | 19 | . | Music 카테고리는 특정 계기로 재발견되어 인기를 얻게 되는 음악 영상들이 존재해 유독 분포의 정상 범위를 크게 벗어나는 outlier가 발견되는 편이라고 생각됨 | 예를 들어, trending까지의 기간이 가장 길게 나타난 Music 카테고리의 ‘손성제-Goodbye’ 영상의 경우, 2018.02 ‘효리네 민박’ 방송을 통해 재발견되면서 trending 영상에 포함된 것으로 추정 | . ",
    "url": "https://chaelist.github.io/docs/kaggle/youtube_trending/#trending%ED%95%98%EA%B8%B0%EA%B9%8C%EC%A7%80%EC%9D%98-%EA%B8%B0%EA%B0%84-%EB%B9%84%EA%B5%90",
    "relUrl": "/docs/kaggle/youtube_trending/#trending하기까지의-기간-비교"
  },"278": {
    "doc": "YouTube Trending Videos",
    "title": "views와 다른 변수 간의 관계",
    "content": "# 상관계수 확인 (heatmap으로 시각화해서 확인) sns.heatmap(videos_df[['views', 'likes', 'dislikes', 'comment_count']].corr(), annot=True, cmap='Blues') plt.yticks(rotation=0); . | likes와 comment_count, views가 강한 상관관계를 보이며, likes는 comment_count와 매우 강한 상관관계를 보인다 | . views와 likes . : views가 많은 영상일수록 likes도 많을까? . # ratings_disabled 기능이 켜져 있으면 likes / dislikes를 누르는 것이 불가능 print(videos_df[videos_df['ratings_disabled'] == True][['likes', 'dislikes']].describe()) . likes dislikes count 1308.00 1308.00 mean 0.00 0.00 std 0.00 0.00 min 0.00 0.00 25% 0.00 0.00 50% 0.00 0.00 75% 0.00 0.00 max 0.00 0.00 . → ratings_diabled 기능이 사용된 영상들을 제외하고 진행 . # ratings_disabled 기능을 사용한 영상들을 제외한 나머지 데이터를 rated_videos_df에 저장 rated_videos_df = videos_df[~videos_df['ratings_disabled']] rated_videos_df.reset_index(drop=True, inplace=True) # reset index print('videos_df: ', len(videos_df)) print('rated_videos_df: ', len(rated_videos_df)) . videos_df: 32251 rated_videos_df: 30943 . | 상관계수 및 선형회귀선 확인 . import scipy.stats as stats # 피어슨 상관계수 검정 corr = stats.pearsonr(rated_videos_df['views'], rated_videos_df['likes']) print('Corr_Coefficient : %.3f \\np-value : %.3f' % (corr)) . Corr_Coefficient : 0.858 p-value : 0.000 . | 강한 상관관계를 보이는 것을 확인 (p-value &lt; 0.01) | . # view와 likes 간의 관계 확인: 선형회귀선 확인 sns.lmplot(data=rated_videos_df, x='views', y='likes', height=5, aspect=1.5); ## default: ci=95 (95% 신뢰구간) . | 카테고리별 차이 확인 . sns.lmplot(data=rated_videos_df, x='views', y='likes', hue='category_name', palette='Set3', height=5, aspect=1.5); . | 카테고리별로 나누면 보다 각각에 잘 맞는 선형회귀선을 그리는 것이 가능 | . ## Music 카테고리 sns.heatmap(videos_df[videos_df['category_name'] == 'Music'][['views', 'likes', 'dislikes', 'comment_count']].corr(), annot=True, cmap='Blues') plt.yticks(rotation=0); . ## Entertainment 카테고리 sns.heatmap(videos_df[videos_df['category_name'] == 'Entertainment'][['views', 'likes', 'dislikes', 'comment_count']].corr(), annot=True, cmap='Blues') plt.yticks(rotation=0); . ## Film &amp; Animation 카테고리 sns.heatmap(videos_df[videos_df['category_name'] == 'Film &amp; Animation'][['views', 'likes', 'dislikes', 'comment_count']].corr(), annot=True, cmap='Blues') plt.yticks(rotation=0); . | 카테고리별로 나누어서 확인하면, views와 likes 간의 상관관계가 보다 강하게 확인된다 | . | . views와 dislikes . : views가 많은 영상일수록 dislikes도 많을까? . | 카테고리별로, views와 dislikes의 관계를 확인 . sns.lmplot(data=rated_videos_df, x='views', y='dislikes', hue='category_name', palette='Set3', height=5, aspect=1.5); . | ‘Entertainment’ 카테고리의 4개 점을 제외하면 대체로 상관관계가 있어 보임 | . | dislike가 많은 영상이 어떤 것인지 확인 . # dislikes 많은 영상부터 순서대로 정렬 videos_df.sort_values(by='dislikes', ascending=False)[['video_id', 'title', 'channel_title', 'trending_date', 'category_name']].head() . |   | video_id | title | channel_title | trending_date | category_name |   | . | 4716 | FlsCjmMhFmw | YouTube Rewind: The Shape of 2017 | #YouTubeRewind | YouTube Spotlight | 2017-12-11 | Entertainment | . | 4538 | FlsCjmMhFmw | YouTube Rewind: The Shape of 2017 | #YouTubeRewind | YouTube Spotlight | 2017-12-10 | Entertainment | . | 4313 | FlsCjmMhFmw | YouTube Rewind: The Shape of 2017 | #YouTubeRewind | YouTube Spotlight | 2017-12-09 | Entertainment | . | 4104 | FlsCjmMhFmw | YouTube Rewind: The Shape of 2017 | #YouTubeRewind | YouTube Spotlight | 2017-12-08 | Entertainment | . | 29013 | 7C2z4GqqS5E | BTS (방탄소년단) ‘FAKE LOVE’ Official MV | ibighit | 2018-05-24 | Music |   | . | 가장 dislike가 많았던 영상 4개는 모두 같은 동영상. | . | dislike가 많은 Entertainment 카테고리의 영상 1개 (= 4개 row)를 제외하고 상관관계를 파악 . # 피어슨 상관계수 검정 drop_index = rated_videos_df.sort_values(by='dislikes', ascending=False).head(4).index dl_outlier_removed_videos_df = rated_videos_df.drop(drop_index, axis='index') corr = stats.pearsonr(dl_outlier_removed_videos_df['views'], dl_outlier_removed_videos_df['dislikes']) print('Corr_Coefficient : %.3f \\np-value : %.3f' % (corr)) . Corr_Coefficient : 0.836 p-value : 0.000 . | dislikes의 outlier 4개 점을 제외하고 나니 상관계수가 강해진 것을 확인할 수 있다 | . | outlier를 제외한 관계를 시각화해서 파악 . sns.lmplot(data=dl_outlier_removed_videos_df, x='views', y='dislikes', hue='category_name', palette='Set3', height=5, aspect=1.5); . | outlier 4개 점을 제외하고 보면, 카테고리별로 현상을 잘 설명하는 선형회귀선을 그리는 것이 가능 | . | . ",
    "url": "https://chaelist.github.io/docs/kaggle/youtube_trending/#views%EC%99%80-%EB%8B%A4%EB%A5%B8-%EB%B3%80%EC%88%98-%EA%B0%84%EC%9D%98-%EA%B4%80%EA%B3%84",
    "relUrl": "/docs/kaggle/youtube_trending/#views와-다른-변수-간의-관계"
  },"279": {
    "doc": "YouTube Trending Videos",
    "title": "기능 사용 유무에 따른 비교",
    "content": ". | comments_disabled와 ratings_disabled 기능을 사용한 비율 확인 &amp; 기능 사용 여부가 views에 미치는 영향을 확인 | . comments/ratings_disabled 기능 사용 비율 . | ※ ‘trending 동영상 중 몇%가 각 기능을 사용했는지’를 파악하기 위해서는 같은 동영상 데이터가 중복되어 포함되면 안된다고 생각 → 같은 동영상인데 여러 번 trending해서 데이터가 여러 번 포함된 경우는 제외 (위에서 저장해두었던 unique_videos_df를 사용) | . comments_disabled_percentage = len(unique_videos_df[unique_videos_df['comments_disabled']]) / len(unique_videos_df) * 100 ratings_disabled_percentage = len(unique_videos_df[unique_videos_df['ratings_disabled']]) / len(unique_videos_df) * 100 print(f'comments_disabled 비율: {comments_disabled_percentage :.2f}%') print(f'ratings_disabled 비율: {ratings_disabled_percentage :.2f}%') . comments_disabled 비율: 1.53% ratings_disabled 비율: 4.57% . | trending한 영상 중 ratings_disabled나 comments_disabled 기능을 사용한 영상을 현저히 적긴 하지만, 전체 동영상 중 비율을 모르기 때문에 어떤 결론을 이끌어내는 것은 불가능… | . comments/ratings_disabled 기능의 views에의 영향 . | ※ 영상별로 data를 1개만 남기되, 가장 마지막으로 trending한 날의 데이터(=데이터셋 내 해당 영상의 최고 views 데이터)만 남긴다 | 특정 기능의 유무에 따라 views가 얼마나 달라지는지를 확인하기 위함이므로, 영상별 최고 views로 분포 파악 (ex. 가설 예시: ratings_disabled 기능을 사용하면 사용하지 않는 것보다 views가 낮게 형성된다) | . | 기능 유무에 따른 views 분포 확인 . # comments_disabled가 views에 어떤 영향을 미치는지 fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(10, 5)) sns.stripplot(data=unique_recentf_df, x='comments_disabled', y='views', palette='Blues', ax=ax1) sns.boxplot(data=unique_recentf_df, x='comments_disabled', y='views', palette='Blues', showfliers=False, ax=ax2) plt.close(2) plt.close(3) plt.tight_layout() . # ratings_disabled가 views에 어떤 영향을 미치는지 fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(10, 5)) sns.stripplot(data=unique_recentf_df, x='ratings_disabled', y='views', palette='Blues', ax=ax1) sns.boxplot(data=unique_recentf_df, x='ratings_disabled', y='views', palette='Blues', showfliers=False, ax=ax2) plt.close(2) plt.close(3) plt.tight_layout() . | 기능 유무에 따라 평균 views에 차이가 있는지 확인 . | 평균 views 비교 후, t-test로 차이가 유의미하다고 봐야 할 지 확인 | . print('--평균 views 비교 -------') print('comments_disabled: {:.0f}'.format(unique_recentf_df['views'][unique_recentf_df['comments_disabled']].mean())) print('comments_abled: {:.0f}\\n'.format(unique_recentf_df['views'][~unique_recentf_df['comments_disabled']].mean())) print('ratings_disabled: {:.0f}'.format(unique_recentf_df['views'][unique_recentf_df['ratings_disabled']].mean())) print('ratings_abled: {:.0f}'.format(unique_recentf_df['views'][~unique_recentf_df['ratings_disabled']].mean())) . --평균 views 비교 ------- comments_disabled: 384423 comments_abled: 369001 ratings_disabled: 209573 ratings_abled: 376942 . 1) comments_disabled 기능 유무에 따른 차이 . comments_disabled = unique_recentf_df[unique_recentf_df['comments_disabled']] comments_abled = unique_recentf_df[~unique_recentf_df['comments_disabled']] # Levene의 등분산 검정 lev_result = stats.levene(comments_disabled['views'], comments_abled['views']) print('LeveneResult(F) : %.2f \\np-value : %.3f' % (lev_result)) ## 대체로 p값이 0.05 이상이면 등분산 가정, 등분산인 독립표본 t-test로 진행 . LeveneResult(F) : 0.05 p-value : 0.819 . # 등분산인 독립표본 t-test 실행 t_result = stats.ttest_ind(comments_disabled['views'], comments_abled['views'], equal_var=True) print('t statistic : %.2f \\np-value : %.3f' % (t_result)) . t statistic : 0.12 p-value : 0.902 . | p값 &gt; 0.05이고, 위에서 실제 값으로 살펴봤을 때에도 평균 views의 차이가 크지 않으므로, 유의미한 차이는 없다고 봐도 무방할 듯. | . 2) ratings_disabed 기능 유무에 따른 차이 . ratings_disabled = unique_recentf_df[unique_recentf_df['ratings_disabled']] ratings_abled = unique_recentf_df[~unique_recentf_df['ratings_disabled']] # Levene의 등분산 검정 lev_result = stats.levene(ratings_disabled['views'], ratings_abled['views']) print('LeveneResult(F) : %.2f \\np-value : %.3f' % (lev_result)) . LeveneResult(F) : 3.94 p-value : 0.047 . # 등분산이 아닌 독립표본 t-test 실행 t_result = stats.ttest_ind(ratings_disabled['views'], ratings_abled['views'], equal_var=False) print('t statistic : %.2f \\np-value : %.3f' % (t_result)) . t statistic : -2.78 p-value : 0.006 . | p값 &lt; 0.01이고, 위에서 실제 값으로 살펴봤을 때도 평균 views의 차이가 꽤 있다고 보이므로, 유의미한 차이가 있다고 생각. | . | . views 기준 사분위 분류 → 분위별 비율 확인 . # views_quartile 칼럼을 새로 생성: 가장 views가 낮은 집단이 1st_q ~ 가장 높은 집단이 4th_q q1, q2, q3 = np.percentile(unique_recentf_df['views'], [25, 50, 75]) def get_quarter(view): if view &lt; q1: quarter = '1st_q' elif view &lt; q2: quarter = '2nd_q' elif view &lt; q3: quarter = '3rd_q' else: quarter = '4th_q' return quarter unique_recentf_df['views_quartile'] = unique_recentf_df['views'].apply(lambda view: get_quarter(view)) unique_recentf_df[['views', 'views_quartile']].head() . |   | views | views_quartile | . | 0 | 27352 | 1st_q | . | 1 | 345008 | 4th_q | . | 2 | 45444 | 2nd_q | . | 3 | 467546 | 4th_q | . | 4 | 9628 | 1st_q | . 1) views 사분위별 comments_disabled 기능을 사용한 영상의 수 . sns.countplot(data=unique_recentf_df[unique_recentf_df['comments_disabled']], y='views_quartile', order=['4th_q', '3rd_q', '2nd_q', '1st_q'], color='skyblue'); . 2) views 사분위별 ratings_disabled 기능을 사용한 영상의 수 . sns.countplot(data=unique_recentf_df[unique_recentf_df['ratings_disabled']], y='views_quartile', order=['4th_q', '3rd_q', '2nd_q', '1st_q'], color='skyblue'); . | views 기준 4사분위 중 1사분위(하위 25%)에 ratings_disabled 기능을 사용한 영상의 수가 유독 많이 분포 (4사분위의 약 4배) | . → 결론: 평균의 차이와, 4사분위 내 분포를 고려해보면, ratings_disabled 기능을 사용하지 않는 것이 보다 높은 views를 기록하는 데에 유리할 수 있다고 생각됨 . ",
    "url": "https://chaelist.github.io/docs/kaggle/youtube_trending/#%EA%B8%B0%EB%8A%A5-%EC%82%AC%EC%9A%A9-%EC%9C%A0%EB%AC%B4%EC%97%90-%EB%94%B0%EB%A5%B8-%EB%B9%84%EA%B5%90",
    "relUrl": "/docs/kaggle/youtube_trending/#기능-사용-유무에-따른-비교"
  }
}
